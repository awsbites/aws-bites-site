{
  "episodes": [
    {
      "title": "1. When should I use serverless?",
      "url": "https://awsbites.com/1-when-should-i-use-serverless/",
      "publish_date": "2021-09-09T00:00:00.000Z",
      "abstract": "In this episode Eoin and Luciano talk about what it means to adopt serverless, when you should use AWS Lambda, and when you should not use serverless.\n",
      "transcript": "Eoin: Hello, everyone, and welcome to another episode of AWS Bites, the weekly show where we try\nto answer questions about AWS in just five minutes. My name is Luciano.\n\n\nLuciano: My name is Eoin.\n\n\nEoin: And today we have a very interesting question. So when should they use serverless? Eoin,\nwhat do you think?\n\n\nLuciano: I like to think that you should use serverless always. It's not practically possible, but\nI suppose the mindset we use is serverless first. There are a few cases where you might\nnot be able to use serverless and everything. But maybe before we go deep on that, what\ndo we mean by use serverless? Do you mean use Lambda functions for everything or is\nthere something more to it than that?\n\n\n\nEoin: Yeah, I think that's a very legitimate question because I think that the common conception is that serverless is just functions as a service like Fuzz and Lambda. In reality,\nI think that there is a lot more to that. It's more of a mindset. It's not just that\nspecific tool, but it's more the idea that you should worry less about the infrastructure\nand managing that infrastructure. So you should focus more on the business logic, try to write\nonly the code that adds value to your company, your business mission, and not focus so much\ninto how do I provision this machine? How do I keep it up to date? What operative system\nam I using? Is it secure? Is it fast enough in terms of network? So I think serverless\nis when you start to think more in terms of business value rather than servers, I guess.\nYeah, I like the idea that serverless is a journey and not a destination.\n\n\nLuciano: So it's more about the fact that you're constantly looking for things you're doing every day that's taking\nyou time that isn't really critical to your business and you're trying to figure out how\nyou can move that responsibility to somebody else, AWS or some third party SaaS with an\nAPI. And I really like that. So if that's the case, where does serverless not work or\nwhere would you not use serverless Luciano?\n\n\n\nEoin: Yeah, I've seen a few interesting cases in my career where even though I really wanted to use serverless, I don't think it would have been the most practical choice for the\ntype of context. For instance, very often I had to deal with companies that had long\ninvestment in products that were very stable, of course, built even before serverless was\na thing. So just the idea of transitioning all that big application, maybe it's like\na monolith written in Ruby or Rails or Django or something like that to serverless will\nbe such a big investment that it becomes really prohibitive and doesn't really give anything\nback to the company to try to do that. So that's definitely one case.\nAnother one is where I've seen companies going to the cloud, but with this idea that they\nwanted to keep themselves as abstracted as possible from the vendor. So they chose something\nlike Kubernetes and they spend a lot of time making sure that they have a solid way to\nship code into Kubernetes. At that point, I wouldn't really suggest that they move from\nthat to serverless. I don't know. Have you seen any other interesting case that might\nbe worth bringing to the table?\nYeah.\n\n\nLuciano: I mean, I think if you've got a stable application that already makes a lot of money, I wouldn't be rushing just to migrate it to serverless just because you feel like using\nthe latest and greatest in cloud tech. There's a cost of switching in all cases. And if you\ndon't really have to go touching something that works and makes you money, why bother?\nBut on the other side of the scale, if you've got something that's new, you want to do a\nlot of experimentation, maybe something where you're working in a real kind of lean organization\nand you're happy to build, throw stuff away, get that feedback loop going, rinse and repeat\nand really iterate on the product. Serverless is really, really good for that because the\ncost of creating and destroying resources that are serverless is next to zero. Whereas\nif you spend a lot of time, you know, racking and stacking virtual infrastructure, you know,\nbuilding auto scaling infrastructure with EC2, there's an investment up front and all\nthat and into, you know, your operating system and your security patching and all of that\nwith services like Lambda, DynamoDB, Aurora Serverless, you don't have to worry with that\nso much. So you can adopt them and drop them really quickly.\nRight. So where do you think that serverless really shines?\n\n\nEoin: Oh, yeah.\n\n\nLuciano: Well, like I say, if you've got, you know, a lot of potential for change, which let's face it, we all do, especially at the start of new product development cycles. Also,\nbut if you've got unpredictable workload, you know, if you're a startup and you're happy\nto adopt this technology and you're, you know, you're willing to invest the time into the\nskill set, it's really suitable because if you're not earning any revenue, you're not\npaying for anything generally. So it scales with your business. And that seems really\nfair and it means you're not likely to, you know, incur significant costs when your company\nis at a nearly stage of growth. But I'd say the same, same goes for other, even in enterprises\nwhere you're trying to build new products, serverless really shines there too because\nyou the cost of experimentation again, it's just really low. So it really fosters that\nmindset where, you know, people can just really push the boundaries of what you're doing and\ndon't have to be adhering to existing models for building software anymore.\nFair enough. Yeah. Yeah.\n\n\nEoin: As a closing note, I just want to say that it's easy to say, let's do everything in a serverless way, but of course there is a learning curve as well.\nSo if you never done a serverless project, it might take you a little bit of time to\njust get comfortable with all the tooling, how do you do testing, local development,\ndeployment. So that's also something to keep in mind whenever you are starting a new project\nwith serverless for the first time. I think it's going to pay off of course, but don't\ndiscount that initial learning curve. So, okay. I think that that's all we have for\ntoday. So thank you very much for being with us and let us know what do you think? Leave\nus comments. We are really curious to know what do you think about serverless? If you're\nusing it, what are your challenges? And make sure to follow and subscribe. So we'll see\nyou at the next episode. Bye.\n"
    },
    {
      "title": "2. Should you go for multi-cloud or go all-in on AWS?",
      "url": "https://awsbites.com/2-should-you-go-for-multi-cloud-or-go-all-in-on-aws/",
      "publish_date": "2021-09-16T00:00:00.000Z",
      "abstract": "In this episode Eoin and Luciano talk about whether you should go all in and build multi-cloud applications or stick to one cloud provider. But first of all we try to define what multi-cloud means for us and how it is different from hybrid-cloud and cloud-agnostic deployments. We also discuss the perils of cloud-agnostic and why you should rather consider having “only” a migration strategy.\nIn this episode we mentioned the following resources:\n\nAmazon ECS everywhere\nAmazon EKS everywhere\n\n",
      "transcript": "Eoin: Hello and welcome to another episode of AWS Bites, the weekly show where we answer questions\nabout AWS in about five minutes.\nMy name is Eoin Shanaghy and I'm joined by Luciano Mammino.\nToday the question is, should you go for multi-cloud or go all in on AWS?\nMaybe the best place to start there is what is multi-cloud?\nMaybe that is not clear. What is multi-cloud?\n\n\nLuciano: Let's start by defining what is multi-cloud.\nAnd I guess I'm going to try to provide my definition of it.\nI'm sure that's arguable.\nI'd say that multiple services across multiple clouds is kind of the way.\nI'd like to see multi-cloud.\nSo for instance, you have developed an API and this API runs on AWS.\nThen maybe you have other services like, I don't know, databases.\nEven other APIs running in another cloud provider and so on.\n\n\nI would say that there are other things that might be confused with multi-cloud and I wouldn't\ninclude them into this definition.\nFor instance, hybrid cloud.\nSo you have some stuff in AWS and some stuff on premise.\nI would argue that that's not really multi-cloud, even though some people might say the opposite.\nAnd also cloud agnostic is kind of interesting and related, but not really multi-cloud.\nSo basically when you build maybe with containers or Kubernetes, when you build software that\ncan potentially run across multiple clouds.\nSo that doesn't necessarily make it multi-cloud.\nIt's just you have the option, but you are not necessarily deploying it straight away\nto different clouds.\nWould you agree with this definition?\nSo do you want to add something there?\nNo, I think this is good.\n\n\nEoin: Multi-cloud agnostic is definitely something we hear a surprising amount about.\nAnd I still fail to understand why it seems to be popular with a lot of people, but I'm\ncurious to hear what drives people towards this.\nI think multi-cloud seems like a healthy thing.\nI think it's something that we see more and more of.\nI think we even see AWS recognizing that multi-cloud is a thing when they're launching services\nlike EKS Anywhere and ECS Anywhere, which allow you to run some of your workload on-prem\nor in other clouds using their services, which is a bit of a departure for them.\n\n\nBut I think it's an acceptance that for any company of any significant size, you'll end\nup with multiple cloud vendors in the picture somehow.\nLike if AWS shop for a lot of your core workloads, but a lot of enterprises have an investment.\nMicrosoft for AD, SharePoint, Dynamics, maybe some SQL server.\nSo the reality of it is that even companies through acquisition, they're going to end\nup with multiple estates across multiple clouds.\n\n\nSo the question becomes, what do you deploy into what cloud?\nHow can you be clear about that strategy?\nAnd not necessarily saying that we need to build everything so that it can run on multiple\nclouds, because that's a bit, I think that's a really dangerous fallacy, something people\nreally need to be careful of because you have the potential there to waste a massive amount\nof engineering time in trying to be agnostic and building something that's generic enough\nto run on multiple clouds.\nBecause you're already losing the benefits that each specific cloud provider gives you\nin their differentiating features.\nIs that fair?\nAbsolutely.\n\n\nLuciano: For instance, one thing that I've seen is if we want to stick to the example of serverless,\nthat even though on the surface, if you compare, for instance, Lambda with the serverless functions\nyou get in Azure, they look quite similar.\nBut then when you go a little bit deeper, the running model, it's so different that\nit becomes harder to kind of find a way to generalize your code so that it can run well\non both models.\nSo I wouldn't definitely encourage people to do that because it might be very tricky\nto get it right.\nAnd as you said, you're also going to lose some of the benefits because you're basically\ngoing to try to find that subset that works for both, but it's definitely a subset.\nSo you're going to miss out the big picture.\nAnd yeah, on that topic, I'm curious to think, to discuss maybe with you, like what do you\nsee as other issues that might arise from trying to be multi-cloud?\n\n\n\nEoin: So I think where a lot of people come from with multi-cloud is from a fear that there's a risk there that at some unknown point in the future, some unknown event is going to\noccur, which will cause them to have to migrate away.\nAnd I think for any business, you can probably try and quantify that risk and be realistic\nabout it.\nWhen you do the maths on that, you'll figure out that the likelihood of the event occurring\nis pretty low.\n\n\nAnd the cost of switching, if you look at that and compare it to the cost you have to\ninvest upfront in trying to be cloud agnostic, it's probably going to lead you to the decision\nthat look, it's better off to be more agile and rapid in your product development now\nand just be more intentional about what you might do in the unlikely event that you will\nhave to search away from your chosen cloud vendor completely.\n\n\nAnd even if that event occurs, you're going to have time to do it.\nSo I think being realistic about it, you can say, look, our focus typically is on, for\nthe majority of enterprises and startups, it's about time to market and getting product\ninto customers hands so you can iterate on it quickly.\nIf you're spending, I would say 60% of your engineering effort trying to be cloud agnostic\nand building layers of abstraction for some event that might never happen, you're really\nthrowing out your opportunity to be agile and get to market quickly.\n\n\nSo I think sometimes a lot of that is guided by maybe a misguided sense that there is a\nrisk there, which really isn't going to happen.\nAnd I think once you accept that you're all in on a cloud vendor for any given workload,\nit suddenly takes away a huge amount of cognitive load that your developers, architects, and\nproduct managers have to think about, and you can really start going much faster.\nHave you come across a couple of cases where you've seen, I mean, one of the worst things\nfor me is when you see a startup that's been getting some funding to build a product and\nthey're investing it all in deploying Kubernetes clusters on three different clouds because\nthey have some misconception that they're going to have to support them at some point\nin the future.\nHave you seen that?\n\n\nLuciano: I have seen that happening, but I've seen also the opposite where you have large enough\ncompanies that are more motivated by financial reasons in terms of, I don't know, are we\ngoing to lose leverage if we are logged in to AWS rather than another cloud?\nAnd that's maybe their motivation for trying to be more cloud agnostic so that they can\nmaybe try to get discounts or, I don't know, get more things from the cloud provider.\n\n\nAnd I don't know, that's kind of understandable, even though I think it's still not worth the\ninvestment personally.\nMost of the time, I think you're going to waste resources and time that you can spend,\nfor instance, in terms of build more feature, make the product more optimized for the use\ncases that your customers have, rather than just making the software layer and the architecture\nlayer so generic that it can work everywhere.\n\n\nAnd then one final remark I would like to make is probably that in general, I don't\nthink I'm advocating for that needs to be only one cloud.\nI think the competition is really good and it's good to be aware of different clouds\nand what kind of features do they have and how similar services compare across multiple\ncloud vendors.\nBecause I think we'll need that competition going forward because that's definitely going\nto help us to have better services across all the offering that you can get from cloud\nproviders.\nAbsolutely.\nAgain, don't pick your favorites and just stick to that without looking at the others.\nKeep looking at the other cloud providers and support them if you think they are doing\nsomething better than your favorite cloud provider.\nSo yeah, competition is good, but don't over engineer your software and architecture just\nto be multi-cloud.\nThat's great.\n\n\n\nEoin: Okay, well, on that note, I think that's all we have today, but thank you very much again for listening and let us know what you think in the comments.\nTell us what you'd like us to talk about in future episodes.\nFollow us and subscribe on YouTube and all your favorite podcast platforms and we'll\nsee you in the next episode.\n"
    },
    {
      "title": "3. How do you deploy a static website on AWS?",
      "url": "https://awsbites.com/3-how-do-you-deploy-a-static-website-on-aws/",
      "publish_date": "2021-09-23T00:00:00.000Z",
      "abstract": "In this episode Eoin and Luciano talk about static websites. What are the main requisites for a static website, where to host them and how to do it in AWS. Finally we touch on some common gotchas and tools that can make it easier for you to deploy static websites on AWS (spoiler: Amplify!).\nIn this episode we mentioned the following resources:\n\nSLIC Starter\nNetlify\nVercel\nAmplify\n\n",
      "transcript": "Eoin: Hello and welcome to another episode of AWS Bites, the weekly show where we answer questions\nabout AWS in about five minutes.\nMy name is Eoin and I am joined by Luciano.\nToday's question is how do you deploy a static website on AWS?\nSo Luciano, what do you think?\nFirst of all, when you want to deploy a static website, what are your requirements or must-haves?\nYeah, that's actually a very good first point to try to answer to.\n\n\nLuciano: To me, when you deploy a static website, the first thing that you need is definitely the\nstorage.\nLike where do you store the static assets?\nThen of course there is an element you need to make them available through HTTP and HTTPS.\nSo all the certificates and domains, and even better if you have something like a CDN, which\nwill do global distribution so you have very quick access for users all around the world.\nSo those for me are the main requirements that you should have for a static website.\nI guess this is an interesting topic because it's not very common in my opinion to do this\nin AWS.\nLike I've seen a lot of people use a bunch of other services.\nWhat do you think?\nWould you use AWS?\nWould you use other services?\nYeah, it definitely depends.\n\n\nEoin: I've done it in AWS quite a lot, but to be honest, if I just wanted to deploy a static\nwebsite and it didn't really matter what cloud service I had to use, I'd probably just for\nthe sake of speed and efficiency, go with something that is more specialized in that\narea like Netlify, Frisell, Cloudflare, some of these options because the developer experience\nand the time to deployment is generally a lot faster.\nThere's a lot less detail you have to think about.\nBut on the other hand, if you've already got AWS infrastructure, you've got deployment\npipelines and you're already set up to use identity and access management and cloud formation,\nall of that great stuff you get with AWS, then it probably makes sense to think, okay,\nI can manage all of this static website infrastructure in the same way.\nWhat do you think?\nYeah, no, fair enough.\n\n\nLuciano: I wouldn't use it for instance, for open source projects.\nI wouldn't use AWS.\nI generally use a combination of GitHub pages, maybe with Cloudflare on front of it.\nBut yeah, when it's more corporate environment, I get it that it makes a lot more sense to\nstick to AWS because you have everything more managed and you can give access to everyone\nworking in the team more easily.\nSo definitely I agree with you on that.\nSo let's try to see what are the options in AWS.\nLike what would you do if you have to set up a website, would you use S3, would you\nuse something else?\n\n\n\nEoin: Yeah, we've got a reference project called SlickStarter where we actually have a static website as part of that serverless application suite.\nThe way that we do that typically is with you put your assets into an S3 bucket.\nSo you create a bucket for your website.\nAll of the HTML, JavaScript, CSS, your images get synchronized to that bucket.\nIn front of that, you've got a CloudFront distribution so that it's globally distributed.\nThe access times around the world should be fast.\nAnd then we'd integrate it with the AWS certificate manager for HTTPS certificates and also route\n53 for your DNS.\nAnd that's generally the basic recipe for a kind of modern website infrastructure on\nAWS.\nAm I missing anything there?\n\n\nLuciano: I don't think so, but there is quite a bit of setup, right?\nSo I don't know if there is any common gotcha that you can share.\nLike to me, I've been beaten in the past by caching invalidation because it's just tricky\nto get it right.\nEspecially the first time, doesn't really matter.\nYou just publish and it works.\nWhen you want to do an upgrade, change a few things on the website and they don't really\nshow up immediately on the website.\nSo that's when it gets tricky and you realize you did something wrong.\nDid it happen to you?\nYeah, it's true.\n\n\nEoin: CloudFront has a bit of a bad reputation for the time it takes to synchronize to all\nthe edge locations as well.\nSo you really have to understand that it will take time.\nIt could be five to 10 minutes.\nI think these days typically to synchronize out your edges and then you have to understand\nhow invalidation works.\nSo that's definitely true.\nAs with all AWS services, you're given a toolkit of resources that you can assemble to build\nsomething really great, but you still have to have a reasonable amount of knowledge.\nSo you just have to be prepared to invest a bit of time.\nI know that AWS Amplify is there to kind of make this easier as well.\nIt's not something I use very frequently.\nI typically use the Amplify SDK rather than the CLI.\nHave you heard good things, Luciano, about what the Amplify CLI can do for users starting\nwith this kind of thing for the first time?\n\n\n\nLuciano: Yeah, I haven't had the chance to use it myself yet, but few people have recommended it as a much simpler and faster way to deploy static websites to AWS.\nAnd especially there are a bunch of additional bits and pieces.\nLike if you are building a single page application type of website, they make your life easier.\nFor instance, if you want to call Lambdas on the backend because you want to build something\nmore dynamic.\nSo I think you also have an additional layer on top of just plain static websites when\nyou use Amplify.\nBut again, I still have to try it, so I'm not going to spend more time talking about\nit because I might say something stupid.\n\n\nEoin: Fair enough.\nI mean, there's lots of...\nAs with AWS and everything, there's always many, many ways to do any given task and to\nachieve a goal.\nSo I know you've got alternatives that have been there for a while, like Elastic Beanstalk,\nI think is quite popular with agencies deploying websites.\nYou've got LightSail, which is designed for, I suppose, users who just want a simple developer\nexperience.\nAnd then you can also deploy your own, right?\nYou can deploy containers with a load balancer.\nI'm sure you've done that in the past one or two times Luciano.\nOh yeah, for sure.\n\n\nLuciano: I even did it once in a Lambda, which is I think a little bit extreme because of course\nwe have all sorts of limitations that come with the Lambda runtime.\nBut I think for small websites, like maybe you are building a form where we are collecting\njust few information, that can work quite well.\nIn the Lambda, you can have serving the HTML and the few assets you need and also the backend\nlogic to collect the information for the form.\nSo yeah, maybe it's not by the book how you should do that, but if you're working on a\nsmall enough project, that can also be an option.\nGreat.\n\n\nEoin: Well, I think what we can do is certainly put some links to everything we've talked\nabout today, some of the services, resources in the description and the show notes.\nAnd otherwise that's all for today.\nSo thank you for listening.\nLet us know what you think in the comments.\nIf you have any further questions, let us know what you'd like us to talk about too.\nFollow us and subscribe and otherwise we'll see you in the next episode.\n"
    },
    {
      "title": "4. What language should you use for Lambda?",
      "url": "https://awsbites.com/4-what-language-should-you-use-for-lambda/",
      "publish_date": "2021-09-30T00:00:00.000Z",
      "abstract": "In this episode Eoin and Luciano talk about programming languages in the context of Lambda. What language should you use? Actually we really answer the question “what languages do WE like to use in the context of Lambda”. Be ready for a lot of strong opinions in this episode!\nIn this episode we mentioned the following resources:\n\nLambda Rust Runtime\n\n",
      "transcript": "Luciano: Hello and welcome to another episode of AWS Bites, the weekly show where we answer questions\nabout AWS in just about five minutes.\nMy name is Luciano and together with me, we have Eoin.\nAnd today we have a very interesting question that I really, really excited about, which\nis what language should you use for AWS Lambda?\nSo I'm going to let you start Eoin, what language do you use for your Lambdas?\nI'm going to resist the temptation to be opinionated.\n\n\nEoin: Really when you're selecting language, I think the first rule is always pick the language\nthat you know and you're comfortable with, especially if you're starting with Lambda\nfor the first time, there's no point trying to learn too much at once.\nThat said, my own personal favorites for Lambda attempt to be the dynamic languages.\nSo most of the Lambda functions I've written have been Node.js runtime, so JavaScript.\nAnd my second favorite would be the Python runtime.\nBut you can use up whatever language you want actually these days.\nWhat do you think Luciano?\nIs there a good set of recommendations for people using Lambda for the first time in\nwhat language to start with?\nYeah, first of all, I'm not going to resist the temptation to be opinionated.\n\n\nLuciano: So I'm going to say use Node.js because in my opinion is the best language, the best\nruntime in general and in particular for Lambda, it's a very good fit for the way Lambda works.\nAnd I've been writing a lot of Lambdas in Node.js and so far it has been working very,\nvery well in terms of both performance and ease of writing and deploying and testing\nthe Lambdas.\nSo that's definitely my favorite.\n\n\nEoin: I got to ask then.\nSo what is it about JavaScript compared to the alternatives?\nBecause if you're coming from a.NET developer background, enterprise Java background, you've\ngot a lot of tools there to help support you.\nThe languages give you type safety and the languages themselves are both very performant\nin Java when it comes to runtime code execution.\nWhy would you use a dynamic language instead of choosing one of those?\n\n\n\nLuciano: Yeah, I think in particular with JavaScript, the main benefit is that the bootstrap time of your Lambda is very well optimized.\nLike the Node.js runtime starts very, very quickly and this is something you should care\na lot when writing Lambdas because of course you want to minimize that cold start time\nevery time you are spinning up a new Lambda.\nSo with languages like Java, I've seen that generally you tend to have much longer cold\nstarts and in Java you have...\nI suppose that the trick is always the same even outside the scope of Lambda.\nWith Java it's very good if you're doing like CPU intensive type of operations while Node.js\ntends to be much better if you're doing a lot of IO.\nAnd in Lambdas generally I've seen you write a lot of integration code.\nSo most often you end up doing HTTP calls or connecting to external sources.\nSo much more IO than you do CPU intensive tasks.\nSo maybe that's why I've seen Node.js being a much better fit compared to languages like\nJava.\n\n\nEoin: Isn't it strange though, the way when Node.js became initially popular a number of years\nago, it was because of its ability to handle many multiple requests at scale because of\nhow it worked with asynchronous IO and handling multiple connections on a single server.\nWith Lambda, it's always handling one single event at a time.\nSo how is it that this model translates well?\nIs it just because it's JavaScript?\nDo you just have to...\nDo you just dispense with a lot of the advantages of Node.js and it still doesn't matter?\n\n\n\nLuciano: Yeah, I think you are touching a very good point and I think this is a common complaint from many people using Lambda, especially for the first times that if you are getting\nmultiple requests at the same time, you will see multiple Lambdas spinning up, even though\nNode.js, if you had one server, would be very capable of handling even thousands of connections\nat the same time.\nSo definitely that's an interesting technical choice and I've heard different reasons from\nAWS why that's implemented that way.\nFor instance, isolation, if your Lambda crashes, you are not going to affect other users asking,\ntaking other requests at the same time, which could be an interesting point.\nTo answer your question, whether you are losing the benefits on Node.js, maybe partially so,\nbut at the same time, you are still keeping the main benefit that if you're doing a lot\nof IO, you can easily use the synchros model of Node.js and JavaScript and avail of that\nconcurrency in a very easy way.\nSo if your Lambda needs to connect to multiple places and do multiple things for a given\nrequest, you can still do that in a very efficient way.\nOkay.\n\n\nEoin: So we've talked about JavaScript, Python, I think, you know, they get all the benefits\nof being able to rapidly develop.\nYou don't have a compile step.\nThey start quickly.\nI think Java,.NET are reasonable options these days too, because there are ways to\nmanage cold start issues.\nI think.NET itself is quite performant, the.NET core runtime.\nWhat about other languages?\nBecause last year we had the arrival of Lambda container image support.\nSo anything that can run in a container can now run Lambda code.\nAnd before that we had custom run times, which was the ability to do the similar kind of\nthing with a zip packaging.\nSo what other less frequently used languages would you be using in AWS Lambda?\n\n\nLuciano: Yeah I think there are two that I am particularly excited about.\nOne is Go, which I actually had the chance to use it.\nAnd it's actually supported quite well by AWS these days.\nAnd the reason why I use it in the past is because I actually had to do a CPU intensive\ntask.\nIt was a lot of data transformation and I was doing that in Lambda in Node.js.\nAnd eventually we realized that in our entire pipeline that was one of the main bottlenecks\nand we could probably try to do something to speed it up.\n\n\nWe rewrote that in Go and it was like 10 times faster.\nSo that was definitely a very interesting experiment and I was very happy to how easy\nit was to write and test and ship a Lambda in Go, which was actually quite unexpected.\nI was expecting a lot more friction.\nAnother one that I'm really excited to try, I haven't tried it yet, is Rust, which is\nnot officially supported as a runtime yet, but there is a very good package that is provided\nby AWS itself that you just use it as a Rust crate.\nAnd then when you build your executable file, it's already called Bootstrap.\nAnd that's really the only file you need to put in a zip package or Docker container.\nYou just ship it as a Lambda and it should work out of the box.\nAnd they give you all the nice, in the library, they give you all the nice things that you\nexpect to handle, the context, the event, exceptions.\nSo that seems to be a new interesting contender if you really care about performance and quick\nBootstrap times.\nSo really excited to have a chance to try that out.\n\n\nEoin: One of the considerations I think it's worth also thinking about, and I think we always\nfall into the pattern of thinking of how will it perform at production time, but we forget\nas developers that performance and developer time and your developer feedback loop and\nhow quickly you can deploy, test and iterate is really important.\nOne of the things I observe is that with the most popular tooling for building and deploying\nLambdas these days is still the serverless framework.\nAnd it seems in my experience, at least to have a strong bias towards Node.js as the\nbest supported runtime for packaging all your node modules and everything else.\nEven if you start using Python, then shipping your dependencies has a little bit more friction.\nHave you noticed that?\n\n\nLuciano: I did.\nYeah.\nAnd I've been working a lot with Python Lambdas lately.\nAnd yeah, it seems a little bit more complicated than it used to be with Node.js.\nI think it's something that is going to get better over time because of course the tooling\nand the ecosystem are always evolving and Python is a widely used language.\nSo I expect that the future is going to be greener, but right now I will agree with you\nthat Node.js is probably the easiest way to get started with Lambda, especially if you\nalready know JavaScript.\nYeah.\n\n\nEoin: It looks like AWS Sam has a little bit more, I suppose, of a cross language support, at\nleast for the common runtimes and Python in particular, but maybe that AWS Sam versus\nserverless framework is a question for another day.\n\n\nLuciano: Yeah.\nYeah.\nI'm looking forward to try to answer that one.\nBut for today, that's all.\nAnd thank you everyone for listening.\nWe are really curious to know what's your favorite language for Lambdas.\nSo please leave us a comment or reach out to us on Twitter and make sure to follow and\nsubscribe so we can see you next time.\nBye.\n"
    },
    {
      "title": "5. What are our favourite AWS services and why?",
      "url": "https://awsbites.com/5-what-are-our-favourite-aws-services-and-why/",
      "publish_date": "2021-10-07T00:00:00.000Z",
      "abstract": "Eoin and Luciano talk about their favourite AWS services and why they like them. Spoiler: we talk about EventBridge, Lambda, CloudFormation, CDK, S3, ECS/Fargate plus some honorable mentions like CloudWatch and IAM.\nIn this episode we mentioned the following resources:\n\nSLIC Watch\n\n",
      "transcript": "Luciano: Hello everyone and welcome to another episode of AWS Bites, the show where we discuss interesting\nAWS questions in about five minutes. My name is Luciano Mamineo and today I am with Eoin\nShanaghi and the question of the day, it's a very interesting one, is what are your favorite\nAWS services? So Eoin, if you should pick like your top three favorite services, what would\nyou pick?\nOkay, so it's not an easy decision, but I've given this one some thought.\n\n\nEoin: For number three, I'm going to go with EventBridge and EventBridge is reasonably new. It came, it kind of spawned\nout of a rebranded version of CloudWatch Events, but you got a lot of options for messaging\non AWS, SQS, SNS, they're all really good services, Kinesis also for streaming. I could\nhave gone with either of those really, but EventBridge for me is really good because\nit's like the most serverless of services almost. There's so little to configure, you\ncan start publishing messages to it without provisioning anything and then you just create\nrules with nice pattern matching if you want to consume these events. So that's why I've\ngone for EventBridge and I think it's a really popular service and gaining more and more\ntraction.\n\n\nNumber two, I'm going to go for Lambda. A lot of people might've expected I'd go for\nit as number one, but I think we're going to see a lot more improvements to Lambda in\nthe coming years. It's already quite different than when it first launched. It really threw\nout the rule book for how you do compute in the cloud and it's so scalable. It has so\nmany integrations into other services and because of how it runs, it's really easy to\nmonitor secure things and individual function level and scale things at a really fine grain\nlevel. It's got an amazing number of features these days and I think it will really take\nover compute into the future, especially if the pricing model changes to support longer\nkind of more long running computational workloads. So that's number two.\n\n\nAnd then for my number one, probably not the most popular choice, but I think it's fundamental\nif you're doing AWS in any really serious way and that's cloud formation. Continuous\ndeployment in the cloud is a must have and how you manage your deployments and your cloud\nresources is really critical to how that works. So that means the features that cloud formation\ngives you that other options don't give you like the ability to manage a stack as kind\nof almost an atomic unit. You've got a collection of resources that AWS is going to deploy for\nyou based on added character template. Then you've got stack sets so you can see the different\nchange sets so you can see you can break things down into logical structures. You can see\nhow things will change before you deploy. It's very easy to underestimate the value\nthat cloud formation can bring to the table and it really gives you a lot of confidence\nin your deployments. So those are my top three. What about you, Luciano? Are you going to\ngo for three completely different ones?\n\n\nLuciano: Yeah, I'll try to differentiate, but I'm glad that you mentioned cloud formation because it's an interesting one for me. I used to,\nI don't want to say hate, but I'm going to say dislike cloud formation and I rather prefer\nto use Terraform every chance I got. And I think it's just because of the syntax. Like\nI found Terraform to be much more user-friendly and simple to understand and write. So in\nthe past I did use a lot of Terraform. Although I can see the value in cloud formation because\nonce you try to deploy with cloud formation, everything is done by AWS. Like you don't\nneed to manage your own secrets, your own state, which is something you have to take\ncare if you use Terraform. So I'm going to pick CDK, even though it's\nnot really a service, it's more of a tool that you get from AWS to use cloud formation.\n\n\nI'm cheating a little bit, but please allow me that because CDK, I found it to be like\nthe nice middle ground. Like you still get a very good experience in writing your infrastructure\nas code, probably even better than Terraform, but again, take it as an opinion. But at the\nsame time, you get all the advantages of cloud formation because CDK will generate cloud\nformation code for you. So at the end of the day, you also get all the benefits of that.\n\n\nSo CDK, probably one of my three favorite services that I want to mention. The other\none is S3, which probably a little bit obvious, but the reason why I wanted to mention it\nis because I have seen that it's kind of the entry point for many, many companies to AWS\nand the cloud in general. It might seem surprising today that so many companies start to think\nabout the cloud when they think, okay, what are we going to do with all these files? Like\nwe don't trust our disks and our backups anymore. What do we do? How do we make them highly\navailable and resilient? And S3 then becomes kind of the gateway to the cloud for them.\n\n\nSo definitely want to give it a shout out, even though it's, I think one of the oldest\nservices together with EC2 and has been there for a while. So maybe not unused, but still\nvery, very well-worked service to mention. And the last one I want to mention is ECS\nand Fargate, which I will depict Lambda if you didn't do that already, but I've seen\nthat ECS and Fargate are very good replacement for Lambda, especially in cases where you\nhave an application that has been built for a number of years, maybe even before the cloud,\nand you are trying to figure out, okay, what is the fastest and easiest way for me to bring\nthat application to the cloud? And if you know a little bit of Docker and you can write\na container for that application, ECS and Fargate will allow you to rather quickly deploy\nall of that in the cloud. So another thing that kind of goes in tandem with my first\nchoice CDK is that I didn't like as much ECS and Fargate before CDK, because there is,\nagain, a lot of configuration you need to write, but with CDK, you get this library\nof higher level constructs, which are called ECS patterns, and those will make your life\nmuch, much simpler. Like you really need to write a very minimal amount of configuration.\nOf course, there are a lot of assumptions that are baked in in these patterns, but for\nmost use cases, you'll be able to ship things to production very quickly. So those are my\nthree picks. Is there anything you think is also worth a mention? Maybe let's do some\nhonorable mentions.\n\n\nEoin: Yeah, what didn't quite make the list? Yeah, I'm with you on S3 for sure is just outstanding\nin what it can deliver. Yeah, I think one of the honorable mentions I'd have to mention\nis CloudWatch, and this is another one that kind of flies under the radar quite a lot\nbecause it's been there for a long time, but there's actually a constantly growing number\nof components to CloudWatch. And over the last couple of years, I've found myself using\nCloudWatch logs insights, like to do powerful distributed log querying really frequently\nwithout having to go to third parties for decentralized logging solutions. But we also\nuse metrics and alarms. I think CloudWatch can be really powerful for creating your own\ncustomer application metrics and alarms as well. And with the right setup, you can go\nreally far with it. And we should probably also mention the open source project we've\nbeen working on called SlickWatch, which allows you to create a lot of those resources automatically\nfor your applications. So I'd give CloudWatch as my honorable mention. What about you, Luciano?\nDo you have another one? Yeah, that's a good one.\n\n\nLuciano: I'm going to just mention really quickly IAM because it's one of those unescapable services that you need\nto learn. Again, not the easiest. Initially, I had a little bit of love and hate relationship\nwith it, but once you understand the building blocks and how well integrated it is with\nevery single service, then yeah, you really get the value out of it and it's something\nyou really need to use, you cannot avoid. So definitely a good service. If you haven't\npaid too much attention to it, I would encourage you to have a second look and try to understand\nbetter all the different concepts around it. And with that, I think we are at time for\ntoday. So thank you very much for following us. I'm really curious to know what are your\nfavorite services. So if you want to leave us a comment or connect with us on Twitter,\nplease send us your top three and we'll probably make some statistics with that. That will\nbe an interesting experiment. So thank you very much. Make sure to follow us and give\nus a like, thumbs up, follow us in your favorite podcast software and we'll see you at the\nnext episode. Bye.\n"
    },
    {
      "title": "6. Is AWS Lambda cheap or expensive?",
      "url": "https://awsbites.com/6-is-aws-lambda-cheap-or-expensive/",
      "publish_date": "2021-10-14T00:00:00.000Z",
      "abstract": "In this episode Eoin and Luciano talk about the pricing model for AWS Lambda and describe some cases where it could be a very convenient solution but also cases where it might end up being very expensive (compared to EC2 spot instances).\nIn this episode we mentioned the following resources:\n\nLambda power tuning\nYou are thinking about serverless costs all wrong\nWhy AWS Lambda Pricing Has to Change for the Enterprise\n\n",
      "transcript": "Eoin: Hello and welcome to another episode of AWS Bites, the weekly show where we answer questions\nabout AWS in about five minutes.\nMy name is Eoin and I'm joined again by Luciano.\nAnd today's question is, is AWS Lambda cheap or expensive?\nSo I suppose the great thing about AWS Lambda is you pay for it only when it runs.\nThat's the whole business model behind Lambda.\nAnd if you imagine Lambda when it runs, it's like this ephemeral resource.\nIt starts when a given event occurs, it processes that event, and then it shuts down and you're\nnot paying for it after that point.\nSo how does that fit into the pricing model, Luciano?\nCan you take us through that?\nYeah, so in short, there are like two parts to the pricing of Lambda.\n\n\nLuciano: One is called like the fixed cost of Lambda and it's basically you pay per number of invocations.\nIt's like, I think, 20 cents of dollar per one million invocation, which in my experience\nis quite negligible because in reality, the bulk of the cost is going to be the variable\npart.\nAnd the variable part is basically based on the actual execution, as you said.\nAnd it's a function of the time that you are actually running a given Lambda, but also\nthe amount of memory that you are allocating for that Lambda.\n\n\nSo you can imagine that these two dimensions, you multiply them together and you get the\nactual price for a Lambda execution.\nThe interesting bit, there is a caveat there that you might think, okay, then I'm going\nto be very cheap and not allocate a lot of memory.\nSo I kind of save money that way.\nIn reality, memory and CPU are very closely related, which basically means that if you\nallocate more memory to your Lambda, you will also get proportionally more CPU allocated\nfor that Lambda, which sometimes it means non-obvious things like depending on your\nworkload, if you're using a lot of CPU, you might be better off allocating more memory,\neven though you don't really need that memory because you get more powerful CPU, then you\nare probably going to complete the execution faster.\nSo at the end of the day, it's going to be cheaper for you.\nAnd of course, all these mechanisms are not very easy to predict in all sorts of different\nuse cases.\nSo there is an interesting tool that I want to mention by Alex Casalboni called AWS Lambda\nPower Tuning that allows you to actually run a simulation of a given Lambda with all the\ndifferent dimensions and see what is the configuration that is probably going to be cheaper for you\nlong term.\nSo definitely have a look at that in the cases where you are not 100% sure what's going to\nbe the best configuration for your Lambdas.\nBut yeah, I have to say that it's hard to estimate that.\nSo I don't know, Eoin, if you want to add anything to this observation.\nYeah, it is.\n\n\nEoin: I mean, if you compare it to EC2, it's easier to understand when you're billed by the hour\nbased on the machine size and the configuration.\nThe cost is just based on how long that machine is active, whether you're doing anything with\nit or not.\nBut if you're going to say, OK, well, how does Lambda pricing compare to EC2, then it\ndepends on how much traffic you have at any given moment.\nIf you've got stable, consistent workloads running all the time and you're optimizing\nyour EC2 instance to those workloads, it actually might be cheaper over time.\nBut that's just one of the factors, right?\nSo there's other cost factors you need to consider, like the time it takes to maintain\nprovision, run these EC2 instances, patch them.\nThere's a lot of factors involved.\nSo it's not exactly a simple use case, a simple calculation, is it?\n\n\n\nLuciano: Yeah, I remember there is a very good article by Yan Cui, I believe, that is like the total cost of ownership of serverless.\nAnd I think it was the first time where I got this idea that, yeah, you shouldn't look\nonly at the bill that you get from AWS when you think about the cost, but you should think\nmore about your entire operation, how much is it costing you to operate on Lambda compared\nto operate on EC2?\n\n\nLike how many people do you need to hire and responsibilities?\nHow do they spend their time compared also to the features that will make you money?\nIs it just patching instances, something that will make your business more successful or\nyou can just avoid to do that because with Lambda you can focus more on features and\nbe more productive?\nSo that's definitely another read that I recommend people to have a look at.\nOkay, so yeah, it's interesting that you say that pretty much depends on the use case,\nbut do you have an expectation that maybe, because Lambda is so convenient, right?\nAnd we are seeing more and more adoption.\nSo is there maybe any hope or a chance that AWS will revisit the price structure to make\nit more, I guess, palatable, even if you are doing long running jobs?\n\n\nEoin: If I was to place a bet, I would expect that the need for wider Lambda adoption will result\nin some price changes at some point.\nIf we look at, there was an article I wrote last week, last actually this year, I think,\nin response to this, having had experience running Lambda in various different workloads,\nwhen we see where you've got constantly running functions doing machine learning or business\nrule execution for hours at a time with thousands of containers, it really starts to look expensive.\n\n\nAnd sometimes it's just optics and perception when people look at this price and compare\nit to EC2.\nI covered all of those calculations in the article, but sometimes it's a massive factor\nof difference, like 10 to 20 times difference if you compare it to EC2 spot.\nBut people don't generally consider the total cost of ownership, and that's the unfortunate\nreality of it.\nThey look at the ticket price.\n\n\nSo I think in order to just meet people where their thought process is, it will require\nsome more adaptive pricing in the future.\nBut look, I wouldn't say people should fear Lambda pricing.\nLambda is still the best service to adopt for compute just because it allows you to\nget things done so quickly.\nSo I would say, be aware of your cost, but don't fear your cost.\nJust understand how the model works.\nGo with it and try and optimize when you need to.\nSo we will have all the links.\nI'll link into that article in the show notes.\nBut otherwise, let's wrap it up for today.\nThanks again for listening and let us know what you think in the comments.\nSend us your own questions for future episodes.\nFollow us and subscribe us and we'll see you next time.\n"
    },
    {
      "title": "7. When do you use Step Functions?",
      "url": "https://awsbites.com/7-when-do-you-use-step-functions/",
      "publish_date": "2021-10-22T00:00:00.000Z",
      "abstract": "In this episode Eoin and Luciano try to reply to a question suggested by Emily Shea: When do you use Step Functions?\nOur answer describes what Step Function is and what you can build with it. We discuss some examples of features that we built in the past using step functions (a billing engine and a crawler) and why. We also discuss what are the main advantages of Step Functions and some things to be aware of, including limitations, cost and when not to use Step Functions.\nIn this episode we mentioned the following resources:\n\nStep functions home page\nExpress step functions\nAI As a Service (Book by P. Elger and E. Shanaghy)\n\n",
      "transcript": "Luciano: Hello and welcome to another episode of AWS Bites, the weekly show where we try to answer\nAWS questions in just about five minutes.\nMy name is Luciano and today I'm joined by Eoin.\nAnd before we get started, I would like to suggest you to follow and subscribe so you\ncan be updated the next time that we publish a new episode.\nThis question was suggested by Emily Shih and it was on Twitter.\nAnd the question is, when do you use step functions?\nI will say before we try to answer that question, maybe we should give a very quick description\nof what step function actually is.\nYeah.\n\n\nEoin: So step functions are AWS's service for implementing state machines.\nSo what that allows you to do is to create workflows that are composed of sequences of\nsteps and they might have, you know, control flow if statements in them and you create\nthem declaratively in JSON or YAML rather.\nSo it's an alternative to trying to implement workflow in something like a Lambda function\nwhere you might be using code or using an event driven kind of orchestration approach.\nSo that's what step functions are.\nYeah, to me, I like to describe them as like the power user version of something like Zapier.\n\n\nLuciano: So where you can easily integrate different services.\nAnd the reason why I like to say that is because especially recently they published a new visual\neditor.\nSo I think the barrier to entry is much lower now and it's easier to just even drag and\ndrop different things and integrate them together.\nSo I like that analogy.\nAnd I like to actually to give an example of what was the first thing I implemented\nusing step functions.\n\n\nI was working in the electricity industry.\nSo we were working in a company that was selling electricity and we needed to calculate billing\nfor every customer.\nAnd that process was actually quite complex.\nWe needed to fetch data from a number of different sources in a different format, different databases.\nThere was like FTP, SQL, CSV repositories.\nAnd after we got all the data, basically we needed to do a bunch of different calculations.\n\n\nAnd then finally we had that billing figure.\nNow interesting enough, there was also manual step to somebody actually review the numbers\nbecause those were big industrial customers.\nSo we wanted to be double sure that everything was okay.\nAnd then when the final bill was reviewed, somebody will click a button somewhere and\nthat will make the step function continue.\nAnd the step function will send the final PDF by email, save it in a bunch of different\nplaces and finalize the billing process.\nSo that was an interesting use case.\nAnd using step function was helpful to us because we would actually for every customer,\njust open the step function page and see literally what was the state, the current state.\nIf there was some error, we would implement the retry mechanisms.\nSo it was very convenient rather than orchestrating all of that in like one big Lambda.\nDo you have any use case on your side Eoin that you want to share?\n\n\n\nEoin: Yeah, you can use it for so much these days because now it's actually integrated into the AWS SDK.\nSo it used to be able to integrate with DynamoDB and Lambda and a few other services, but now\nyou can actually call any AWS SDK service.\nSo the possibilities are pretty unlimited.\nOne of the examples I covered off in the book I co-wrote with Peter Elger, the AI as a service\nbook is actually implementing a simple web crawler in the step function.\n\n\nSo that was an interesting use case.\nI wouldn't necessarily implement Google using step functions, but it was a case where you\nhave a page to crawl.\nYou need to traverse through the page recursively and do some analysis on it.\nYou're actually extracting text and then using AWS Comprehend to analyze the text and figuring\nout if there are any dates or locations mentioned in the webpage.\nSo we use step functions to implement that.\n\n\nAnd it's quite good for that kind of thing where you've got loops, recursive flows, or\nyou've got AWS services you want to call out to that might take a long time to execute.\nIt's like some of the AI services.\nSo that's one thing.\nI also find it, I think where it really shines and will be adopted more and more is in enterprise\nworkflow cases.\nWhen you've got businesses that need to do process business rules, and that could be\nlike batch processing based on some rules relating to pension schemes or insurance policies\nor financial rules.\nAll of the FinTech banking insurance sector has a lot of that kind of stuff that might\nrun on a scheduled basis or in response to an event.\nAnd it needs to evaluate some complex workflow and perform a series of steps.\nAnd being able to visualize them in the step functions console is really handy, especially\nwhen you're troubleshooting for standard workflows that is.\nMaybe we can get into express workflows too.\nSo I think those kind of business process modeling cases where you might have used other\nvery complex custom code or business process modeling engines in the past, those are really\ngood places where you can use step functions.\nYeah.\n\n\nLuciano: All the use cases you just described struck me as like long running, I don't know, pieces\nof business logic.\nSo I'm starting to ask myself, what are the limitations that maybe you should consider\nbefore using step function for something?\nYep.\n\n\nEoin: Yeah, that's a good one.\nThe interesting, slightly amusing limitation for step functions that you can run each execution\nfor up to a year.\nSo I think it's the longest time limit of any service.\nBut yeah, and obviously that brings its own challenges because how do you test something\nthat expires after a year?\nIt's an interesting challenge in the development and test.\nSo you can have long run against executions.\n\n\nAnd the reason for that is because you can have manual approval steps like you've already\noutlined Luciano.\nSo that's why you might need that.\nPeople might take months to actually approve something.\nYou also have a limit on the number of transitions.\nSo in a standard workflow, that's 10,000 transitions.\nAnd that can hit you, especially if you've got a wait and retry event loop, if you're\nwaiting for a service to run, or if you've got a lot of items.\nYou've also got a limit into the number of the amount of data you can process, right?\nSo you can't throw megabytes of data into a step function, and you can't process millions\nof items in a map state or a parallel state, some of those really useful control flow states.\nHave you hit some of those limits?\n\n\nLuciano: I have hit some of these limits once, and I needed to figure out a different solution.\nI don't remember exactly what it was, but I think I was doing all the orchestration\nin a lambda for that particular step, which was a little bit unsatisfying, I'd say, because\nI lost the benefit of step function at that point.\nBut on the number of transition, you reminded me that that's actually an interesting thing\nto consider, because the pricing model is actually based on how many transitions you\nare doing in the execution of a step function, including even start and end.\nSo even if you have just one step, you are still paying at least three transitions there.\nIt might get very expensive if you do a lot of transitions very quickly and you do a lot\nof executions, so that's maybe something else to consider.\nIf you have very simple use cases, maybe with a lot of transitions, maybe if you need to\nsave money, don't go directly into step functions.\nThat's also something that could become a limitation, the pricing model there.\nYeah.\n\n\nEoin: And they introduced the Express workflows then, so that can help with the pricing, but\nonly if you have a very short running execution.\nSo when would you use Express workflows as opposed to standard workflows?\nYeah, as far as I know, Express workflow, they work in synchronous mode.\n\n\nLuciano: So basically it's like you can implement a request response type of pattern.\nSo if you need to have, maybe you have still a complicated workload behind the scene, but\nthe way of consuming that workflow, you just make a request and you expect a response as\nsoon as possible.\nI think in that case, using the Express version of step function will be a much better suit\nfor that use case.\nYeah.\n\n\n\nEoin: And then you've got a five minute limit, which is obviously a completely different scale to a standard workflow.\nYeah.\nSo it's for really, really fast executions and things that might be behind an API.\n\n\nLuciano: Okay.\nSo now we are approaching the 10 minutes mark, so I think this is time to do the closing.\nThat's all we have for today.\nThank you for listening.\nAnd if you enjoyed this episode, of course, make sure to subscribe and like.\nAnd we are always curious to know your opinion.\nSo if you have other interesting use cases that you want to share, or if you disagree\nwith our opinion, absolutely send us a comment or reach out to us on our social channels.\nWe'd be more than happy to chat with you and compare our experiences.\nAnd with that, thanks again.\nWe'll see you at the next episode.\nBye.\n"
    },
    {
      "title": "8. What are your AWS horror stories?",
      "url": "https://awsbites.com/8-what-are-your-aws-horror-stories/",
      "publish_date": "2021-10-28T00:00:00.000Z",
      "abstract": "In this Halloween-themed episode, Eoin and Luciano tell some AWS horror stories! Get ready for some trick or treat!\nOf course we have to start with billing and we tell some of our failures with predicting cost ending up with a nightmarishly bad billing surprise! We also discuss some horror stories from the perspective of AWS developer experience and finally we touch on some Cloudformation terrors!\nIn this episode we mentioned the following resources:\n\nAWS NAT Gateway\nAWS High Performance Block Storage\nCloudformation UPDATE_ROLLBACK_FAILED state\n\n",
      "transcript": "Eoin: Hello and welcome to a special Halloween episode of AWS Bites, the weekly show where we answer\nquestions about AWS in about five minutes.\nMy name is Eoin and today I'm joined by Luciano.\nBefore we get started, be sure to give us a follow and subscribe so you can get notified\nwhen the next episode goes live.\nBut today our Halloween themed question is, what are your AWS horror stories?\nI guess the scariest thing about AWS has to be billing.\nLuciano, do you have any billing nightmares that you can tell us about?\n\n\nLuciano: Of course, that was actually the first thing that came to my mind about AWS horror stories.\nSpecifically one time where I was working on a very new project, it was a startup, we\ndidn't have big funding.\nAnd then suddenly we had a bill of like $7,000 I think.\nAnd then I realized that it was actually my fault because by mistake in Terraform, I ended\nup provisioning a number of IOPs in an EBS volume that was way over what we needed.\nAnd yeah, we just did realize when we got the bill and thankfully we did have some credit\nwith AWS at the time.\nSo we didn't just bankrupt the company straight away.\nSo there is a little bit of a silver lining in that story.\nYeah.\nDo you have any things like that related to billing?\nYeah, I have.\n\n\nEoin: I've had a few scary stories.\nI think in general, if that kind of thing happens, you can try and reach out and clear\nyour case and get some mercy from AWS.\nYeah, I've had a few of them.\nI have to say I've left EC2 instances, not one or two, but thousands running over the\nweekend and realized on a Monday morning when I saw the billing alerts.\nBut yeah, look, you got to put billing alerts in place and try and react to them as quickly\nas possible.\n\n\nI also had a really interesting one.\nI mean, one of the really, I suppose, standout billing nightmares with AWS is the data transfer\ncost.\nAnd another particular one is NAT gateway cost.\nSo managing that gateway cost is pretty expensive.\nAnd I did have a case where it was working in an enterprise that had, you know, a direct\nconnect link into their corporate data center.\nBut I was using CDK actually to use one of the ECS patterns for CDK.\n\n\nSo it's a really nice construct that will create all of the infrastructure you need\nautomatically.\nSo you don't have to worry about what's under the hood.\nExcept I didn't realize, I didn't pay enough attention to realize that what it was creating\nunder the hood included its own NAT gateway and its own VPC and its own network constructs\nas well as the ECS infrastructure and the cluster and SQSQs and all the great stuff\njust from having one line of code that creates this nice AWS queue processing service.\nAnd yeah, because we were doing a lot of S3 traffic, we ended up getting charged quite\na significant amount of money.\nI can't remember exactly what it was, but that was completely not obvious to me at the\ntime.\nBut I've definitely learned my lesson and checked these things and double check each\ntime.\nWhat else have we got in terms of AWS horror stories?\nIs there anything not building related?\nYeah, I think in general there could be a lot of other stories regarding developer experience.\n\n\nLuciano: And we all know, for instance, the web console is not great.\nSometimes you struggle to do even things that you expect to be like basic features.\nBut recently I struggled a lot with EventBridge, for instance, which is even a new service.\nI was expecting this problem not to be there.\nAnd it's kind of a funny one because in EventBridge, the way it works is you can trigger events,\nyou have AWS events, you have custom events.\n\n\nAnd then the way you hook into those events and react to them is that you have to create\nrules.\nBut the problem is that when you are publishing an event, if you are not the one publishing\nit, maybe it's an integration.\nIn order to visualize that event, you need to write a rule.\nBut if you don't know the shape of that message, it's very hard to write a matching rule.\nSo I did find myself spending a lot of time in this kind of chicken and egg problem trying\nto figure out, OK, how am I supposed to write a rule for a message that I don't really know\nabout?\nAnd I know that there are solutions, but I did end up spending a lot of time trying to\nsolve that particular problem the first time I was using EventBridge.\nSo maybe something to be improved there in terms of user experience.\n\n\nEoin: Yeah, for developer experience, there's so many horror stories we could probably talk\nabout and so inconsistent as well across services.\nOne of the things I find is that if you decide to go with a third party service versus AWS\nand you're trying to weigh up the pros and cons, a lot of people will lean towards AWS\nbecause they've already got everything set up in AWS and you've got the IAM permissions\nall in place.\n\n\nBut that can bite you when it comes to developer experience.\nAnd one of the nightmares I've been through a few times is setting up build pipelines.\nAnd people can assume that code pipeline and code builder are something analogous to using\nGitHub actions or GitLab CI or Jenkins.\nBut in fact, it's a very different beast.\nYou really have to invest a lot in understanding what the constructs are.\nYou can't just create a YAML file with declarative workflow and get it working.\n\n\nYou have to create and configure individual resources and every step in your pipeline\nis an AWS resource.\nAnd it's powerful in a lot of ways that it's well integrated into AWS.\nBut it doesn't have the flexibility of all those other options, unfortunately.\nYou can't dynamically control the workflow.\nYou can't just skip actions in a code pipeline stage very easily.\nAnd you don't get that nice kind of visualization that you get in GitHub or other services where\nyou can see your code pipeline really easily and link it through to your pull request and\neverything.\nSo I've definitely had a case where I thought, okay, this is going to be easy to set up a\ncode pipeline.\nBut it ended up really being a horror story and something that burned me badly.\nSo I'm very cautious about recommending code pipeline as a result.\n\n\nLuciano: So one last one.\nYeah, go ahead.\nI was thinking about deployments and well, pipelines.\nYou make me think about deployment.\nAnd one thing that I'm sure that many people have been through is when you have to fix\na very urgent bug in production and you are working really hard, you have the changes,\nyou are ready to ship them.\nAnd suddenly, CloudFormation is throwing at you that was the actual name update rollback\nfailed which takes whatever to settle.\nAnd then you are there just waiting like, AWS, please do something for me.\nI really need to ship these changes.\nSo I don't know if that ever happened to you, but it happened to me a bunch of times and\nhas been an horror story every single time.\n\n\n\nEoin: Yeah, I think everybody who uses CloudFormation probably has a nightmare where they wake up in a cold sweat dreaming about update rollback failed.\nIt's really so difficult to recover from.\nYeah, that's definitely a true horror story.\nMaybe with that, we scared people enough and we should let people go have a lie down.\nBut thanks very much for joining us and do make sure to like and subscribe so that you\ncan find out about the next episode and we'll see you next week.\n"
    },
    {
      "title": "9. How do you get started with AWS?",
      "url": "https://awsbites.com/9-how-do-you-get-started-with-aws/",
      "publish_date": "2021-11-04T00:00:00.000Z",
      "abstract": "In this episode, Eoin and Luciano try to answer a beginner question: how to get started with AWS. Special thanks to Adam Mann for suggesting this question!\nAWS is so big that it doesn’t make any sense to try to learn it all! It’s better to learn the fundamentals and then focus on specific areas of interest to get the best out of AWS.\nBut what are these fundamentals and where do I start if I have a background as a web developer? Check out this episode to find out!\nIn this episode we mentioned the following resources:\n\nIAM\nS3\nCloudFormation\nThe Serverless framework\n\n",
      "transcript": "Luciano: Hello everyone and welcome to another episode of AWS Bites, the weekly show where we answer\nquestions about AWS in more or less five minutes.\nMy name is Luciano and today I'm joined by Eoin, but before we get started I want to\nask you to make sure you give us a follow and subscribe so you can be notified every\ntime we publish a new episode.\nThis question is, how do you get started with AWS?\nIt's a very exciting question and I have to thank Adam Mann for suggesting that question\non LinkedIn.\nSo, Eoin, what do you think?\nHow would you get started with AWS?\n\n\n\nEoin: Yeah, well, first things first, I think one of the things you do need to get started with is a credit card.\nAnd you know, that's not really great because if you're a student or if you're someone coming\nfrom a less privileged background, that's not a simple thing to make sure you have.\nAnd then also you need to be aware of your cost because there's no way to cap your cost\nand have guarantees.\nSo that's the first thing and you need to get that out of the way first because right\nnow that is the only way you can get started and create an AWS account.\nWhat do you think yourself, what would you recommend for somebody who's coming to you\nsaying, I want to get into AWS, where do I start?\nYes, I think the problem with AWS is that it's incredibly big.\n\n\nLuciano: Like there is so much you could possibly learn that I think it's not humanly possible for\none person to know everything that there is to know in AWS.\nSo with that being said, of course you need to pick an area of interest.\nSo I would probably ask somebody like, what do you already know?\nWhat are your skills that maybe you can learn something in AWS to try to amplify those skills?\nAnd I think that the very common use case that I'm seeing is you maybe are a full stack\nweb developer, you never use AWS, maybe learning how do you deploy a full stack web application\nin AWS can be useful because at that point you are kind of expanding your skills and\nalso learning how to ship your code to production.\nSo that's maybe one angle you could take when approaching AWS.\nAnd yeah, but at the same time, there are maybe things you cannot escape, right?\nHow do you feel about that?\nYeah.\n\n\nEoin: Okay.\nYou've got probably around 200 services.\nYou have to focus on something.\nThere's always going to be the fundamental services that you can't escape.\nAnd while you don't have to learn them completely when you start off, you have to be aware of\nthem.\nI am, I am is one of them.\nSo you need to understand how to manage roles and policies and what the essential details\nare around that.\nThere can be a lot of complexity to it, but it's good to get started with that pretty\nearly.\n\n\nAlso understanding users and security and how they relate to roles and policies is important\nbecause you need to know when you're doing something that isn't secure from the start,\nespecially when your credit card is behind it again, right?\nSo IAM is another one.\nS3 is a service you probably you're not going to avoid for long.\nAnd it's also quite simple to get started with.\nSo much, much more understandable and very useful and broadly, broadly applicable across\nlots of different things.\n\n\nSo those are the, those are a couple of fundamentals.\nAnd I'd say that when you're starting to learn about services, there's a couple of different\nways of doing it.\nYou can use the web console, the AWS management console.\nIt's quite a good way to visually create resources and understand how they work.\nBut I would say as early as possible, you should try and think about getting into creating\nresources as code and using infrastructure as code.\n\n\nSo cloud formation or Terraform are good tools for that.\nAnd the reason I'd recommend those, even when you're getting started pretty early on, is\nthat they're very good for helping you understand how these services are actually configured\nunder the hood, what their properties are and how they relate to each other.\nSo it helps you to build a good mental model.\nAnd it's also just a good practice to build up anyway.\nBut the other fundamental that of course is important relating to cost is to understand\nbilling and cost and usage reports, or to have some way of getting insight into your\ncosts, because it's always something you have to be aware of.\nIt's a tool you need in your developer toolkit on AWS.\nWhat do you think?\nHave I missed anything there?\n\n\nLuciano: I think you gave a pretty comprehensive idea on things you're definitely going to bump\ninto.\nSo it's better to approach them early and also that they can help you to learn faster\nall the different concepts in AWS.\nOne thing that I want to add is that probably after everything we just said, you still might\nnot know where to start, because again, it's kind of a jump in the dark.\nIt's very hard to pick an area and just start.\n\n\nSo if that's the case, I'm going to give you an opinionated recommendation.\nJust start with the serverless framework and try to build an API.\nThere is plenty of tutorials out there.\nSo just pick whatever seems the most comprehensive or easier to you.\nAnd the interesting thing is that you're probably going to learn about how to build an API,\nhow to configure API gateway and Lambda and use maybe DynamoDB as a back end to store\nthe data.\n\n\nAnd all these things are core competencies that are going to be very valuable for you.\nOne reason is because they are very on demand in the market.\nThey are growing a lot in popularity.\nSo definitely there is going to be a lot of adoption in the future.\nAnd the other thing is that with those skillset, as somebody that is just studying or maybe\nstarting to build some products, it's something that can allow you to build different things\nvery quickly.\n\n\nSo you can easily put things in production without having to learn.\nI don't know.\nHow do you manage an EC2 instance or a server?\nHow do you put those things in production?\nHow do you maintain them?\nYou don't need to worry about all this stuff.\nYou just learn the minimum required to ship code to production and run it, which I think\nis something very useful to have in your skillset if you're starting as a web developer especially.\nOkay.\nSo with that being said, I think that's all we have for today.\nSo thank you very much for listening and definitely let us know what you think in the comments.\nIf you started using AWS in a different way, we are really curious to know what was your\npath.\nSo definitely share that with us.\nAnd if you enjoyed this episode, please give us a thumbs up and remember to follow and\nsubscribe.\nWith that, we'll see you next time.\nBye.\nAnd we'll see you next time.\n"
    },
    {
      "title": "10. Lambda or Fargate for containers?",
      "url": "https://awsbites.com/10-lambda-or-fargate-for-containers/",
      "publish_date": "2021-11-11T00:00:00.000Z",
      "abstract": "In this episode, Eoin and Luciano try to explain whether you should consider Lambda or Fargate for containers.\nWe start by clarifying what we really mean by containers and what kind of container support you get in both Lambda and Fargate. Then we go into more detail about the characteristics of both services including limits and pricing. By the end of this episode you should be able to understand how the two services compare and which one might be more suitable for your next project!\nIn this episode we mentioned the following resources:\n\nAWS Lambda\nAWS Fargate\nContainer Image Support in AWS Lambda Deep Dive\nContainer Image Support on AWS Lambda Bridges the Gap to Much Wider Adoption\n\n",
      "transcript": "Eoin: Hello and welcome to AWS Bites, the weekly show where we answer questions about AWS in\nabout five minutes.\nMy name is Eoin and again, I'm joined by Luciano.\nBefore we get started, just make sure to give us a follow and subscribe so you can be notified\nwhen we release the next episode.\nAnd today's question we've got is, should you use Fargate or AWS Lambda for containers?\nWhat do you think, Luciano?\n\n\nLuciano: Yeah, I think it's important to start by clarifying what do we really mean with the word container,\nbecause I think there are at least two different ways of seeing a container.\nOne is when you look at a container as a packaging tool, so you put all the code in this unit\nand there is also the idea of running that code.\nSo considering that container as a runtime environment, right?\nSo in that sense, in the context of Fargate and Lambda, if you're looking for a container\nruntime that is maybe closer to something like Kubernetes, probably Fargate is a better\ndefinition, a better service for that concept.\n\n\nAnd instead, if you're looking just for something that helps you to run code without worrying\ntoo much about how it's running, then maybe Lambda is a better service for that type of\nidea.\nAnd in general, also, it might make sense to think in terms of long-running services\nrather than just run some code when an event happens.\nSo also that's another way of seeing the difference between Fargate and Lambda, because Fargate\nis definitely more suitable for long-running services, maybe more traditional web servers\nlike, I don't know, an application written with Django or Spring Boot or Laravel, so\nmore traditional MVC frameworks where you are running a long-running server, that server\nis accepting multiple requests, and all of that could be a unit that you are running\nas a container on Fargate.\nInstead, with Lambda, it gets much different, even the way you are going to write your code,\nbecause you generally receive an HTTP event, there will be one Lambda dedicated to respond\nto that HTTP request, and it's going to have to try to respond as quick as possible, because\nof course your pricing is also related to how quick you can respond.\nYeah, but with that being said, I guess it might be interesting to see how do you get\nstarted with both?\nWhat do you think on that front?\n\n\nEoin: Yeah, Lambda is still the more serverless kind of solution, so it's easier to get started,\nyou have less to do.\nWith Fargate, you have a lot of configuration to do, you have to create a cluster and a\nservice and you have to understand what a task definition is, and what a task is, so\nit's a lot more of a container orchestration kind of an environment.\nWith Lambda, it's much more about run this code in response to this event.\n\n\nSo like you say, containers then are just a packaging mechanism, so if you're looking\nfor something that takes a lot of the heavy lifting off you, Lambda is definitely the\nway to go.\nYou can simplify the Fargate deployment process like the CCS patterns in CDK, which are quite\nuseful for that, but realistically they're just hiding a lot of that complexity for you,\nyou have to kind of understand what magic is going on under the hood.\n\n\nThe other thing about the difference then between Lambda and Fargate is it's all about\nlimits, and Lambda's limits are reasonably well known, but when you deploy Lambdas with\na zip, you're limited to 250 megs of code, but with container images which were announced\nlast year, you have 10 gigabytes, you can put 10 gigabytes of a container image up there,\nwhich really changes the number of use cases you can use for Lambda.\n\n\nYour memory limit is also 10 gigs in Lambda now, but you don't get any disk, right, you\nwill leave temporary storage up to half a gig, but that compares with Fargate where\nyou have up to 30 gigs of memory and you can get up to 200 gigs of ephemeral storage.\nSo Fargate in that sense, in terms of memory and storage allows you to do a bit more, but\ninterestingly enough Lambda allows you to get up to six CPUs if you've got a lot of\nconcurrency going on in your compute, and Fargate's limit is actually just four CPUs,\nso it depends on your context and you have to evaluate each thing separately.\nSo understanding that Fargate allows you to control CPU and memory separately, but with\nLambda you move the memory slider that also moves the CPU slider and the amount of networking\nbandwidth you have, so it's all about your context and understanding which works best\nfor your workload.\nSo is there anything else we need to cover off when we're comparing Lambda and Fargate\nfor containers?\n\n\nLuciano: Maybe we can talk very quickly about cost, but again, it's very hard to give you a recipe\nto calculate the cost because it really depends on the type of application you are trying\nto run and how you build that, but I suppose in general we could say that Fargate tends\nto be cheaper and more controllable.\nAs you said, you can control memory and CPU independently, so if you have applications\nthat don't require a lot of CPU and maybe a lot of memory or vice versa, you can probably\nfine tune Fargate more than you could do with Lambda, so that could help you to save money.\n\n\nAlso Fargate runs on spot instances if you want to, so that's another option that will\nhelp you to save a lot of money.\nI think it's up to 70% and I think we have the article we already mentioned in a previous\nepisode that you wrote that can give you a lot more details around that, so we're going\nto put that in the links.\nAnother thing in general, another dimension that is worth mentioning in terms of cost\nis that Fargate, there might be a little bit more waste in terms of bootstrapping the container\nand starting to run it and also shutting it down.\nWhile with Lambda you tend to get a lot more instantaneous times for bootstrap and shutdown,\nso if you have very short running tasks, maybe it's better to just write a Lambda and get\nthe Lambda to do that work.\nYou can also save money that way because you're going to be able to fine tune for those times\nand save money on the bootstrap time, but again, you'll need to make an analysis and\nsee exactly how the different rules that you need to satisfy for your application will\naffect the pricing and the characteristics of the runtime.\nYeah, that's a really good point.\n\n\nEoin: With cost, always have to be aware of it and don't make assumptions about what's more expensive.\nYour mileage may vary.\nSo we'll link to that article in the show notes again.\nAnd also there's an article that we created around the topic of how Lambda container images\nwork, which might be useful for people who are coming from a traditional container deployment\nenvironment.\nSo let's leave those in the show notes and wrap it up there.\nI think that's all we've time for.\nPlease do follow us so that you get notified about the next episode and we'll catch you\nfor the next episode next week.\n"
    },
    {
      "title": "11. How do you move away from the management console?",
      "url": "https://awsbites.com/11-how-do-you-move-away-from-the-management-console/",
      "publish_date": "2021-11-18T00:00:00.000Z",
      "abstract": "In this episode, Luciano and Eoin discuss the good and the bad of the AWS Management Console (a.k.a. the web console) and why you should consider migrating to Infrastructure as Code (IaC) as soon as possible, especially for your production applications.\nIn this episode we mentioned the following resources:\n\nCloudformation\nCDK\nServerless Framework\nSAM\nTerraform\nFormer2\nImport or create (e.g. if the resource already exists in production) with CDK\n\n",
      "transcript": "Luciano: Hello everyone and welcome to another episode of AWS Bites, the weekly show where we try\nto answer interesting AWS questions in about five minutes.\nMy name is Luciano and today I'm joined by Eoin and before we get into today's question\nI will kindly ask you to like and subscribe what we are doing so you can be notified every\ntime we publish a new episode.\nSo today's question is how do you move away from the management console that is the AWS\nweb console?\nAnd before actually we try to answer that particular question, I would like to try to\ndescribe what are the pros and cons maybe of that web console and why maybe you might\nwant to move away from that.\nWhat do you think Eoin?\n\n\n\nEoin: Yeah, the AWS console is, you know, everybody has some difficulty with it, but it's actually really good for learning about AWS services for the most part because it gives you a visual\nrepresentation.\nYou can try things fairly quickly and it's just good for looking around, debugging, understanding\nwhat components things are made of.\nAnd if you're getting into AWS early on, it makes sense to do things that way because\nthe other methods could be quite a bit daunting.\n\n\nBut when it comes to, you know, getting into production mode, it's not so great really\nbecause you want to have repeatable, probably immutable deployments, you know, where you\ncan create a stack with all your code and your infrastructure and it's reproducible,\nit's deterministic, and it can be tracked in your source control.\nAnd then when you deploy it to the cloud, you've got predictability around what you\nhave and then you can deploy it to multiple environments.\nYou can destroy it and recreate it easily.\nAnd it makes change management and iterating on features a lot easier than if you're trying\nto remember what you did in the console the last time and what you've configured and what\nyou've forgotten to configure and what you've tagged and what you haven't tagged.\nSo that's what the problem with the console is.\nSo if you're using the console, what are the alternatives you think that we could suggest\nto people?\nYeah, I think there are many alternatives.\n\n\nLuciano: Like for sure you could use the command line interface with the AWS CLI.\nYou can also use the SDK in any language of your choice.\nBut I would argue that those are not really tools for infrastructure as code.\nLike at best you can do either some scripts or maybe if you need to create resources at\nruntime inside your application, you can use the SDK.\nBut they are useful alternatives, but I don't think they solve the problem of infrastructure\nas code.\n\n\nFor those I would recommend definitely CloudFormation, which is kind of the canonical service in\nAWS to do infrastructure as code.\nAnd that one will allow you to, in a declarative language, express all the resources that you\nwant in your stack and allows you to deploy them kind of atomically.\nIf something goes bad, you have a rollback process that is going to bring it back to\nthe previous version.\n\n\nSo all those cool things.\nOf course the choice is not limited to CloudFormation.\nEven AWS itself has things like CDK and SAM, which are somewhat higher level abstractions\non top of CloudFormation.\nYou also have the serverless framework as another alternative that you can use, especially\nif you are building serverless projects.\nAnd also there is something like Terraform that maybe you are using already, especially\nif you use other cloud providers.\n\n\nAnd that tool can be useful because with the same tool you can manage different cloud providers.\nSo yeah, definitely there is no shortage of options, even though my recommendation would\nbe to eventually learn CloudFormation because that's kind of the golden rule in AWS.\nThat's the lower level and chances are you would need to deal with CloudFormation directly\nat some point.\nSo yeah, I suppose at this point it could be interesting to explore a use case where\nmaybe you have a production application that was, let's say deployed by clicking in the\nUI and provisioning things manually.\nWhat could be a reasonable path to move from that particular deployment to managing this\napplication with infrastructure as code?\nYeah, I think the use case is actually kind of more common than people would like to admit.\n\n\nEoin: It's probably a secret shame in having created a massive application using the console.\nBut you know, people do it and people do it successfully.\nThe question is how do you get out of it because it's not very sustainable.\nAnd I suppose the first thing is try to not mess with what you have.\nYou know, if you've got something, some people call that like a snowflake infrastructure\nbecause it was created.\n\n\nAnd once it's gone, it's gone, you can't really repeat it.\nIt's completely unique.\nWhat you can do is just, I would recommend trying to create a replica of that in a new\naccount rather than trying to transition a set of resources in one account over to infrastructure\nas code.\nBecause it's very difficult to make all those changes without breaking anything.\nSo it's actually a good opportunity to get your organization and your account structure\nin place.\n\n\nCreate a new set of accounts, start to build a replica of what you have and then switch\nover.\nAnd you could do that by, first problem is what do you have already, right?\nSo you kind of need to audit all of your infrastructure and resources.\nYou might have a good handle on that, but you could also use maybe AWS config to query\nall the resources or you can use third party tools for that.\nThere's also AWS used to have a tool called Cloud former, which isn't really maintained\nand I wouldn't recommend it.\n\n\nBut there's an open source product called former two, which we can link in the notes\nand it seems to be really good.\nI haven't used it yet, but it looks pretty good at generating Terraform, HCL code or\nCloudFormation templates from your existing resources.\nSo then once you build up those templates, you can deploy them, test them in this new\naccount structure.\nAnd then you have to just do a cut over using say DNS at the front end, synchronizing and\nmigrating your data at the backend.\nAnd it's not going to be an easy process, but it's something if you do take that approach,\nyou can at least rehearse it, practice it multiple times.\nYou could even have two production environments that are almost replicas, A, B test it and\nthen cut over when you're confident.\n\n\nLuciano: That's great.\nYeah.\nI think I want to mention another thing, which is that in CloudFormation, you can import\nexisting resources, but that's kind of a tricky thing.\nIt's probably not the right thing to do because when you import a resource and then you create\na template, if you try to deploy that template to another environment or another account,\nit's going to try to import the same resource and it suspects the resource is already there.\n\n\nSo that's not going to work for you to generalize your stack.\nIt's just kind of an escape hatch.\nIf you already have things in production and then you are managing something with CloudFormation\nand you want to reference that particular resource, we are going to link an article\nin the notes on how you can do that with CDK, but again, this is not something you should\ndo to fully migrate something from clicky-clicky to infrastructure as code.\n\n\nIt's more if you really have to use a resource that exists in a production environment, then\nyou can import it that way.\nIf you need to recreate in other environments, you can recreate, for instance, for testing\nreasons, but yeah, it's kind of a borderline approach.\nAll right.\nI think that's all we have for today.\nSo thank you very much for listening and we are really curious to know what you think\nabout this episode if you have been through this journey of trying to migrate something\nthat was done by clicking the UI to infrastructure as code and what kind of solution did you\nfind in the end.\nSo please share your experience with us.\nAnd if you have any question that you would like us to answer next time, then let us know\nand we'll try to do that.\nThank you very much and we'll see you in the next episode.\nBye.\n"
    },
    {
      "title": "12. How do you manage your AWS credentials?",
      "url": "https://awsbites.com/12-how-do-you-manage-your-aws-credentials/",
      "publish_date": "2021-11-26T00:00:00.000Z",
      "abstract": "In this episode, Eoin and Luciano talk about how to manage AWS credentials and different ways to manage them. From the more traditional (and not recommended) IAM credentials to SSO.\nIn this episode we mentioned the following resources:\n\nGitHub integration with OIDC\nMFA access for assumed roles\nAWS vault\nAWS SSO utils\nAWS SSO export credentials\n\n",
      "transcript": "Eoin: Hello and welcome to another episode of AWS Bites, the weekly show where we answer questions\nabout AWS in about five minutes.\nI'm Eoin and again, I'm joined by Luciano.\nAnd before I get started, make sure to give us a follow and a subscribe so you can be\nnotified when the next episode goes live.\nAnd today's question is, how do you manage your AWS credentials?\nWhat do you think of Luciano?\n\n\nLuciano: Yeah, this is really a big topic and I hope we are going to be able to summarize it effectively in the, with the time we have today.\nBut I think we generally have to distinguish between two different scenarios.\nOne is when you are a user, a person trying to access AWS and you can do that through\nthe web console or you can do that through the CLI.\nAnd another one is when you are an application.\nSo imagine like a service and you need programmatic access through the SDK.\n\n\nThe traditional way that you will get access to AWS, which is no longer recommended is\nbasically that you go into IAM, you create a user, you create programmatic credentials\nfor that user, that user has a role that allows the user itself to access different resources.\nAnd at this point you take those programmatic credentials and you use them to access AWS.\nNow why this is not recommended, first of all, it's not secure because the credentials\nare always the same until you revoke them.\nIt's not particularly easy to rotate those credentials.\nSo that discourages, I suppose, people from having good practices and rotating them often.\nAnd also they are very easy to steal because for instance, if you configure the AWS CLI,\nthose credentials are just text in plain text in a file in your own directory.\nSo it's very easy to grab them and use them for all sorts of purposes.\nSo I don't know, what do you think would be a better alternative, Eoin?\nYeah, so I suppose it depends on what we're actually trying to achieve.\n\n\nEoin: And for applications that are running on AWS, like in an EC2 instances or an ECS or in a\nLambda function, AWS manages that for you.\nSo you don't actually have to manage any credentials yourself.\nEC2 would use a metadata service and Lambda will inject temporary credentials for you.\nSo you don't have to store and should not be storing any credentials there or any access\nkeys.\nBut you might still need to manage credentials for external applications, things that aren't\nrunning in AWS.\n\n\nSo they might be running on a third party SAS or on an on-prem system.\nAnd people have frequently used access keys and secret access keys for that in the past,\nbut generally you don't need to do that anymore.\nThere's alternatives there.\nSo one example of that is, let's say you're building a CI CD and you're using GitHub actions.\nWith GitHub actions, you can use a Web Identity Federation and OpenID Connect to assume a\nrole from your GitHub action.\nAnd you don't have to do any credentials storage in GitHub.\nSo you don't have to worry about long lived credentials in secrets in GitHub and the possibility\nof those being leaked as you build your application.\nSo I think it's great that we're moving away from the ability to store these really dangerous\naccess keys all over the place.\nThat's for kind of application access.\nSo let's go back to user access.\nWhat do you think we can use instead of IAM users?\nWhat are the options there?\n\n\nLuciano: Yeah, there are a few things I've seen in the past to try to protect a little bit more of those credentials.\nOne is using MFA.\nWith MFA, you can basically create a user with a role that is very restricted.\nThe only thing that it can do is assume another role.\nAnd in the policy that allows you to assume that role, you can put a condition that says\nyou can do that only with MFA.\nSo that adds another layer of security, even if the credentials are stolen, then you still\nneed that MFA token to be able to do anything useful with those credentials.\n\n\nSo that is, I think, a good enough solution for starting.\nThere is an article from AWS that describes how to enable all of that.\nWe're going to put it in the description.\nAnother thing that I've used in the past is something called AWS Vault.\nIt's a third-party utility that you can find on GitHub, and we're going to provide the\nlink as well.\nAnd that one basically allows you not to keep the credentials in clear text in your system.\nSo as a developer, you are going to store the credentials in the system keychain, which\nshould be a little bit more secure.\nAnd then there is a CLI tool that allows you to extract these credentials only when you\nneed them.\nAnd it also exposes a mock metadata API.\nSo when you are developing, you can still interact seamlessly using the SDK from your\nmachine.\nSo that actually works really well.\nBut again, I suppose there is a need of moving away from IAM entirely.\nSo I don't know if you have any suggestion about that.\nOh, yeah, definitely.\n\n\nEoin: I mean, the way to go for moving away from IAM users is to put AWS SSO in place.\nAnd this has all sorts of great benefits.\nIt works really well from a developer user experience point of view.\nIt's just much easier to manage users' permissions, groups, assignments.\nWith SSO, it gets rid of a lot of the drawbacks you have with IAM users and access keys.\nIt also allows you to use your existing identity provider.\n\n\nSo if it's Active Directory or Okta or even Google Workspaces, you can use those as your\nidentity source.\nAnd then manage permission sets within SSO, assign groups, and ensure that credentials\nare short-lived.\nSo that's really where it shines in comparison to users.\nWith SSO, you always get short-lived credentials that will expire within like one to eight\nhours or up to 12 hours, I think.\nSo SSO is really the way to go.\n\n\nAnd there's good tooling around this.\nNow, there are still some gotchas.\nSo if you're logging on to the web console, SSO works really well and makes it really\nseamless.\nIf you're using programmatic access, so from the CLI or from one of the SDKs, there's a\nlittle bit more to it because not every SDK version supports the SSO process really well.\nSo that's a little unfortunate.\nBut until those things have come up to speed, there are some really good tools.\n\n\nAnd Ben Kehoe, who's an amazing expert in the space of SSO, has put together some really\ngood tools, which I use on a daily basis.\nOne is AWS SSO util, which is just adds a lot of nice gloss on top of SSO for the CLI.\nAnd it allows you to populate all of the roles available into your config file, for example.\nIf you're using things like the serverless framework or the CDK, they don't support SSO\nfor the reasons I outlined because they're using the JavaScript SDK version two, which\ndoesn't support it fully.\n\n\nAgain has another tool called AWS Export Credentials, which is like a shame that allows you to just\nprovide credentials as environment variables or somewhere else that you can use.\nThere's still short-lived credentials.\nIt adds a little bit more friction, but it means you're still able to get the best of\nboth worlds and use SSO with those tools.\nSo I know a lot of people have asked us to cover SSO on the podcast.\nSo we're definitely going to come back and talk about SSO and all of the details around\nit in more detail.\nBut I think as regards credentials, we've covered most of the main topics there.\nSo that's all we've got for today.\nBut if you want to find out about more of this stuff in future episodes, subscribe to\nthe channel and do give us comments as well and give us a thumbs up.\nAnd we always like to hear your feedback.\nSo thanks very much for that.\nAnd we'll see you next time.\n"
    },
    {
      "title": "13.1. What’s on your re:Invent 2021 wish list?",
      "url": "https://awsbites.com/13.1-what-s-on-your-re-invent-2021-wish-list/",
      "publish_date": "2021-11-27T00:00:00.000Z",
      "abstract": "In this special episode, Eoin and Luciano talk about their wishlist for AWS re:invent 2021. Based on our experience and personal AWS pain points, we share some of our wishes for new announcements during the biggest cloud event of the year. We also discuss some of the biggest announcements of last year and a few tips on how to get ready to follow the announcements of the next few days.\n⚠ CORRECTION on Data Transfer cost: The changes in data transfer were not reported accurately in this episode. The monthly data transfer free tier limit has changed from 1 GB/month per region to 100GB/month for all regions. Data transfer out of CloudFront is now free for 1TB/month, up from 50GB/month. See the official announcements linked below.\nIn this episode we mentioned the following resources:\n\nServerless Airline booking app example\nAWS Wild Rydes example\nAWS Workshops\nData transfer free tier increase:  1\nData transfer free tier increase: 2\nExport Amplify projects to CDK\nCDK hotswap\nPartial SQS batch response\n\n",
      "transcript": ""
    },
    {
      "title": "13.2. AWS Bites special re:Invent 2021 day 1",
      "url": "https://awsbites.com/13.2-aws-bites-special-re-invent-2021-day-1/",
      "publish_date": "2021-11-30T00:00:00.000Z",
      "abstract": "In this special episode, Eoin and Luciano talk about their impression on the announcements from the first day of AWS re:invent 2021.\nIn this episode we mentioned the following resources:\n\nAWS Lambda now supports event filtering for Amazon SQS, Amazon DynamoDB, and Amazon Kinesis as event sources\nAmazon CodeGuru Reviewer now detects hardcoded secrets in Java and Python repositories\nAmazon ECR announces pull through cache repositories\nIntroducing recommenders optimized to deliver personalized experiences for Media &amp; Entertainment and Retail with Amazon Personalize\nAWS Chatbot now supports management of AWS resources in Slack (Preview)\nAmazon CloudWatch Evidently\nIntroducing AWS Migration Hub Refactor Spaces - Preview\nCloudWatch RUM\nCloudWatch Metrics Insights\nAWS Karpenter\nAWS Data exchange for API\nS3 Event Notifications with EventBridge\nAmazon Athena ACID Transactions (Preview)\nAWS Control Tower introduces Terraform account provisioning and customization\n\n",
      "transcript": ""
    },
    {
      "title": "13.3. AWS Bites special re:Invent 2021 day 2&3!",
      "url": "https://awsbites.com/13.3-aws-bites-special-re-invent-2021-day-2-and-3/",
      "publish_date": "2021-12-02T00:00:00.000Z",
      "abstract": "Sharing our insights on the announcements of re:Invent 2021\nIn this special episode, Eoin and Luciano talk about their impression on the announcements from day 2 and 3 of AWS re:invent 2021.\nIn this episode we mentioned the following resources:\n\nKinesis on-demand data streams\nRedshift Serverless\nServerless MSK\nServerless EMR\nSagemaker Serverless Inference (preview)\nAWS Lambda ephemeral storage - pre-announcement\nSQS Redrive\nSageMaker Studio Lab\nSageMaker Studio integrates with EMR\nSageMaker Ground Truth Plus\nAI&amp;ML Scholarship Program\nAmazon Lex Automated Chatbot Designer (Preview)\nDynamoDB Infrequent Access\nLake Formation: Security and Governed Tables\nAWS S3 Glacier instant retrieval\nAWS new S3 intelligent tiering\nS3 console security warnings, errors and suggestions\nS3 Object Ownerhsip\nAWS Microservices Extractor\nMainframe modernisation: convert Cobol to Java (in lambda functions)\nCarbon Footprint Tool\nNew service: AWS private 5G\nNew service: TwinMaker (digital twins)\nGraviton 3 instances\n\n",
      "transcript": ""
    },
    {
      "title": "14. What can you do with Amplify Studio?",
      "url": "https://awsbites.com/14-what-can-you-do-with-amplify-studio/",
      "publish_date": "2021-12-09T00:00:00.000Z",
      "abstract": "In this extended episode, Eoin and Luciano talk about Amplify with a particular focus on Amplify Studio, a new visual way to manage Amplify applications announced during the recent re:Invent event during Werner Vogels’ keynote.\nIn this episode we mentioned the following resources:\n\nAmplify Studio\nFigma\nThe 2021 AWS re:Invent Werner Vogels keynote segment on Amplify Studio\nAli Spittel\n\n",
      "transcript": "Eoin: Hello and welcome to another episode of AWS Bites, the weekly show where we talk about\nAWS and answer questions in around about five minutes.\nI'm Eoin and we're joined by Luciano again.\nAnd before we get started, make sure you give us a follow and subscribe so you can be notified\nwhen the next episode goes live.\nAnd today we've got a different kind of a question focused on something we didn't talk\nabout before, which is Amplify.\nAnd the question is, what can you do with Amplify Studio?\nSo I think we maybe did touch on Amplify once or twice just in passing, maybe when we were\ntalking about static hosting and cloud formation versus console.\nBut let's dive into Amplify Studio a little bit more because we had an announcement at\nreInvent recently and it was about Amplify Studio.\nBut maybe it's best to start with a bit of a history of Amplify and what else Amplify\ndoes.\nHow would you describe it, AWS Amplify?\n\n\nLuciano: Yeah, Amplify is an interesting product because I think it's a little bit of a shift from AWS in many ways.\nLike in general, in AWS, I feel that you have always like very low control and you have\nto understand all the nitty gritty details.\nSo it has been like always a little bit of a challenge for many types of developers to\ncome and use AWS because they need to kind of approach everything at a very low level\nand then build up from there.\n\n\nSo Amplify is a shift because they try to give you like an easier environment to build\napplications.\nAnd I have a feeling that they are targeting primarily front end developers because front\nend has been really like the main thing that you still have to build on your own.\nAnd they try to abstract everything else to make your life easier.\nSo that's why I feel that front end developers are the target users for this product.\nAnd the idea, I suppose, is that AWS wants to give front end developers all the other\ncapabilities that they need to build a fully fledged application without having to know\nthings like CI-CD or I don't know, cloud formation or how to create user pools incognito\nbecause all these things are nicely abstracted for you from Amplify.\nYeah.\nAnd it's got a few different features, hasn't it?\n\n\nEoin: Like there's a few different ways to get started with it.\nSo I know that there's an Amplify CLI and I think when it started, you had the Amplify\nSDK and the CLI.\nWhat other parts do you have?\nYeah.\n\n\nLuciano: Also you mentioned the CLI, you mentioned the SDK, so a client library that you can\nuse either in a mobile... because you can build mobile and web projects.\nSo you will have SDKs in both environments.\nBut other components are, for instance, an integration with your Git repository, whatever\nthat is, doesn't have to be necessarily in AWS.\nAnd that will automatically create a CI-CD pipeline for you.\nSo you can basically get GitOps by default.\n\n\nYeah.\nVery nice.\nAnd the new thing that was announced recently is Amplify Studio, which is, I would say,\nkind of a UI and more UI friendly environments where you can do many of these things in a\nkind of more clicky, clicky way.\nFor instance, you can create data models by just clicking in a UI and defining all your\nfields and you can select the constraints and relationship between maybe fields from\na table with fields in another table.\nAnd then at the end, when you just click save, that is going to provision a DynamoDB table\nfor you.\nIt's going to provision GraphQL APIs to access this data.\nSo Amplify Studio is like, okay, I don't know anything about how to provision all these\ndifferent things, but I can easily explore these concepts and they are very friendly\nfor me to create things and deploy them.\n\n\nEoin: Yeah, I can imagine with all of these things, if you're building them from scratch, like you said, AWS, you usually operate at a low level, it requires a lot of expertise.\nAnd if you look at the feature list on Amplify and the level of abstraction it provides,\nit's giving you authentication and data and file storage, serverless APIs with GraphQL\nor just REST, and then it gives you messaging, analytics, push notifications, geolocation,\nand even like ML features like predictions, predictive analytics, and AR and VR support,\nwhich I don't know if I'd ever use it, but it's interesting that it's there.\nBuilding all these things yourself.\n\n\nLuciano: And I think the reason they also announced like a storage abstraction, so probably you'll get more and more support for applications that require, I don't know, users to upload\nfiles or to manage, I don't know, content for user that maybe you will start normally\nin S3 without having to know all the nuances of S3 and uploads and all these things.\n\n\nEoin: I'm just thinking about the amount of time I've spent building these things manually\nfrom the, I suppose, raw AWS components.\nAnd of course it's really useful to know how all these things fit together, but if you're\nstarting from scratch with AWS, I can imagine the lead time to getting all of these things\nup and running and understanding them is going to be many, many months realistically.\nSo if Amplify lets you get started in hours to build this basic capability, I can certainly\nsee the attraction of that.\nAnd so is there an AWS Amplify service?\nSo is it doing anything special that you can't do in those raw services?\n\n\nLuciano: I would probably say for the most part, no, meaning that at the end of the day, Amplify is managing a cloud formation stack for you.\nSo if you're really curious, you can just go in your cloud formation panel and see exactly\nlike what are all the resources that have been provisioned by your interactions with\nAmplify either from the CLI or the Amplify studio.\nSo there is really no magic to it if you want, but it's still nice that you can get a lot\nof things done very quickly.\nLike if you will have to do all these things using infrastructure as code, probably it\nwill take you much longer to ship anything to production.\nSo I suppose it's more of a question of how much control do you want to have compared\nto like, do you just want to build something simple and ship it quickly?\nSo I don't know.\nWhat do you think in terms of maybe like what kind of use cases would you use it for?\nYeah, what kind of use cases?\n\n\nEoin: This is a good question.\nSo if I look at the way it's built data models, authentication, storage as the fundamental\ncomponents you can imagine using for any kind of CRUD app where you want users to sign up\nand log in and create something, you know, and that could be uploading images, filling\nout forms, adding entries.\nSo basically CRUD apps and everything is kind of a CMS or a CRUD app at its core.\n\n\nIt feels like most applications are.\nBut if you don't have very, very complex business logic at the backend, I think it seems like\na reasonably good fit and you want to get started pretty quickly.\nSo I think there's a couple of questions with stuff like this.\nWe've learned over time to be a little bit fearful of high level abstractions in case\nthey leak eventually or you're trying to move forward and ultimately you want to drop down\nto the low level components and does it make it possible and is it a reversible process?\n\n\nSo can you get the best of both worlds?\nCan you continue to use Amplify and keep customizing those components at the backend?\nFrom what I've seen with Amplify, yes and no is the answer to that.\nSo you can add custom cloud formation, for example, which is nice.\nYou can incorporate existing CDK components.\nThat's a new enough feature.\nAnd you can also eject or export the application to CDK and incorporate it into a CDK pipeline.\n\n\nSo it's interesting to watch it evolve because I was, as Amplify, the story has emerged.\nI have been a bit skeptical, but I'm kind of surprised that it's keeping a good level\nof quality there, adding a lot of useful features.\nAnd they seem to be very serious about investing in it and supporting lots of different users\ncoming from lots of different frameworks, technologies, working on different types of\napplications.\nSo it's really trying to have a broad reach, which is ambitious, of course, but they're\nclearly serious about it and it was a big feature of Werner Fochel's keynote at Reinvent.\nSo it kind of bodes well.\nAnd on that topic then, there was at the keynote, Ali Spittel was on and talking about the Amplify\nStudio, and I suppose that's what we're here to talk about.\nSo I know that last year we had Amplify admin UI, and now we've got Amplify Studio.\nSo what does Amplify Studio give you that we didn't have before?\n\n\nLuciano: Yeah, I think the most impressive thing that was added is the ability to easily collaborate with designers through Figma integrations.\nBasically what you can do, I was playing with this a little bit yesterday, you could, as\npart of your Amplify project, you could have Figma project.\nYou can, for instance, download a template that you get from AWS to easily get started,\nbut then you design all your components in Figma and you have a process that allows you\nto synchronize those components back into Amplify as React components.\n\n\nOf course, these React components are in a way immutable, like you are not supposed to\nchange the code because, again, if the designer changes some details, they will be resynchronized\nso all your changes will be overridden, but I suppose you can wrap those components with\ncomponents that will provide all the additional business logic.\nIt is still a little bit limited, meaning there are things that are missing, like, I\ndon't know, pagination was one that wasn't obvious how you could build it or routing,\nlike it's not obviously described how would you build the routing.\nIn this, probably, again, you need to wrap the components with your own custom components.\nSo I think it's not perfect yet, but it's impressive to see, first of all, that there\nis a collaboration at that level with something like Figma, which is a well recognized industry\ntool.\nSo AWS rather than building their own UI designing tool, I guess, they decided to create this\ncollaboration and it's actually very smoothly integrated, like you don't have to do, I don't\nknow, weird permission screen or stuff like that.\nIt's very intuitively how you connect the two.\nBut yeah, it seems like there is still a long way to go to actually make it really powerful\nso that you can build more complicated applications with that.\n\n\nEoin: Yeah, it seems like you still need to know what you're doing and you need to know your way around a React app.\nSo it's still kind of aimed at front end developers and it's still React only, right?\nSo even though Amplify supports lots of different front end frameworks, including Angular and\nVue and Ionic and as well mobile front ends, so it supports Flutter, you can do React Native.\nSo who is it aimed at then?\nSo what kind of users, is this something for everybody or is it aimed at, you seem to mention\npeople just getting started doing personal applications.\nDoes it have broader appeal?\nWould you use it for an application that you expect to be around for five years?\nYeah, that's an interesting question.\n\n\nLuciano: I guess it depends on how standard I'm going to say is your application.\nAnd by that, I mean, I don't know, is it just managing CRUD type of data?\nSo you can just define your entities and then you just need to have easy ways to, I don't\nknow, create, update, delete.\nI think if that's your use case, probably you can get a long way with Amplify for the\nfeatures you get right now.\nI can also see more sophisticated use cases where, I don't know, maybe you have data models\nthat are more dynamic or where you need to have, I don't know, integrations maybe with\nexternal services where maybe Amplify, it will be, maybe you'll need to work around,\nlet's say the limitations of Amplify and maybe the solution is not going to come so easily.\n\n\nMaybe we'll become more as a blocker than as an enabler in that case.\nBut definitely, I don't know, I can see it being very useful for either an MVP where\nyou just want to prototype something very, very quickly to see if it makes sense as a\nbusiness or if you're doing a small site project or a personal utility application where, I\nmean, you are not opening it up to a big public.\nYou just need something that works for a specific need that you might have.\nProbably Amplify is a big win because we'll give you a much faster time to actually use\nin the product.\nI am a little bit skeptical on enterprise type of applications, mainly because there\nare some important missing features.\nFor instance, the ability to deploy into a private network, a private DPC or SAML authentication\nis another thing that it wasn't obvious how to do it.\nMaybe it is possible, but the default authentication system doesn't seem to provide an option for\nthat.\nSo maybe, again, something you could bend Amplify to be able to achieve, but it doesn't\nseem easily supported.\n\n\nEoin: Actually, this is an area where I think Amplify could really win if they added a few features like that because I can see lots of enterprises out there who are struggling to get the time\nand the people to be able to build custom applications for line of business applications\nor applications that integrate existing data sources.\nI think if you have AD integration for authentication, internal hosting, like you say, and the ability\nto integrate into some existing APIs and integrate systems together in a lightweight way with\nadding some of these features like storage and data behind them, I think you could end\nup with something really powerful that would be adopted and gain a lot of popularity in\nenterprise.\nSo while it might feel like something that's suitable for startups or for people building\nthe MVP of a SaaS application, I think that's actually where it could really shine in the\nfuture if we get those kind of features.\nIs there anything else that you can see in the future direction of Amplify then?\nBecause I guess I feel, as you alluded to, with these UI components that have recently\nbeen announced, it's like giving you that automated front end capability in collaboration\nwith designers.\nIt kind of feels like it's just the start though.\nYou can't build a full UI application automatically with Amplify yet.\nIs that where it's going?\n\n\nLuciano: I would expect this to be definitely one of the next milestones for the Amplify team to\nallow designers and developers to work together even more closely by defining not just the\nshape of the components, but also how the components come together and the interactions\nbetween the components.\nSo again, navigation, signup, pagination, all these kind of things, I think they will\ncome in the next releases.\n\n\nBut also we probably mentioned that already, right now you can only integrate with the\ndata models that you define in Amplify or at least in an easy way.\nI think it might make sense for Amplify to also show easy way to also integrate with\ndata sources that are not in your Amplify project.\nFor instance, other APIs that maybe exist in your enterprise or other external APIs\nor even things that are not classically exposed to a REST or a GraphQL API.\nMaybe you just want to interact with, I don't know, even a SOAP system.\nRight?\nYeah, exactly.\nSo in an enterprise, I can definitely see that still happening a lot.\nSome random FTP servers.\nIt might be nice to have that too.\nSo yeah, those things will make it Amplify closer, I suppose, to a low code type of solution.\nYeah.\n\n\nEoin: That's really interesting.\nI think this is, we're probably getting to the end of our time here and going a little\nover as usual, but I think one of the things that I feel about this is that from your perspective\nand my perspective, we're looking at Amplify from our existing bias of understanding how\nto use CloudFormation.\nWe've got a comfort level with AWS that just comes from using it over the course of many\nyears, but we're probably not the primary target audience for Amplify.\n\n\nSo really interested to know what other people are doing with it.\nSo this is one where user feedback and listener feedback is really going to help us to shape\nhow we talk about this kind of thing in future.\nSo if people have used Amplify either to build applications from scratch or to augment existing\napplications, maybe you're doing something really interesting and quirky with Amplify,\nwe'd love you to give us a message on Twitter or in the comments on YouTube and let us know\nwhat you think.\nAnd we'd really like to follow up on it.\nIt doesn't have to be, we don't have to cover it all in this episode.\nSo please, and also please share it if you know other people using Amplify, share links\nto the podcast and make sure to follow and subscribe because we'll come back to this\nin the future I feel because I feel like there's definitely a lot more to happen.\nOkay, so let's wrap it up.\nThanks very much for listening everybody.\nAnd we'll talk to you on the next episode of AWS.\nBye.\nBye.\nschemes.\n"
    },
    {
      "title": "15. Is serverless good for startups?",
      "url": "https://awsbites.com/15-is-serverless-good-for-startups/",
      "publish_date": "2021-12-16T00:00:00.000Z",
      "abstract": "In this extended episode, Luciano and Eoin try to cover a recurring topic around Serverless: is it a good or bad idea for startups?\nWe start by giving a brief description of the different definitions and perspectives on serverless. Then, we try to explore some cases in which we believe serverless might not be the best fit for a startup. We follow on by revisiting some cases where instead we believe serverless can actually be a great fit. We finish by discussing some suggestions on how a startup (or even a more established company) could start approaching serverless in a more cautious and incremental way.\nIn this episode we mentioned the following resources:\n\nAuth0\nAlgolia\nFirebase\nLaravel\nDjango\nExpress\nAWS Amplify\nOur previous episode on Amplify\n\n",
      "transcript": "Luciano: Hello everyone and welcome to another episode of AWS Bites.\nAWS Bites is the weekly show with bitesized episodes where we try to answer your questions\nabout AWS.\nMy name is Luciano and today I'm joined by Eoin.\nSo before we get started, let me kindly ask you to give us a follow and subscribe so you\ncan be up to date with our latest news and every time we have a new episode.\nAnd the question for today is actually one I really, really like and I'm really excited\nabout and it's, is serverless good or bad if you are a startup?\nSo I will probably ask you Eoin, first of all, to explain briefly what do we mean by\nserverless?\nWhat do you think?\n\n\nEoin: That's a really good place to start because it means different things for almost everybody.\nAnd we could probably divide it down into two categories.\nThere's the mindset that serverless is all about using off the shelf managed services\nand offloading whatever effort you can to a third party provider.\nNow that could be AWS, but it could also be a provider of a specific service.\nSo it could be like using DynamoDB as an example of a serverless approach, but also using Auth0\nto manage your user accounts, authentication and authorization is serverless or using Algolia\nas a search service or even using something like Firebase is an example of a serverless\napproach.\n\n\nSo that's one part, right?\nIt's the mindset of just doing the simplest thing possible and getting rid of all the\nmaintenance effort.\nBut the other perspective on it is serverless is about using Lambda and API gateway and\nstep functions and in order to take that approach, I mean, that is definitely a serverless approach\nbecause you're not managing any servers, but there's a trade offs there, right?\n\n\nYou still have to understand how they work together.\nWhat are their limits?\nWhat are the failure modes and how do you fit them all together to make a coherent system\nthat you can sustain and is going to be robust?\nSo then in that case, thank you so much to think about like, how do you do deployment?\nHow do you do security and identity and access management?\nHow are you going to do local development?\nWhat about observability?\nWhat about infrastructure as code?\nWhere do you store your secrets?\nYou know, you get the idea where there's a lot of considerations.\nThat's not everything that you have to think about.\nSo it's not necessarily, depending on your perspective on serverless, I guess the answer\nis going to be different, right?\nAnd I think it also depends what kind of startup you are and what kind of mode you're in and\nwhat your goals are.\nSo that's the first thing.\n\n\nLuciano: Yeah, I would say maybe we'll talk a little bit more about this later on during the episode, but I will say there is also a little bit of perspective of cost where to be serverless,\nit needs to scale to zero.\nSo that can be another thing that might be advantageous depending on the circumstances.\n\n\nEoin: Yeah, that's a good point because some of these approaches, like with the third party services, sometimes you have higher cost considerations with Auth0 and other, they can be quite expensive\nin certain cases.\nSo you do have to think about that.\nAnd unfortunately, it's a dilemma that startups will find themselves in where they feel they\nhave, they might have no budget and they might have time, but do you want to spend your precious\ntime assembling lots of disparate AWS services or do you want to be focusing on your business\nand product market fit?\nYeah, exactly.\n\n\nLuciano: So I suppose we can summarize what you just said from the opposite perspective, like when\nit's not good.\nSo if we reiterate over what you just said, basically, I think if you are trying to build\neverything on AWS using the serverless services like Lambdas, the function, DynamoDB and so\non, of course there is a learning curve there.\nSo if you're starting from scratch, that's something that you're not going to acquire\nmagically the next day.\n\n\nLike it's going to take you some time to build up the knowledge and the experience with these\nparticular systems.\nAnd maybe you're coming from, I don't know, that type of company that has been using something\nlike Django or Laravel or Express for a long time.\nAnd you already have a set of tools and practices where you can be very, very productive with\nthese tools.\nMaybe it can be good for you to stick with the tools.\nLike if you're building a new product and you already have all that knowledge, you can\nfocus on the business logic and get the product delivered very, very quickly.\nSo in that case, if you are that kind of a startup, maybe serverless is not good for\nyou.\nYeah, I agree with that, Luciano.\n\n\nEoin: I would also say, if you're coming from like that Django Laravel world and you know exactly\nwhat scale you're going to reach.\nSo if you've got a situation where you're not going to have bursty scale, you've got\na fixed limited number of users and the load is going to be pretty even, then that's fine.\nServerless isn't, there's a lot of benefits there, but it's not necessarily simple out\nof the box.\n\n\nYou don't get simplicity for free.\nAnd sometimes people on the serverless side of these debates presented as, oh, it's so\neasy to get started with.\nAnd all you have to do to create a Lambda function is follow these quick steps.\nAnd then you already have a complete RESTful API up and running.\nBut the getting started costs and the getting started simplicity factor is just one part\nof it.\nWhat you really need to think about is what is the long-term cost of developing, changing\nand maintaining a system like that.\nSo while we were true believers in the world of serverless, we try not to oversell it because\nyou certainly can.\nAnd you have to understand that there is a cost to understanding how to build services\non AWS.\nAbsolutely.\n\n\nLuciano: And I have seen many companies that they are already struggling to move Monoliths to the\ncloud because even just doing that, it's an effort on its own.\nSo going to serverless is like an additional gear, right?\nSo you need to change even more.\nAnd sometimes you fail to account what is the cost for the company to do that.\nAnd it gets of course more risky, the more far away from your starting point it is that\nparticular target.\nI agree.\n\n\nEoin: Yeah.\nI mean, it's a trade off.\nSo once you understand that and you're happy to make that investment as you migrate, there's\ndefinitely long-term benefits, but you should go into it with your eyes open.\n\n\nLuciano: And maybe with a more gradual approach.\nBut yeah, let's maybe explore some of the goods, I guess.\nLet's say that you are in a position where it makes sense for you to go with serverless.\nWhat kind of benefits do we think as a user of serverless you are going to get?\nWe already mentioned briefly that the cost can scale to zero, which basically means that\nyou provision a bunch of infrastructure.\nMaybe you are doing an MVP so you don't really have a lot of traffic.\nMaybe you have test users that you are onboarding and they will be using the system only for\na little while and give you feedback and that's it.\nI think that it's great to use serverless because you are only going to pay for that\nshort period of time where you're actually using the system.\nSo you can even afford to try a bunch of different things without being concerned of a very big\nupfront investment.\nSo that's definitely an advantage.\nWhat do you think?\nIs there anything else worth mentioning?\nYeah, cost is definitely a good one.\n\n\nEoin: Also in cost, I would say what we really see with serverless is that it gives you a mechanism\nto have a low cost for innovation, which is really, really important for startups.\nThe ability to create complex systems pretty quickly once you understand how the services\nwork and then refactor them, remove them.\nEverything you create is like this immutable infrastructure approach.\nSo you can create massively scalable systems without having to provision lots of infrastructure.\n\n\nSo you don't have all that upfront racking and stacking costs that you would have with\ninstances, machines or Kubernetes clusters.\nYou can really get event-driven systems or web-based systems up and running pretty quickly\nonce you understand how these things work, but then remove them if they don't work.\nAnd that's really important for startups to be able to iterate quickly.\nSo that's a really, really big one.\n\n\nSo if you have the skills, you understand how these things work, it's definitely a good\napproach to take.\nBut there's also, I suppose, going back to our first perspective, when we talked about\nthe definition of serverless, we talked about the two perspectives and the first one wasn't\nnecessarily AWS focused.\nIt's like find the cheapest way you can build, you can get up and running.\nSo if you're using a higher level of abstraction, that can also make it a very good choice for\nstartups because you're not necessarily going all in on AWS and understanding all how things\nwork together, but you can use something a bit more with a higher level of abstraction.\nAnd I know in the last episode, we talked a lot about Amplify.\nSo that's one way.\nIf you understand the things that Amplify is a good fit for, then why not use that?\nThere's also other systems like Arc, which is like a simple kind of well thought out\ndeveloper experience that hides a lot of the complexity for you and makes the developer\nexperience a lot more seamless and kind of removes all these moving parts from you.\n\n\nLuciano: Yeah, I think this is something we mentioned even in the previous episode that we have seen that AWS is recognizing that there is a need of a lower barrier to entry to all\nthis ecosystem.\nLike AWS has always been a lot like it gives you a lot of control.\nIf you understand the details, you can do a lot of things that are actually very...\nGive you a lot of powers, but it takes a while to get to that level of understanding.\nAnd with Amplify, I think AWS is recognizing that not everyone wants to start at that point.\nMaybe they need an easier way to get started.\nMaybe they need to be productive very quickly, but then they also give you ways to go back\nto the roots of every single system and get that control back after you have started.\nSo maybe Amplify is a good trade off.\nIf you don't have all the skills, you can get started.\nAnd then later on, when you feel you are more confident, you can start to use other things\nor export Amplify to CDK and work with the services directly.\n\n\nEoin: And we're kind of assuming, I guess, Luciano, that we're always dealing with kind of crowd\napplications and web based front ends and maybe mobile front end with an API behind\nit.\nYou're talking about like a REST API, GraphQL, some database in the background.\nWhat if it's a different kind of ecosystem you're involved in?\nLike you're building at IoT devices and you need infrastructure to support that, or you\nhave some sort of machine learning algorithm and you're building a service on top of that.\nNot everything is API driven and front end driven.\nHow does the serverless equation fit into those classes of startup?\n\n\nLuciano: Yeah, I think if we stick to that definition of serverless where you are basically using services that are most appropriate for your use case, I think AWS has a lot of services\nthat can help you out.\nFor instance, in the case of IoT, you have all sorts of different services, even to provision\nthe different devices to update them, to keep them in sync or in a network where they can\ntalk to each other.\n\n\nSo definitely you are going to have big advantages in picking that option because otherwise you\nprobably are on your own to rebuild all this infrastructure yourself.\nAnd similarly for machine learning, maybe not my best area of expertise, but I know\nthat there are plenty of tools and services in AWS to basically satisfy all the different\nneeds that you can have in terms of AI and machine learning.\nSo if you're doing something that is standard enough, I'm going to say probably you can\njust use something off the shelf like that rather than building from zero your own platform.\nYeah, I could definitely testify to that.\n\n\nEoin: Having worked with a startup previously that had developed an idea around IoT devices and\nthe machine learning algorithm, and it was actually astonishing for both me and for them\nhow quick it was to be able to productionize, I suppose, a prototype that wasn't running\nin AWS, bring it into AWS with AWS IoT, Lambda functions, DynamoDB, and machine learning\nall together.\nI mean, a couple of years ago, this would have been a project that ran into months or\nyears, but in the world of AWS IoT and things like SageMaker, it's dramatically reduced\nthe amount of effort you have to put in.\nOkay.\n\n\nLuciano: Maybe just to close this episode off, let's try to explore another final question, which\nmaybe some of you might have, which is what if you don't have the experience yet to go\nwith serverless, but you see a long-term advantage in going with it because we described a bunch\nof pros of this approach, and maybe you see that there is a lot of value in some of these\nthings that we mentioned.\nSo what can you do?\nOn one side, we said it's good for you to stick with what you know and be productive.\nOn the other side, you might be missing out the benefits of serverless.\nSo do we have any advice there?\nOkay.\n\n\nEoin: Yeah, that's a really good one.\nI think the question itself should prompt people to think, okay, what am I optimizing\nfor here?\nAm I trying to get to market quickly with the basic MVP, or am I trying to strategically\nselect a technology that's going to sustain me for years and allow me to hire developers\nin 18 months time who have the right skills to maintain this?\nAnd I think that's a really good one.\n\n\nIt's a really good thing to think about.\nAnd I guess the answer I've kind of alluded to there, if you're willing to make that investment,\nyou have the funding and you have the time.\nSay if you're really going to focus time on building a complete product and you wanted\nto have a sustainable, stable technology set of choices underneath it, then absolutely\ngo for serverless and invest the time in acquiring the skills, especially if you have the funding.\nIf you're budget constrained, you're bootstrapping and you're trying to get something out there\nto onboard users quickly, and you're happy to iterate and change the technology over\ntime, then go with the technology you know.\n\n\nLuciano: Absolutely.\n\n\nEoin: What would you say?\nIs there any other way you'd respond to that question?\nBecause I think that's probably the most important question we're covering.\n\n\nLuciano: Yeah, I think there might be another case where maybe you are a company that builds multiple products and you already have a bunch of different products in the market.\nIf you're in that position, probably you can afford to experiment a little bit because\nit's always going to happen that in an existing product, you need to build a new feature.\nWhy not trying to build that feature?\nIf it's not the primary feature, of course, with something like serverless, maybe you're\ngoing to try to run that particular functionality in a Lambda, store some data in DynamoDB.\n\n\nAnd that way you can have a very low risk way of starting to try out these technologies.\nAnd probably at that point, you can see a lot more clearly whether that technology is\ngoing to give you benefits or not.\nAlso people in the team, they can start to recognize if they feel more productive in\nusing these technologies, if they can ship faster.\nAnd at that point, it's so new to decide, okay, I want to invest more or maybe my next\nproduct I'm going to try to build this from scratch using what I learned from these experiments.\nSo that can be another way, but I suppose that makes more sense for more established\ncompanies that are seeing a longer term return on this kind of approach.\nAnd they want to de-risk, I suppose, jumping straight into serverless.\nYep, yep, good call.\n\n\nEoin: Okay.\n\n\nLuciano: And with that, I think we have covered enough for this episode.\nSo thank you so much for being with us today.\nPlease give us your feedback.\nFeel free to leave us comments or to reach out to us on Twitter or LinkedIn.\nAnd if you have any question that you would like us to address for the next episode, send\nit our way and we'll try our best to reply that question.\nAnd with that, thanks again.\nWe'll see you in the next episode.\nBye.\nBye.\nBye.\n"
    },
    {
      "title": "16. What are the pros and cons of CDK?",
      "url": "https://awsbites.com/16-what-are-the-pros-and-cons-of-cdk/",
      "publish_date": "2021-12-23T00:00:00.000Z",
      "abstract": "In this episode, Eoin and Luciano explore the various pros and cons of AWS Cloud Development Kit (CDK).\nWe start by describing what CDK is and what it is used for. Then we spend a bit of time covering the details of how CDK actually works defining L1, L2 and L3 constructs and the integration with CloudFormation.\nIn the central part of the episode we deep dive into a bunch of pros and cons of CDK, mostly trying to describe the tradeoffs and the pitfalls.\nFinally, we close the episode by giving a piece of advice on what we believe is the best way to get started with CDK to minimize the surprises and be able to reap all the benefits of this amazing tools.\nIn this episode we mentioned the following resources:\n\nCDK\nCloudFormation\nTerraform\nCDK Pipelines\nCDK Patterns\nCreate Resources conditionally with CDK\n\n",
      "transcript": "Eoin: Hello and welcome to AWS Bites, the weekly show with bite-sized episodes where we answer\nyour questions about AWS.\nMy name is Eoin and I'm joined by Luciano.\nBefore we get started, make sure you give us a follow and subscribe so you can be notified\nwhen the next episode goes live.\nToday's question is, what are the pros and cons of CDK?\nAnd as always, let's start off by talking about definitions.\nWhat is CDK?\n\n\nLuciano: Yeah, CDK is a relatively new tool slash service from AWS, which the name actually means Cloud Development Kit and is essentially another way of writing infrastructure as code.\nBut this time, rather than using a declarative language like JSON or YAML or something similar,\nyou can actually use real imperative programming languages and many of them are supported,\nfor instance, TypeScript, Java, C-sharp, Python.\nI actually don't know if even Go is supported.\nMaybe it's in beta, but yeah, the idea is that you can use probably a language that\nyou are already comfortable with when you're writing code.\nYeah.\nDo you want to tell us how it works?\nYeah.\n\n\nEoin: There's a lot of complexity under the hood and a lot of magic actually at supporting\nall those languages.\nI know that they've got this project called JSII, which allows, I think they write basically,\nwrite it in TypeScript and then it gets all these other languages get generated.\nBut yeah, so it's generating cloud formation, as we said.\nWe talked a lot about cloud formation in the past and how important cloud formation is\nas a service if you're using AWS.\n\n\nBut CDK gives you layers of abstraction on top of it.\nSo the question becomes how much of an abstraction do you want to have on top of your cloud formation?\nGood resources.\nBecause cloud formation can be very verbose, but it's also very clear and declarative in\nsome ways.\nWhen you're reading a JSON or YAML file, you can get very comfortable with that and it's\nvery easy to see what you're about to deploy.\n\n\nCloud formation will give you a number of levels.\nSo there's these concept of L1 and L2 constructs.\nSo an L1 or level one construct is basically a programmatic object oriented wrapper around\nall of these cloud formation resources.\nSo you have these classes that would provide that begin with CFN.\nAnd those are really just a typed layer on top of the resources you're going to generate.\nThen you've got these L2 patterns, which are higher levels of abstraction.\n\n\nAnd those are like CDK classes that will give you same defaults for a lot of use cases.\nSo you can create an SQS queue, but you don't have to declare all of the properties.\nIt will give you some sensible defaults.\nAnd then beyond that, you've got higher level patterns and constructs that can be whole\napplications even.\nThey're groups of cloud formation resources with lots of defaults, some configurability,\nbut the idea about those is that they're kind of patterns that allows you to deploy a lot\nof resources with a couple of lines of code.\nAnd that'll lead us into, I guess, some of the advantages and disadvantages of CDK because\npeople who are experienced with lots of abstractions in various different programming paradigms\nover numbers of decades will understand that abstractions don't always come for free.\nSo let's go through the pros and cons.\nThat's what we're here to talk about.\nWhat do you think are some of the advantages with CDK Luciano?\n\n\nLuciano: I suppose the most obvious ones, you're probably getting that already, are that if you are\nalready comfortable with a programming language because you are mostly writing code most of\nyour time, you are already in a way good to go.\nYou don't need to learn anything new.\nOf course, you need to learn what are the classes that you need to use to build different\nthings.\nThe different levels that you described are not so obvious at first.\n\n\nSo there is still, of course, a learning curve, but at least you don't have to learn an entirely\nnew language.\nLike for instance, when I started doing Terraform, I needed to learn all the syntax and the nitty\ngritty details of HCL, the AshuCorp language.\nAnd that sometimes is a little bit of a barrier that you can avoid with something like CDK.\nAlso we mentioned that there are all the different levels and these levels exist for what you\nget out of the box from AWS.\n\n\nLike if you start to use CDK, but there are also third party patterns that you can just\nuse, you can search your line and you can download the ones that you think they make\nthe most sense for you.\nAnd you can even avail of work that other people are doing, just bringing it into your\nproject and you are more or less, let's say, good to go.\nProbably we'll spend a little few words later on that.\nAnd I suppose another great thing is that because you are using all the languages that are supported are typed languages.\n\n\nAnd because of that, you get a very good level of auto-completion and type checking.\nSo for instance, if you are writing, let's say a low level construct, like a CFN one,\nwhen you start to initialize the classes, you're going to get good auto-complete.\nYou can immediately see all the properties.\nYou can see documentation in line.\nSo I feel that that flow that I used to have when I was doing a cloud formation or Terraform\nwhere you always have on one page of the documentation, on the other page, your editor, or maybe two\ndifferent screens and you're always looking at two different places to try to reconcile\nthem.\nNow it's a little bit more streamlined, like in one window, you are going to have everything\nyou need to figure out like what kind of properties you need to set and where.\n\n\nEoin: When I started using CDK, when it first came out and it was in beta and I opened up, I\nthink VS code and started writing TypeScript and I got all this auto-completion and type\nchecking and immediate error feedback on what properties were missing.\nIt was like, you know, it's something we're used to for developing code, but with cloud\nformation, even then, I don't know if CFN lint was available.\nIt's deploying, getting feedback on cloud formation was typically, you know, something\nyou needed to deploy in order to get, and this, that was a big productivity win.\nSo it just, yeah, that was really good for me.\nAnd then other things as well, like IAM policy generation, have you found that like with\nCDK, you spend much less time hand crafting and tuning policies and figuring out why you're\ngetting malformed, you policy document legacy particles failed, these kinds of errors.\nYeah.\n\n\nLuciano: And I really like, actually, this is a really good point because basically what you can\ndo, I'm going to give a practical example.\nFor instance, you define an S3 bucket and then you want to allow a particular EC2 instance\nto be able to read and write in that particular S3 bucket.\nWhat you will typically do with cloud formation, you create three distinct resources, the bucket,\nthe EC2 instance, and then you craft your own policy that ties the two together, giving\nthe right permission.\n\n\nWith CDK, because you have this idea of kind of an object oriented approach, what you could\nsay is you use the object that in your code describes the S3 bucket and you just say dot\ngrant read and write to and pass a particular instance reference.\nAnd that will automatically generate a policy for you.\nSo it's a little bit of abstraction, but I think it feels a lot more readable and it's\neasier to get the link between the two resources right because you don't really have to manually\nreference things.\nYou just essentially let the autocomplete guide you and it will most likely do the right\nthing for you.\nSo that's something I really liked and I think even when I gave that code to somebody else\nthat wasn't familiar with either cloud formation or CDK, they immediately realized, okay, you\nare creating this instance, you are creating this bucket, and then you are granting permissions\nfor the instance to read and write in that bucket.\nSo I think that's another very clear advantage.\n\n\nEoin: There is one thing I like as well, which is that with cloud formation, you have the concept of change sets and it has a lot of features like change sets and stack sets, but change\nsets allow you to kind of make a plan for what you're about to deploy, inspect that\nplan and then apply it in separate steps.\nAnd Terraform also has this concept with Terraform plans.\nBut change sets don't tend to be used very commonly.\n\n\nI think I heard somewhere recently saying that some ridiculously high percentage of\ncloud formation in the world is deployed using serverless framework, like 80 or 90%.\nAnd serverless framework doesn't use change sets by default.\nI think it's a plugin for it, but it tends not to get used.\nSo people tend to just deploy.\nBut CDK is very much built around change sets.\nSo you create a change set, it allows you to inspect and verify.\nSo it has this synthesis process, which allows you to see the template.\nThen when you're deploying, you can see the change set and you can verify the change set,\nparticularly the security changes before you deploy.\nSo this is nice.\nIt's allowing you to follow best practices by default rather than having to add that\nin yourself.\n\n\nLuciano: Yeah, this is something I used to do a lot with Terraform where I was prototyping something and even before deploying, I would do like a Terraform diff, which is pretty much the\nsame thing we are describing.\nAnd you will get a list of, okay, this is what is going to change if you actually want\nto deploy right now.\nMaybe I didn't want to deploy, it was just a good sanity check to see, okay, I'm really\ngoing in the right direction where I am describing the changes that I want to happen in the infrastructure.\nAnd now with CDK, you can just do CDK diff, I think is the command, and it will give you\nlike a list of, okay, at this stage, if you apply this, this is what's going to change\ncompared to your current infrastructure.\nAnd I think that that's really powerful and really useful, especially if you are starting\nto use infrastructure as code, it will give you a lot more confidence when you are writing\nfor the first time that you are going in the right direction.\nYeah, yeah.\n\n\nEoin: One of the other things that is kind of really rising to the fore with CDK is how easy it\ncan make to do pipelines.\nHow easy it can make doing pipelines.\nSo if you use CodePipeline and CodeBuild, CodeDeploy, all these services, creating pipelines\nfor them with CloudFormation is hard work and maintaining those pipelines is really\nhard work.\nAnd I think pipelines are the first, building CI-CD stacks, this is one of the first things\nI ever used CDK for.\n\n\nAnd I still use it very commonly because it just makes that process much easier when you\nhave particularly dynamic behavior in your pipelines.\nYou know, the stacks you are deploying change, the number of steps in your pipeline change,\nyou want to be able to replicate a deployment pipeline for a dev environment or for a new\nset of accounts, CDK really facilitates that.\nAnd beyond that, if you are really going all in on CDK and you are using L2 constructs\nfor creating Lambda functions and all of your resources for each service in your application\nand multiple stacks within your application, it has a pattern of, a pattern, so it's kind\nof level three constructs for CDK pipelines.\n\n\nAnd this will basically create pipelines out of the box with very good defaults that would\ndeploy all of the stacks in your application.\nAnd so this is really good.\nIt means you have to go all in on CDK.\nThat's the only thing, but it is a really nice advantage.\nAnd CDK pipelines then are also self-mutating.\nSo if you have got the pipeline code in the same repo as all of your application stacks,\nthen you commit to a branch or trigger a release, then when the pipeline runs, it will first\nmake sure that the pipeline itself is running the latest version of that code, and then\nit will deploy everything else.\nSo it's really nice from a change management point of view.\nYou can imagine a PR that introduces a new service into your application and it includes\nthe pipeline changes as well.\nSo it makes it really easy from a code review, collaboration point of view.\n\n\nLuciano: So that's really nice.\nAnother thing that is probably relevant to what you just said is the concept of assets\nthat exist in CDK, which I think is really clever and can simplify a little bit your\nlife in many use cases.\nFor instance, if you use, I don't know, something like Terraform, every time you need to, let's\nsay, deploy a Lambda or a container in ECS, you need to, of course, to specify, okay,\nwhere is the source code for that?\n\n\nAnd generally that means, okay, I need to create an artifact, publish it to S3 or to\na container registry, and then I can reference that particular artifact in my infrastructure\nas code.\nWith CDK, there is a way that you can abstract all that work, and if you have the code collocated\nwith your infrastructure as code, you can just reference assets in the same project.\nAnd then behind the scene, that asset abstraction will, for instance, deploy the source code\nof a Lambda in S3 or use a container registry to deploy the source code for a container.\nAnd they would even do the build phase within the context of CDK.\nSo it's a little bit more streamlined process where you don't end up using different tools\nand different steps of a pipeline to just deploy your changes, which can be nice.\nI guess there are pros and cons, but it can be nice, especially if you're working on a\nsmall project because it makes your life easier.\nBut now that we've talked a lot about the goods, what do we have to say about that?\nDo you want to mention something on that one?\n\n\nEoin: Yeah, there's quite a lot to cover here.\nIn my own experience, one of the big things is that there hasn't been a lot of consistency\nin it, especially when you're talking about some of the L2 constructs.\nNot every service in AWS has L2 constructs.\nSo I remember trying to deploy batch and you had to use the L1 constructs.\nSo sometimes it falls a little bit behind the CloudFormation and it takes time for those\nL2 constructs to emerge.\n\n\nBut I suppose one of the main things, if we're looking at disadvantages of CDK and approaches\nlike that, is that this is an abstraction layer and abstractions should always be treated\nwith a decent amount of caution.\nIf you look at object relational mapping as an example of that, there's always a price\nto pay for abstractions.\nOne of the things then is if you don't understand the details of what is being generated, this\nis a dangerous thing.\n\n\nSo if you've got a client-side application that's making decisions for you about the\nresources that would be deployed on your cloud.\nAnd if you're using this as a way of escaping, getting the understanding of what you're deploying\nin the cloud, that's a dangerous thing, right?\nBecause you can really end up having performance or cost or other unexpected behaviors emerging\nfrom what you're deploying.\n\n\nSo there's a couple of cases where CDK is really good, but I wouldn't say it's good\nif you're just coming at it from a beginner and using it to completely skip having a good\nunderstanding of what CloudFormation does and what it's doing for you.\nI think going back to the episode where we talked about our favorite service, we mentioned\nit as one of our top service.\nI think it might've been on my number one spot.\n\n\nThere's a reason for that, right?\nIt's just critical to everything you do and having a predictable deterministic deployments\nis really important.\nIf you don't understand what's being generated, you might lose that.\nAbsolutely.\nSo yeah, I can't emphasize that enough, right?\nSo it's important to go in with your eyes open.\nSo we have seen a lot of change as well.\nI started using it in the beta phase and as you would expect when it went to general release,\nthere were some breaking changes, but you also have a lot of style changes since then,\neven across the version one series and deprecation of methods of doing things.\n\n\nAnd recently the last couple of weeks we've had version two come out and I've already\nseen a lot of people complain about breaking changes and how it's not ready for general\nrelease and how they have to start redesigning their stacks.\nSo this is something that's important to clarify.\nThere's always a trade off with these levels of abstraction.\nThe other thing that can be quite confusing to new users is that you begin deploying a\nCDK application by executing this bootstrap phase.\n\n\nAnd it's not immediately obvious what this is for or why you would do it, but you need\nto start, I suppose, with a bucket and some things that CDK can use to deploy its resources.\nSo it needs to prepare sometimes a bucket for assets and it actually now generates an\nECR repository for container images and it'll generate some policies for deployment across\naccount policies.\nAnd these are all useful things, but when you're deploying your own application stack,\nsometimes it's a surprise to see that you need to deploy another stack first in order\nto just deploy the stack you're actually targeting.\nSo that's a bit of complexity and something that's not universally understood.\nSo I put that down as a disadvantage.\nYeah, actually I'm not sure I understood that so well until now.\n\n\nLuciano: So thank you for explaining that to me as well.\nI always done the bootstrap phase, but I was a little bit like, okay, whatever, I guess\nI have to do this, but it's good to know what actually happens behind the scene, I think.\nI have another one, which is something that has been beaten me quite a few times actually.\nAnd that's basically you think about CDK as, okay, this is just code.\nI can do whatever I want.\n\n\nYou can write all sorts of business logic in there, loops and if statements and do things\nas you would do in any other regular programming language, but sometimes you don't get the\nbehavior you would expect.\nAnd this is because in reality, what CDK is doing is managing an entire life cycle and\nyou are defining resources that will need to be provisioned at certain points.\nSometimes you reference resources that might or might not exist at that particular moment\nin time and CDK will try to figure out that when you actually try to deploy.\nSo for instance, one thing that happened to me a few times is that I was trying to read\nthe content of an SSM variable and decide like a piece of business logic.\nIf this is the content I'm going to, I don't know, maybe want to provision something else\nor I don't know, whatever business logic trying to make the type of resources that I wanted\nto provision determined by the value in an SSM parameter.\nAnd that doesn't always work.\nI mean, you need to do different things to make it work.\nSo that was an interesting one.\nAnd it might be confusing why that happened.\nSo that's a quirk, I would say, of CDK.\n\n\nEoin: There's another thing.\nI mean, I think there's a lot of debate in the community and on public discourse around\nCDK benefits versus disadvantages.\nAnd one of the things that's cited quite often is, you know, it's not a deterministic deployment\npath because if your client code that's generating your cloud formation code changes, that the\ncloud formation template can change without you having changed any of the inputs.\n\n\nSo it's not really a deterministic path.\nWe kind of talked about that when we're dealing with the disadvantage of abstraction.\nAnother kind of disadvantage that I've seen referred to are the kind of cultural barriers\nthat it can bring about in an organization or that it fails to deal with.\nAnd if you look at how things have gone over the past few years in cloud and the emergence\nof a DevOps culture, what you're trying to do is break down barriers and walls between\noperations and developers.\n\n\nAnd if you want to be able to create these cross-functional teams that can build it and\nrun it, I think there's a danger that using programmatic constructs and using imperative\nlanguages can actually rebuild some of those walls because if you look at people who have\ngreat expertise and experience in SysOps and are coming maybe from a background of using\nchange management tools like Ansible and Chef and Puppet and lots of other tools and even\nTerraform, if you start coming along and saying, well, this is better because it uses imperative\nlanguages and imperative languages are real programmatic languages and they're better\nbecause they allow you to do all of this great object-oriented stuff.\n\n\nI don't think that's a really genuine or helpful argument.\nThese languages aren't inherently better at all.\nThey're just more familiar to people who are used to writing software with business logic.\nIt doesn't necessarily mean that they're a better tool for this job.\nIn fact, there are plenty of arguments to say that infrastructure should always be declarative.\nSo I think organizations kind of need to think about this before just going all in because\nsometimes what you can actually do is end up isolating people with really good skills\nand SysOps experience in your organization by essentially potentially gatekeeping by\nsaying you now need to have this set of skills in order to be able to do infrastructure in\na modern software application.\nAnd I don't think that's the case.\nI think we should be able to have cross-functional teams where we meet people where they are\nand understand that sometimes for infrastructure, imposing your set of tools on top of them\nisn't necessarily the best thing.\nI think that's really important disadvantage that can emerge and something that should\nkind of slow people down from just adopting it just for the sake of it because it seems\neasier to get started.\nYeah, that's an amazing point.\n\n\nLuciano: I totally agree on that.\nAnd it's interesting to see what the industry is going to decide, I suppose, in a few years\nif tools like CDK are going to be more mainstream or if eventually we are going to go back to\nmore declarative approaches maybe, I don't know, with different tools or different languages.\n\n\nEoin: So how do you think people should get started then if they're, I guess, people are going\nto come at it from different angles.\nMaybe people are using CloudFormation already or using something else.\nWhat's the best place to get started with CDK?\n\n\nLuciano: Yeah, I think I would like to suggest a little bit of a backward approach because what we say is that the main advantage of CDK is that it's a level of abstraction and you can deploy\nthings probably quicker than you will do with just writing CloudFormation from scratch.\nBut at the same time, we say that there is a danger that if you do that, you're not going\nto really know what's going on really at the stack level, like what kind of resources are\nyou going to end up deploying.\n\n\nSo what I would like to suggest, and maybe this is a little bit of an experiment, so\nplease let us know if you do that, what do you think about this experiment, is try to\nstart maybe using CDK almost like CloudFormation.\nAnd by that I mean, use just level one constructs.\nSo you are literally just writing CloudFormation but in something like TypeScript.\nAnd that will give you a good approach, in my opinion, to the tooling around it and to\nlike what's really happening in the different phases.\n\n\nAnd then from that point on, you can start to avail from the different abstraction.\nYou can use level one constructs or you can use level two constructs or third party constructs\nto use that more level of abstraction where maybe, I don't know, you just want to create\na VPC and you are okay with some of the defaults.\nBut at that point, you should be comfortable enough knowing where to check to see what's\nactually going on behind the scene.\nSo this is what I'm suggesting.\nStart from actually the lowest level and then add abstraction as you feel more and more\ncomfortable and as you feel you understand what those abstraction are really doing for\nyou.\nSo maybe that will give you less surprises, I would say.\nMaybe a little bit more painful to reap the benefits of CDK, but also probably a safer\napproach and less surprises at the end of the day.\n\n\nEoin: I think that's good.\nYeah, yeah, yeah.\nIt's good to not jump into these things feed first and go into these high levels of abstraction.\nSo that makes sense.\nAnd sometimes I've actually used CDK just to generate cloud formation so I can see what\nsyntax I should be writing manually and then I just create the template.\nSo you can always use it in that mode of operation as well.\nMaybe that's the best of both worlds.\nOkay, so I think given that we've finished up on how to get started and we've covered\nall the disadvantages and advantages we can think of, it's time to finish up and maybe\nask for your feedback to anybody who's listening to let us know what you think in the comments,\nto share it with your colleagues and friends and let us know how you get on with CDK.\nAnd if you've enjoyed the episode, give us a thumbs up as well and follow and subscribe.\nSo we're going to see you in the next episode.\nThanks very much for listening and goodbye.\n"
    },
    {
      "title": "17. How do you keep up with AWS in 2022?",
      "url": "https://awsbites.com/17-how-do-you-keep-up-with-aws-in-2022/",
      "publish_date": "2021-12-30T00:00:00.000Z",
      "abstract": "In this episode of AWS Bites podcast, Luciano and Eoin talk about some ideas to stay update with the evergreen world of AWS. This is a much less structured episode than usual and we informally discuss some of our favourite resources including blogs, articles, YouTube channels, other podcasts and Twitter profiles.\nIn this episode we mentioned the following resources:\n\nAWS what’s new (official)\nWhat's new on AWS Twitter bot\nAWS News RSS Feed\nAWS Podcast\nWerner Vogel’s blog\nRebecca Marshburn &amp; Jeremy Daly's Serverless Chats podcast\nJeremy Daly’s Off by None Newsletter\nCloudonaut’s blog\nCloudonaut’s YouTube channel\nYan Cui’s blog\nYan Cui’s Real World Serverless\nAWS Builder Library\nBorislav Hadzhiev’s blog\nMarcia Villalba’s Foobar on YouTube\nAdam Elmore’s AWS.fm podcast\nEmily Shea on Twitter\nEmily Shea’s re:Invent 2021 “Getting started building your first serverless web application”\nDanilo Poccia on Twitter\nHeitor Lessa on Twitter\n\n",
      "transcript": "Luciano: Hello and welcome to AWS Bites, the weekly show where we answer your questions about\nAWS. I am Luciano and today I'm joined by Eoin. And before we get started, please let\nme remind you to please follow and subscribe this channel so you can be up to date every\ntime we publish new episodes. And today's question is actually somewhat relevant to\nthis because we want to try and answer the question, how do you keep up with AWS in 2022?\nThe new year is just around the corner. So what are the best resources out there for\nyou to know about all the news about AWS every time there is something worth knowing about?\nEoin, do you want to try to start to give an answer to this question?\n\n\nEoin: Yeah, I guess we have to start off saying that everybody should keep listening to AWS and keep subscribing and listen to all the episodes because everything that we think\nis relevant and interesting to talk about will be having at least weekly episodes throughout\n2022. There's so many resources out there. The question is how do you filter out and\npick something? And I guess my habits change quite regularly and I dip in and out of various\ndifferent pieces of content and it depends on my mood and what I'm doing and how much\ntime I have. And sometimes I don't keep up to date for periods of time. And I guess that's\nthe same for everybody. There are some people who are going to be obsessively keeping up\nto date and following RSS streams, RSS feeds of news announcements. That's not really how\nI like to do it. I suppose my strategy is kind of best effort, but also trying to keep\na passive awareness of what's happening in the world of AWS in the background. And there's\na couple of fairly fundamental AWS update resources I use just to kind of keep that\npassive awareness going. And one is the AWS official podcast. And sometimes I just listen\nto that when I'm traveling or have headphones in and I have a bit of time and they basically\njust read out all the announcements on that podcast. And so it's not particularly thrilling\nor opinionated, but it covers absolutely everything. So if you want to know what easy to instance\ntypes there are, or what new languages are available for Lex, then it will tell you everything\nin great detail. And it's kind of something that doesn't sink in too deep, but it's there\nin the back of your mind. So if you encounter it in the future, you might think about it.\nAnd another thing that's really useful for that is there's a Twitter feed. It's Twitter\nbot, not official, but it's called What's New on AWS. And I think it basically just\ntweets out all the announcements that come on the AWS official blogs. And you end up\ngetting a few notifications per day. If you tick the bell on Twitter, then you get a few\nnotifications every day and most of them will be pretty irrelevant to you and you can just\nignore them instantly, but others are interesting and then you can dig deeper and read about\nthem. So I find that really efficient way of keeping up to date. So those are pretty\nboring ones, but very useful.\nYeah.\n\n\nLuciano: I suppose on the last one, if you are more of a feed reader type of person, you can just subscribe to the feed reader from the official AWS announcement blog and you'll\nget the same type of content from the Twitter post. So yeah, it's interesting that we have\nthese days a huge variety of medias where you can consume information from. I don't\nknow, another option could be YouTube channels. There is no shortage of YouTube channels related\nto AWS and definitely a few shout outs to Marcia Villalba, FUBAR. It's a really good\none. And I know that Marcia is also doing content in Spanish, so that could be also\ninteresting. Could be another way to access this information if you prefer content in\nanother language. There is again, no shortage of material out there. So that's another one\nthat is definitely recommended. I'm always flipping between, I suppose, more YouTube\nvideos and classic, more traditional blog posts or written articles. And I want to give\nanother shout out to Borislav Atziv. I hope I am pronouncing it right. I ended up reading\na lot of his CDK blog posts. They are very good, very in detail, and generally you find\nvery complete examples. So that was a very specific quest for me at the beginning of\nlast year to start learning more and more about CDK. So I ended up in this blog post\nand then started to read the next one and the next one and the next one and there is\na lot of good material out there. It's very niche. I think it's mostly CDK, but if that's\nsomething you want to learn more about, definitely I recommend that particular blog. And by the\nway, you'll find all the links in the description of this episode. And yeah, let's see. Another\none that I really like in terms of blog, and it's a little bit different, I guess, from\nmost blogs is Werner Vogel's own blog post, because there is a huge effort in publishing\nregularly, I would say, but there are some blog posts that are actually very deep and\nyou get a lot of principles and ideas about how to architect distributed systems, which\nis not always strictly related to AWS, but of course, it's some knowledge that you need\nto have and you need to keep building if you want to use AWS efficiently. And with that,\nof course, there are mixed blog posts where the focus is more maybe on a new service of\nAWS. So you can definitely also find that kind of, okay, what's new on AWS type of material.\nSo I think it's an interesting combination of strictly related AWS topics, but also things\nthat you might need to know in terms of how to architect good distributed systems. Yeah.\nAnything else on your radar? Yeah.\n\n\nEoin: There's a couple, I suppose, the first few things we mentioned that were about just keeping up to date with news announcements.\nWe're getting into more like, how do you keep up to date with community and best practices\nand how to's and there's so much there. There's one set of resources I've been using quite\na lot, which is the CloudAnnot blog and YouTube channel. And this is from Andreas and Michael\nWittig, who are brothers who have an AWS consultancy. And the one thing that stands out about this\ncontent to me is that if you're looking for knowledge, as opposed to just discussion and\nopinions, these videos and blog posts are just really well crafted and very well thought\nout and informative. So they're always, I think, done in such a way that you have the\nvideo and the accompanying blog posts. So you've got both formats and they're just really\nwell done. So if you're looking for how to set up SSO, for example, for the first time,\nor basic stuff around IAM or how to do code pipeline and code build, they've quite a lot\nof blogs that are just really informative and really well thought out. And I think they\nstand out as being different because they're just really knowledge rich and well structured\nand they're not so opinion based.\n\n\nLuciano: They look like tutorials, right?\n\n\nEoin: Yeah, exactly. Yeah. And they're just really well written and you always leave with a sense\nthat you have a much more complete understanding than you started with.\n\n\nLuciano: Yeah. And I also like that they have this kind of double format video and text so you\ncan pick your favorite or even consume both as a second way of digesting the same type\nof information.\nYeah, that's good. That's good. Yeah. And you were talking about Werner Fokkel's content.\n\n\nEoin: That's quite, I suppose, like you say, deeper. The builders library in AWS is kind of similar\nto that, isn't it? In that it's deeper architectural practices and organizational practices that\nare, I suppose, require a little bit more study.\nYeah. Yeah.\n\n\nLuciano: There are a couple actually of interesting articles that were even mentioned by Werner Vogels at his latest re-invent keynote. And one is about the importance of constant\nwork. Another one is about how to design idempotent IPIs. And I think those two articles are actually\nvery good in giving you information that maybe you have been exposed to, but probably you\nkind of underestimated. And it's good that they remark why these kind of concepts are\nso important to build very good distributed systems. But also they are kind of representative\nof the type of content you can find in this particular section of the AWS website. So\nall the content is pretty much that in-depth focus on one specific architectural topic.\nSo if you are trying to become an architect or if you are already an architect and you\nwant to explore more in-depth topics, I think that that's really a good resource.\nYeah. Okay. That sounds good.\n\n\nEoin: I mean, when I'm in kind of busy periods, I guess it gets very difficult trying to watch videos or read blog posts and getting the time to actually\nsit down and focus. But podcasts can be pretty useful sometimes because you can have them\non in the background and you mightn't hear everything and you might be able to focus\non everything, but sometimes some of it just does stick and that could be useful and serve\nyou in the future. So do you have any go-to podcasts?\n\n\nLuciano: I actually want to mention two of them. One is probably the newest I've seen in this space\nis Adam Elmore's AWS FM. And I really liked that one because it feels a little bit lighter\nthan at least many of the other types of content we have been mentioning because it's more\na conversation between people that are prominent in the AWS ecosystem. And it's more about,\nI don't know, their life story, what they built, how did they get to the place where\nthey are today. And of course, you get to learn a lot of interesting information about\nAWS, but it's more about the personal journey of the guests. So I think this is the kind\nof podcast you can listen without necessarily being too focused on the content, but still\nyou can get a pretty good value for the time you spend listening to the podcast. So I really\nliked that particular format. But there is another one that is a little\nbit more technical, it's from Yan Cui, which is Real-World Serverless. And that one is\na lot more, well, first of all, it's focused on serverless, but it's more the story of\npeople that have built successful systems using serverless and what they learned. So\nthere is a lot more like serverless from the trenches or you get to hear, okay, this did\nwork, this did work. So that's a lot more practical and focused. So if you are interested\nin serverless, that's really a great resource. Yeah.\n\n\nEoin: And Yan Cui also has a bunch of really good blog posts on serverless that are, I suppose, indispensable for anybody in the\nspace. I really like those two podcasts too. I mean, and there's the serverless chats one\nas well. I guess they all have different styles and approaches and it depends on the kind\nof mood I'm in, which one I listen to. But I do really like that real world serverless\napproach where you get people talking about the use cases and stuff they're really building\nand ultimately what doesn't work, which is sometimes missing from a lot of other content.\nLike what are the gotchas and the failure modes?\n\n\nLuciano: Yeah, I think in a similar vein- Is there anybody?\nExactly. Yeah, because Twitter is another way to consume pretty much like random content,\nright? But if you follow people then in a specific niche, then you get more focused\non that particular type of content. And there are in particular three profiles that I really\nlike. Emily Shea is probably one of the new profiles that I started to follow recently\nand she has been producing a lot of amazing content, especially targeting people that\nare starting with AWS. So if you are in that particular part of the journey, definitely\nfollow Emily. You'll get a lot of value for that. And then other two are evangelists in\nAWS, Danilo Poce and 8OrLess. I think we mentioned them already in a previous episode. They are\nalways producing a huge amount of material and even just giving you information about\nnew services or things that you can now do in AWS that maybe you couldn't do before or\nit was more complicated. So it's good also to get few bits of information by just following\ntheir profiles. So definitely recommend it. Do you have any other profile you would recommend?\n\n\nEoin: I guess, yeah, I just wanted to call out, since you mentioned Emily Shea, I think we\nboth managed to catch Emily's talk at Reinvent, which was a really, really excellent talk\non serverless. And I think a lot of people who are using AWS, but thinking about getting\ninto serverless might be looking at that in the new year and how do you discover more\nabout serverless? And I think Emily's talk is a really good place to start. But another\none I wanted to mention, and I think we mentioned serverless chats and Jeremy Daily as well.\n\n\nJeremy's off by none newsletter is a really, really good resource. So it's a weekly newsletter\nand because there's so much content out there, I think Jeremy puts a lot of time into distilling\nit down into like, here's some articles that I'd like to highlight for you. And it just\nmakes it really efficient to keep up to date without having to do all the work. He's kind\nof done all the effort for you. So I just wanted to shout that out. And he's on Twitter\nas well. So we'll add his Twitter handle into the feed as well. But I think that's covered\neverything I had anyway. I think there's plenty there for people. But I'd be interested if\nwhat are we missing? Maybe people have some good suggestions out there.\n\n\nLuciano: Yeah, it would be great if you can just leave us a comment or a tweet saying, okay, I would recommend this particular resource for whatever reason you like it. I think that will help\nus to give even more ideas and content to people following us. And also for us, maybe\nwe can discover resources that we didn't know about. So that would be amazing.\n\n\nEoin: Yeah, maybe a good new year's resolution is to switch the people you follow and change the content you're consuming just to get out of the filter bubble and get some new perspectives\non everything AWS. Absolutely. Okay.\n\n\nLuciano: I think that's all we have for today. Thanks again for being with us and listening to this episode. Again, make\nsure to follow and subscribe us. And yeah, again, if you have comments or opinions, please\nshare them. We'd love to hear. And if you have any question you would like us to answer,\ndefinitely send it our way and we'll try our best in one of the next episodes. And that's\nall. See you in the next episode.\n"
    },
    {
      "title": "18. How do you move to the cloud?",
      "url": "https://awsbites.com/18-how-do-you-move-to-the-cloud/",
      "publish_date": "2022-01-06T00:00:00.000Z",
      "abstract": "In this episode of AWS Bites, Eoin and Luciano discuss some interesting strategies to take a workload to the cloud. Most of this episode is built around “6 Strategies for Migrating Applications to the Cloud” by Stephen Orban in which you can find the concepts of the “6 Rs for cloud migrations”: Retire or Retain, Re-purchase, Re-host, Re-platform and Re-architecture.\nThroughout the episode we share our take on this approach and we try to provide our perspective and suggestions on how to apply this strategy and get some significant ROI for this project. Moving to the cloud is never a trivial thing and there are many elements to consider, especially in a medium/big organisation. This episode will help you to lay out all the elements that matter the most and come up with a solid plan for your next cloud migration.\nIn this episode we mentioned the following resources:\n\n6 Strategies for Migrating Applications to the Cloud\n\n",
      "transcript": "Eoin: Hello and welcome to AWS Bites, the weekly show with byte-sized episodes where we answer\nyour questions about AWS.\nMy name is Eoin and I'm joined by Luciano.\nBefore we get started, make sure to subscribe to our YouTube channel, give us a follow and\nshare the episode with others.\nToday the question is, how do you move to the cloud?\nSo that's a pretty fundamental question and probably one that's relevant to more people\nthan any of the topics we talked about today because most people aren't in the cloud yet.\nSo Luciano, where would you start?\n\n\nLuciano: Yeah, I hate to do this, but I suppose the real answer as in everything technical is\nit depends, right?\nAnd the reason why I say that is because of course it depends from, first of all, what\nis your starting point?\nWhat do you have today that you want to move to the cloud?\nBecause depending on the answer to that question, you'll probably find different strategies.\nAnd I suppose things that are important to understand are what kind of architecture do\nyou have today?\n\n\nLike, do you have a monolith running on premise maybe or in a virtual private service?\nYeah, how do you deploy that particular monolith?\nIt could be important to figure out what is an equivalent path to bring that monolith\ninto the cloud.\nBut another interesting point for instance is what are the skills of your team?\nDo you have anyone in your team that already knows a cloud provider of choice?\nDoesn't have to be AWS, but yeah, is there at least one or do you need to hire somebody\nthat has the skills and needs to train your team?\nSo I suppose if you try to start to assess what is your current starting point, that\ncan help you to figure out what is the leap forward that you need to do to move to the\ncloud.\nBut I know that that sounds probably very vague.\nI don't know if you have more kind of step-by-step strategy that can help people to figure out\nexactly what are the options at least to move to the cloud.\nYeah, you're right.\n\n\nEoin: It depends.\nMaybe it's worthwhile to say that for...\nLet's try and focus on the most complicated cases or at least the more organizationally\ncomplex situations.\nSo enterprise workloads where you've got a large enterprise with multiple different workloads\nmigrating to the public cloud.\nIf you've got a single workload, it's probably a little bit more tractable as a problem,\nbut there was a really good blog post and it's about five years old now, I think by\nStephen Orban on the AWS blog and it's called Six Strategies for Migrating Applications to\nthe Cloud.\n\n\nAnd I've seen it referred to over and over and over again over the years and it uses\nthe six R's for cloud migration.\nSo if you're looking for a framework, I think this is a really good one because it just\nhelps you kind of make a system for cloud migration and kind of categorize things.\nReally helps you to make sense, especially if you're like some enterprises have hundreds\nand hundreds of workloads they're going to migrate to the cloud.\n\n\nSo it's no mean feat.\nYou got to start somewhere and you need to have some structure around it.\nSo the six R's are, I think, retire, retain, repurchase, and then you have rehosting, replatforming\nand rearchitecture.\nSo we can go through them, but retire is, okay, you're not going to need this system\nanymore.\nYou can bin it, which is the best outcome because you don't have to worry about it anymore.\n\n\nAnd then retain is like, okay, well, we're not really going to get a lot of value out\nof this migrating this one to the cloud.\nLet's just leave it where it is for now.\nIt's not hurting anybody, which is also fine, right?\nBecause if it's providing value and you don't have to move it, it's not going to cost you\nanything.\nAnd then repurchase one is another really nice one actually, because it's saying, well,\nlet's not take any responsibility for this workload anymore.\n\n\nLet's just buy something in the cloud like a SaaS provider that's going to manage all\nthis workload for us.\nAnd so these three are actually really good wins.\nIf you can categorize some of your workloads under those three R's, you've already made\na lot of progress in my view.\nBut then those three are kind of, I suppose, the cop outs.\nThe rest of them are actual cloud migrations.\nSo then you're talking about rehosting, replatforming and rearchitecture.\nSo rehosting, I guess, is what most people started to do when we started looking at cloud\nmigration over the years.\nThat's common now, but it still happens a lot and makes a lot of sense.\nIt's essentially lift and shift.\nSo you're taking what runs on-prem and in a data center and moving it into the cloud.\nSo is there anything we need to say about that?\nOr what do you think, Lithianna, before we move on to the others?\n\n\nLuciano: Yeah, just think that that one makes sense when, again, you have something a little bit monolithic.\nSo you have one instance or a VPS and you have maybe an image that contains everything\nyou need to run that particular application, like operative systems, all the tools and\nso on.\nIf it's like a VMware instance, I think there are many ways to run that directly in AWS.\nSo you don't even need to do any particular work in reconverting that instance into something\nelse.\n\n\nBut if you want to be a little bit more native to AWS, you can build an equivalent EC2 image\nand then you can just run that application as an EC2 instance.\nYeah.\nSo in that case, yeah, it's very easy.\nYou just need to figure out how to rebuild the image if you have to and what is the size\nof the instance that you need to run it to basically have all the computing power and\nmemory that you really need for that particular application.\n\n\nBut yeah, one that I think it's really interesting are probably the other two, actually, the\nre-platforming and re-architecture, because those are the ones that could generally become\nlike much bigger projects in a company, in a cloud migration, a list strategy.\nAnd re-platforming is not necessarily that crazy complicated, but it could be depending\non how you do it.\nBut the idea is that you basically change, I don't know, your database is still the same\ndatabase, but it runs on RDS, for instance.\n\n\nSo you need to change a few things, but they are not really changing the structure of the\nentire application.\nAnd I have seen recently a couple of interesting projects that did that, where they moved applications\nlike classic web framework running in one monolithic approach.\nThey moved it to either EC2, multiple instances, or maybe using ECS as a way to run the application\nas a container.\nAnd it was quite interesting because you could see that immediately there is a benefit there\nthat you realize, okay, I've been running this particular application as one instance\nand it didn't scale so well.\nNow that I'm moving it to the cloud and making it a little bit more distributed, I get a\nlot of features in terms of scalability, even not just scaling up, but also scaling down\nwhen things cool off a little bit.\nAnd the only caveat I've seen that always bites people off a little bit is make sure\nthat your application is stateless.\nSo in one instance of your application, you are not keeping any state, for instance, I\ndon't know, user session or files in the disk, but you need to use services like Redis for\nkeeping the sessions, which could be an elastic edge in the case of AWS, or maybe use S3 to\nkeep persistent files around and everything else maybe in RDS or DynamoDB.\nSo that's the only-\n\n\nEoin: I guess there's a couple- Yeah, no go ahead.\nSorry, I was just going to say.\nI guess there's a couple of other levels.\nI mean, even if you look at some on-prem systems, you might have like a hardware load balancer\nor a software load balancer, reverse proxies.\nThere's lots of services that you might be running on-prem, which you could take advantage\nof a load balancer in EC2 then instead, and you might use a DNS for a failover and you\nmight think that up to Route 53 is, so there's a lot of different levels you can go to in\nterms of re-platforming when you go to AWS.\nAnd it's just a question of how much work do you want to take on and how much of an\nadvantage do you want to take of AWS?\nAnd how, I guess, because sometimes you could be just moving, it seems simpler to move exactly\nwhat you have into AWS, but it still takes work.\nSo is there a lot of value in just moving something that will eventually be replaced?\nYeah, exactly.\n\n\nLuciano: And of course you can do that in multiple steps.\nYou don't have to re-platform everything in one go.\nMaybe you can do that in few different releases, but with every release you have a usable application\nand you can start to get the benefits of that migration.\nAnd yeah, the next one is re-architecture.\n\n\nEoin: Yeah, it goes back to your question around skills as well, because even if you just lift and shift, it sounds like it's a simple copy paste exercise, but you still have to set\nup your AWS accounts and your network infrastructure and your VPNs or your Direct Connect link,\nmaybe a transit gateway and your IAM roles in SSO and your VPCs and your IP CIDR blocks.\nThere's so much at the foundation of it, you have to put in place, especially for an enterprise\nbefore you migrate your first workload, really.\nSo lift and shift, there's a lot of work just to get the first step.\n\n\nLuciano: Absolutely.\nYeah, I would say that the next or is re-architecture, which is probably one of the more elaborate\nwhere you are literally redesign and re-architecting the entire workload to be, I suppose, more\nsuitable to what the cloud can offer.\nAnd that could be, for instance, I don't know, you start to use serverless, but your starting\npoint was not a serverless application.\nSo you have to reimagine the entire application from a serverless perspective.\nAnd probably that if it's not like a full rewrite of the application, it's definitely\na huge amount of work where you will have to rewrite and re-architecture many components.\nSo that's, I don't know if I would recommend that because it's always like longer term,\nso it's always the riskiest approach, but it's probably the one that will give you more\nbenefits in the long term because you get a lot more flexibility and you get all the\nadvantages of the cloud.\nSo you are less constrained, I suppose, at the end of that process.\nBut yeah.\n\n\nEoin: I would say that this is one to go for.\nIf you can already measure the fact that you're constantly firefighting bugs with your existing\narchitecture.\nSo if you move it as it is to AWS, that's not going to change.\nSo if you're prepared to say, okay, let's just invest in refactoring this platform and\ntaking advantage at the same time of paper use, serverless managed services, using DynamoDB,\nAPI Gateway, S3, event driven architectures with EventBridge, then yeah, I'd say go for\nit definitely.\nIf it's going to take away from that firefighting pain you're in now, that's a good strategy.\nBut it's not going to happen overnight.\nIt requires you to start small in this rate.\nBut there is a long term payoff for sure.\nYes.\n\n\nLuciano: Yeah.\nAnd I suppose, yeah, the next question is, let's say that you decide what you want to\ndo already.\nWhat are some good guidelines to start to execute on a plan that you might have?\nDo you have any recommendation there?\nYeah, I'd say it depends on the nature of your organization and the style of your company.\n\n\nEoin: There's two approaches.\nYou could say, let's take a very methodical approach with a lot of upfront planning.\nAnd this is like really measured approach.\nSo you break it down into steps.\nYou first do a discovery phase where you assess all that exactly what you have.\nFor a lot of enterprises, this can take a significant amount of time because it's rare\nthat you find a large enterprise and a very good centralized understanding of what all\nthe systems are and how they work and who they're used by and what capabilities are\nrequired and what all the gotchas are there.\n\n\nAnd then you would categorize them, create a plan.\nThe plan involves training, upskilling.\nIt revolves proofs of concept, MVPs.\nThen you have a project plan and you iterate on it and it's usually a multi-year plan.\nBut you can categorize each of your system using the six R's and execute on that plan.\nBut another approach, you could do that, but as an alternative or as a first step before\nyou do that, you could say, let's just take a more lean, agile approach and pick a good\ncandidate project that you can migrate and create, I suppose, an isolated team that's\ngoing to execute on it and try and pick a project that doesn't have interdependencies\nto a lot of other systems and migrate that and use it as a learning exercise.\nBut try to do it in a lean way.\nTry to get an MVP of it out there in a matter of weeks, a couple of months maybe, but not\na nine month or 12 month project because that's too long to be getting feedback.\nTry and execute on this thing quickly and learn about it and learn what works and what\ndoesn't work and try and simplify it as well.\nDon't try and boil the ocean.\nThis is a good thing because even if you're making a plan thereafter, a more comprehensive\nplan, then you've got an informed plan and you've got some evidence to back it up and\nyou're not just speculating on how long things are going to take.\n\n\nLuciano: Yeah, that's true.\nI think the tricky bit there is to figure out which service or which workload in your\norganization can be not too small and not too big so that you have a good enough risk\nthere to understand what is the effort needed to move more and more things to the cloud,\nbut also what are the benefits that you can get once you have done this step of moving\na workload to the cloud.\nOn that same topic, maybe we can mention what could be a good measure of success for a cloud\nmigration.\n\n\nAnd I think that that's important because I feel that there is a little bit too much\nof a hype in terms of we need to move to the cloud, but it's not always clear why, like\nwhat is the advantage of doing that.\nIt feels like it's trendy to do it and you are missing out if you're not doing it, but\nI think it's very important as a company because it's a very serious investment to really understand\nwhat is the return of that investment.\n\n\nAnd I think there are some pointers that you will keep in mind.\nFor instance, you might have increasing performance, you might have cost benefits, you might get\na different speed of innovation because you might make it easier for your team to build\nnew services, to try new things.\nSo maybe that's one of the main benefits you can get.\nAnd of course, there is no one single answer for every company.\nI suppose every company will be looking at different types of return.\n\n\nSo try to do also that exercise where you try to understand why it is important for\nyou to move to the cloud and what is your expectation because then you can use those\nexpectations to measure your experiments and basically understand, okay, is this really\nwhat I wanted or maybe do I need to do something different?\nAnd of course, that's going to take a little bit of time, it's never an immediate return.\nSo you have to have a little bit of trust in the process and realize that it's going\nto take a little bit of time to sync all this new information and make sure you do all the\nthings correctly so that you can get the benefits.\nYeah, that's a good point.\n\n\nEoin: I would also add that there are plenty of resources and tools that AWS will provide,\nparticularly if you're a large enterprise trying to migrate to the cloud.\nI mean, they will clearly provide people to support that.\nThey've also got a whole set of services that will help you to do the rehosting or replatforming,\nlike migrating VMs in an automated way, doing discovery, doing database migration, but they'll\nalso help you with people that will help to create a migration plan and execute on it.\n\n\nOne of the things that you kind of have to do yourself is figure out the measures of\nsuccess and the cost equations and the overall impact and the success factors.\nThat's going to be quite challenging.\nIf you're looking at it just from a cost point of view, you really need to look at the total\ncost of ownership and the amount of time your existing teams are spending in, say, firefighting,\non-prem system, and really strategically think about, okay, if we're going to migrate to\nthe cloud, it's not really just about the different cost model and moving from CapEx\nto OpEx.\n\n\nYou've got what you should really be thinking about is how can I get out of the business\nof patching servers and troubleshooting network hardware, and how can I move to things that\nare provided by AWS so then I can free up people to execute more on actual meaningful\nbusiness capabilities.\nSo it's very hard to measure, but it is worthwhile spending some time actually defining what\nthose measures of success are going to be.\n\n\nAnd not just focusing on the easy to measure things like just the running cost.\nYeah, that makes a lot of sense.\nSo yeah, I mean, I think we've given some decent words of caution there in terms of\nwhat to expect.\nI mean, there are definitely great benefits in moving to the cloud, but it's just not\novernight.\nIt's going to take time, I guess, especially if you're a large enterprise, but the benefits\nas well actually, it's worthwhile mentioning that there's always going to be organizational\nchange challenges.\n\n\nIt's not just a technical exercise, of course.\nAnd some of that can be helped by taking that kind of lean agile approach and doing that\nproof of concept project in a bubble at the start because it helps once people see that\nit can be done and successful and you can gather some excitement around it, then people\nwill follow.\nI think it makes it easier to convince people and to show people that it can work and to\nprevent any, I suppose, unnecessary dissent that can happen when you haven't really delivered\non anything yet.\nIf you've got a long plan and you haven't actually executed on anything yet, it can\nbecome increasingly difficult to convince everybody that it's worthwhile.\nI think that's reasonably a comprehensive overview of the very complex world of cloud\nmigration and so we can probably leave it out there, leave it at that for today.\nSo thanks very much for listening everybody.\nAnd again, make sure to subscribe to our YouTube channel and keep following for more episodes\nand we'll see you in the next one.\n"
    },
    {
      "title": "19. Is the serverless DX still immature?",
      "url": "https://awsbites.com/19-is-the-serverless-dx-still-immature/",
      "publish_date": "2022-01-13T00:00:00.000Z",
      "abstract": "In this episode of AWS Bites podcast, Luciano and Eoin talk about what the serverless Developer Experience (DX) is like and whether we should consider it still immature or not.\nIn this context we discuss some of the struggles with local development, what are some tools that can make our lives easier and how we can improve the overall serverless experience.\nBig thanks to Padraig O'Brien for suggesting this topic!\nIn this episode we mentioned the following resources:\n\nLocalStack\nServerless offline plugin\nSAM Local\nSAM Accelerate\nCDK HotSwap / CDK Watch\nSST\n\n",
      "transcript": "Luciano: Hello, today we're going to answer the question, is serverless developer experience still immature?\nAnd with that question, we are going to cover topics like what the local development looks\nlike, what are the best tools that you can use for developing serverless applications,\nwhat is the serverless ecosystem, and how it can be improved.\nMy name is Luciano and today I'm joined by Eoin and this is AWS Bites podcast.\n\n\nSo before we get started, I want to thank you, Project O'Brien, for suggesting us this topic\ntoday and we remind you that we are very open to accept suggestions, so feel free to send us your\nquestions. But let's start by clarifying what is the context for today. We are talking about when\nwe mention serverless, we're talking about building applications not just with Lambda,\nbut the entire ecosystem like DynamoDB, SQS, Cognito, Kinesis, EventBridge. So in general,\nthat large variety that you get of managed services in AWS. So maybe we can start by\ndiscussing how does the serverless experience compares with more traditional ways of building\napplications in the cloud. What do you think, Eoin?\nYeah, I guess it depends where you're coming from.\n\n\nEoin: I remember when we were building monolithic applications and you essentially had one process that would run all of the various different\ncapabilities of your application, maybe talking to a database. Sometimes you could run all of\nthe code for the application in an IDE with a click of one button. Then I guess a lot of\npeople moved into microservices development where things were a little bit more fragmented\nand you had containers for different pieces of functionality. And from a development perspective,\nthings started to change around that time because you needed to run multiple containers,\nbut it was still doable. And I guess one of the things you're talking about when you're looking\nat developer experience for serverless, a lot of people will compare it to containers and Docker\nand Kubernetes and what it's like to run containers locally. One of the things containers do really,\nreally well, I think, is making sure that you've got a developer environment that performs very\nclosely to what your production environment is going to do. Like you've got an immutable\ncontainer that's going to run in the same way pretty much locally and in production.\nWith serverless, it's a lot more complex, right? It's not as simple.\nYeah.\n\n\nLuciano: And I know for instance, even with things like Kubernetes, which it is a very complex piece of machinery when you run it in the cloud, you still have very good tooling to simulate all\nthat ecosystem in one machine as you are developing. So that's an interesting thing to\nstart with. So yeah, we're trying to compare the serverless experience with monoliths,\nmore traditional way and microservices using containers. I think that the first thing that\ncomes to mind when thinking about what is the main difference between serverless and this kind of\narchitecture is that the unit, that the level of granularity is much smaller with serverless\nbecause you are generally thinking in terms of functions rather than services. Even though,\nyeah, you might argue that a set of function logically create a service, but when you are\ndeveloping a specific capability, you are writing one particular function at the time.\nSo the level of granularity is like it's immediately smaller and it's forcing you to think\ndifferently about how to build that particular capability. And it's something you can see,\nfor instance, even in terms of security, right? You are going to be able to write\none particular policy for a particular function that is extremely locked down only to the things\nthat that function is supposed to do, which is very hard to achieve in a bigger, more\nmonolithic application or even a microservice. Absolutely.\n\n\nEoin: It's a huge benefit and we might talk about that in more detail in a while, but it also brings a set of challenges then because\nsuddenly you have the kind of responsibility now to make sure that you've got minimal privilege\nwith every IAM policy you write. Yeah.\n\n\nLuciano: And that's tricky because of course, we know the pain of writing IAM policies and it's very hard to get them right at the first try. So you generally go\nby trial and error until it does what you want it to do. Exactly. Yeah. Unfortunately that is the case.\n\n\nEoin: Yeah. So I like the fact that you've got this much more fragmentation really.\nSo you move to microservices, you had lots of small pieces with serverless. Generally, you have\nmany, many more pieces. They might be grouped together into services, but you're deploying lots\nof individual pieces of code, but you're also deploying resources very freely.\nSo I suppose before serverless, you deployed infrastructure and then you wrote code and\ndeployed your code onto the infrastructure. But with serverless, that separation doesn't really\nclearly exist anymore. You're deploying infrastructure and code together in many cases.\n\n\nYou might have some base foundational resources that are more long-lived and you don't deploy\nthem every time, but you can deploy an SQS queue very easily, very dynamically. You can even create\nthem at runtime. So it's not just about how you get your code running. It's about how you get all\nof those other little pieces that build up your architecture and how you get those, how you develop\nwith that mindset then. So I suppose one of the big things, the big elephant in the room sometimes\nwhen it comes to serverless is local development. And when you've got a mix of code and AWS resources,\neach AWS resource is very complex in its own right. Can you simulate that locally? And is it\nworthwhile simulating that locally? It's the million dollar question. What's your opinion on it?\n\n\nLuciano: Yeah, I don't know if I have a fully formed opinion yet, meaning that I've been working in some projects where somehow we managed to have a good enough local environment and everyone has\nbeen happy with it. But in other cases it has been much more tricky and we ended up relying a lot more\non the real AWS environment running in the cloud, even for development. So while we change things\njust to test our changes, we end up deploying and checking things in the real AWS environment.\n\n\nSo I'm still a little bit conflicted on whether one way is better than the other. And I think it\ndepends probably on the complexity of the application, meaning are you using just few\nsimple AWS services or maybe you're using more advanced services and a mixture with many, many\ndifferent services. And I think also that depends on the tooling that is currently available.\nFor instance, I used local stack, which is actually a very good tool that allows you to\nsimulate many different AWS services locally. It runs in Docker, so you can easily run one or more\ncontainers and use them to simulate, I don't know, S3, SQS are probably the most common ones,\nbut you can simulate a range of different services. But it gets tricky, of course, when you are doing\nmore advanced things like EventBridge comes to mind, Step Functions comes to mind. And in those\ncases, you get something very basic, but you don't get the level of accuracy that you might need,\nor that it might make you feel confident that what you're doing is actually going to work\nin production. Yeah, I think that's really good point.\n\n\nEoin: I think local stack is great, as long as you know where the limits are and where you need to actually rely on the cloud. So\ndon't over rely on it and assume that it's a drop in replacement for AWS. Of course it isn't,\nand it never will be, but it can help you in certain cases if you want to have really fast\nfeedback in a local developer environment. There are some places where it does really well.\n\n\nIf you need a local S3, then yeah, it'll work with the limitations. So it can help to optimize\nyour developer workflow, but it's not going to replace ultimately the cloud. You need to\ntest your code in the cloud as quickly as possible. Question is, how satisfied are you\nwith deploying to the cloud every time you make a code change? So what is your developer flow?\nHow fast do you want your developer feedback loop to be? And when do you go from working\nin local mode to working in cloud mode? And there's a lot of different factors at play there.\nIt depends on a lot of things, including your internet connection. I've had that experience\nwhere developing on a slow internet connection and deploying a cloud formation stack and even\nuploading it was a bottleneck. So if you're working with a gigabit connection all the time\nand that's not a problem, then that's good for you, but it's not like that for everybody. So\nthe tooling has to be broadly applicable if it's going to really be successful.\nAbsolutely. That's a great point.\n\n\nLuciano: Is there any other tool that you know, aside from local stack for trying to get something locally or maybe to simplify the developer experience in\ngeneral? Yeah, there are a lot of options and there's a growing set of options.\n\n\nEoin: So for a long time I've been using serverless offline when using the serverless framework successfully.\nAnd if you've got like a traditional API stack with Lambda behind it, maybe DynamoDB behind it,\nyou can run all of that locally with reasonable results, pretty good results. It can start to\nbreak down a little bit when you've got other triggers. If you're triggering from the event\nbridge or SQS or SNS, then you need, this is where it gets a little bit more complicated.\nYou've also got SAM local, which is also pretty good. Probably even more robust than serverless\noffline because it's, I guess, built in support by AWS and also uses Docker by default to give\nyou more of an isolated runtime. So that's good. And then you've got some of these new tools that\nare coming out like SST, we were discussing earlier, right? There are some third parties\nwhich are really kind of pushing the boundaries and trying to make local development and the\nbridge between local and cloud development a little bit more seamless, letting you do\nkind of local troubleshooting. So what do you think?\nYeah.\n\n\nLuciano: And it's interesting that I haven't tried SST yet, but it seems that they are promoting more this idea that if you could get your code faster to the cloud, then maybe the\ncloud becomes your development environment. And I think this is maybe the big question.\nThe big question, is this going to become a reality or it's just because we cannot\nsimulate everything locally, this is the only approach we could reasonably take. So I don't\nknow, again, what is the real answer, but it's interesting to see that there is a push even from\nAWS itself to say, yeah, you can do things locally, but the real environment is the cloud. So try to\ndo as much as possible in the cloud straight away. And that hybrid approach, I think is still a\nreality for most of the cases where maybe you are still running your code locally, but your code\nreaches out to services that are already available in AWS, like, I don't know, read a table in Dynamo\nDB or write a message in IQ or send an SNS notification. You can definitely do all these\nthings from the code running locally. So you get more or less something that is trustable enough\nat the end of the day as you change your code. So, yeah, I don't know. Do we want to do some\nfinal remarks of what do we think is good and what do we think is bad right now?\nYeah, that's a good idea. Maybe it sounds like we've taken a kind of a negative view.\n\n\nEoin: I mean, it is, I think, let's face facts, we're serverless advocates, but the tooling is immature compared\nto container-based development. That's my view on it. And I think that's fine as long as you\nunderstand that and understand the limitations and the workarounds and keep an eye out for\nimprovements there because there's the number of benefits outweigh these disadvantages in my view.\nIf you've got a good approach to constantly improving your development environment,\nyou can live with these limitations. But I think there's a lot of scope for good improvement as\nwell. So what do you think could be better?\n\n\nLuciano: Yeah, I think what can be better, as we said, is the ability to be able to push code faster. And we have seen before and during re-invent\na number of promising initiatives from AWS. For instance, I remember we discussed some accelerate\nor also some sync, sometimes it's referred to, which should allow you to be able to\nsynchronize the code in your Lambda if you're using some, like straight away. As you do a change,\nthere is probably like a watching mechanism that will automatically publish your code without you\nhaving to invoke a separate command which triggers cloud formation and so on. So that should give you\na better way of just changing your code and seeing the changes straight away.\nAnd similarly, I think there is something called CDK-Otswap. If I understand correctly,\nit will work with CDK and it will allow you to update Lambdas, but also ECS, code inside\nECS containers, and also step functions, state machines, definition. And as far as I understand,\nthat doesn't use cloud formation. So that's probably why you get a much faster feedback.\nProbably you don't get rollbacks and all the other things that cloud formation gives you,\nbut for development is probably fine. Yeah. I mean, that's what it's about.\n\n\nEoin: It's not about making your production deployment faster because that needs to be safe and predictable using cloud\nformation, but it's about just getting code up to AWS faster and running it faster. Of course,\nwe said at the start that serverless applications aren't just about the code. They're about all\nthose other resources that comprise your architecture. So that's one of the things\nthat can slow you down and could be improved. If you want to deploy a new queue, we create\nand destroy AWS resources all the time when we're in development mode on AWS with serverless\napplications. So how can you make that faster? If cloud formation was orders of magnitude faster,\nthat would make the developer experience instantly way, way better. And it can be a bottleneck.\nYou also mentioned security, Luciano, getting your permissions right. When you're in that\nphase of development where you're constantly tweaking IAM policies, what's the best way\nto do that? Are you updating the policy programmatically? Are you using the console\nto update your policy? If you're using cloud formation every time, you have to wait for\nthe stack to update before you can test every version of the policy. So improve tooling around\nthat and maybe better predictive generation of policies and validation of policies.\nSome intelligence in there would go a long way to improving the developer experience. I really\nthink so. I absolutely agree. I think that this is all we have for this episode.\n\n\nLuciano: And I'm really curious to know what you all think about this and what are you doing today to build your\nserverless applications. If you found any sweet spot that works well for you, we are really\ncurious to know about that. So definitely share your experience with us. And please\nremember to follow us and subscribe so you can be notified the next time we publish an\nepisode, which is generally every Friday. So we'll see you next time. Bye.\n"
    },
    {
      "title": "20. Should I get an AWS certification?",
      "url": "https://awsbites.com/20-should-i-get-an-aws-certification/",
      "publish_date": "2022-01-20T00:00:00.000Z",
      "abstract": "In this episode of AWS Bites podcast, Eoin and Luciano talk about whether it is worth to get an AWS certification and why. We discuss why a certification can be important from the perspective of individuals and companies, what are the certifications available and how are they grouped. Finally we try to provide some suggestions for a study plan and give various useful resources and tips.\nIn this episode we mentioned the following resources:\n\nOfficial AWS certifications landing page\nPassing all the AWS certifications, article by Adam Elmore\nLuciano’s post about AWS Solution Architect Associate exam notes and tips\nUdemy\nTutorialspoint\nAdrian Cantrill’s AWS certification training material\nA Cloud Guru\n\n",
      "transcript": "Eoin: Hello today we were going to answer the question should I get AWS certification\nAnd we're going to cover why a certification can be important from the perspective of individuals and companies\nWhat are the certifications available? We're going to give some suggestions for a study plan and various resources and tips\nMy name is Eoin and today I'm joined by Luciano and this is the AWS bytes podcast\nThe first question I think we can answer is is it worth it to get AWS certification?\nDo you want to try and answer that Luciano?\n\n\nLuciano: Yeah, I guess that's it's the good first question and the answer of course again as usual is it depends and\nI suppose we can discuss different perspectives\nThe first one is you are somebody that already has some degree of experience with AWS or the cloud in general\nAnd in that case, maybe it's a little bit trickier to say yes or no\nIt yeah, it depends on your current situation\nBut most likely it's gonna add less value to you because you already have experience you can probably prove\nIn your workplace or if you're doing an interview that you already know your stuff and you don't need a certification\nBut if you are just starting your cloud journey or your software engineering journey, and you are interested in the cloud in general\nTrying to get a certification could help you a lot boosting your well first of all your self-confidence\nBut also going into interviews or trying to grow in your existing workplace\nIt can be like very beneficial to get that certification\nAnd on the other side other than just the title itself. I think going through the journey of studying for the certification\nIt's something gives you a very good overview on what are all the different services. What are the things that are more important?\nHow to learn all the different concepts like in which order it makes the most sense\nI think a certification is definitely gonna give you all that good\nbase to keep growing from there\nDo you agree with this perspective or is there anything else you might want to add?\n\n\nEoin: Yeah, I do agree And my own experience of it as well\nI probably previously had that common perception that as long as you had the experience you don't really need the certification\nBut I've been through the journey with the AWS certification\nand I've also realized that\nThat kind of perception kind of comes from a position of privilege as well\nAnd it's less true for underrepresented groups in tech\nSo in the end I've heard many cases where people have said that where you're fighting that bias all the time\nHaving a certification can really help to overcome that\nSo I think that's that's really worthwhile to mention because everybody's context is very different\nBut in general, I think there is a lot of value to certification for a lot of people and it does help for hiring, right?\nSo it can kind of give you access to better roles and salaries. Like you said it can help you get jobs and\nChange your role in your existing company and it really helps for companies that are early on in the cloud journey\nIf you've got a lot of people already with a lot of experience in-house, it may matter less\nBut if you're trying to build a lot of expertise, it's something that\nboosts credibility, but also helps to\nFocus people on AWS and all of the different options available there and also build a culture probably of\nProfessional development in that area within the company and you'll have people encouraging each other and\nExchanging tips on how to achieve the certifications and when they see other people doing it, they might want to be more encouraged to do it themselves\nWhat do you think are there other areas that\nAWS certification is useful for that we didn't mention yet?\n\n\nLuciano: Yeah, I think another interesting one could be if you are a freelancer or like a consultant as an individual Then having a certification is definitely something it's like a badge that is worth having because it boosts immediately your credibility in the market\nBut at the same time it's other than the budget itself\nThere is for instance the AWS IQ platform\nWhich I think is available in the States and UK maybe some other countries in Europe, I think France\nAnd of course that it's very likely they will open it up to more and more countries in the next few years\nBut the point is that if you are in that platform\nIt's a platform managed by AWS where you can find customers as a freelancer and having certification is definitely gonna give you more\nVisibility so you'll be able to access more opportunities to get hired for four projects and there is a very good blog post\nI think it's a blog post by Adam Elmore where he discussed his own experience\nHe has achieved all the available certifications and he discussed\nHow doing that and being in that platform has massively increased his chances to get interesting jobs\nAnd grow his freelancing career. So that's an interesting case study\nIt's probably like the very extreme but it's good to see that that perspective in my opinion\nYeah, it was an interesting one.\n\n\nEoin: He did 12 in six weeks, I think which is a pretty commendable achievement Do you think it's useful for employers? Like if you're an employer you're hiring a lot of people and you're doing a lot on AWS\nHow do you perceive people who do and don't have AWS certification badges?\n\n\nLuciano: Yeah, I think from the perspective of an employer there are definitely two possible positions One is that you are trying to hire somebody and then seeing that certification might be more or less important to you\nIt's definitely as we said like an additional signal of credibility, but I would say trust but verify\nOf course as in anything we know you there is still a factor that you can game a little bit the exam and get certified\nSo the certification is definitely not a replacement for experience. Make sure you validate that certification\nThat certification may be asking the candidate for specific examples of projects they've been working on\nAnd what they think is more important. What did they learn? So definitely don't just rely on the certification to hire somebody\nOn the other hand, if you are an employer you probably want to care about certifications even for the people you already hired\nThey might be interested in getting a certification and I think if that happens in your company I think you should support that\nEspecially if AWS is something you basically work on it every day you need that experience\nAllowing your team to get a certification can definitely increase their productivity, can increase their skill\nBut not just that you are also supporting every team member to grow on their career so I think that's definitely something good to do in the company anyway\nAnd my immediate recommendation would be don't just say yes do it but actually support them for real\nAnd that could mean a number of different actions for instance help them to create a study plan\nFind a portion of their work time that they can spend studying, maybe give them access to paid material\nAnd also it could be interesting to pay for the actual exam as a company I think it's more convenient to do that rather than letting them spend their own money on the certification\nSo I would definitely recommend that for employers\n\n\nEoin: Yeah absolutely, so we should probably give then an overview quickly of the different certifications available And we've got various tiers right so the first one you would probably start with is the foundational training\nYou know if you're a developer and or on that track the developer the foundational one is your cloud practitioner certification\nAnd that's going to give you an overview of basic understandings of cloud concepts and the various AWS services\nAnd it is what it says, it's a good foundation and I think that's something that you could do without having significant experience\nI think they give a guideline of six months of experience with AWS but six months of experience could mean a lot of different things\nAnd it's something you could probably do without having much hands on keyboard experience with AWS\nAnd it'll help you to navigate AWS a little bit better so I'd definitely recommend that for anyone getting started\nAnd the next up is associate so then within the higher tiers you've got a division between architect and developer and operations\nSo I think we've both followed the architect track and you've written a particular blog post around solutions architect\nSo maybe do you want to talk about that one and how the associate solutions architect works and what it gives you\n\n\nLuciano: Yeah I wrote a blog post I think it was almost three years ago at this point so I hope it's not too outdated But I try to give like generic tips from what I've seen so maybe most of them are still relevant\nBut my experience with that particular certification is that it is really good because it really lets you focus on the pillar type of knowledge\nLike principles of cloud design but also the most interesting services like S3, DynamoDB, EC2\nAnd you really get to learn what are the characteristics of those services and how you should rely on some particular characteristics\nNot just the strengths but also the weaknesses of every different service\nOne thing that it might be a little bit annoying or at least it was for me is that there are a lot of questions that are like very specific\nSo you end up needing to memorize a lot of concepts like I don't know how to do capacity planning with DynamoDB\nThere are questions where you really need to do the calculations so you really need to know by heart the different values there\nSimilarly there are questions around the different number of nines that you have for all the different characteristics of S3\nDepending on which kind of storage do you end up using\nSo it takes a little bit of effort in memorizing certain concepts so that might be a little bit annoying for some people\nBut then those things are really useful like you really need to understand why those things are important\nSo it pushes you in the right direction in my opinion\nAnd I'll just leave a link to the article there are a lot more tips in terms of how to manage time, how to do the exam\nAnd also some links to resources that I use to study\n\n\nEoin: But yeah that might take another episode if we want to go too much into detail in that Absolutely, yeah and once you've got your associate then you probably look at professional\nI think it's interesting that they say if you're looking at associate you want to have one year experience\nAnd then you want to have professional two years\nI think I was, I'd been working with AWS for about five or six years before I did any certifications\nSo it's, your mileage is going to vary here\nI would suggest that if you're going to do the professional two years of experience is actually not a lot\nI mean it would certainly mean that you probably have to study a lot more and put a lot more effort into it\nAnd rely less on your experience and kind of inherent knowledge that you've built up\nProfessional exam is a good jump in challenge\nAnd it's, I would suggest it's probably harder to bluff\nI mean you can always cram and study and just try and memorize everything\nBut professional is definitely a significant challenge\nI've heard some people describe it as the hardest certification in tech\nI don't know if that's true or not or that's just hyperbole\nBut yeah with a lot of experience it'll help\nI'm not somebody who particularly enjoys the exam experience\nI know that Adam Elmore in his blog says he loves doing exams\nSo doing 12 exams in six weeks was enjoyable experience for him\nFor me it's like pulling teeth so I'm at the other end of the scale\nI'm glad to have it over with\nBut yeah I think if you're doing professional I think there is a lot of credibility associated with that\nAnd it's certainly a good validation of your experience like we said\n\n\nLuciano: Yeah I think in the professional one the main difference I'm seeing, I'm preparing for that one right now So take this with a grain of salt\nBut what I'm seeing by doing the mock exams is that there is a lot more of practical knowledge\nMost of the questions are you need to build this, what's the best solution for this particular use case\nOr there is a particular problem in an existing architecture\nWhat can be the most likely cause for that problem\nSo you really need to know a bunch of different architecture which is not something you can improvise by just reading around\nSo I think having that experience makes a lot of difference in being able to pass the exam\n\n\nEoin: One of the things I haven't done is the specialty exam Or the specialty certification track and there are various different ones like advanced networking\nDatabase, machine learning, security\nWhere do you think those would fit or do you think it's\nIs it for people who are really specialists or do you think it's also something that if you're like an AWS generalist\nIt would also be worthwhile pursuing one of those\n\n\nLuciano: Yeah I haven't done them so again I have a very kind of distant opinion So take it again with a grain of salt\nBut my guess would be that you can either be somebody that is building up interest in a particular area that you haven't been exposed to\nSo you might take that certification as almost like a challenge to see what that area of AWS looks like\nWhat kind of knowledge do you need to build\nAnd maybe you will end up learning a bunch of new things and why not decide maybe to do a little bit of a career switch\nTo specialize even more in those areas\nSo that's definitely an extreme. The other case could be that you already have a lot of knowledge in that area\nSo you want to get certified that you know your stuff and maybe that can give you access to more specialized roles in your existing company\nOr maybe do interviews for very specialized roles that you are seeing in companies that you really like\nSo maybe that's another option why you should consider one of those kind of certifications\n\n\nEoin: Yeah this is a good one For people actually thinking about the exam or about to embark on it or in the middle of study\nHow do you suggest preparing for the exams\nYou mentioned your blog post which will definitely link in the show notes\nAnd I suppose I've got mixed experience you know experience is obviously good preparation\nYou need to look at the certification guide and see what the topics and syllabus is\nBut there's plenty of resources out there sometimes it's a mind boggling array of content\nSo I know that when I was looking at the professional exam I was using a cloud guru which is incredibly popular\nAnd a lot of companies use it and a lot of individuals use it\nI found it to have slightly mixed advantages and disadvantages\nIn that it was very good content a really good platform and the sandbox environments they give you to experiment with stuff is very useful\nOne of the things I found is that it seemed to lack a little bit of depth\nAnd that a lot of the detailed topics which are required for professional level weren't explored in a lot of great detail\nBut instead they just told you to go and read the white papers\nSo I think it's worth stating that I found you know it's definitely worth looking at\nIt's just at that professional level I thought it could do with some more detail\nAnd one of the other platforms I found other people recommend quite a lot for that is Adrian Cantrell's platform\nWhich is you know his own platform but he's got a lot of detailed tutorials there so we'll link that through as well\nYou mentioned the practice exam so where do people go to get practice and to see what they're like\nBecause that's something you could do even before you apply for the exam just to get a sense of readiness\n\n\nLuciano: Yeah as far as I know I have been doing the mock exams in three different platforms One is a Cloud Guru itself they do have that feature as well and I think that one is actually quite well done\nBecause it doesn't just give you like okay correct answers you did pass or you did not pass\nBut for every single question after you do the exam you have an entire like in-depth section that tells you how to approach that question\nIn the case you failed why maybe you failed it like maybe some tips on okay look for this keyword next time\nBecause that really has a lot of value so it can give you I think it can give you like a good way of approaching the exam\nEven if you just fail it I think you will just learn a lot on how to approach the next time you try to do the exam\nSimilarly there is one on Udemy that I'm currently using for studying for the professional certification which is built pretty much in the same way\nIt's a mock exam you get the time that you really get during the exam you try to answer the questions and at the end you get your own score\nBut also for every single question you get a lot of context and suggestions\nAnd then there is the AWS itself allows you to do practice exam I think you need to pay some small fee to do that\nBut I would recommend doing that before any exam because chances are that the AWS one is much closer to the one we will actually have to face during the exam\nIn terms of actual content and type of questions so these are the three I know and I used in the past I don't know if you have anything else there\n\n\nEoin: I've also used the Udemy ones which I think are essentially resold from the tutorials point Tutorials is the same content and you know they're relatively inexpensive and they're often discounted to like 20 euros that level\nThe one thing I found about them is that I thought they were more challenging than the real exam\nSo from that point of view it's quite good because it sets a higher bar than the actual exam and I found that the questions were often longer, took longer to read\nA lot more subtle detail in them. In fact the first time I did a cloud partitioner mock exam with Udemy it really frightened me into getting my act together\nBecause it was so challenging and I flunked it so badly the first time\nI think that's good right because it really helps you to prepare by really setting a high bar and then you actually find that the exam itself isn't so bad\nSo I really recommend those\nWhat about during the exam itself? So you've got two options for the exam now\nPreviously I think you could only do the foundational level ones online and everything else you had to go to a test centre\nBut with Covid everything is now available through what they call online proctoring\nThere are two platforms for that one is Pearson View and the other is PSI\nAnd I've had experience with Pearson View and I know other people who have had experience with both\nAnd I just wanted to relay some of my experience with those because there's good and bad\nIf you can get to a local test centre safely and do your exam I would probably recommend that because you don't have to worry about the environment\nYou just turn up, sit at a desk, do the exam and leave with your result\nWhen you do the online proctoring it's an advantage that you can do it from your office or from your home\nBut you really have to make sure that the environment is correct so in advance you have to make sure that the room is clear, your desk is clear\nThere's no devices or books or paper within reach\nYou will have to use your phone to take a picture of your environment from all angles and somebody's watching you for the duration of the exam\nAnd it's quite amusing sometimes because if you're thinking about a question and you're focusing and you might put your hand over your mouth\nOr you might mumble to yourself they'll tell you to stop because they don't know that you're trying to communicate with somebody outside the room\nOr you're trying to work around the system in some way or cheat\nSo it's a little bit amusing and kind of restrictive in that sense, you have to put time into preparing for it\nAnd the platform doesn't always work unfortunately\nThe first one I ever did with online proctoring didn't work at all, I had to cancel it and it took weeks before I was able to reschedule it\nWhich is really inconvenient\nI know a colleague of ours at Fortheram had that same experience recently, it's no fun when that happens\nAnd I've heard about problems with the actual platform on both options there\nSo there are pros and cons to it, it is nice that you have that option to do it online\nBut you really have to prepare, make sure the environment's working and then hope that it just works on the day\nAnd I'm sure it usually does but you know things happen, it's technology\nWhat about other preparation? During the exam itself Luciano, I think you had a lot of good recommendations on how to approach the exam in your blog post\nDo you want to talk about some of those?\n\n\nLuciano: Yeah, I think at least what was my approach after failing a few mock exams I started to develop a little bit of a system for how can I be more effective\nBecause I think the main concern is that it's not just challenging because of course it's an exam, you need to have the knowledge, there is a stress factor\nBut also from what I've seen it's very tight, you have a certain amount of time and a lot of questions and you need to really manage your time well\nSo what I would recommend is be aware of the time but also don't let that stress you out, don't freak out just because you see the counter going down\nTry to focus on the questions, try to allocate enough time for you to understand the question well enough\nAnd I think when you start to do a few mock exams you'll start to realize that there are some patterns\nFor instance some keywords will have a certain weight in a question or even in an answer\nAnother thing that happens often is that you have to pick between possible answers and some of the answers are almost identical\nSo basically there is one or a few keywords of difference between each other\nSo most of the time that means that the choice is either one or another question so you can probably ignore the other two\nBut that means that it's really important that you understand the difference between those few keywords\nSo really try to do the exercise of seeing, okay, this is just not the most likely answer because there is another one that is almost identical\nSo try to focus on finding out what's the difference, why the difference might be important\nAnd other than that if you see that you are spending too much time on one question there is an option to flag the question you don't have to answer straight away\nSo you can leave it for the end and do all the other questions and then before submitting your results, your answers, you can go back to the flagged ones\nAnd try to figure it out with a fresher mind if you can understand what the question means and if you can find a good answer for it\nThose are I think my basic suggestions, I don't know if you have anything else\n\n\nEoin: No, I'd agree with that, yeah, if you've got time left over at the end rather than walking out early take the time to review the answers and flagging the ones where you had some doubt is a good idea I'm really interested to hear what other people's experiences are with certification and if people have any questions about approaching certification we'd love to help and we'd love to learn more about what other people's experiences are\nSo let us know in the comments or ask on Twitter, LinkedIn, wherever you find us and wherever you hang out in YouTube comments\nIf you enjoyed the episode give us a thumbs up, make sure that you subscribe on YouTube or follow the podcast and share it with your friends and colleagues and we'll see you in the next episode\n"
    },
    {
      "title": "21. What services should I use for events?",
      "url": "https://awsbites.com/21-what-services-should-i-use-for-events/",
      "publish_date": "2022-01-27T00:00:00.000Z",
      "abstract": "In this episode of AWS Bites podcast, Luciano and Eoin talk about AWS services related to events and message passing like SQS, SNS, Event Bridge, Kinesis and Kafka (MSK).\nWe discuss in which context is convenient to use messages and events and we deliver a quick walkthrough of all the services discussing major features and some practical examples on how to use them.\nIn this episode we mentioned the following resources:\n\nSNS\nSQS\nEvent Bridge\nKinesis\nKafka (MSK)\n\n",
      "transcript": "Luciano: Hello, today we're going to answer the question,\nwhat services should they use for events?\nAnd we are going to cover in which context\nis convenient to use messages and events.\nWhat are the major services available in AWS\nfor these particular cases?\nAnd then we are going to basically discussing\nall the services at high level,\nwhat are the major features and some of the examples.\nMy name is Luciano and today I'm joined by Eoin\nand this is AWS Bites podcast.\nOkay, so Eoin, do you want to start by giving us an idea\nof contexts and use cases where it might make sense\nto use a message passing or AWS services related to that?\nYeah, there's a lot to cover here.\n\n\nEoin: And I suppose we're talking about asynchronous communication\nbetween services or components or systems.\nThere's a lot of things you can do\nwith all of these services,\nbut we might be talking about integrations\nbetween two different systems,\njust communication between microservices\nand event-driven serverless architectures,\nwhich are really exciting when you look at what you can do\nwith things like Lambda and all these services.\nYeah, so I think we've got,\nthere's a lot of things we could cover.\nWe're going to cover the five different services,\nSQS, SNS, EventBridge, and then Kinesis,\nbut also Kafka, managed streaming for Kafka, MSK.\nSo should we start with SQS,\ngiven that it's, I think, the oldest AWS service,\nwhat would you use SQS for?\n\n\nLuciano: Yeah, I suppose one first good way of seeing all the services\nis trying to understand like high level,\nwhat are the categories?\nFor instance, talking about SQS,\nI will say that SQS is more like point-to-point\ntype of communication.\nYou generally have one producer that is creating tasks\nor task definition, job definition,\nwhatever we want to call them.\nThey are stored in a queue.\nSo this idea that you push things from one side\nand you consume them from another.\nAnd generally you have one consumer\nor a group of consumers,\nbut they are kind of consuming the same type of tasks.\nYeah.\nSo you generally would say that it's like one producer,\none consumer type of configuration when you use SQS.\nFor any usage, yeah.\n\n\nEoin: Yeah.\n\n\nLuciano: While in comparison, we have something that is more PubSub,\npublish and subscribe event,\nand I would probably put SNS and EventBridge in that bucket.\nAnd the reason is that with PubSub,\nyou generally have, again, one producer,\nbut on the other end, you might have from zero\nto many consumers.\nLike you are just saying, this is happening.\nThis is the definition of an event.\nNobody might listen to it.\n\n\nSo the event just phase away,\nor you might have one or more consumer actually reacting\nto that particular event.\nAnd then the other case or group, if you want,\nis a more streaming based use case.\nAnd this is where I will put Kinesis or MSK.\nAgain, you have one producers, zero to many consumers.\nThe difference that messages are,\nI would say somewhat more important\nbecause they are persisted and they're kept around,\nand that gives you more features.\nFor instance, you can replay them.\nAnd also generally streaming as higher throughput.\nSo definitely a good candidate for Kinesis or MSKs\nwhen you are really processing high throughput data\nand you need to do like real time operations\non the incoming data.\n\n\nEoin: That makes a lot of sense.\nYeah, so should we start with SQS first then\nand think about some of the use cases\nand where would you use it?\nWhat do you think?\n\n\nLuciano: Yeah, so in general, I will use SQS in all those situations\nwhere I want to make a system more resilient\nbecause you can just put a queue\nbetween a particular operation.\nSo whenever you receive the input\nand where you are producing your output,\nif you put a queue in between,\nthen that gives you the ability of first of all,\npersisting all that input,\nthen decoupling that the collection of the input\nfrom actually doing the work to produce the output.\n\n\nAnd with that, you get a lot of features like,\nI don't know, if something fails, you can easily retry it.\nAnd you create that letter queue.\nIf you are actually unable, you retry it multiple times\nand you are still failing, you can save it.\nAnd somebody can analyze it manually,\nfigure out what went wrong.\nAnd then from that letter queue,\nmaybe you can re-ingest the message and execute it again\nafter you realize what was going wrong.\n\n\nSo that's generally a good use case for SQS,\nbut it can also help for performance.\nFor instance, you can use a queue\nfor every single action in a system\nwhere maybe you are receiving a user request,\nyou need to do some actions,\nbut those actions can be postponed.\nYou don't need to do them right now just to answer,\nto produce a response for the user.\nJust few examples, very common ones.\nA user does something, you need to send a confirmation email.\nYou don't need to do that in line in the user request.\nYou can just save it to a queue\nand a background process can pick it up\nand actually send the email.\nVery similarly, a user is uploading a picture,\nyou need to resize it.\nYou don't need to let the user wait in line\nwhile you are resizing the pictures.\nYou can just say, okay, the picture was received\nand you can do all the resizing\nin the background using a queue.\nAnything else worth adding?\n\n\nEoin: I think that really covers it very well.\nLike if you imagine at a very simple level,\nif you've got a synchronous request\nprocessing flow right now,\nif you want to add resilience to that,\njust put a queue in the middle.\nAnd that's a good first step\nto making your architecture more resilient\nand more performant.\n\n\nLuciano: Yeah.\nDo you want to move to SNS then?\n\n\nEoin: Yeah, yeah.\nSo SNS is, back to the categorization,\nit's a pub sub mechanisms.\nSo instead of having a point to point thing like SQS\nwhere you typically have each message being processed\nby one consumer, with SNS,\nyou want to target multiple subscribers.\nSo a subscriber could be something like a Lambda function.\nIt could be one of the other things that SNS supports\nlike email and SMS messages.\nThat's how it's kind of got started.\n\n\nBut it's essentially when you want to be able\nto publish something and have other systems react,\nbut you don't necessarily know\nwhat those other systems are in advance.\nYou can anticipate multiple potential subscribers\nin the future.\nSo the thing about SNS is that you need to create\na couple of resources.\nYou need to create both the topic resource\nand then you need to create one or more subscription\nresources in order for messages to flow.\n\n\nAnd it's also not inherently resilient.\nSo SNS, if you want your messages to be stored,\nyou would typically have an SQS subscriber\nand then have your subscriber action for actually react\nto the queue rather than the topic directly.\nAnd that's very typical way of doing a fan out with SNS\nand then multiple SQS queues at the end.\nSo the examples where you'd use them, I think,\nare in a microservices architecture,\nif you want to communicate events between domains.\n\n\nI suppose, if you imagine an e-commerce application\nwith multiple microservices,\nand let's say you've got an order service\nand an order is created following a web request,\nthen you might have an analytic service\nthat picks up that order event and stores it\nin the data warehouse to do some analytics\nor ETL downstream.\nThen you might have another service\nthat sends a confirmation email to a customer\nand another service that starts the fulfillment workflow.\nSo if you're doing this kind of event-driven orchestration,\nthat's one of the ways you could do it.\nSNS has so many different use cases.\nWherever you would use something,\neven like a topic in some of the traditional services\nlike ActiveMQ or RabbitMQ, you could use SNS instead.\nAnd it's pretty performant as well.\nSo the throughput is pretty good.\nAnd so that brings up an interesting point\nbecause SNS and EventBridge can seem quite similar\non the face of it.\nHow would you describe if somebody was choosing,\nshould I use SNS or should I use EventBridge?\nThey both seem like PubSUB.\nWhat's the difference?\n\n\nLuciano: Yeah, that's a good question.\nAnd if I have to be honest, I was a little bit confused\nmyself when EventBridge was announced to try to understand,\nOK, why a new service when we have SNS?\nAnd I think it's just it covers a very similar space.\nSo definitely there is an overlap between the two\nservices.\nIt's more the way that they provide you\nall the tools and features that you might need to fulfill\nthat particular need.\n\n\nSo I think EventBridge is definitely more flexible than SNS\nand in different ways.\nSo just to try to make a list, the first obvious difference\nwe will notice is that with EventBridge,\nwe don't need to explicitly create topics.\nYou have a default bus, it's called,\nand you can just use that to publish all your messages.\nAlso, the other thing is in the way\nyou consume those messages with SNS,\nyou are basically listening on that particular channel\nfor all the events that are published on that channel.\n\n\nWith EventBridge, the mechanism is based on pattern matching.\nSo you could describe pretty flexible patterns\nthat will allow you to capture even different types of events\nwith just one expression.\nSo for instance, you could say, OK,\nI want to listen for all the events produced\nby a particular source, all the events that\ncontain a specific ID in a given field of the event.\nSo you can build all the patterns\nin the way that makes the most sense for your application,\nand that can be very powerful.\n\n\nOther than that, it is very interesting\nthat EventBridge supports out of the box what I will call AWS\ninternal events, if you want.\nSo not necessarily specific for your application,\nbut events that happen in the context of a given AWS account.\nAnd that's something you can use to build specific integration.\nFor instance, you could listen for a particular step function\nthat is changing its own state, and maybe you\nwould want to react to that.\n\n\nOr very interestingly enough, you can listen for S3 events.\nSo you can listen for new files on an S3 bucket,\nor files that are updated and deleted.\nAnd similarly enough, you can listen for CrowdTrail events.\nAnd in all these cases, you can still use pattern matching,\nso you can have very sophisticated ways\nof capturing very specific events that\nmatter to your application.\nOther thing is that it supports many more targets than SNS.\n\n\nFor instance, you can use EventBridge\nto basically propagate events to SNS itself, SQS, Lambda,\nstep function, log groups, event batch, an EC2,\nor even other event buses in other regional accounts.\nSo definitely much more powerful in terms\nof all the different ways you can distribute the messages.\nFinally, there are other features,\nlike you can have schema, the schema registry where\nyou can visualize the shape of all the messages that\nare going to the bus.\n\n\nYou have discovery functionality so that every time there\nare new messages, the schema is registered,\nand you can use it.\nAnd then you can archive the messages for long retention,\nand you can even replay the messages.\nSo definitely much more features from EventBridge\ncompared to SNS.\nIn terms of examples, I guess they\nare pretty similar to what you will do with SNS.\nSo I think your re-commerce example is still valid.\n\n\nYou could implement that even with EventBridge.\nDefinitely.\nBecause you can also listen to AWS-specific events.\nAnother example could be you are interested in files\nthat are uploaded to an S3 bucket, maybe,\nand you can build easily a pattern that\nwill capture those events.\nAnd you can do, I don't know, a virus scan, maybe,\non files that are uploaded by users.\nOr maybe if those are text files,\nyou can pick them up and index their content\nso you can create a search functionality\non top of these files.\nAll of that is something you can build easily\nbecause you don't need to create an event bus for them.\nYou don't need to create the events yourself.\nBut they will happen automatically\nin the context of your AWS account.\nSo yeah, that's all I have.\nAm I missing anything important from your perspective?\n\n\nEoin: I think that covers a huge amount.\nOne of the things I've seen is that the performance\ncharacteristics of EventBridge are slightly slower compared\nto SNS.\nSo that's one consideration people\nmight want to bear in mind.\nSometimes if you need to process it within a few hundred\nmilliseconds, SNS might be the best option.\nBut generally, for these kind of events,\nmost of the events in EventBridge\nare going to arrive pretty quickly anyway.\n\n\nYou may have some outliers that are a little bit slower.\nSo that's one thing to bear in mind,\nespecially when you're doing this common event\nprocess you mentioned.\nIt's a very common thing to have an event bus for all\nthe lifecycle events for all of the resources\nin your application get published.\nBut if you've got performance characteristics around that,\nthen you might want to think about something else.\n\n\nTraditionally in microservices, people\nmay have used Kafka or a stream processing system in the past.\nSo maybe that brings us nicely along to the other category\nwe mentioned, Kinesis and Kafka and stream processing.\nSo I think we mentioned that it's\nsuitable for high throughput.\nSo definitely with these Kafka and Kinesis,\nyou're going to get lower latency,\nlike really low latency.\nSo when you really want low latency\nand have to react in milliseconds to events,\nthis is what you need to be thinking about using\na stream processing.\n\n\nSome people do use these stream processing systems\nas pub sub buses because you can.\nThey all have a concept of like a channel or a stream\nthat you can treat like a topic.\nAnd then you have consumers for it.\nThe difference is that the way they work\nand the way data is stored is completely different.\nIt's almost like instead of a message bus,\nit's almost like sequential lines in a file.\nAnd the consumers are just pointing at a given line\nnumber.\n\n\nAnd that's kind of a simple model\nfor how those things work.\nBut what that means is that you can get guarantees\naround ordering.\nAnd that is one of the fundamental things that\ncan be beneficial with Kinesis and Kafka.\nBut you've got retention.\nSo Kinesis will allow you to store messages\nfor up to a year.\nKafka, you can serve it forever.\nBut you have to think about scalability\nbecause if you're looking at a Kinesis consumer,\nthere's consumption throughput limits you need to think about.\n\n\nThere are producer throughput limits.\nAnd in all these cases, you need to size the cluster\nif it's Kafka, or you need to size the streams\nif it's Kinesis.\nSo you really have to think about the numbers\nand the mathematics around your event flow\nif you want to use those things.\nSo there's more of an investment,\nI would say, needed for them.\nIf you can get away with using SNS, SQS, and EventBridge\ninstead, it's going to be much simpler.\nBut they do have their places.\nSo I guess, yeah, maybe we should give some examples.\nDo you have actually some examples, Vigianno,\nthat you could say are good?\nIllustrate the differences between stream processing\nand what you'd use it for as opposed to a PubSub?\n\n\nLuciano: Yeah, absolutely.\nSo one example that comes to mind is, for instance,\nyou have an application where you\nhave users interacting with, let's say, for instance,\nproducts.\nAgain, just to stick with the e-commerce example.\nMaybe you would be really interested in trying\nto observe what the user is doing in the page\nto understand maybe what other products you\ncan suggest to that user.\nAnd you could implement something\nthat captures all the user clicks,\nbut not just the clicks, maybe even\nthe user offering specific elements in the page,\nor even scrolling the page and looking\nat a specific area of the page.\n\n\nAnd you can create streams where you are basically\nsending all these events in real time.\nAnd then you can have analytics consuming\nall this information real time and responding\nwith suggestions for other products\nthat the user might be interested to watch and maybe\npurchase.\nOr other examples that I've seen are, in general,\nwhen you need to transfer large amounts of data\nbetween systems.\nAnd you need to do that as quick as possible.\n\n\nFor instance, logs or network traffic,\nmetadata about network happening in a particular, I don't know,\nprivate network or something like that because you might want\nto do a security analysis or things like that.\nOne cool thing about Kinesis is that basically you\nsend messages in batch.\nAnd you can immediately receive the batch\nas soon as it's available.\nSo for instance, as soon as you accumulate, let's say,\nI don't know, 30 messages in a batch,\nand that can happen in literally single digit milliseconds,\nyou will receive that message.\nOr alternatively, you can define time-based window.\nSo if the batch is not complete within one second,\nI still want to receive it straightaway\nbecause I want to process as fast as possible.\nSo I think Kinesis will give you these capabilities that\nare not easy to replicate with something like SNS or Event\nBridge.\n\n\nEoin: Yeah, for sure.\n\n\nLuciano: Yeah, I think with that, we have a good overview of Kinesis.\nMaybe it's worth spending a little bit more time trying\nto highlight some of the differences with Kafka, MSK,\nand why you might want to use that as an alternative\nto Kinesis.\nWhat do you think?\n\n\nEoin: Yeah, that's true.\nBecause Kafka, I guess, is similar,\nbut has way more features than Kinesis and Minkisa,\nKinesis is deliberately simple.\nYou have to size it correctly.\nEven both of those services recently\nannounced they have serverless mode.\nSo it's kind of auto-scales a little bit.\nIt's not completely serverless, but it helps.\nYou still have to think a lot more about sizing.\nWith Kafka, you'll get more features.\n\n\nIf you've already got Kafka, I would say,\nit's a reason you would use MSK.\nIf you don't already have an existing investment in Kafka\nand all of the tooling and the ecosystem around it,\nwhich is really large and fairly complex,\nthen it's probably OK to go with Kinesis\nif you need stream processing.\nAnd I think that's really where MSK fits, is really\nfor people who are migrating to AWS,\nand they've already got Kafka somewhere.\nBut it does have a huge feature set.\nSo if you've got lots of advanced stream processing\nuse cases at scale, then it's definitely worth a look.\n\n\nLuciano: Yeah, absolutely.\nI will probably summarize it that Kafka\nis more of an industry standard for this kind of task.\nIt is.\n\n\nEoin: It's de facto.\n\n\nLuciano: So if you probably work in multi-cloud environments\nor, I don't know, run your own Kubernetes clusters,\nprobably it's better to use something like Kafka\nbecause you will have it available in all\nthe environments you need it to be, while Kinesis is AWS\nspecific, so you can pretty much run it on AWS.\nAnd yeah, I think that brings us to the conclusion\nof this episode.\nJust to summarize, I think you can probably\ncover 90% of the use cases by just sticking with SQS\nand then SNS or EventBridge.\n\n\nAnd then you can use only Kinesis or Kafka\nfor more advanced use cases where you need high throughput\nor where maybe you need to store all the events for a long time\nand you need all the additional features\nthat you get from something like Kafka.\nSo with that, I think we have covered pretty much all\nthat we wanted to cover today.\nWe will be doing a more in-depth series on all the services.\nSo if that's something that interests you,\nmake sure to subscribe and click that Like button\nso you can support us and be notified every time we\npublish the next episodes.\nThank you very much for being with us,\nand we'll see you in the next episode.\nSlac.\n"
    },
    {
      "title": "22. What do you need to know about SQS?",
      "url": "https://awsbites.com/22-what-do-you-need-to-know-about-sqs/",
      "publish_date": "2022-02-03T00:00:00.000Z",
      "abstract": "Luciano and Eoin take a deep dive into SQS as part of a series on AWS event services and event-driven architecture. We talk about the kind of problems SQS can solve, all of the SQS features and how to configure and use SQS to achieve reliability and scalability without all the complexity. We also take some time to detail how SQS works with Lambda in terms of scaling, batching and filtering.\nIn this episode we mentioned the following resources:\n\nSQS\nUsing Lambda with SQS\nLambda SQS Scaling\nSLIC Watch (Serverless plugin for easy dashboards and alarms)\n\n",
      "transcript": "Eoin: Hello, today we are going to answer the question, how do you use SQS?\nAnd so by the end of this episode, you will know when to use an SQS queue and we'll give\nsome example use cases. We'll talk about the main features of SQS, how to send and receive messages,\nhow to customize the configuration of a queue, and we'll also talk a lot about the integration\nbetween SQS and Lambda. My name is Eoin and I'm joined by Luciano and this is the AWS Bites podcast.\nIn the last episode, we talked about all the different AWS event services you have\nand today we're going to do a deep dive on SQS. So I think we had a classification of event systems\nthe last time Luciano, we had point-to-point systems, we had PubSub and we had streaming.\nSo SQS is a point-to-point system. What is it good for?\nYeah, so in the last episode we gave a few high-level details about SQS, so let's deep dive.\n\n\nLuciano: We mentioned that decoupling producers and consumers is generally a good use case for SQS.\nAlso, it's a good service to add reliability because basically when you add SQS, you have an\neasy way to store messages persistently, so you can consume them later and this is good for\ninstance in cases where your consumer might not be immediately available or it might be overloaded.\nYou can have a queue in between and that allows you to be able to manage these kind of situations\nin a more reliable way. The other thing is that a characteristic of SQS is that each message is\nexpected to be processed by one consumer and that will have implications that we'll discuss later\nwith some of the examples. Also, multiple consumers allows you to scale highly. For instance, if you\nare producing many, many messages and for some reason your application is not working, you can\nfor some reason your application is more and more successful, so the number of messages grows\nexponentially, you can keep allocating more and more consumers and you will just get all the\nmessages being distributed and you can compute in a more parallelized way. So that let's say SQS is\ngenerally a good way to scale workloads when the number of items to process increases over time.\nAnd the other thing is that it can be used even for cross-region or cross-account communication\nbecause from one region you can publish messages in another region or even from one account you\ncan publish messages in another account. So it can be used for communicating that way across\nregions and accounts. Is there any example use case that you want to mention to try to clarify\nthese points we just mentioned?\n\n\nEoin: There are loads of use cases where you can really make great use of SQS but I suppose some of the simple ones we talked about the last time were you want to send\nan email so you could have a service that consumes email sending requests and SQS is ideal for that\nor if you have some sort of batch processing like a service to process picture resizing requests.\nThat's a typical example and you can imagine the same thing being applied to a lot of enterprise\nbatch processing workloads as well like if you're doing some sort of calculation or modeling an\naggregation task these are all jobs that you can put on an SQS queue and then have one or many\nmany workers that pull from that queue. It could be an AI modeling workload as well so you can just\nimagine having a pool of workers and that pool can scale auto scale according to the queue depth the\nnumber of messages in your queue. That's a pretty typical pattern. You've also got I suppose thinking\nlike about enterprise architecture or event driven microservices the architecture and decoupling\nsystems SQS is really useful in all of those situations so decoupling within systems at a\nfiner grain or at a macro level across a big enterprise full of applications.\nSo yeah that's that's particularly useful you could also use it as an SNS subscription so\nwe mentioned that it's a point-to-point channel but you can also use it with PubSub\nand together you get point-to-point with reliability or sorry PubSub with reliability.\nYou also have DLQs so DLQs there's lots of enterprise integration patterns and DLQ is\none of the most well known and that's essentially a dead letter queue which so it's a queue where\nif you have messages that have failed to be processed repeatedly you can put them into\na dead letter queue and then manually re-inspect them and schedule them for redelivery later.\nSo SQS queues can be used as DLQs in their own right with any system but SQS also has a feature\nbut SQS also has a feature which allows you to send failed messages to another queue which is a DLQ.\nSo that's one of the one of the cool features of SQS. Should we go through and run through the\nhighlight features of SQS quickly? Where would you start? Yeah I think so.\n\n\nLuciano: So yeah the first thing that we also mentioned in the previous episode is that we have two different types of queues in SQS.\nOne type is called regular queues I guess for lack of better naming and the other type is FIFO queues.\nSo with regular queues you get different types of guarantees and the idea is that a queue will do\nlike a best effort delivery in terms of ordering so you are not guaranteed to have messages\nstrictly ordered when you consume them and the other thing is that you get at least once delivery\nwhich basically means that when you receive the messages it might happen that you get the same\nmessage more than once. When you need a little bit more strict guarantees you can use FIFO queues\nand FIFO queues will give you ordering guarantees so if you produce messages with a certain order\nthose messages are consumed in the same order and also get exactly once delivery so FIFO queues have\na mechanism to remove potentially duplicated messages coming in again into the queue\nand we'll probably give you a few more details about that later on during this episode.\n\n\nAnother interesting feature is DLQ. We already mentioned it. It's something that needs to be\nenabled and we'll discuss a little bit more how to enable that but it's very easy to have it\nbuilt in and this is very convenient because it's very common that you define a number of messages\ntypes in your application then your code evolves eventually you might be introducing a bug in the\ncode that is processing a job and what happens is that if your job if your worker is always crashing\nthen you're not going to be able to process that message ever so rather than keep retrying it\nindefinitely it will eventually be moved to a DLQ somebody can investigate and then when you\nrealize what's the problem you fix the bug you can easily reingest from the DLQ and actually process\nthe message so this is actually a very critical feature that I think most applications using\nIQ should avail of. Another interesting detail is that the protocol of Sqs is not one of the common\nprotocols generally seen in other queuing systems like RabbitMQ or ActiveMQ that uses protocol like\nAMQP or MQTT. In the case of Sqs the protocol is HTTP. I don't think it makes a huge difference at\nthe end of the day because the way you interact is through the SDK so you don't get to really work at\nthe protocol level but there might be different features and different characteristics in terms\nof performance because of the underlying protocol so that that might be interesting to know for some\npeople that are coming from other queuing systems. Then we also have a server-side encryption so\nmessages are encrypted in transit and I suppose also they are stored encrypted.\nWe have message delays which basically allows us to\nconfigure in different ways how and when the message should appear in the queue. We'll mention\na few examples later on. The other very interesting thing is that Sqs is durable and available pretty\nmuch by default, also scalable. That means for instance if we look in contrast Kinesis, with\nKinesis you need to do a little bit of capacity planning. You need to understand how many shards\nyou need to provision and that's generally based on what is the throughput that you want to achieve.\nWith Sqs you don't have to worry about all the stuff. Queues will automatically scale for you\nand you don't need to pre-provision any of that stuff. In general I would say that the biggest\nfeature of Sqs is that it is a very simple, true to its name, a very simple queuing system\nand therefore it's very easy to integrate in most applications and you get very good performance\nbasically straight away without having to go crazy with configuration.\nIs there anything else worth mentioning? Maybe, I don't know, some interesting integrations with\nother services. What do you think? Yeah, for sure.\n\n\nEoin: On the integration with other services side we can think of I suppose about the producer side and the consumer side. So when you're producing messages\na lot of services are there that target Sqs already. So if you've got an API gateway you can\nback that with a queue. So you can send messages from API gateway directly to a queue.\nWe already mentioned SNS subscriptions so SNS and Sqs play very nicely together. EventBridge too,\nin the same way and you can also integrate it into step functions. So it's pretty well integrated on\nthe production side. Of course we could talk about the consumption side but for both consumption and\nproduction you can also use it programmatically. So let's talk about how do you send a message and\nhow do you receive a message. So on the sending side there's a send message API and there's also\na send message batch API. So you can send a single message or you can send up to 10 in a batch.\n\n\nSo that's the limit in a batch. Then when you receive you call on the consumption side\nit's essentially a pull mode. So you have an SDK or an API that you use to call receive message and\nreceive message and it also allows you to receive up to 10 messages at a time. So you can choose\nto receive one or up to 10. And that operates in two modes. You've got short polling mode and long\npolling mode. So short polling is basically saying give me a message if one is available\nbut if no message is available just return. So that's essentially a zero seconds wait time.\n\n\nBut you can also do long polling where you can say wait up to 20 seconds for messages to appear\nand then return. So the difference there is it depends on the volume of messages you're\nexpecting and the nature of your system. I suppose it's important to kind of bear in mind that if\nyou're polling more frequently that's an extra request. SQS is priced essentially on the number\nof requests. There's also data transfer but it's fundamentally about the number of requests so you\ncan bear that in mind. So once you've called receive message then you can do your message\nprocessing and then delete. So it's essentially like you're starting a job and then you're\ncommitting to the fact that you've processed the job by calling delete message at the end.\nSo there's three steps essentially when you're a consumer. So the interesting thing there is\nwhat happens when you forget to delete and yeah that's really important. So can you describe that\nLuciano? What would you expect to happen if you forget to delete a message from a queue after\nyou've processed it? Yeah so I'm gonna try to give a brief description of that.\n\n\nLuciano: I think we will understand more how that really works when we deep dive into the actual configuration. But in\ngeneral SQS never really deletes messages that have been delivered to a consumer because it's\nwaiting for the consumer to acknowledge that the job was completed and that's done through\nan explicit call to the delete message API. And so if that doesn't happen because either you\nforget to do that in your code or maybe there is a bug and the worker crashes before is actually\nable to delete the message, the only thing that SQS can do is assume that something went wrong.\n\n\nMaybe the message was not delivered, maybe the job was not processed correctly, so it's gonna\nmake the job eventually reappear in the queue so that it can be processed again. So it's kind of\ntaking a sane default to make sure that you have a chance to process that message again in case\nsomething went wrong. So yeah make sure to delete the message when you completed processing it,\notherwise you'll end up reprocessing the same job over and over again. And of course your queue\nwill grow indefinitely because you keep accumulating more and more messages that will\nalways reappear in the queue until they are completely expired because of the direction of a\nmessage in the queue. So given that we are starting to talk more and more about the different\nconfiguration options, should we deep dive into that? Let's do that.\n\n\nEoin: You mentioned around the deletion, it's all down to visibility. You mentioned this thing about messages,\nthey're not deleted, they just become invisible. So then my understanding of message visibility,\nmaybe you can chime in here, but if you receive a message, it remains in the queue,\nso we said that prevents other consumers from seeing it. So it gives you, the first\nconsumer, a chance to process it. There's a visibility timeout, so the clock is ticking\nand the consumer has to process it within this visibility timeout. And after that timeout has\nelapsed, if it hasn't been explicitly deleted, it's going to reappear. So this is a configuration\nsetting then you can set a queue level, but you can also set it at an individual message level\nwhen you call receive message. So it can be zero seconds, it can be up to 12 hours,\nbut the default is 30 seconds.\n\n\nLuciano: Yeah, I guess one case where this can be important is if you have a job that you know is going to take you a long time to process, make sure to find the\nfine tune this visibility timeout, because if it's too low, while you are still processing\nthat message, it will already reappear in the queue. So you end up with duplicated processing\nbecause of that. So that can be another issue that can happen when you have long running\nprocessing jobs. Yeah, that's a good one.\n\n\nEoin: And we're going to talk about Lambda later, but Lambda has its own timeout and you want to make sure that they align the SQS timeout and\nthe Lambda timeout because it doesn't make sense for your Lambda to take longer than your visibility\ntimeout because you'll end up with that situation. Another configuration option I think worth\nmentioning before we kind of move on is message groups because we talked about FIFO queues and the\nordering guarantees that you get. So those ordering guarantees aren't per for the whole\nqueue, you can actually partition it into ordered streams using message groups. It's a bit like\nthe way ordering works with Kinesis shards except different, but the concept is the same, right?\n\n\nBecause it means that you can say define multiple groups and then you can still get parallel\nprocessing with ordering. So it's a nice way to balance that. But then the order guarantees and\nthe delivery guarantees are per message group ID. So that's interesting to know. There's a whole\nset of other configuration options like you can set up message delay, you can set up the queue\nretention up to 14 days, you could put a specific resource policy in there for security, a redrive\npolicy for DLQs, we mentioned that already. And on FIFO queues actually you can also do\ndeduplication. So you can ensure that you got the exactly once delivery semantics by making sure\nSQS can recognize when you've got a duplicate. These FIFO queues also support high throughput\nmode, which is interesting because when you do those FIFO queues, because you've got ordering,\nby its very nature, you're limiting throughput because you have to process them in order.\nSo there's a number of settings like setting the FIFO throughput limit and stuff that you can use\nto make sure you get the maximum throughput for FIFO queues. Throughput for standard queues,\nit's essentially unlimited. So those are the configuration options. Are there any kind of\nconstraints, limitations, a lot of AWS services, you know, you have to understand all the quotas\nand limitations. What's the soft limit? What's the hard limit? There aren't a lot with SQS, are there?\nYeah, there aren't.\n\n\nLuciano: I think the main ones is that there is a limit on the message sides, which is 256 kilobytes. But a common pattern that I've seen is you can use, for instance, S3 and\njust put a reference to the file in S3 in the message when you need to use more data.\nThen it's interesting to see that there are no limits in terms of the number of messages that\ncan be stored in a queue. So you can keep pushing more and more messages and you don't have a limit\nthere. And there aren't API limits in terms of requests except in the FIFO queues where you have,\nI think, 300 API calls per second. Is that right? Yep.\nAnd 3,000 if you use batching because you can do 10 at a time, basically.\nAnd also there is another interesting limit, which is the number of messages that can be in-flight.\nWe didn't... I don't think we explained what in-flight means, but basically it's those\nmessages that are currently being processed by workers on the other side.\n\n\nEoin: Received but not deleted. Exactly.\n\n\nLuciano: Yeah. And the visibility time out hasn't expired yet, so they haven't reappeared in the queue.\nYep. And there is a limit of 120,000 for regular queues and 20,000 for FIFO queues.\nNow, this is something I wanted to mention because one of the tools that we have mentioned also in\nprevious episodes called SlickWatch, which allows you to easily get observability in serverless\nprojects if you're using the serverless framework. In that serverless plugin that we built,\nwe already give you a pre-configured alarm and dashboard that you can have in your application\nin your application to monitor if you are actually reaching this threshold\nof too many in-flight requests. So if you want to check that out,\nwe'll put the link in the description. Yeah, that's good.\n\n\nEoin: It's a bit of a shameless self-promotion, but it's definitely worthwhile because those are the kinds of things you\nnever think about. And then in the rare situation where you've got a problem and your number of\nin-flight messages goes through the roof, you'll get an alarm with it. I think it's worthwhile\ntalking about AWS Lambda because AWS Lambda works really well with SQS, but it has a whole set of\ndifferent considerations. So I think it's worthwhile talking about it. When you've got EC2 or ECS\nconsumers, containers somewhere, the operation model for consumers is relatively straightforward\nbecause you're in control and you have to build all the infrastructure and scale it out yourself.\n\n\nWith Lambda, there's something called an event source mapping. It's the same part of the Lambda\nservice that's also used with Kinesis and Kafka sources and DynamoDB streams. But event source\nmapping is essentially a consumer for SQS that is managed for you within the Lambda service,\nand it pulls those messages for you. So we mentioned how SQS integrates with other\nservices on the producer side. On the consumer side, it basically doesn't integrate with anything\nbecause you have to pull messages out of it. But the exception is Lambda because it does that for\nyou. So one of the interesting things that I came across with Lambda and SQS is how it scales\ndifferently than other Lambda sources. So if you're expecting that if you've got, we talked\nabout the batch processing workload, if you've got many, many containers or many Lambda instances\nrunning and they're taking a long time to process SQS messages, some type of machine learning workload\nrunning in Lambda might take 90 seconds to process an event, for example. The event source mapping is\nonly going to scale to 60 concurrent instances per minute. And this is dramatically different\nto other event sources where you, I mean, if you call invoke Lambda, you can get a thousand\nrunning instantly and you can get another thousand every minute. And that scales really fast. But\nwith SQS, you can't scale that way. And even if you use provision concurrency, which I tried,\nit's still 60 per second to consume your SQS messages. So that can be a limit, but it depends\non how long it takes to process your messages. Obviously, if your messages are going to be\nprocessed in seconds or hundreds of milliseconds, you're still going to be able to process\nthousands of messages or thousands of batches of messages very quickly. But it's just important\nto be aware of that. Well, if you've got FIFO queues then as well, you get one batch at a time\nper message group ID. So that's, I suppose that's probably intuitive, but if you've got five\ndifferent message group IDs, then you're going to have a maximum of five consumers.\nThere's also some interesting configuration options like the batching. So you can configure\nif it should invoke your Lambda after a predefined number of seconds,\nlike every six seconds, for example, or if it had received a certain threshold number of messages,\nor also just based on the payload size. So the number of megabytes that it has accumulated.\nSo that's a whole set of configuration that you get with the Lambda service in SQS.\nAnd a really new one is the event filtering, which came out just late last year.\nAnd this is kind of interesting because you can filter at the event source mapping level and say,\nI only want to filter messages matching this pattern. And you can do, you know,\nJSON filter or a string filter. What that actually means sometimes is that if your consumer\ndoesn't match a filter, you can still end up losing messages because they've been processed\nby the event source mapping, but they just haven't been sent onto your Lambda because\nyou filter them out. So you have to really think about the semantics there and whether you can\nand if you want another consumer to be able to pick up that message, you might need to\nre-architect the message delivery setup. And the last thing I'd say about Lambda is\njust that cross account Lambdas with an SQS in a different account are also possible,\nwhich is really helpful. I wish that was available for all the services, including Kinesis,\nbut that's really helpful for integration across multiple applications when you've got a multi\naccount set up, which is best practice these days, you know, separate account per application,\nper environment. So if you want to communicate across applications, it's a really good way to do\nit.\n\n\nLuciano: Yeah, I think the way you described the integration with Lambda, it feels like there is a lot of magic that AWS does for you. So you have, you can basically build something quicker,\nbut I think it's interesting to understand what's really going on under the hood. So you don't have\nsurprises there. So I think that that was a good one to cover.\nYeah. Yeah.\n\n\nEoin: It's an interesting one because Lambda is simple, SQS is simple, but they've got, they're building more and more configuration options to make it more powerful.\nSo, you know, you sacrifice some of that simplicity with the power then you get.\nI think that Lambda kind of concludes a lot of the topics around SQS, but I did want to call out\na couple of talks that came up at reInvent last year, some really good new talks all around the\nidea of enterprise integration patterns and message driven architectures, and it covers SQS,\nbut also all the other services that we're going to talk about in this series.\n\n\nAnd one of them was by Gregor Hopa, who was one of the authors of the enterprise integrations\npatterns book, a book I read a long time ago, which is very good for understanding all the\ndifferent types of message driven workflows you can have in applications. So there's one by him,\nand there's a couple of others that we're going to put in the show notes. And if people are\ninterested in event driven architectures and how you can build really powerful architectures with\nvery simple services without having to build a whole lot of infrastructure, I think these\nare really, really worthwhile. So really strong recommendations on those. And with that, I think\nwe'll leave it for this episode, but please follow us, especially if you want to hear more about the\nevent driven architecture series, we're going to cover the SNS in the next episode. So thanks very\nmuch for being with us and we'll talk to you then.\n"
    },
    {
      "title": "23. What’s the big deal with EventBridge?",
      "url": "https://awsbites.com/23-what-s-the-big-deal-with-eventbridge/",
      "publish_date": "2022-02-10T00:00:00.000Z",
      "abstract": "Eoin and Luciano continue their series about event services. In this episode, they chat about EventBridge and explore why this AWS service has such a great potential for event-based serverless applications. This episode presents some interesting examples of when and how to use EventBridge. It also covers all the different classes of events that you can manage with EventBridge: AWS events, third-party events and custom events. We discuss limits and pricing and, finally, we show how things can go wrong and how much you can end up paying for it. We conclude the episode with some tips and resources to avoid shooting yourself in the foot and get good observability when using EventBridge.\nIn this episode we mentioned the following resources:\n\nOur previous episode about all things SQS\nOur introductory episode about what services you should use for events\nList of AWS services that can trigger EventBridge events\nAn example of how to make HTTP calls directly from EventBridge (by Sheen Brisals)\nHow to test when using EventBridge (by Paul Swail\nEventbridge CLI tool\nLumigo CLI\nEventBridge Atlas\nEventBridge Canon\nAccelerate Serverless Adoption with EventBridge (talk by Sheen Brisals)\nSeries of Articles by Sheen Brisals on EventBridge\n\n",
      "transcript": "Luciano: Hello everyone, today we're going to answer the question how do you use event bridge?\nAnd by the end of this episode you will know why event bridge is very different from everything\nelse in this space. We will also discuss some interesting examples on when and how you can use\nevent bridge, what can possibly go wrong with it and how much are you going to pay for it,\nand finally we are going to give you some tips to avoid shooting yourself in the foot and get good\nobservability. My name is Luciano and today I'm joined by Eoin and this is AWS Bites podcast.\nSo before we get started let me mention that this is a series about AWS services that cover the\nspace of event and message passing. We just finished an episode about SQS, the previous one,\nand we also had a more generic introduction to all these services. Today we're going to focus\non event bridge but if you're interested in the space make sure to follow and subscribe so you\ncan be notified whenever we publish the next events. But back to event bridge, Eoin what can\nyou tell us about what are the main features or the main things why event bridge is so interesting?\n\n\nEoin: Yeah event bridge if we talk about the classification we previously used we had queues and then we had pubsub and then streaming. Event bridge we put in the pubsub category but\nit's a little bit different to other pubsub systems it's got some interesting features.\nSo event bridge is nice in that you can publish events without really provisioning anything up\nfront that's I think the thing that people really like. You don't have to provision anywhere\nyou don't have to provision any resources in AWS if you want to send events to it\nbut if you want to consume them then you just need to create a rule and that's how it works.\n\n\nSo you don't really have the subscriber concept like you do with SNS. You don't necessarily have\nto have listeners instead you have rules and a rule is slightly different to a subscription.\nWith a rule you define a pattern and it is really like a pattern matching concept similar to the\nkind of pattern matching concept you get in some programming languages where you have an event\nyou're trying to say I want to receive events that look a little bit like this pattern here\nand then you can specify some properties in that event and their values this partial string that\nit might begin with integer comparison and then you say okay with this rule if you find any\nmatches then can you please send these events to these targets and one of the great things about\nEventBridge is that it supports lots of targets so EventBridge is really good it's almost like the\nintegrate anything with anything service on AWS so it's of all the services we're going to talk\nabout in this series it's probably the most general purpose it can do almost anything you\ncan imagine. There are of course some limitations which we'll talk about but it is really really\npowerful and it's one of the services that has really got a lot of people excited since it was\nannounced it was actually based off a service that had existed for a long time CloudWatch events\nbut they rebranded it and gave it a new purpose and added a lot of new features and suddenly\npeople are getting as excited about it almost as people did when Lambda came out so there's lots of\ndifferent different features in it so you've got different event types do you want to talk about\nmaybe what what are the different events you can get with EventBridge?\n\n\nLuciano: Yeah before that I just want to mention that it reminds me a little bit in its purpose and the way it has been marketed as the\nif this then that or Zapier answer from AWS right? Yeah absolutely yeah different type of events so\nthere are actually I would say three main categories of events that you can deal with\nand the first one is that there are events that are I would call them native in AWS so things that\nhappen in your AWS account or other accounts and you might want to listen for and react to them\nand just to give you a few examples for instance you can be notified whenever a spot instance has\nbeen interrupted or maybe you have step functions and you want to react to a step function changing\nstate maybe it's started maybe it's failing maybe it's timing out and for instance you can even\nenable notifications and events for S3 objects so you could say every time there is a new\na new object in a bucket trigger that this is an event and you can create a rule to match that\nand very generic thing you can use CloudTrail as well and that will give you access to another\nwide range of events there is a list that we will put in the show descriptions there is a link there\nand we'll you will find that all the events and services from AWS that will trigger events that\nyou can capture in EventBridge there is an interesting small detail that you need to be a\nlittle bit careful about there are different guarantees in terms of delivery depending on the\ndepending on the type of event source that you are trying to to use sometimes you have kind of a\nguaranteed delivery and then sometimes you have best effort so I suppose that pretty much means\n\n\nEoin: when you get at least once delivery or at most once type of delivery yeah I guess if it's best effort it's kind of at least zero right because you just for some of those services you just won't\n\n\nLuciano: get an event so yeah it's good to check the docs so this is the first category and it's something that it's there for you if you want to use it you just create a rule and you use it in some cases\nyou need to enable the events but most of the time it's there and it works for you then there are\nanother category that is partner events and this is actually really interesting and this is why\nsometimes I like to think about EventBridge as sort of a Zapier or if this then that because\nyou can plug in events from other SaaS platforms for instance I don't know Salesforce, Outzero,\nPagerDuty so if you integrate the platform with your AWS account basically you can start to get\nevents from those other external platform and consume them I don't know an example that comes\nto mind maybe you have a sales pipeline that you are mapping in Salesforce if that pipeline changes\nmaybe you have new deals coming in you can probably receive events and react to those and create\nintegrations and finally the third category of events is custom events so you can use the SDK\nto dispatch your own totally custom events for instance I don't know if you are building any\ncommerce which seems to be our favorite example you could create your own custom events every\ntime there is a new order and you can define the structure of those events and then different parts\nof your application can create patterns and rules to capture those events and react to them\nthat's all I have for events but what about targets\n\n\nEoin: that's yeah I mentioned that one of the advantages is that you have loads of targets so compared to SNS or SQS you have many services that you can integrate directly from EventBridge\nyou also have HTTP destinations so if you've got a third-party API you can use EventBridge to route\nall of these different types of messages that can come in to a third-party API or an external API or\none of your other APIs and it also supports neat things like throttling and exponential backup and\nback off and retry that's really useful and one of the great things that EventBridge has that a\nlot of other services doesn't have is cross account of EventBridge so you can very if you've\ngot different applications different domains running in different accounts or even third-party\nevents you can set other EventBridges in other accounts as a target so that makes it really\neasy to integrate without having to have you know specific services that are there with an IAM\npolicy and you know network routing that can route from one account to another that you have to do\nwith some other kinds of services and EventBridge takes care of that so there's definitely the most\nrich the richest supportive supported set of supported targets available.\n\n\nI suppose we there's a couple of other features that came out after EventBridge was launched\nthe schema registry and then the archive and replay and for people who have a lot of events\nin their system and they're trying to support a large number of developers and trying to share\nknowledge around what kind of events they can listen to the schema is really useful for that\nuseful for that EventBridge can discover what kind of events are coming through your system\nand automatically register schemas for it and then you can use\nEventBridge to generate code samples there are code bindings so if you're using an object\noriented typed language it can generate classes for you for example yeah that's really that's really neat for those absolutely yeah it can get really difficult if everything is dynamic and event driven if you don't have types it can get pretty difficult to kind of understand the\nstructure of events and what what properties are supported and then you've got the archive so\nEventBridge archive allows you to retain have retention on those events\nso you don't lose them because they're otherwise ephemeral really once if nobody's listening if\nthere are no rules the events magically disappear but if you've got an archive you can actually\nreplay events so you can add rules change rules fix problems and then replay events which is\npretty good for resilience yeah should we go through the process of using EventBridge it's\npretty simple right but uh i guess it depends what are the steps involved if you wanted to\nstart sending and receiving events we talked about how to do it with sqs how do you do it with\n\n\nLuciano: EventBridge yeah i think it's a good thing to split the process into parts of sending events and receiving events because of course you care about sending events only when you are creating\nyour own custom events if you want to listen for aws events or third-party events you you just get\nthem so you just need to focus more on the receiving part so if you're trying to create your own custom\nevents basically the first thing you need to do is select a bus that you want to use there is a\ndefault bus so i will say that most of the time that's good enough but of course you also have an\noption to create more specialized buses if you if you have to the other thing you have an api to send\nevents so you can do that of course through the sdk is probably the most common pattern or you\ncan just call the api or the cli and there is a specific interface that you have to use but it's\npretty free form because you can structure the event in pretty much as you want is a big blob\nof json and we'll talk in a second more about that and then finally when you want to listen to the\nevent you'll need to create the rule and the rule also is another json object with properties so\nlet's zoom in into those two json objects so when you create an event there is kind of a best\npractice that is pretty much something you will see also in the aws events and the third-party\nevents as well and what there are like fields that are expected to be there and they have a meaning\nso and these are the ones you will be using also when matching for those events the first one is\nsource and source pretty much describes who is creating the event so it could be your own service\nmaybe i don't know again in the example of an e-commerce you could be the order service so you\ncould literally say source order dot service or something like that in the case of aws you will\nsee something like i don't know aws dot states for instance for step functions and for third parties\ni expect that you have something similar for instance i don't know salesforce dot i don't\nknow pipeline maybe and then another field is detail type which is a little bit more descriptive\nand it tries to describe the specific type of event that was generated by that source\nfor instance in our e-commerce example that can be order created in the case of aws sometimes they\ntend to be a little bit more verbose and they have like entire sentences like step function execution\nstatus change and it's like an entire sentence that you need to match on and finally the the\nmost interesting part is the detail attribute and generally you can use that as you want meaning that\nit's an object and you can store inside that object all the data that you think is relevant\nto describe that example and this is where it gets a little bit tricky because because you have a lot\nof freedom you need to be careful and on one side you can say i'm gonna just store the minimum amount\nof data that represents this event for instance i don't know if it's an order just the order id\non the other side you might sort all the data that represents that order right and and the two\nsides are basically they will force a new different constraints if you go with a very many\nvery minimal approach maybe whoever is consuming the event will need to query different data sources\nor fetch additional data somewhere else so that might be consuming that might be cost efficient\nor on the other hand if you end up with very big messages that can also be a problem especially\nif you are storing these messages you might incur an additional cost i'm not really sure if there\nare limits on the sides of the payload but we'll talk more about limits later i believe it's the same as s and s and sqs actually so 256k for the for the whole message yeah so yeah be careful that also have to to respect that limit yeah absolutely yeah one suggestion there will be try to start\nwith something that is relatively small and that you think it makes sense and then if you realize\n\n\nEoin: over time you need additional feeds you can always add them later yeah it's interesting when you talk about event driven architectures people say oh you should do event driven architectures because\nit means you're then loosely coupled as if it's like a very clear yes no thing loosely coupled\nor not but i've heard the term semantic coupling being used whether that means when you've got when\nyou're relying very tightly on the structure of an event and all its fields this is a form of\nsemantic coupling so you still end up with it it's not the same as having you know location coupling\ncoupling or time coupling but it it means you're bound to the event that you're used to seeing so\nit makes it very difficult for the producer of that message to change it and to change the\n\n\nLuciano: structure of their data types over time so you really have to get that balance right yeah i think to to to all extents you should consider those messages as an interface right because as soon as\nanother service is starting to consume them then yeah you might break things if you suddenly change\n\n\nEoin: dramatically the structure of the events yeah for example you mentioned that in the source field you might put the order service and so if somebody says oh well then i should match on the source\nbecause i want to get order of events so i should match on the order service but it probably doesn't\nmake a lot of sense to match on the source a lot of the time because this is kind of coupling\nto the producer you might it makes the detail type is usually a little bit more semantically\nmeaningful like order created or order created that sounds like a lifecycle event that you can\n\n\nLuciano: use and you don't need to match on the source then yeah what else can we add we mentioned that you can use this to communicate across domains so it could be a good way to do event choreography\nand we gave you a few interesting details other interesting detail that could could change i\nsuppose the way you implement that the integration between services is that if you if you really need\na semantic that is like exact ones processing of a particular event it's on you to to do that like\nyou need to provide your own the duplication ids and you need to handle duplication because\nby default you get at least once delivery so you might get the same message more than once\nand finally another interesting detail is security you need to of course use iam and define all the\nresource constraints but you can be very very granular and for instance you could say that is\nnot possible for instance to create a rule that is using i don't know certain detail types like\nyou can limit also that the structure you can leave it based on the structure of the events\nand that that can be useful because of course if you have total freedom on listening for events\nthat can become a side channel for services to to listen for information that they are generally\nnot allowed to listen for so you you might be very strict in that regard as well\nyeah an interesting thing is the other side of the coin i suppose is how do you\nactually write the rules do you want to tell us something about that howan\n\n\nEoin: yeah so when you create a rule you can create it using cloud formation or terraform or the api or the console and the other rule you will you'll define the pattern but you'll also define the\ntargets and for targets you know we mentioned you can target a lambda function you can target an\nsqsq or another event bus lots of other services but you can also do mappings so if you're want to\ntransform the data and if you're transforming it to a htb destination you know if you want to match\nthis the required payload structure that's required you can do an input transformer on that\nwhich allows you to extract the data and then you can do a map and then you can do a map\nand then you can do a map and then you can do a transform around that which allows you to extract\nout fields and map them into a different kind of a structure you can do that with any event actually\nand any destination you've also got destinations that you can configure that are specific to an\nindividual service and examples of that would be kinesis or sqs so with kinesis and sqs5 as we\nspecify at what level you want to order guarantees so with kinesis you might put in a partition key\nand with sqs fifo you could put in the message group id and when you write an event bridge rule\nthe targets one of those things you can say this is the you can say this is the field that i want\nto become the partition key or the message group id so it makes sure that if you've got events\ncoming in through event bridge you can specify the order thing you want or the shard essentially that\nwould be processed now remember that event bridge doesn't give you any order guarantees because it's\nit's not that kind of service so you don't have order guarantees on the input side so it's really\nmore or less about doing best effort ordering or just controlling the shard they get allocated to\nso that you can process them concurrently that's really the the benefit there\nand another example of that would be ecs so you can fire off an ecs task so run a container in\nresponse to an event and you ecs task configuration in a rule will allow you to\nspecify what container image to use what the task definition is all of that stuff so there's\nso much you can do with this it's worth looking into the cloud formation documentation i always\nfind that's a good way to see what are all the configuration parameters for these things\nyeah it really does allow you to integrate anything with almost anything yeah the cool\n\n\nLuciano: thing about that is that most of the time you don't i mean it's so configurable that you just need to write the right configuration to achieve a good integration you don't even need for instance\nto write a lambda to reshape the data and call the destination service which is under you maybe\nwhole thing you would need to do but because you have such flexibility you can probably build\nsophisticated integration with the data and then you can do it with the data and then you can do\nsophisticated integration without provisioning any like compute functionality it's just event\n\n\nEoin: bridge configuration yep and that that's that's a good thing because okay it might be a little bit more difficult to troubleshoot an input transformer than a lambda function that's that's the trade-off\nbut you don't have the latency and the extra lambda function to manage so\nyou just got to pick your battle i suppose and figure out which is best for your use case\nso since i mentioned latency should we start talking about limitations and constraints and\n\n\nLuciano: performance characteristics yes i think that's a good follow-up to how do you use event bridge yeah i think that the main one is latency like you you said that already but how how different\nit is for instance from sns where you have very small latency like we're talking about 30\nmilliseconds i believe in the case of sns with event bridge is a little bit unwavy but i think\nit's generally around alpha second which is yeah quite different from sns right sns is very\nperformant event bridges suppose you need to start from the premise that you don't really care about\nextremely fast delivery it's more it's gonna happen relatively fast but not like milliseconds fast\n\n\nEoin: yeah i think this is this is exactly i mean this is one of the rules where you can say am i going to use event bridge if you need strict performance and latency high throughput\nyou have to go with sns or one of the streaming solutions but if if you're talking about okay\ni just need to react to business events and process them and it doesn't have to be like if\nyou want to fulfill an order half a second is probably not a significant latency in the grand\nscheme of things you know if you're talking about package delivery but if you're talking about a\nuser who's clicked an action and maybe you're going to do some processing on it they're waiting\nin the web browser on a mobile device and they're waiting for a green tick to show that something\nhas been processed you might think twice about event bridge in that kind of flow because\nthe the guarantees like the aws documentation says typically around half a second\n\n\nLuciano: so it's not exactly confidence inspiring if you're looking for real-time event processing so yeah i suppose it could be a good idea that becomes especially true if you have like a cascade of\nevents events depending on one another of course that that effect can compound so yeah be be even\n\n\nEoin: more careful if you have that kind of situation i would still say that for a lot of you know across domain events and micro even inter microservice communication this should be\nsufficient in vast majority of cases and i would i would recommend against using this latency limit\nas a false argument just to reject event bridge and go with something that could be really more\ncomplex because if event bridge is so simple and manages so much for you that it will it could has\nthe potential to save you massive amounts of engineering time so it's worth sticking with it\nand only if you really need to tweak performance you know you might need to deal with optimizations\n\n\nLuciano: for specific cases yeah there are other limitations this is what i'm mentioning quickly but nothing extremely special for instance you have invocation quotas they are slightly different depending on\nthe region so watch out for that they can also be increased if you need to have more invocations\nalso there is a limit on the number of put events that you can do over a certain period of time\nthere is a number i think it's 300 rules per bus but it's only you can get increased if you need\nto create more rules and also for every single rule you create you have up to five different\ntargets that you can trigger if that rule matches yeah in terms of pricing is there anything\n\n\nEoin: interesting worth mentioning but the pricing for sending events is pretty straightforward so i think for every region you're looking at a dollar for every million events and that's whether\nit's custom events or third-party events or aws events so a dollar for a million events you have\nin the case of ws events that counts only for the ones that you need to explicitly enable like the s3 objects but yeah i think not all of them are actually charged right absolutely yeah they're sent by default they're sent already even if you don't know it so that's true you're not going to\nbe billed for that there is an additional cost though for things like http destinations it's\nlike a supplemental cost of 20 cents for a million deliveries and those features we talked about like\narchive and replay those are the areas that you might want to be a little bit more careful about\nwhen looking at pricing because if you're archiving events sometimes you don't notice that\nevents are the volume is escalating and if you've got millions of events you're going to be charged\nper gigabyte of archive per month so that that's an ongoing cost that will escalate over time\nand replay you'll get charged by the events processed replay is probably not something that\nyou're going to do as part of a programmatic workflow it's probably more likely to be done as\npart of remediation in the event of a failure so it's just important to be aware of and there's\nalso a charge for a schema registry as well in terms of events ingested when you're discovering\nschemas automatically so that said that's the pricing spiel done and i think one of the one of\nthe things that our channel is challenging with all event driven systems is how do you test it\nand you've been reading some pretty good articles on this lechano and i know paul swale has covered\nthis in in detail a couple of times what do you think are the best recommendations there\n\n\nLuciano: yeah i think at least for me the main pain point is to make sure that the rules that i write like the patterns are actually doing what i think they should be doing because this is of course\na json based language with its own nuances like you have a lot of power but you need to learn\nhow to use the different constructs to do that the pattern matching and of course you you need\nto find a good way to test if your rule is actually correct and i found that that there is\na little bit of friction maybe something to be improved in aws in terms of the tools you could\nget to actually get the reassurance that your patterns are actually doing what you want them to\ndo and so there are really a lot of great ways to do that in aws what i discovered is that there is\nan api called test event pattern that you could use either from the cli or from an sdk and with\nthat api what you can do is send an example event and your pattern and that api will just tell you\ntrue or false based on is your rule actually matching that particular example of event so\nthat could be a good way of testing things it's just a little bit annoying that you need to write\neither as code through the sdk or if you want to write it through the cli like it's it's very hard\nto write json in the cli especially for big events or rules so maybe an area where somebody can write\na little tool or a little ui to do that that would be definitely useful at least for me\nmaybe aws itself could do that that would be amazing then in the article that you mentioned\nfrom paul it's actually a really good article and it goes in depth in terms of all the things that\ncould possibly go wrong when you start to use event bridge like from the moment you publish\nare you actually able to publish and if not why yeah the moment you try to consume the message\nare you actually receiving the message and if not why and then after you receive the message\nare you actually processing incorrectly and if you fail to process it what can possibly happen\nso that that's an interesting article and if you are interested in all these possible edge cases we\nrecommend you to check it out there are another couple of tools that can help you in terms of\ntroubleshooting where whether you are actually delivering like from source to destination\ncorrectly one is something called event bridge cli again we will have the link in the description\nand this event bridge cli it's a tool written in go i believe that it's very interesting because\nyou just run it locally and it will provision in your account a rule that you are actually defined\nthat that's the one you want to test but it will automatically dispatch that rule into sqs\nand then it's going to create for you it's going to poll at the cli so you get like an interactive\ncli it's going to keep pulling from that queue so you can start to try to fire events and see\nif they appear in your local terminal so it's a very powerful and convenient way to test that\nto test that your patterns and your events are actually connected correctly together and i think\nthere is another tool which is probably similar from lumigo but i haven't used that so we'll put\nthe link in the description as well other interesting tool are event bridge atlas and event\nbridge canon which help you out as well with writing the events and testing them and sharing\nthem in a team so i haven't used them i haven't used them extensively so i cannot give you a lot\n\n\nEoin: of detail but definitely interesting to to check them out yeah event bridge atlas and event bridge canon both come from david boin who's also tweets and writes a lot about event bridge and has some\nreally interesting tools so yeah definitely worth checking those out um should we talk about\nintegration with sqs because i know we've got um we talked about dlq's last last time with sqs\ni know that um you can use dlq with event bridge which is really good because event bridge doesn't\ngive you reliability built in if you don't get to catch the event if you missed it it's it's gone\nright it can be in an archive but it's essentially gone but you it does have retry policies you can\nconfigure retry rules in your rule and you can set a dlq as well then your undelivered messages will\ngo to sqs um but when do you know so here's an interesting one when do you decide to process\nthings directly using a target that might be a lambda function or when should you put an sq\n\n\nLuciano: sq in between those two things so that you get that extra piece of resiliency yeah this is this is something that suppose we mentioned also in the in the previous episode about sqs that\nsqs can be used in general to to give you that extra peace of mind that a message is actually\nbeing recorded and then you can consume you can retry so i think that that's that should answer\nyour question right whenever you want to have that use case that when you really care about\na message and you want to be sure that it's being processed i think it's worth putting an sqs in\nbetween and then consuming the message through sqs rather than letting event breach invoke directly\nyour target in other cases where maybe you don't really care that much you can afford to lose a\nmessage or two probably it's not worth it you can just keep that integration and yeah and go ahead\nwith that yeah that's good god do you need best effort durability or most time durability i guess i guess isn't it okay cool what else should we cover relating to event bridge i think i will just mention few additional resources i i really like the work that sheen\nbristles from the lego group has been doing around event bridge so definitely check out his blog\nagain we'll have a link in the description there are i think six or seven different articles that\ncover pretty in-depth like tutorial style how to do different things with event bridge and also\nthere is a video that where he's talking a lot about different characteristics of event bridge\n\n\nEoin: we'll have a link for that as well and i think that's really good it's good to see how i think sheen in that talk gives good examples of how they structure their events and they've got a different\na different style to what we mentioned in the detail which is really interesting and really\n\n\nLuciano: worth watch so yeah definitely recommend that and yeah i think with this you get a pretty good overview of what you can do with event bridge how to use it what to watch out for so this is it for\nthis episode but make sure to follow and subscribe because in the next episodes we'll continue the\nseries about event systems we'll be talking about sns kinesis and gafka so yeah stay tuned we'll\nwe'll talk more about this stuff bye\n"
    },
    {
      "title": "24. What's SNS and how do you use it?",
      "url": "https://awsbites.com/24-what-s-sns-and-how-do-you-use-it/",
      "publish_date": "2022-02-17T00:00:00.000Z",
      "abstract": "Luciano and Eoin deep dive into SNS discussing what it does, how it differs from EventBridge and SQS and how you can use it to send messages to customers but also for microservices communication. In this new episode dedicated to AWS events and messaging services, we learn everything there is to know about SNS including advantages, limitations and cost. This episode complements the episode about EventBridge, giving another perspective on when to use SNS and when to pick EventBridge instead.\nIn this episode we mentioned the following resources:\n\nOur previous episode about EventBridge\nOur previous episode about all things SQS\nOur introductory episode about what services you should use for events\nA comparison between EventBridge and SNS by Cloudonaut\n\n",
      "transcript": "Eoin: Hello, today we are going to answer the question, how do you use SNS?\nBy the end of this episode, you will know how SNS compares to EventBridge and SQS.\nYou'll know how to use it to send messages to customers, but also for microservice communication.\nWe'll tell you what the costs and limits are, and also how you can integrate SNS with other\nservices in AWS. My name is Eoin, I'm joined by Luciano and this is the AWS Bites podcast.\nThis is the fourth episode in the series all about services for events and messages on AWS.\nWe covered SQS and EventBridge in detail already, and today we're talking about SNS,\nthe simple notification service. Luciano, would you like to tell us how would you describe SNS?\n\n\nLuciano: Yeah, so based on how we categorize the other events and messages services, I will place SNS in the category of PubSub. So you have this Publish and Subscribe API. It's interesting\nbecause it can target what is generally called in AWS A2A, which is like application to application,\nbut also something that they call A2P, application to person, which basically means that if you\npublish something to an SNS topic, then that message can be delivered also as an SMS or as\nan email or even a push notification. By the way, this is interesting because it feels like a little\nbit of a legacy feature because since this feature was available in SNS, we know that now we have\nother services that are more specialized to do that, like Pinpoint or even third parties that\nyou would generally use for SMS or emails like Twilio or SendGrid or for push notification,\nprobably you would use Firebase, Cloud Messaging. So it's interesting to see that, and we have to\nsay that also SNS is one of the oldest AWS services. It's 12 years old, I believe now.\nSo maybe this is a little bit of a legacy feature that was built there initially, but now you will\nprobably use other services. So in terms of advantages, what do you think are the strengths\nof SNS compared to other services?\n\n\nEoin: I'd say that the main strength compared to things like EventBridge and some of the other services, again, simplicity, but also really fast delivery. I think we mentioned\nthat in the last episode that SNS is typically lower latency. It's also push-based. You don't\nhave any polling like you do with SQS. You can have subscribers that are notified immediately.\nSo there's lots of really good things you can do with SNS. Typically what you're doing with\nSNS is fan out. You're taking a message and broadcasting it to multiple subscribers.\n\n\nSo the things you can do with it are like event-based communication across parts of\nmicroservice architectures, similar to what we discussed with EventBridge actually. So if you've\ngot a fan out pattern, so if you want to have the kind of behavior we talked about with SQS, where\nyou're sending message from one system to another, and you need that durability with SNS, you can do\nthat, but get multiple consumers involved rather than a single consumer. So you can combine SNS and\nSQS. For example, in our e-commerce application, a new order arrives, you can send the event to\nyour fulfillment service, the analytic service, and your billing or invoicing service. One of\nthe other things you can do with SNS is send handle system alerts. AWS services will often send\nmessages to SNS when certain things happen, like CloudWatch alarms or change state. That's how you\nget the notifications from CloudWatch alarms. And then you can take those alarm notifications,\nsend them by email, or send them to a webhook or a Lambda function, which could then send it to\nSlack or Teams or wherever you want to receive those. It's also used for code pipeline\nnotifications. And if you use AWS config for compliance, it can send SNS messages when\nthe compliance date changes. So I think that's a pretty... That's those are the kind of things I\nwould generally use SNS for. How would you describe the usage of SNS? We talked through\nthis, I think, for EventBridge and SQS. What are the steps?\n\n\nLuciano: Yeah, it's interesting because we will see that compared to what we described for EventBridge, using SNS requires a little bit\nmore work, I would say. And that's because for instance, just for starting, you need to create\na topic. And a topic is effectively the place, the channel where you're going to be able to publish\nmessages. Once you create a topic, you can create a subscription on that topic. And a subscription\nbasically allows you to say, somebody wants to receive the messages once they are published on\nthis particular topic. And a subscription, sometimes it's something you need to confirm.\n\n\nFor instance, if you do a subscription that requires the destination to be an email or HTTP\nor endpoints that live in another AWS account, AWS will actually ask you to confirm that that\nsubscription needs to be confirmed before it's actually activated. And sometimes, for instance,\nyou can receive an email and you have to click a link on that email or there are APIs that you can\nuse if you need to do that in an automated fashion. Then at that point, once you have a topic\nand a subscription, you can publish one or more messages to that topic and you can use\neither the publish API, which gives you one message, or the publish batch, which allows you\nto publish more than one message with one single request. And that's interesting. But another thing\nthat is very interesting is that very often you need to also configure what are called access\ncontrol policies, which are something very similar to IAM policies, at least they have\nthe very similar structure. But those are basically the way you can authorize, for instance,\ncertain users to publish to a specific topic or SNS itself to be able to deliver to a queue.\nSo give SNS the permission to use the put message API to SQS. And you can also put different\nlimitations in places. For instance, you could say for this particular topic, I only want\nsubscription that deliver to HTTPS, for instance. So you can use all this feature to create\nrestrictions on how messages are consumed, but also how messages are published to the topic.\nAre there other interesting details that are worth mentioning in terms of, I don't know,\nthe user experience of creating messages and consuming them?\nIf you remember with EventBridge, we were talking about rules and how you do pattern matching.\n\n\nEoin: And SNS has kind of a similar feature called message filtering. But the important\ndistinction there is that message filtering is based on message attributes and not the\nmessage content itself. So it's more like a filtering on metadata. So it can be used\nto partition and filter messages. And that works pretty well, but it just doesn't have\nthe flexibility that you have with EventBridge. It feels like we're comparing it with EventBridge\nquite a lot when we mention all these features. But one of the interesting ones is that you can\nyou can have up to 12.5 million subscribers per topic in SNS. EventBridge doesn't give you\nanything like that. You generally don't need it, but you would have five targets per rule\nin EventBridge. Of course, you can have as many rules as you want.\nSo then I guess we should talk about pricing. And it does seem like it's a little bit cheaper than\nEventBridge because it's about 50 cents for a million SNS requests. And then you've got\nadditional costs for things like HTTP notification deliveries and email deliveries. So\nif you want to target those endpoint types, that's something you should look at.\nWhat about reliability? With EventBridge, you have DLQ. With SQS, you can have another Q,\nwhich is a DLQ. You get that with SNS too, right? But you also have other similar features like\nFIFO topics, which is quite a new one for SNS. Do you want to talk about what that can give you?\nYeah.\n\n\nLuciano: The only thing I know about that one, I haven't used it yet, but it's basically it allows you to push messages to FIFO queues and make sure that the order is respected in the process. So end\nto end, you will be able to pull messages from the queue with the same order that the messages were\nproduced in the topic in the first place. This is as much as I know. I don't know if you have any\nmore detail that is worth sharing about that one. Yeah.\n\n\nEoin: I think the feature is, I guess the important thing is that with SNS, FIFO, the only subscriber type is an SQS queue. That's the only\none that makes sense. But yeah, it's really important, obviously, I guess if you have the\nkind of use case that you might normally use something like Kinesis or Kafka for, where you've\ngot strict ordering with multiple subscribers. So it's a really good addition. A lot of people\nwere very excited when that came out. We should also talk about, since we've been compared to\nEventBridge, how it compares to SQS. So you've got obviously multiple consumers and you don't\nhave durability built in. So I mean, those are really headline differences between SNS and SQS,\nbut they combine really well, don't they? So it's very common to see SNS with SQS subscribers.\nI think we covered when we were talking about SQS, that if you need the best of both worlds,\nyou can just essentially glue them together and you've got queues on the receiving end,\na topic on the publication end. And once you link the two, you can even have cross account SQS\nsubscriptions from an SNS topic. So you can imagine how that would work in a microservices\ncontext when you have services deployed in different accounts or even across applications.\nYeah, there are different patterns that you can probably achieve by just combining SNS and SQS.\n\n\nLuciano: So that's a very common integration, I will say. In terms of comparison with EventBridge,\nwe already mentioned a few points. I think there are other ones that are also very interesting.\nAnd we already said, for instance, that topics need to be created in advance. In SNS,\nwhile in EventBridge, you get a default bus, you can create new bus if you want,\nbut you get something by default. So you don't necessarily need to provision any infrastructure\nwith EventBridge. In SNS, a subscription is bound to a topic. So you say, I want to listen for\nmessages published in this particular topic, while in EventBridge, you listen for messages in a bus,\nwhich is kind of considered less granular if you want because a bus is more general purpose,\nwhile a topic is more specialized, at least. This will be the most common conception, in my opinion.\n\n\nAnd the reason why is because in a topic, when you subscribe, you generally get all the messages\nin that topic, while in EventBridge, you generally provide rules that will allow you to select only\nmessages with a particular structure that you are interested in consuming. So EventBridge is\ngenerally, you have the common bus for everything, and you just select things that you are interested\nin by providing a pattern. With SNS, you select the specific topic that has the information you\nwant. You can probably achieve something similar with filtering, but we already mentioned that\nSNS filtering, you can only filter on metadata and not the message content. So you will need to craft\nthat metadata in advance for the rules that you expect if you want to use filtering to select a\nsubset of the messages in the topic. Also, another interesting thing that we mentioned in the\nprevious episode that EventBridge gives you AWS events or even third-party integration events,\nlike Salesforce or stuff like that, while in SNS, you don't have any of that. It's up to you to\npublish something into the different topics. SNS is much faster. Typically, you have a latency\nof 30 milliseconds compared to something that can be around 500 milliseconds for EventBridge.\nSo definitely, this is probably the killer feature that is going to make SNS win against EventBridge\nin certain scenarios. Like when you really need good latency, SNS is your choice.\nEventBridge has many supported targets, while SNS is a little bit more limited. We'll mention some\nof them in a second. And the other interesting thing, this is very cool feature that I really\nlike from EventBridge, that when you do HTTP targets, you can define a template that allows\nyou to change the structure of how the HTTP request is going to be sent. Instead, with SNS,\nthere is a standard format and that's it. So you'll need to build your APIs to accept that\nformat. Instead, EventBridge is a lot more flexible in that sense.\nOr you'd have to put a Lambda function in between, I guess, if you need to transform it.\n\n\nEoin: Exactly. And then you use that seamlessness.\nYeah. Do you want to tell us something about the integrations then?\n\n\nLuciano: Yeah.\n\n\nEoin: So we talked at the start about the consumer targeting versus application to application cases. So you've got SMS, mobile, push, and email. But in terms of\nservice integrations, you can target SQS endpoints, Lambda, HTTPS endpoints, and Kinesis data,\nfirehose. So that can be a sync for all of your events. And on the publication side, S3\nnotifications can target SNS. API gateway can target SNS. DynamoDB. Also, SES, EventBridge\nitself can target SNS. But in the same account only, whereas with EventBridge to EventBridge,\nit's much easier to do cross-account. And Step Functions, of course, can publish to SNS,\nbecause Step Functions can call any AWS API pretty much these days. So that, I think, covers the\nprimary integrations you'd use. Obviously, a limited subset compared to EventBridge, but\nthat's one of the trade-offs. And I think that pretty much covers all of the details on SNS.\nAnd wraps up this part of the series. Next focus will be on streaming, which is a different ball\ngame and should be really interesting. Before we go, I just wanted to call out one article,\nwhich is going to be in the show notes, which is from Cloud or Not blog, and it compares SNS\nto EventBridge. I think it's a topic people are really interested in these days, so I'd recommend\nyou take a look at that. So thanks for listening, and we will see you in the next episode.\n"
    },
    {
      "title": "25. What can you do with Kinesis Data Streams?",
      "url": "https://awsbites.com/25-what-can-you-do-with-kinesis-data-streams/",
      "publish_date": "2022-02-24T00:00:00.000Z",
      "abstract": "Eoin and Luciano are back with some more AWS messaging services material! This time we talk about Kinesis Data Streams and everything there’s to know about them! In this episode of AWS Bites, we cover the difference between data streaming and queue or message buses. How Kinesis can let you process large batches of messages in near real time. Why you might use Kinesis as a queue or pub/sub bus in some cases. A few different ways to use Kinesis with the focus on avoiding too much complexity. We also provide some insights based on our experience with Kinesis Data Streams including real use cases, monitoring tips and some gotchas to be aware of.\nIn this episode we mentioned the following resources:\n\nOur introductory episode about what services you should use for events\nAmazon Kinesis Producer Library (KPL)\nAmazon Kinesis Consumer Library (KCL)\nKinesis Library wrapper for Node.js\nKinesis Library wrapper for Python\nKinesis Data Stream with captures from DynamoDB (for CDC)\nKinesis Data Stream with captures from Aurora\nKinesis Data Analytics\nKinesis Firehose\nMust-read on Kinesis: Anahit Pogosova’s two-part blog series, part 1\nMust-read on Kinesis: Anahit Pogosova’s two-part blog series, part 2\nCloudonaut Kinesis vs MSK\nDeep Dive on Lambda Consumers for Kinesis / Heeki Park - re:invent 2020\n\n",
      "transcript": "Luciano: Hello everyone, today we're going to answer the question what can you do with Kinesis data\nstreams and today we're going to explore this topic and talk about what are the main differences\nbetween data streams and queues or message buses, how Kinesis can let you process large\nbatches of messages in near real time, why you might want to use Kinesis as a queue or a pub-sub\nin some particular scenarios and also we're going to explore a few different ways to use Kinesis\nwith the focus on avoiding too much complexity. My name is Luciano and I'm joined by Eoin\nand this is AWS Bites podcast.\nSo this is one of our last episodes in our series of AWS events services and we are now focusing on\nstreaming data. So I suppose the first question that we should try to answer is what is streaming,\nwhat do we mean by that and in particular in the context of Kinesis data streams what are the\npieces of functionality that we get from this particular service. What do you think Eoin?\n\n\nEoin: Kinesis data streams we're talking about, so this is a fully managed event streaming service,\nso yeah like you say streaming is different to queuing and pub-sub systems, the ones we've\ntalked about already, so we should probably talk about what streaming is and how it's subtly but\nquite different. With streaming you're talking about processing continuous streams of events\nin near real time and typically in batches right, larger batches with SNS or SQS you might be\ntalking about individual messages normally they're standalone units but with pub-subs you're talking\nabout single message to fanning out to multiple consumers but streaming you're typically talking\nabout larger volumes of messages from multiple producers being processed in bulk so in batches\nby one or more consumers and more or less in real time so you're really talking about a continuous\nstream of batches of records and another difference with streaming with things like\nKinesis and Kafka it retains the records even after they've been processed so you can always\nhave new consumers join or you can reprocess data by going back to the data that's stored\ndepending on how long you've configured the retention of your streams.\n\n\nLuciano: Yeah that can be very useful also when you have a bug in your code and you realize that maybe you you're processing\ndidn't really do what you wanted to do in the first place you can fix the bug and then basically\nrewind your ingestion start from scratch and re-ingest everything again.\n\n\nEoin: Yeah that's true that's true I mean we should also say that even though we're clarifying the differences between\nstreaming and queues or pub-sub it's actually very common well not uncommon I would say for people to\nuse Kinesis or Kafka as a queuing system or as a pub-sub system and then there's the question why\nwould you do that and I suppose you'd only really do that if you need some of the specific features\nthat Kinesis or Kafka provides that aren't provided with SQS or SNS so those we'll go into\nthose in a bit more detail but we should probably do our usual rundown through some of the use cases\nfor Kinesis data streams where should we start?\n\n\nLuciano: Yeah I always like our e-commerce example so in the context of an e-commerce how can we avail of Kinesis data stream like what kind of utility can\nwe get from from this service and one example that we mentioned before in one of our other episodes\nwas if you need to collect clicks let's say or mouse movements from the users in the e-commerce\nbecause that will give us an information what kind of products they're possibly interested in what we\ncould do is that we can collect this data almost in real time from the UI send it to a Kinesis data\nstream and then we can have some processing logic that it's getting the data in real time\nfiguring out which products maybe are relevant for the users and we can send this data back maybe to\na web socket or some other API and refresh the UI in real time with interesting suggestions so that's\na cool use case and we could definitely implement that with Kinesis data streams. Also another thing\nthat I have done in the past is basically you can capture logs for instance from CloudWatch logs\nor other sources and you can store them in Kinesis data streams and then you can let\nthe Lambda integration process these logs pretty much in real time and you can do all sorts of\nthings with that. One use case I've done before is basically if you store things that allow you\nto extrapolate metrics then from your Lambda you can actually ingest this information as\nCloudWatch metrics so you can basically transform logs into metrics by using Kinesis data stream as\na quick way to transport the data and then start some processing. Any other example comes to mind\non your side?\n\n\nEoin: Yeah, speaking of metrics there are quite often a few cases where AWS, CloudWatch metrics for some services they're missing some of the metrics you want. Often for\nthings like when you're trying to track the number of resources for pricing you know you want to be\nalerted when you're using too many containers or number of vc2 instances you can use the last time\nwe were talking about event bridge events that come from AWS out of the box so you'll get state\ntransition events when a container starts or stops similar with EC2 instances. With event bridge it\nintegrates into Kinesis data streams quite easily so you can set a data stream as your target for\nan event bridge rule you can take all of those events that are coming relating to ECS containers\nand then you can process them in in batches in lambda or another consumer and use it to figure\nout okay at any given point in time how many containers are running how many containers are\nrunning in each state what is the duration of a running container and you can create your own\ncustom metrics then. This is something I've done a few times with things like ECS and AWS batch\nwhere the metrics out of the box have a few gaps and it works pretty well.\nI suppose you also see other interesting use cases like people are building a delivery service you\nknow like uber deliveroo that kind of service if you can imagine an architecture for that system\nyou've got a lot of real-time event processing so you're collecting orders you're collecting\nresponses from drivers all of those cases are things where you would say okay this is a\ndelivery stream I could use Kinesis or Kafka for this. Yeah absolutely. So I think that's\na good thing. Yeah absolutely.\n\n\nLuciano: Yeah we should maybe talk now about some of the main features of Kinesis Data Stream. So we said that the data is going to stay there but of course it's not\ngoing to stay there indefinitely you can configure a retention time by default is 24 hours but you\ncan scale that up to one year so you can keep your data around for a very long time if you want to.\nAnother thing that is worth mentioning is that you get very very good latency it's under a second to\nbasically when you store that first record from when you actually consume it\nand can be around a second or 70 milliseconds depending on how you use it so we will talk\nmore about this later on. And then messages in Kinesis are called records as you might have\nguessed at this point but a record generally has a sequence number and a partition key\nand then there is of course a date of art so the payload itself which is the part you are\ninterested in processing. Another thing that we mentioned to some extent but is worth highlighting\na little bit more in detail is the concept of shards. Shards are the unit of scale for\nKinesis and I suppose you can imagine shards as different locations where the data is being\ndistributed and partition keys are basically is the concept used to select given a record in which\nshard is going to end up so this is very similar to many other distributed systems so it's not\nmagic in that sense it uses the same principles as many other distributed systems.\nAnd of course that gives us other guarantees so you're not going to have absolute ordering\nof messages but you will have relative ordering per shard so only the data ending up in the same\nshard will respect partial ordering there but if you look if you fetch data from different shards\nthey will not necessarily be in order.\n\n\nEoin: Exactly that's a little bit like what we talked about SQS and message group IDs it's a similar principle yeah and of course what that means is that with\nguaranteed ordering you always have physics getting in the way and putting limits on you\nso we should talk about those limits because the limits of Kinesis and what you can do with a\nsingle shard really dictate the architecture of your system so normally we talk about quotas and\nlimits at the end but we're going to talk about this a little bit earlier because you get very\nfixed capacity and you have to think about that and how it impacts the architecture the scalability\nand the level of concurrency. It makes your architecture a little bit more complex and it\nkind of makes you realize that streaming data applications are inherently more complex than\nqueues and pubsub so it's probably worth saying at this point if you don't need any of the specific\ncharacteristics of these services it's always better to stick with something simple like SQS\nand SNF Enbridge because they do give you you can get low latency and scale and massive concurrency\nwith those systems you don't need a streaming data system to get that it's only if you really have\nspecific needs that some of the Kinesis or Kafka features will give you just around the number of\nmessages you can get in a batch for example or the integrations with specific other systems\nthose are the only reasons where you really need to reach for those more complex streaming\napplications but anyway with that out of the way what are these limits so we have strict limits of\none megabyte write per shard or a thousand records per second write capacity so that's how much you\ncan write per shard in a second or you can read two megabytes or two thousand records per second\nfrom a shard so the right read capacity is double the right capacity and it's very easy to remember\nbecause it's one megabyte and one thousand for right but you're limited to how often you\n\n\nLuciano: on this one i want to mention sunday because the first time i noticed these limits i was a little bit confused i was thinking okay why can you write at a certain speed and then you can read\na double the speed and that might seem counterintuitive at first but we need to keep\nin mind that you might have multiple consumers so you are writing from one side but you have\nmultiple consumer on the other side and these limits needs to be accounted for all the consumers\n\n\nEoin: at the same time so they they compound you they are not specific for consumers exactly yeah and you also sometimes need to read more than you're writing because you might be catch up you might\nbe reading from older records in the stream so you have to catch up so you can't catch up if the\nlimits would be only the same as the the right limit but there's also the five get you can only\ncall get records you can only have five read transactions per shard per second as well which\nis another interesting interesting one so you could that means you can read up like with a\nsingle read transaction you can actually read 10 000 records so remember we were both talking about\nthis is about batches that gives you an indication right in one read transaction\nyou might be reading 10 000 records or 50 megabytes so you just need to think about that\nwhen you're thinking about latency you might say okay i can get like 50 milliseconds i i i've\nobserved that i can get messages in 50 milliseconds so if i just call get records\n20 times a second that's fine but you can you can only call it um five times per second so that's\nreally the unit of scale the size you can think about if you if you know what rate you're going\nto be ingesting at and reading at then you can do the maths and figure figure the rest out as\nregards other limits soft limits you have a soft limit of either 200 or 500 shards per stream\ndepending on the region but you can always increase that to i think tens of thousands of shards\nso you can really get high volumes through kinesis and i guess since we're talking about\nunits of scale we know the limits what do you do then how do you scale how does kinesis scale for\n\n\nLuciano: you yeah that's an interesting one and it's a little bit of a controversial topic because in some cases you will hear kinesis being addressed as a serverless service but i suppose\nthat really depends on the definition of serverless that you have in mind yeah sometimes the argument\nto that is that it doesn't really scale to zero that means that you have to think about how do\nyou provision kinesis data streams up front and once you do that if you don't use them they don't\nautomatically scale to zero so an interesting news on this field was that in the last reinvent\naws announced the on-demand mode for kinesis data stream which helps a little bit to see kinesis data\nstream in a more serverless way but again they don't scale to zero so you have to worry less\nabout provisioning them in advance but they will never scale to zero and even if you don't use them\nthere is always some cost that you will need to pay anyway so it's interesting like you could\nargue it's serverless but not quite anyway talking about scalability we at this point we understand\nthat the shard is probably the primary unit to understand how kinesis data stream will scale\nand the interesting thing is that if you use the provision mode it's up to you to basically do all\nthe calculations up front and allocate the number of shards that you think are going to be enough\nfor you to sustain your read and write throughput you can change that over time like if you realize\nat some point you need more or you over provision and you want to scale it down you can do that but\nthere are restrictions you can do it only 10 times per day meaning a 24-hour period and then\nit's interesting that you can only either double or decrease to half the original number of shards\nfunny enough you can do arbitrary amounts if you use the web ui but you can see that the value\nfluctuates until it actually converts to the value one so there is some magic there if you use the\nweb ui looks like you can go to any arbitrary number but there is still this restriction where\nyou are doubling and decreasing enough until you get to the number you really want to use yeah and\n\n\nEoin: of course the reason for that essentially is because if you look at the apis for doing it if you're implementing this programmatically as a lot of people do especially before on-demand mode came\nout the way you increase the number of shards is by splitting existing shards and the way you\nand the way you reduce the number of shares is by merging two existing shards together\nso that's where the doubling and the half comes from but i guess yeah you don't have to do that\n\n\nLuciano: so much so how does on-demand mode work then yeah on-demand mode is pretty much aws will figure it out for you and it's based on the actual throughput that you have while working on the kinesis data\nstream so you start with a default write capacity of four megabytes per second or four thousand\nrecords which is probably for shards behind the scenes we just don't have to worry too much about\nthe concept of shards in this case and if you keep writing and reading more so if you will need more\ncapacity the on-demand mode will automatically scale the stream and you will be able to go up\nto 200 megabits per second or 200,000 records for writing into the stream of course it doesn't scale\ninstantaneously so be careful with that like if you have spikes that are like instantaneously\ndoubling or tripling your throughput don't expect to scale immediately it will take a little bit of\ntime i think it's around 15 minutes before it will decide to double the capacity so you might still\n\n\nEoin: get throttled with with this approach if you have spikes so be careful with that yeah i also think it's kind of worth mentioning that while it says you you get a default write capacity of four\nmegabytes and four thousand records and like you say that equates to four shards and you might think\noh that's great i don't have to worry about my shards anymore of course at the consumption side\nyou need to think about the read uh the limits because you still have a per-shard read limit\nand if you've got four shards you kind of need to know about that because it affects the concurrency\nof your consumers and if we'll talk a little bit more about lambda but with lambda the number of\ninvocations of lambda will depend on the number of shards so if you don't know the number of shards\nit's very difficult to predict how many lambdas are going to be invoked so you always need to\nkind of think about that anyway that's true even if even if it's happening in an on-demand fashion\n\n\nLuciano: you should probably do some monitoring on that absolutely yeah do we want to mention also there is another mode called enhanced fan out which it's it makes sense it's relevant only for consumers\nand it's a different way that allows you to consume the data because in this case you basically\nget a pipe so every consumers will have a pipe and the capacity of how much data you can read\nper second is basically per consumer in this case and it goes up to two megabytes per second per\n\n\nEoin: shard if i remember correctly so the interesting thing is that i guess the sorry to jenna but yeah i guess the interesting thing here is that with these limits we talked you mentioned the read\nlimit of 2000 and how it's greater than the right limit but it's shared with your consumers so\nlike you said you've got five if you have let's say two consumers they're sharing that they get\na thousand each or a megabyte each and that can be quite difficult if you've got multiple consumers\nif you've got you know one consumer that's aggregating events one that's sending them to\nfirehose and another one that's doing another lambda based aggregation with enhanced fan out\nyou're basically saying give me a dedicated pipe and it's using http2 and instead of you're pulling\nthe events it's pushing them to you so you essentially get 2000 records or two megabytes\nper shard per consumer per shard per consumer right instead of per shard which is really good\nwhich is really good yeah and you also get lower latency for free as well because it's push mode\nit can it can give you that latency of yeah i think it says average 70 milliseconds\n\n\nLuciano: instead of the average of 200 milliseconds with a standard shared consumer yeah thanks for clarifying that do you want to mention something quickly about the different pricing model\n\n\nEoin: especially comparing the on-demand and the provision mode yeah this is this was a surprise because a lot of people have been anticipating an on-demand or kind of serverless version of kinesis\nfor a while and the slight disappointment came from the pricing because it looks like in order\nto take advantage of on-demand mode it doesn't immediately make it free but it's because it\ndoesn't scale down to zero so the kind of entry level pricing for each if you compare them on\ndemand is actually more expensive than provisioned because with on-demand you pay per stream per hour\nand with provision you pay per shard per hour so if you if you go with provisioned the lowest\nprovisioned level which is one shard you'll pay about a third almost a quarter of the price of the\non-demand baseline which is around four and a half cents per hour because on-demand is charging you\nper stream it's giving you four shards out of the box but if you don't use any of those shards\nyou're still being charged right so the baseline is higher for on-demand and then with on-demand\nyou're being it is kind of more senseless and serverless in the sense that it's charging you\nby the data that's flowing through the system the data ingested and the data retrieved whereas with\nprovisioned you're paid by the shard and then the shard gives you a capacity you don't pay as you\nread so it depends i think the the the if you're trying to judge which is the right mode from a\npricing point of view you just have to compare your use case it's going to vary for everybody\nsome cases on demand will be cheaper in other cases provision will be cheaper it depends on\nyour trade-offs between performance scalability cost and the value you're getting out of this\n\n\nLuciano: feature so it's yeah it's really a business context how frequent do you actually write and read to and from the stream like is it a constant stream or is it something that can be a little bit more spiky\nyeah yeah all right in terms of observability because we slightly mentioned that before\nbut yeah let's leave let's see maybe what are the most interesting metrics to look after to to see\nif we are actually using the data streams correctly so of course the the first one and\nprobably the most obvious is the iterator age which can basically tell you the age of let's say\nthat the message the latest messages you still have to process so it's kind of an indicator of\nhow far behind are you in the stream in consuming it so it's a good way to see if your read capacity\nis is not well tuned as opposed to the rate of producing data so keep that in mind because that\nmight might be very useful for you to to guarantee that you are actually producing and consuming data\nas in real time as possible another one is throttling and this is a little bit of a\ncontroversial topic in my in my opinion because it's you can get accurate throttling metrics\nbecause of course you'll get a data point every time you get throttled but if you want to try to\nanticipate throttling you have to do a little bit of maths on your own and you can look at the get\nrecord and put record matrix and the problem with those metrics is that the ones you get by default\nin cloud watch they are aggregated by minute so if you have very bursty data producing and\nconsuming the data then you don't really know in a minute how much are you doing for instance in a\nsecond so you only see the minute value so you might see a value that looks okay but then you\nsee that at the same time you got a throttling for that particular minute so be careful with that\nand if you really have bursty use cases maybe you want to do something a little bit more custom\nand maybe record metrics on your own to make sure that you understand if you want to understand in\nadvance when you might incur in throttling otherwise the simple way of looking at this is\njust look at the matrix for when the throttling actually happens in reading and writing so when\nyou exceed the capacity and then you can adjust your shards or you can adjust your process\nto avoid the throttling in the future sometimes even a little bit of throttling can be fine\nbecause of course the sdk will retry for you so if that's not really slowing down your pipeline\nprobably you don't even need to change the chart so keep a look at this matrix because they will\nbe telling you a lot and you can understand whether you need to change some configuration\nand how you read and write the data or the number of shards itself\nAt this point maybe we should talk I think you probably have a fair idea on how to use\nkinesis data stream but this may be good to do a recap and maybe provide a little bit more details\nright?\n\n\nEoin: Yeah okay I'll try there's generally one way I usually try to use kinesis but I'll talk about that last because there are kind of three different ways to use kinesis and I'll talk about\nthat last because there are kind of three different ways to use it there's the api there's the libraries\nand then you've got lambda and if we talk about the api usage first it probably gives you a sense\nof how it works under the hood even if you use some of the abstractions so when you're producing\nmessages and sending them into kinesis there's a put records api and it allows you to put 500\nrecords at a given api request and it does that really quickly as well so when you're again when\nyou're comparing to queuing or pubsub kinesis is all about big lots of messages big batches you can\nput in 500 in a single request you'll remember like with sns and sqs it's 10 with the batch api's\nso you can put in a megabyte total per record or up to five megabytes for the whole request\nso there's a bit of maths to be done there as well and when you specify that you put in the\npartition key which you already mentioned so the partition key is useful for dictating ordering\nand also the shard it's allocated to so aws will take your partition key produce a hash of it\nand use that hash to dictate which shard your message goes into because every shard basically\nhas a range of hash keys that it will accept so it's just basically an integer allocation for each\nshard a range of integers but if you want to override that and you want to be really have\nlots of fine control over which shard a message goes into you can actually override the key and\nspecify that integer in the message as a decimal value and that's called the explicit hash key\ngenerally you don't have to do that but in some cases if you want to be really\nif you want to control that ordering and the shard allocation yourself you can do that\nI suppose since you're putting 500 messages as well it's also really important I think a lot of\npeople using Kinesis have been here if you use the put records api you have to ensure that you\nhandle the errors that come back because you can get partial success and the put records api will\ntell you which ones have failed and then you need to reprocess those so it's not like a you're\ngoing to get an exception or just an error code back you have to look at the error response and\nfigure out which ones have succeeded and which ones have failed and redrive so that's the\nproduction side the producer but if we look at the consumer side retrieving manually using the api\nthere's quite a lot of complexity to it or at least there's a lot of heavy lifting because\nyou've got multiple shards you can re you can call get records for a single shard so then you need to\nget first of all find out how many shards there are you establish an integer and then you can\nyou establish an iterator by calling get shard iterator and the iterator is like a pointer\nthat the service is maintaining for you and then with that pointer you call get records\nbut when you establish your connection get an iterator you can specify whether you want to\nread from the latest message the oldest message in the stream or you can specify a specific point in\ntime or a specific record and then once you have your iterator you call get records in sequence\nand you can do it up to five times per second and every like we said already i think you can get\n10 000 messages in each get records request then you just it's essentially like pagination after\nthat but of course like there's a bit of complexity in that managing multiple shards\nyou might need like multi-threading to do that you have to keep track of where each thread is storing\n\n\nLuciano: it's you know is what chart each thread is retrieving from and that's one one good analogy that we can use to understand really what's going on there is that imagine that every single shard\nis a file that where some all the producers are pretty much appending more and more data\nwhile at the same time you have consumers that have a pointer in every single file\nyou're trying to track okay this is the point where i arrived consuming the data\nand they need to figure out how to move forward not just on one file but for every single shard\nyou have like one file yeah multiple files multiple pointers with data coming in all the time\n\n\nEoin: in every single file yeah i mean these systems are ultimately very fully featured distributed logs and i do like that i think that's really good analogy if you think of each shard or partition\nas just a text file and every record is on a line and then you just need to keep track of\nthe line number and that's like your iterator um the yeah i mentioned that there's complexity here\nso the kinesis client library is a consumer library written in java that aws provides to\nmake this a bit easier now you do have to understand how it works a little bit it manages\nthat pool of workers and multi-threading for you and it uses dynamo db to track the state um so it's\nhas it can recover and it can keep track of things as shards increase and decrease\nit does it is written in java it does have some findings for other languages like node js and\npython that essentially use the java process running as a daemon and use s standard input\nand standard output to communicate with the node js or the python process so that's the consumer\nlibrary but there's also a producer library the kpl and that allows you to send messages but it\nhas multi-threading retry logic batching in there for you and it can also try to reduce the chances\nthat you're going to get throttled on you know the number of messages per second limit by packing\nlarge numbers of small messages into a single record and that's something that might be complex\nto implement yourself but if you use the producer library and the consumer library together\nit happens seamlessly for you so i think especially if you're in a java ecosystem and\nyou're using instances or containers with kinesis those libraries are a good fit but there's still\nyou still have to understand how they work you still have to manage the extra resources\nso the third way of using kinesis is definitely my favorite and that's with lambda so i i've\ni've talked enough about the first two do you want to try cover um how lambda works with the\n\n\nLuciano: event source mapping that we talked about previously sure yeah i'll try to explain how that works but feel free to add more if i'm missing any important either so we already\nexplore something similar when we talk about sqs where basically we say sqs has a bunch of api\nis for you to fetch the data but then when you use it with lambda it becomes a little bit more\nmagic and you get a lot of stuff magically integrated and working well out of the box\nand the reason why that happens is because lambda has this component called event source mapping\nwhich is basically something that can be used to pull information and trigger a lambda for you\nso in the case of kinesis this integration exists as well and basically what you can do you can just\nsay use a kinesis stream as a source for a lambda and what happens is that as new data is available\nin the stream your lambda will be triggered and you will get an event that describes the data\nin the stream a little bit more involved than that of course because you might want to configure\nhow you get the data how many lambda gets executed concurrently and things like that so of course\nthese integrations don't you can configure and there are specific parameters you can use you can\nalso do things like aggregations i think it's called tumbling window aggregation so you could\nconfigure this integration too as it's going through the stream aggregate some data and then\ntrigger your lambda with already a partial aggregated result so you can even let the this\nevent source mapping do some of the work for you and just trigger the lambda with some partial\nvalue already calculated other interesting thing is that you could define rules to filter events\nso you could specify a filter that tells you i'm only interested in triggering the lambda for\nevents that match certain conditions another interesting one is the batch size window count\nuh count so for instance you could say i want to receive let's say i don't know batches with 30\nmessages so i want to trigger a lambda when i accumulate 30 messages but you can also say unless\nmaybe a certain amount of time has passed so i want to receive a smaller batch if i'm waiting\ntoo long so you could configure this batch size and window to say what's the maximum amount of\ntime you want to wait or if you can produce batches of a certain size within that unit\nstart to trigger the lambda anyway another interesting one is the parallelization factor\nwhich is what is going to tell us how many lambdas we are going to potentially spin up\nconcurrently for processing that particular data stream and basically what what you can\nsay you specify this value and that value gives you the number of lambda concurrent repair shard\nso let's make an example if you have two shards and you say that parallelization factor is four\n\n\nEoin: i think you get eight up to eight concurrent lambdas at any given point right yeah yeah am i missing an interesting thing i just wanted to chime in there to mention you might be we\nmentioned that ordering is strict per shard so if you if you haven't dealt with this with lambda\nbefore you might be wondering well if ordering is strict per shard how can it do concurrent\nmultiple concurrent lambdas for one shard and event source mapping also manages that for you so\nwhat it means is that if you've got the same partition key so the partition key is quite\nimportant here if you've got the same partition key for a message it will guarantee that messages\nfor the same partition key will still be order guaranteed for each concurrent lambda processor\nso even though the hash key is different it'll it'll it'll let the hash key will allow it to\ngive that concurrency per shard but you'll still get strict ordering per partition key\nwithin that so it's like an extra level of order guarantee there so it's um it works\npretty well but you just have to make sure that the ordering guarantee matches your expectation\n\n\nLuciano: that's an interesting one i was not aware of that one okay do we want to get to the end of this episode by talking maybe maybe about some of the integrations that you will get\nwith kinesis data streams and other aws services\n\n\nEoin: yeah we mentioned event bridge already because we said we you could take events from event bridge and put them into kinesis data streams to make metrics and all sorts of useful things but there's\nalso a couple of interesting kind of change data capture possibilities you can do with data streams\nyou know when people are using streaming applications one of the other use cases we\ndidn't really talk about is kind of like event sourcing or you're using a complete history of\nevents to accumulate the state of a system like in a financial application so change data capture is\nis quite common in some fields and people are using the changes in the database as an event\nit changes in the database as an event source for aggregation for reporting but also for\nreal-time calculations and you can integrate kinesis data streams into dynamo db and get a\nchange data capture and also for aurora in rds so it'll give you a continuous stream of database\nchanges coming from your tables and you can either aggregate that or use it to build accumulate state\nbased on a history of records so you also got the other two products in the kinesis family and this\nis sometimes you know the branding and naming confusion but we talk about kinesis sometimes as\nif there's only kinesis data streams but you've also got the other ones do you want to talk about\n\n\nLuciano: what those are and what they can give you yeah so you have kinesis data analytics which is if you ever use fling apache fling is something very close to that probably implements the same api or\nprobably is using yeah but it's basically you can define processing logic and aggregation in real\ntime directly on the stream so you basically define yeah how to get the data aggregated and\nproduce new data so it's kind of that analytics use case is the best use case for seeing this\nparticular kinesis variation the other one is kinesis pharaohs which is also quite common and\nthe idea is that sometimes you just want to flush all the data in a stream somewhere so rather than\nwriting all that integration yourself you can use pharaohs and integrates automatically with things\nlike s3 redshift elastic search or even apis through api gateway or directly calling http\nendpoints so that's a good way when you just want to store the data or propagate it somewhere else\nyou just use pharaohs write it to the stream and let pharaohs do the rest of the integration\nyou can also use that by the way to connect different kinesis data streams together because\nyou could use pharaohs to say move the data from one stream to another so the destination\non pharaohs can be another kinesis data stream there is another one i think related to videos\nyou can even have kinesis video streams but i will leave that aside for this episode because\nit's a little bit of a snowflake i think it's the naming is confusing in reality it's in the family\n\n\nEoin: of video services should we mention maybe some material for deep diving on the topic there's there's a must read on kinesis actually i believe because the documentation can be difficult as it\noften is but there's a two-part blog post by anahit pogosova which is almost like the de facto manual\ni think for kinesis at this point so we'll provide the links to those in the show notes\nand everything here we've covered is covered in really just comprehensive detail really well in\nthose articles we mentioned that we'll be talking about kafka as well so there's a comparison\nby the cloud and us guys between kinesis and managed streaming for kafka which is definitely\nworth checking out there's accompanying video with that i also saw a really good one in reinvent\n2020 which is a deep dive on using lambda specifically with kinesis by hiki park and we'll\nprovide the link to that one too that talks a lot about how it's implemented under the hood and how\nstandard consumers work versus enhanced fan out really worth checking out so if you're looking at\nkinesis because you have to consider the how they work and the limits so much there's a little bit\nmore investment in your time as an architect or developer with kinesis and streaming data in\ngeneral i think it's worth taking you know a couple of hours looking at those resources and\n\n\nLuciano: you'll feel much more prepared that's great yeah i think with this we have covered a lot this is probably our longest episode ever so by the way feel feel free to let us know if you prefer longer\nand more in-depth episodes or if you prefer the shorter ones because of course we are always\nexperimenting and it's great to have your feedback on that with that let's keep in touch make sure to\nfollow and subscribe if you want to be notified about new episodes especially if you're liking\nthis event and messaging series you can get notified when we publish the next one which as\nowen said is going to be about kafka and again don't forget to connect we are looking forward\nfor all your opinions and if you have used kinesis we're curious to know your use cases\nif you had any trouble with it surprises because all of that it's something that we can share\nand learn from each other see you in the next episode bye\n"
    },
    {
      "title": "26. What can you do with Kafka on AWS?",
      "url": "https://awsbites.com/26-what-can-you-do-with-kafka-on-aws/",
      "publish_date": "2022-03-03T00:00:00.000Z",
      "abstract": "Luciano and Eoin explore the wonderful world of data streaming using Kafka on AWS. In this episode we focus mainly on Managed Streaming for Kafka (or MSK) and discuss what are the main differences between MSK and Kinesis. We also explore the main features that MSK provides, its scaling characteristics, pricing and, finally, how MSK works in conjunction with other AWS services.\nWe conclude the episode by providing a decision tree that should help you to decide whether you should use Kinesis or MSK or avoid streaming services entirely in favor of something like SNS or SQS.\nIn this episode we mentioned the following resources:\n\nOur previous episode on Kinesis data streams\nOur series of Event services\nAWS MSK sizing spreadsheet\nShould My Startup use Kinesis or MSK?\nIntro to MSK (reinvent talk from 2018)\nRunning Apache Kafka on AWS (by Frank Munz)\nCloudonaut - Kinesis versus MSK)\n\n",
      "transcript": "Eoin: Hello, today we are going to answer the question,\nwhat can you do with Kafka on AWS?\nWe're gonna take you through managed streaming for Kafka\nor MSK and the main differences between MSK and Kinesis.\nWe're also gonna talk about all the features MSK provides\nand the advantages over other Kafka options.\nWe'll talk about scaling characteristics, pricing,\nand then how MSK works with a lot of other AWS services.\n\n\nMy name is Eoin, I'm here with Luciano\nand this is the AWS Bites podcast.\nThis is the final episode in our AWS event series.\nSo the last time we talked about Kinesis\nand streaming data, and we're continuing to talk\nabout streaming data today with Kafka.\nSo I think Luciano, the last time we said that streaming\nis all about processing patches of messages\nthat are retained in a stream for a longer period of time\nso you can replay them.\n\n\nAnd it's good for a lot of use cases.\nLike we talked about real-time analytics,\nstream processing, cool things like event sourcing\nand then audit logs, that kind of thing.\nI guess Kafka became very popular\nfor microservices communication as well\nbecause it has such low latency, good delivery guarantees\nand has now a really rich ecosystem.\nOne of the things before we get into the details is,\nI think it's fair to say that we don't have as much\nexperience with Kafka as we do with all the other services\nwe've talked about in this series.\nLike we've both used it in the past,\nbut a lot of features that we're gonna talk about today,\nparticularly around MSK,\nare things we've used in production.\nSo we have done the research and evaluated MSK\nin various different ways, but we're really interested\nif you have any hands-on experience with Kafka and MSK.\nWant to share your thoughts and opinions\nand how it compares to the alternatives, please reach out.\n\n\nLuciano: Yeah, that'd be awesome.\n\n\nEoin: What are the options?\nWe've talked about, we're not just gonna talk about Kafka\nbecause it's AWS, we're just gonna focus on MSK,\nbut there's other options out there for cloud-based Kafka\nif you don't want to manage it yourself, is that right?\n\n\nLuciano: I know at least about two of them that are Confluent Cloud, which historically probably the first one\nthat they came up with a service like this.\nSo managed Kafka for you and Confluent,\nthey are the experts in the market.\nThey're all about Kafka, they build numbers of plugins,\nthey contribute to the project itself.\nSo probably they know their stuff,\nbut there is also a very new one\nthat is called AppStash.\nWe mentioned it previously\nregarding their serverless offering for Redis\nand they recently launched also an MSK,\nequivalent let's say, so managed Kafka on AppStash servers.\nSo you might want to look at this other alternatives,\nmaybe they have a different feature set,\nmaybe different pricing.\nSo if you're looking for managed Kafka,\ndon't limit yourself to look at AWS for sure.\nYeah, do we want to start to have a quick walk\nthrough the features of Kafka?\n\n\nEoin: Yeah, let's do that.\nSo I think with Kinesis you've got, obviously the AWS API,\nthe SDK for putting messages.\nWe talked about all that the last time around.\nAnd I know that Kafka has like a producer API,\nwhich does what you would expect.\nIt's for producing messages and a consumer API, right?\nSo those are fairly similar concepts\nto what we talked about with Kinesis,\nbut it's got some other APIs as well.\nWhat are those?\n\n\nLuciano: Yeah, there is a streams API, which is like a consumer on steroid\nand allows you to build like processing pipelines\nin real time where you can do aggregation,\nfiltering, transformation.\nAnd this is probably an alternative to Apache Flink.\nLike I don't really,\nI'm not really sure how it compares\nlike pound to pound to Flink,\nbut it seems that there is a good overlap\nin terms of feature set and things\nthat you can do with this.\n\n\nThen there is also a connect API,\nwhich is kind of a simplified way\nto put data into the Kafka streams\nor to read and consume this data from the streams\nand maybe move it somewhere else.\nExamples that we can find are, I don't know,\nfor instance, get data from S3\nor write data from Kafka to S3\nor integrations with Elasticsearch,\nmaybe for implementing search features\nor the Bizium, which I think is a change detection system\nthat allows you to basically store all the change logs\nfrom your databases into Kafka.\nAnd then you can probably do,\nbuild like event-based systems\nfrom changes happening in your databases.\n\n\nEoin: Yeah, that's cool.\nSo there's a lot more in terms of the rich feature set\naround Kafka than Kinesis,\nwhich is, I suppose, more of a single purpose streaming,\nright?\nIt's just about producing and consuming events.\n\n\nLuciano: Yeah, I think there is a little bit of an overlap\nwith Kinesis Fyros,\nbut Fyros is only thinking about\nonce you have the data in your stream,\nhow do you move it somewhere else?\nHere you can also have like data sources\nand let them push data into your streams.\nSo I think it's a richer ecosystem\nwith more use cases being supported.\n\n\nEoin: Yeah.\nAnd with Kafka, you have the admin API as well,\nI suppose that's worth mentioning,\nfor creating topics and managing your cluster.\nOf course, with AWS and MSK,\nyou can also use the AWS SDK and API\nfor managing those things as well,\nbut it doesn't allow you to create topics.\nThat's something you would do with the Kafka API.\nI think Kafka often is so closely associated\nwith the like enterprise Java ecosystems\nand Java based community.\nSo there's a lot of Java based libraries\nand Scala based libraries,\nwhich provide really rich capabilities\nand a lot more than you would get\nwith just the consumer and producer sending messages\nand receiving the messages.\n\n\nLuciano: Yeah, absolutely.\n\n\nEoin: I know you've used like the Node.js client\nand there's other APIs or software packages out there\nfor working with Kafka.\nThey're probably not as rich, I guess,\nas the Java ones, right?\n\n\nLuciano: Yeah, Java, I think is kind of the default\nand the one that gets all the new shiny features\nbefore everyone else.\nBut I suppose, depending on your use cases\nand the languages that you are using for your project,\nyou'll find good enough clients\nfor pretty much most of the mainstream\nprogramming languages.\nYeah, probably maintained by the community, I guess, right?\n\n\nEoin: Rather than the Kafka core teams.\nSo what are the two different terms?\nI know that they tend to use different terms\nfor the same thing between Kinesis and Kafka.\nSo what are the one-to-one mappings here?\n\n\nLuciano: Yeah, that's an interesting topic\nbecause if you are coming from Kinesis\nand looking at Kafka or vice versa,\ncoming from Kafka and looking at Kinesis,\nit might be a little bit confusing to get used\nto slightly different terminology for similar concepts.\nBut the first concept that exists only in Kafka\nis the idea of a broker,\nwhich is not really applicable to Kinesis data stream\nbecause that concept is totally abstracted to you.\n\n\nA broker is literally an instance\nthat is part of your Kafka cluster.\nAnd we don't get to see that in Kinesis\nbecause AWS is kind of hiding all of that complexity.\nThen we have the concept of a topic,\nwhich in Kafka is called topic\nand it's pretty much equivalent to stream in Kinesis.\nSo it's the idea of one logical stream\nwhere you put your data,\nyou will call that topic in Kafka.\nThen we have the idea of partition.\n\n\nAgain, partition is the Kafka topic,\nbut we used to call that shard in Kinesis.\nSo the idea of once you have a topic or a stream,\nhow do you distribute the data in that topic\ninto multiple instances?\nAnd then we have the concept of producers and consumer,\nwhich surprisingly is the only terminology\nthat matches in both systems.\nAnd finally we have offset, which is a Kafka terminology\nand iterator is the equivalent in Kinesis.\nSo the idea that as you are consuming the data,\nyou are like reading a transaction log.\nSo you have a pointer that is basically used\nto keep track of where are you at reading all this data.\nSo the data is always coming in.\nSo you're trying to catch up\nand process the data real time.\nSo that offset or iterator is what tells the whole system\nwhat to read next, basically.\n\n\nEoin: Interesting.\nAnd I know we were talking about watching the iterator age\nwhen you were talking about Kinesis.\nI think there's also like this offset lag metric in Kafka,\nwhich is, I guess, pretty much one-to-one.\n\n\nLuciano: Yeah, probably it is, yeah.\nOkay, so should we maybe mention more differences\nwith Kinesis?\nIs there something else that comes to mind for you?\n\n\nEoin: Yeah, I think when we were talking about this earlier,\nyou made the point that comparing Kafka sync,\nKafka's, Kinesis and Kafka is like comparing SQS\nto RabbitMQ.\nOne is much more simpler, one is much more feature rich.\nAnd so Kafka has a lot of features\nand configuration options,\nbut in exchange for that richer set of features,\nyou get increased complexity as you might expect.\nAnd you also talked about these brokers.\nSo Kafka has this cluster provisioning model\nwhere you need to scale brokers\nand think about disk size and memory and network,\nall those wonderful things.\nAnd you can create as many topics as you want,\nand you just need to scale your resources accordingly.\nAnd you can also create lots of consumers.\nThere's a whole complexity\naround managing cluster state as well.\nI wanna go into that.\nDoes this whole duality with Kafka\nwhere you need to think about your Kafka configuration\nand your Zookeeper configuration, what's that all about?\n\n\nLuciano: Yeah, so basically the way that Kafka works,\nbecause of course it's a distributed system,\nit needs to replicate data across multiple nodes.\nAnd also you have consumers and the system needs\nto keep track of the state of each consumer.\nSo there is a lot of information\nthat is kind of distributed\nand needs to be kept in sync across different nodes.\nAnd all of that, as many other Apache projects,\nis managed by another system that is called Zookeeper.\n\n\nAnd Zookeeper is something that needs to be provisioned\nin a multi instance mode as well.\nAnd we need to make sure it's highly available\nand resilient because of course,\nthat the cluster is healthy only\nif Zookeeper is available all the time.\nSo it's an additional piece of complexity\nthat you get with Kafka.\nBut the interesting thing is that in MSK,\nall this complexity is managed by AWS for you.\nAnd also the pricing, this is actually the interesting bit,\nis something that you don't pay any additional costs\nfor Zookeeper.\n\n\nSo it's something that it's somewhat included\nin your MSK offering.\nSo AWS is kind of absorbing that cost for you\nor kind of abstracting that cost in different ways\nin the whole MSK offering.\nBut it's not something you need to think about\nin terms how many instances I'm gonna use for Zookeeper,\nwhat kind of size and how that is gonna impact cost.\nIt's not really affecting the cost scheme in MSK.\nAn interesting thing is that I think this has been\na long running conversation in the Kafka community\non whether they should get eventually read on Zookeeper\nand have this kind of internal mechanism\nto synchronize all the data.\nAnd as far as I can tell by reading some blog posts,\nthere has been a lot of progress.\nAnd since version 2.8, I think it starts to be feasible\nto run a Kafka cluster without Zookeeper at all.\nI don't think it's the recommended approach so far.\nAnd also in MSK, it's not really clear what happens\nif you use 2.8.\nI think it still uses Zookeeper,\nbut you don't get like a flag, use Zookeeper or not.\n\n\nEoin: It doesn't matter, I guess, with MSK anyway, really, right?\nIf it's all managed for you. Exactly.\nYeah.\nOkay, cool.\nI know that as related with all AWS services,\nKinesis uses HTTP,\nbut Kafka has its own TCP based protocols.\nI guess some efficiency can come from that.\nThere's also changed a difference in the delivery guarantees.\nWe've talked a lot across this whole series\nabout at least once processing\nand at most once processing.\n\n\nKafka is one of the rare things that actually has support\nfor exactly once delivery of messages.\nBut I think this doesn't work for all consumers.\nYou need to be really sure of what you're doing\nand understand how Kafka transactions work.\nBut it is supported in things like Kafka Streams.\nSo that can be important for me, right?\nIf you don't want to have to build IAM potency\nand you really need those guarantees.\n\n\nSo the provisioning model then,\nwe talked about brokers and everything.\nKinesis uses throughput provisioning.\nWe talked about that and it's very clear.\nThe number of shards,\na single shard has very clear throughput limits.\nAnd if you want more throughput, you need more shards.\nBut you have limits with the number of consumers then, right?\nBecause if you've got a consumer,\nyou can only read like a megabyte a second from that shard\nand you've got these enhanced consumers to help a little bit.\nBut Kafka, you can really have as many consumers\nas you want, right?\nYou just need to again,\nmake sure you've got the CPU storage and partition setup.\n\n\nLuciano: Yeah, I think Kafka is a little bit more traditional way\nof thinking about a cloud service\nwhere you have a set of instances\nthat are taking the heat for everything\nyou want to do with them.\nAnd you might have, I don't know, very small topics\nand very few big topics,\nand maybe your cluster will be able to deal\nwith all of them at the scale you need.\nSo you don't really get to think in terms of topics,\nbut more what are, like is the system under stress?\nIs the CPU or the storage enough for my workload?\nSo you need to look at all these metrics\nrather than having like a fixed unit\nand you just scale based proportionally on that unit.\nYeah, so it could be much more complicated,\nbut also I suppose more flexible\nif you have very diverse type of topics\nand very diverse throughput logics or functions,\nI guess, across different topics.\n\n\nEoin: Okay, okay.\nAnd what about retention?\nBecause Kinesis has,\nthey actually increased the maximum retention\nfrom seven days to one year, not so long ago,\nbut Kafka doesn't have any limits, right?\n\n\nLuciano: Yeah, I suppose the idea with Kafka is,\nagain, it's up to you to decide.\nAnd if you have enough disk space,\nyou can start the data as long as you want.\nThere is no intrinsic limit after which the data is lost.\n\n\nEoin: Yeah, there's a clear benefit there if you're using it for advanced sourcing\nand if you want to rebuild your state\nat any time in the future.\nAre there any other differences between Kinesis and Kafka\nthat we should cover off?\n\n\nLuciano: Yeah, an interesting one we mentioned\nis a little bit already is that being an open source project\nhas been around for a long time\nand it's probably has been like the promoter\nand like the first real project in this space\nthat then maybe started Kinesis and everything else.\nThere is a lot of history there\nand of course the ecosystem is really good\nand there are a lot of open source tools.\nFor instance, you can find all sorts of different admin UIs\nthat can help you to build the data in a cluster\nand understand what's going on.\nOr also there are tools that allow you to define schema\nand have this kind of validation\nthat all the data you ingest in your Kafka\nis somewhat compliant with schema you defined\nor to do discovery of you have been ingesting\ndifferent messages that they will extrapolate the schema\nfrom your messages and you can easily visualize that.\nSo all this kind of interesting stuff is available to you\nbecause there is an entire community\nthat are building tools and building products\non top of Kafka and sharing what they learn.\n\n\nEoin: Okay, yeah, I guess that sounds like it.\nI mean, other benefits, I think in terms of latency\nthey both have a pretty low latency,\nyou know, like 100 millisecond latency.\nSo they're pretty similar in that regard.\nSo let's talk about how you get going with MSK\nand how you set it up.\nSo there's two modes we're gonna talk about\nbecause we've got MSK as it has existed\nfor a couple of years now\nbut we've also got the preview for MSK serverless.\n\n\nSo for the brokers first, I know that we talked about\nwhat you have to scale.\nSo when you set it up, you have to create,\nyou have to select your instance type.\nSo you get a number of options there\nand the minimum kind of production level one\nis like an M5 large.\nYou can also, they do offer a small one\nfor development workloads\nbut generally because it's a distributed system\nand it needs a core room in order to make sure\nthat state is reliable,\nyou need to kind of want three brokers minimum.\n\n\nYou probably wanna set up three brokers\nacross three availability zones.\nSo you might think about that.\nI'm not sure what the story is with inter AZ traffic.\nUsually that's something you have to pay for.\nSo I would be observing that if I was using a cluster\nin AWS with a lot of traffic, think about the cost there.\nAnd then you can set the number of brokers in each AZ.\nSo you might end up with a six broker set up by default.\n\n\nProbably get away with three,\nyou need to then think about EBS volumes\nfor your data storage, right?\nIf you're gonna use infinite storage,\nyou need to know where it's gonna be held\nand of course, because it's an instance based thing,\nyou need to set up a VPC.\nIt needs networking and security groups, private subnets.\nSo there's all of that to set up.\nIn terms of security then, I think you'll have to select\nwhich authentication mechanism you support\nand it supports five options.\n\n\nOne of them being no authentication at all.\nSo I could probably exclude that one.\nDon't do that.\nBut you have username password authentication\nto have this SaaS protocol you can use.\nAnd you put this as with like you would with RDS,\nyou can put the password and username in secrets manager\nand MSK will use that.\nAnd then you can use that on your clients.\nYou can also use TLS authentication.\nAnd the interesting one there with MSK,\nwhich is different to other options\nis that you can use IAM authentication.\nSo they, you can imagine AWS have patched Kafka\nto support IAM as an authentication mechanism.\nSo that's setting it up.\nAnd then once you set it up, you would create a topic.\nSo this is a slight difference because when with Kinesis,\nyou would configure a stream as a resource,\nas an AWS resource.\nWith MSK, you don't, right?\nYou configure the cluster as the resource\nand then you use the Kafka API to create your topics.\nAnd when you create a topic,\nthen you can specify how many partitions\nand how many brokers you need to replicate that across.\nSo how do you think that sounds?\nThere's quite a lot in there.\n\n\nLuciano: It feels again, a little bit more traditional,\nlike comparing, I don't know, RDS to DynamoDB as well,\nwhere RDS you provision, like I want to use, I don't know,\nan instance of Postgres,\nbut then you maybe you don't create any table in it, right?\nCreating a table means you connect to the database,\nyou run SQL and you create tables.\nWhile in DynamoDB, when you decide to create a table,\nthen you are creating an AWS resource\nthat represents that table.\nSo I think it's a similar kind of mindset\nwhen it comes to comparing MSK with Kinesis.\n\n\nEoin: Yeah.\nYeah.\nSo to make all this easier,\nlast year we had an announcement that MSK serverless\nwas in preview mode.\nSo, and it's in preview.\nWhen I used MSK serverless,\nit was only available in US East to Ohio,\nbut hot off the press,\nit's now available in EU West one, Dublin, Ireland as well.\nSo how, what kind of a difference do you think\nthat will make and how does it function compared\nto the laborious configuration,\nhow do you set up by just talk through\nfor the provision mode?\n\n\nLuciano: Yeah, my expectation is that MSK serverless\nwill try to remove a lot of these concerns\nthat we just discussed in terms,\nhow do you even get started?\nHow do you, like before you create a topic, what do you do?\nSo I think this will try to give you a more immediate usage\nand provisioning of MSK that is probably similar\nto the user experience you will get with Kinesis.\nAnd in fact, there is a very clear unit of scale,\nwhich is the right throughput.\nAnd you can, you have like limits that are more set in stone\nbecause of course AWS will take a lot of the work for you.\nSo they will need to work with certain limits\nand you have storage limits,\nI think it's 250 gigabytes per partition,\none day retention,\nand then you have a maximum of 120 partition, I believe,\nwhich maybe can be increased.\nCorrect me if I'm wrong.\n\n\nEoin: Yeah, I think this is probably just because it's in preview mode and they just put a cap on it,\nbut yeah, you would expect all those limits to increase\nbecause they're not particularly high.\n\n\nLuciano: Yeah, yeah, yeah, definitely.\nAnd then you have IAM authentication only.\nAnd one interesting thing is that you might think,\nokay, I'm gonna, maybe I want to work with Kafka\nbecause this is what I'm using for my product.\nI'm migrating to AWS.\nProbably the safest bet is to start with MSK.\nAnd then if you are starting to do okay,\nbut eventually I would want to serverless\nbecause that will remove a lot of the complexity for me.\nThen what you could probably do in the future is\nyou start with MSK and then you can transition\nto MSK serverless by migrating all your data.\nAnd one of the tools that I think is one of the most common\nused in Kafka is mirror maker\nto move data across Kafka clusters.\nSo you can probably do that to migrate your data\nfrom traditional MSK to MSK serverless.\nYeah, should we talk about monitoring maybe?\nLike what do you do once you have everything up and running?\nHow do we make sure it's actually doing what we want\nand it's healthy?\n\n\nEoin: Yeah, well, more configuration will inevitably mean\nmore things to monitor.\nAnd that's something you'll get with Kafka.\nSo you could configure the monitoring level\nthree different options.\nYou can set it to be like per broker, per topic\nor per topic per partition.\nAnd that will give you, I think it's like between\naround 40 and 90 metrics to monitor with MSK.\nAnd that depends on the level of the,\nobviously the monitoring level you have set.\n\n\nSo the fact that there you have up to 90 metrics to monitor\nwill give you some indication of the kind of infrastructure\nand maintenance complexity with a traditional Kafka.\nIt's also worth mentioning that built into MSK support\nfor open monitoring with Prometheus,\na lot of people will be using.\nAnd then in terms of logging,\nyou can set up your broker logs\nlike to go to CloudWatch logs or S3 or firehose.\n\n\nSo that's obviously you want to create a lot of alarms\nand keep an eye on all those metrics.\nAnd there's also a lot of integrations between MSK\nand for something that's relatively new,\nit's pretty impressive the list of integrations.\nI know that Kinesis Data Analytics\nnot doesn't just work with Kinesis Data Streams.\nYou can use it with Kafka as well.\nSo then you can do stream processing.\nIf you don't want to use the streams API in Kafka,\nyou could use Flink on Kinesis Data Analytics\nbecause that is essentially a managed Flink.\nYou can also run Flink on EMR.\nSo you can integrate your streams with EMR.\nAnd I also noticed that you were talking about,\nwhen you're talking about schema registry support\nwithin Kafka, there's a product called the Glue\nSchema Registry as well, which is essentially\na schema registry for real-time streaming.\nSo you can have like Avro schemas and JSON schemas\nand protobuf schemas and enforce the data structure\non the producer side and consumer side using that.\n\n\nLuciano: But I think the most interesting is probably Lambda, right?\nLambda integration.\n\n\nEoin: Yeah, yeah.\nAnd this is again, something which is,\nhas they've had a lot of features\nbecause they haven't just added Lambda integration for MSK.\nThey've also added support for Lambda integration\nwith your own managed Kafka.\nSo you don't have to use MLK to integrate with Lambda.\nAnd it uses the same event source mapping feature\nthat we talked about when we covered Kinesis and SQS.\nSo it also supports MSK, but not different options\nare supported depending on what your event source is.\n\n\nAnd we obviously talked about Kinesis\nand how you've got your shared level,\nbut then if you want like 10 Lambdas processing messages\nfrom each chart, you can set this parallelization factor\nconfiguration and get more parallelism.\nYou don't do that with MSK or with Kafka.\nInstead, you have to think about it a bit differently.\nBy default, you just get one consumer for your MSK topic.\nAnd then Lambda can scale up\nbased on the number of partitions in that topic.\n\n\nBut the maximum scaling is one consumer\nper partition per topic.\nSo again, this is all very use case specific\nand it depends on what your partitioning level is\nand your volume of messages,\nbut it'll only scale up every three minutes.\nSo it might not be reactive enough for your needs,\nwhich is a pity because I think Lambda\nis the ideal use case for these things.\nIf you look at like the Streams API or the Consumer API,\nyou still have to have something running,\nsomething that's pretty stateful to consume these messages.\nWhereas with Lambda, you could have a lot of power,\nbut the scalability, you just have to check\nif it's gonna work for your use case.\n\n\nLuciano: Yeah, exactly.\nEspecially if you don't have like a continuous stream\nof data with more or less constant throughput,\nbut it's very, very bursty ingestion of data.\nYou'll probably suffer because of these three minutes\nchunks of scaling because yeah,\nit will kind of go very slow at the beginning.\n\n\nEoin: Yeah, and then as regards security between Lambda\nand Kafka, it's kind of different to what you'd expect.\nAnd there's authentication, you can use IAM authentication,\nbut you can also use your username password authentication\nand TLS to integrate Lambda with your cluster.\nObviously is required if you're not using MSK,\nbut also you need a VPC for your cluster,\nbut your Lambda function doesn't have to run in that VPC.\n\n\nSo it's slightly different than the mental model\nyou might imagine for this setup.\nInstead, you need to give the cluster's VPC\naccess to the Lambda service and also the STS service.\nSo that usually means either you give it access\nto the internet with like a NAT gateway,\nwhich might set off a few alarm bells\nbased on their pricing episode a few months back.\nBut so you might instead use VPC endpoints\nand create a direct route between your VPC\nand Lambda service.\nSo that's one thing you might not expect.\nYou might think just because the resource runs in,\ncluster itself runs in a VPC,\nthat the Lambda has to be in that VPC, but it doesn't.\nAnd it kind of works the other way around.\nYeah, that's interesting.\n\n\nLuciano: I'm gonna talk very quickly about pricing.\nAnd basically, well, the first case is provisioned.\nYou are of course paying by the number of brokers\nand the size that you pick for these brokers.\nFor instance, if you go for an M5 large,\nthat's about 24 cents per hour.\nAnd depending on how many do you have,\nyou can do the math and see how much are you gonna pay.\nBut of course there is also storage in the equation.\n\n\nSo you have certain amount of cost,\nI think it's around 10 or 11 cents per gigabyte per month.\nAnd yeah, that will add to the cost.\nIf you go serverless,\nthere are actually a bunch of dimensions\nthat will influence your cost.\nSo there is a certain fee that is $075 per cluster per hour.\nSo you pay a certain fee per hour\njust by speeding up a cluster.\nThen depending on the number of partitions you have,\nthere is an additional fee on partition per hour.\n\n\nAnd of course, storage as well will add costs.\nSo the number of gigabytes per month.\nAnd then you also pay for data transfer,\nboth data in and data out in terms of gigabytes.\nYeah, so just do the maths and based on use cases,\ntry to figure out what's the pricing.\nThe interesting thing that I guess we realized,\nit might change depending on your use cases,\nbut instinctively it looks like that Kinesis is way cheaper\nfor lower volumes,\nbut then MSK can be more effective\nif you're really running many, many shards,\nif you're really running data processing at scale\nand ingesting a lot of data.\nSo that can be interesting.\nMaybe it's for a startup,\nyou can start with Kinesis\nbecause it's easier to start with\nand you probably don't need the big volumes,\nbut if you're running serious workloads,\nmaybe that the investment in Kafka could be worth it.\n\n\nEoin: Okay, so it's another serverless option\nthat doesn't scale to zero in terms of pricing,\nwhich is a bit of a pity.\nSo that's maybe something we can hope for\nat some point in the future.\n\n\nLuciano: Should we close this episode by trying to have a recap\nbetween why would you use MSK over Kinesis or vice versa?\n\n\nEoin: What's the decision tree?\nYeah, exactly.\n\n\nLuciano: So yeah, I'm gonna say that first of all,\nwhen you have a large number of consumers,\nprobably MSK is a better solution\nbecause you can be more flexible there.\nAlso, if you need Kafka Streams or Kafka Connect\nbecause you already built a solution\nthat uses those technologies,\nor maybe you have expertise with those technologies\nand you want to leverage that expertise,\nof course, again, MSK is an obvious winner there.\n\n\nAnd similarly, if you just have experience\nor you prefer to work with a technology like Kafka\nbecause it's open source\nand you can easily port it also to other cloud providers,\nagain, that's a winner over Kinesis.\nBut of course there are disadvantages,\nlike we already mentioned that there is more complexity,\nso you need to keep that into account.\nAlso in terms of MSK, so Kafka on AWS\nis a relatively new service.\n\n\nSo sometimes you might struggle\nto find documentation or examples,\nso that's also something else to keep in mind.\nAnd it's less serverless because you need to think about,\nas we said, VPC, EC2,\nrather than just thinking about how much are you scaling,\ngive me one dimension for scale\nand everything else will be managed for me.\nHere you really need to think about many, many concerns,\nmany, many metrics, so everything becomes more complicated.\n\n\nAnd one final remark that I have is,\nagain, you might not even need stream processing at all,\nso always keep that in mind.\nSometimes you can go a long way\nwith just using like SQS, SNS or EventBridge.\nSo keep that in mind\nbecause if you don't really need streams,\nwhether that's Kinesis or Kafka, doesn't really matter,\nbut if you don't need that level of complexity at all,\nand you can go with SQS, SNS and EventBridge,\nyou can probably make your life easier.\nAnd those services will also scale to zero,\nso you could also save a lot of money\nby using those as an alternative.\nSo try to really nail down your use cases\nand the technology you could use for your use cases.\n\n\nEoin: Yeah, yeah.\nPlus one for me from starting with SQS and EventBridge,\nfor example, you can get a lot done and you can always migrate.\nSo to conclude then, we have a few resources.\nI know we've collected a few links here,\nwhich we'll put it in the show notes,\njust some really good ones.\nOne of the interesting things that you might find\nis that Amazon actually provide a pricing spreadsheet,\nan Excel spreadsheet that you can use for pricing your MSK.\nWe have a link to that,\nand we have some pretty good talks.\nSo we've one on whether your startup\nshould use Kinesis or MSK,\neven if you're not in a startup,\nthat's an Amazon talk.\nEven if you're not in the startup,\nthat's useful comparison between the two.\nWe've got another intro to MSK talk\nway back from its launch in 2018.\nAnd you wanted to highlight the Frank Munns talk as well,\nright Luciano?\n\n\nLuciano: Yeah, I really like it because it's not just an intro\nwith a demo on how to use MSK,\nbut there is also a little bit of a preamble\nthat gives you a lot of insights\nabout why would you need stream processing\nand all the value of data over time\nand the value that you can extrapolate\nif you are able to make sense of that data\nas soon as it's available in your system.\nSo I really like that\nnot just as a technical introduction to MSK,\nbut also as a way to reason about\nwhether you really need that type of capability or not\nand what kind of advantages you could get from it.\n\n\nEoin: Okay, maybe the last one we can mention to wrap up then\nis there's another really useful versus comparison,\nwhich the Cloud and Not guys do pretty regularly.\nThey've got a comparison table,\nbut also an episode comparing Kinesis and MSK,\nwhich we reckon you should check out.\nIt's got a demo of MSK in there as well.\nI think this wraps up not just this episode,\nbut the whole series on AWS event services.\n\n\nAnd the next time we'll be back\nwith something completely different.\nSo we've really appreciated all the great feedback\nwe've got on this series.\nIt's actually helped to change how we present them.\nI know some of these talks have been,\nthese episodes have been longer than previously,\nbut hopefully it's been worth it for people.\nAnd we're really interested in your feedback\non how we can make shorter or longer episodes in the future\nand what topics you want to cover.\nSo thanks for all your feedback.\nWe really appreciate it\nand we'll see you in the next episode.\n—\n"
    },
    {
      "title": "27. How do you organize AWS Accounts?",
      "url": "https://awsbites.com/27-how-do-you-organize-aws-accounts/",
      "publish_date": "2022-03-10T00:00:00.000Z",
      "abstract": "Eoin and Luciano try to answer the question of what AWS accounts do you need for your team and how to organize them. In this episode of AWS bites we discuss common ways to organize AWS accounts and environments from the perspective of a company running production workloads. We try to answer questions like “how many accounts and how many environments?”. We also discuss how you and your team can be more productive by effectively managing AWS accounts and environments. Finally we explore some common security and cost-related tradeoffs that are common when it comes to organizing AWS accounts.\nThanks to David Lynam for suggesting this awesome topic!\nIn this episode we mentioned the following resources:\n\nAWS Account vending machines\nOrg Formation\nTerraform for accounts and organizations\n\n",
      "transcript": "Luciano: Hello, today we're going to answer the question, what accounts do you need for your team?\nAnd we will try to explore some of the common ways to organize AWS accounts and environments,\nof course with the idea of a company running production workloads, so how many accounts\ndo they need and how many environments. We will discuss how to become more productive with AWS\naccounts and finally we'll discuss some of the common security and cost-related trade-offs.\nMy name is Luciano and today I'm joined by Eoin and this is AWS Bites podcast.\nSo first of all I'd like to start by saying a big thank you to David Lynam because he\nsuggested this awesome topic on Twitter, so please keep sending us your suggestion,\nwe are more than happy to try to address common questions that you might have.\nI would suggest we maybe start with a quick definition of what do we mean by an AWS account,\nwhat do we mean by an environment, just to make sure we are all in the same line.\n\n\nEoin: Okay, so an account, let's try to define that as the fundamental unit that you get started with on AWS. Most people will start off with one account, put in a credit card and grow from there, but you\ncan add as many accounts as you need across your organization and some companies have hundreds of\naccounts. But an account is really a logical concept but it's a very strict security boundary\nas well because once you try to access resources in another account that's a cross-account access\nconcern. So it's very much a walled garden for everything you're doing in AWS and it's very\nuseful for isolating workloads. So I think one of the things that you need to think about\nwhen you're thinking about an account is what do you want to share with other people\nand what do you want to isolate, right? So that's a concern. Is there anything else we need to say\nabout what an account is?\n\n\nLuciano: Maybe I will only add that you can have this kind of tree structure for accounts so you can group them logically in a way that you will have like a master account and\nall the sub-accounts there, which is not always obvious. Organizational units as well are like\nthese folders really for accounts. So yeah, that's important to know. Yeah, that's true.\n\n\nEoin: So what's an environment then? Because we talk about accounts and environments. Are they the same thing?\n\n\nLuciano: So I would consider an environment, this is more of a loose concept I suppose, so I would consider an environment, this is more of a loose concept I suppose from the perspective of AWS.\nSo it's more for the way that you decide to build and ship applications throughout your pipeline.\nSo probably you're going to have a development environment where you are going to use that\nonly for building new features and as you build maybe you might want to test a few things manually.\nYou might have other environments for instance for QA or for all sorts of automated testing.\nThen finally you will have your production environment which is literally where the\nsoftware you are running is going to be accessed by your users and that's the main source of\nI suppose delivering the product to your customers. So yeah, it tends to be a little\nbit more loose because you don't really have that concept as a structured way in AWS and\nyou'll need to decide how to apply that concept using accounts.\n\n\nEoin: So I guess the one environment that everybody would probably have is the production environment and after that it's going to vary?\n\n\nLuciano: Exactly, yeah. So what do we think is like the most common way at least to try to structure\naccounts and environments for? I suppose a big enough company doesn't have to be extremely big\nbut maybe not just like one person company. Maybe you start to have a few products and a few\nemployees in the team. So what can we recommend?\n\n\nEoin: It used to be a lot more common to mix lots of different environments in one account. These days, 2022, it's more common to have one account per\nenvironment but also per application or domain or a team. It would vary from company to company\nhow you split that up based on what the relationship is between the number of products\nyou have and the number of teams. Really you want to think about what is the security boundary\nyou're trying to enforce? What are the resources you want to share and not share?\nAnd to avoid people getting on each other's toes, you know, stamping on each other's toes by sharing\nan account. You might find that you have to put in place constraints so that people don't delete\nother people's resources. If you're doing that, that's a sign that you need to split into\nmultiple accounts, I think. But I think the AWS best practice is more or less one account\nper application per environment. Yeah.\n\n\nLuciano: So if we want to do an example, that basically means, again, let's take our e-commerce example. If you have, I don't know,\nkind of product application and then you might have a fulfillment application, you might have\nalready a few different accounts because let's just say you have a dev and a production account,\nyou will have two different accounts for the first application and two other different accounts for\nthe other application. So you can see how it's kind of a matrix of all the environments you have\nand all the applications you have. If you want to do very granular kind of control in terms of\nboundaries.\n\n\nEoin: It's probably worth mentioning as well that if those are like microservices and if you've got a team of five people and they're producing the fulfillment application as well as the product\napplication and the billing application, if they're really just microservices and the\ndeployment can all live quite easily together in one account, you don't need to have an account per\nmicroservice per environment. Because in that case, there's also a bit of overhead in managing\nmultiple accounts because you need to, those are more things you have to monitor and take care of.\nSo just because you have multiple separately deployable units in your application, it doesn't\nmean they always need to have a separate account. It's perfectly okay to start with one account if\nyou've got just one team on that because with a small team especially, you're probably going to\nbe able to make sure that you don't have any conflicts there.\n\n\nLuciano: Do we agree that I suppose the rule of thumb is how you structure your organization should be somewhat reflected in\nhow do you structure accounts, right? Yeah, I think so. This is Conway's law, isn't it?\n\n\nEoin: Exactly. Your architecture is going to follow your team structure anyway. So maybe your\naccount structure also fits into that.\n\n\nLuciano: And I suppose to summarize this concept, the benefits of doing that is that when you create accounts, you are trying to enforce autonomy on one side,\nbut also security boundaries as we mentioned. But there is another interesting topic that\nis every account comes with quotas. So AWS will allow you to do a certain number of things\ndepending on the services you use. And of course, the more teams are trying to use the same account,\nthe more likely it's going to be that you hit those limits and that can become a little bit of\na complication. It can slow you down. So also keeping more granular accounts will make it\neasier to use responsibly every account without having to ask for increased quotas or do other\nthings like that. Absolutely. Yeah.\n\n\nEoin: When we talk about the application per account per environment as well, sorry, account per application for an environment, we should also mention it's pretty\ncommon as well to have shared tooling accounts like for CI CD where you might put your pipelines\nand other things that aren't specific to the runtime environment of your application. Because\nthere you have a different set of access criteria and concerns to your runtime environment. And it's\ngood to keep those two things separate.\n\n\nLuciano: It's kind of like a control plane that then can act on behalf of the other accounts, right? Yeah, that's it. Yeah. And it works pretty well.\n\n\nEoin: And you get very familiar then with providing cross-account access, which is a good skill and a discipline to have when\nyou've got multiple accounts.\n\n\nLuciano: So I suppose the next question is, do we think that every developer in a team should have their own account? Is that too much or are there benefits in that?\n\n\nEoin: It's a good goal to consider, but it's not necessarily required for people to be productive.\nAnd it kind of depends on what your account requirements are. Some applications might need\nto have a certain level of setup when you create an account. Like you might need to go to AWS and\nget limit increases. Some of that might take support to get a lead time. So you don't necessarily\nneed to make it one account per developer and make sure that if a new developer joins a team,\nyou stand up a new account. You can certainly do that. And it provides a good level of autonomy.\nBut if you've got a team that's collaborating with each other anyway, you might have pairing sessions.\nYou don't necessarily need one per developer. What we see quite often is that you have a pool\nof accounts and you just allocate them to people working on different features\nin a given sprint, say in your team. And that tends to work pretty well.\n\n\nLuciano: Yeah, I agree with that. I actually prefer that approach because encourages people to talk more\nbetween each other and try to figure out, okay, what are you working on? Maybe because you're\nworking in this environment, let's finish this feature together. Instead with the single account\nper developer is a lot more, this is my account, let me do my thing. And you can keep doing your\nown thing in your own account, which I don't know, I guess encourages a non-collaborative\nenvironment at that point.\n\n\nEoin: I guess the main goal with having developer accounts is that people on the team have an environment where they can experiment freely and don't really have to\nworry about breaking things that matter to other people. I think there's a lot of people doing\ninteresting things like in their continuous deployment pipelines then having automated\nprocesses. So when you create a pull request or when you create a branch, it'll automatically\ndeploy a stack from that branch into an account. And then when you merge the PR and delete the\nbranch, it will tear down that stack. So you can automate a lot of that stuff as well, and it can\nmake it very productive for people so that if you've got lots of microservices they need to\ndeploy, they don't have to go to the manual effort of doing that every time they open a new PR.\n\n\nLuciano: Yeah, that's a good practice because I suppose that will also help to keep those environment as clean as possible so that when people transition across these accounts,\nthey have some expectations on where they are starting from.\nSo I suppose the next topic is because we are talking a lot about freedom and having different\naccounts where you can experiment freely. We should probably also discuss if there is something\nthat you should restrict, right? And how, and I suppose there are two ways of seeing this. You\ncould either be very defensive. You could say, okay, I'm going to allow you in this account only\nspecific actions so you can provision only specific resources. And that's it. You cannot do anything\nelse. Or the opposite mindset could be like trust but verify. You have more freedom, but you also\nhave a way to detect what's going on and maybe realize if something dangerous is happening. So\nyou can have monitoring or configuration tools that allows you to see, okay, somebody is\nprovisioning a very big issue instance and that's going to affect cost or something like that.\nDo you have any preference like in one or the other?\n\n\nEoin: Yeah, I think the preference is to try and err on the side of putting in place just guardrails for people. And you have the concept of detective guardrails, which are freer for your teams,\nbecause it's basically you're just keeping an eye on things. And then if something does something\nthat you don't particularly like, you work with them and you try to remediate the situation.\nThen the more strict approaches you have preventive guardrails where you're putting\nin like service control policies, enforcing permissions boundaries and saying you can't do that.\n\n\nIf you want to do something new, you have to open a ticket. And ultimately it will come down to what\nare the real levels of compliance that you need in your business and trying to be as trustful as\npossible of teams, because that's the most productive way to do it. It's just to let people\nmove forward. And there's lots of things you can use like CloudTrail and AWS Config,\nwhich allow you to observe what's going on and then get alerts. So if you say, OK,\nwe don't want people to create buckets that are not encrypted. So one of the things you\ncould do there is put in place a config rule. And if you get alerted then when people create\nan unencrypted bucket. Right. So a much more strict approach is don't allow people to create\nbuckets because only we know how to create buckets properly. But that's something that\nwill guarantee your compliance at all times. But it is going to slow down your teams because\ncreating buckets is something developers need to do to try things quite frequently. So understand\nthe tradeoffs there. And I think you can approach this also from a cost perspective.\n\n\nLuciano: For instance, you could have other rules that will prevent people from spinning up very expensive, easy to\ninstances or maybe even at the regional level because we know there are certain regions that\nare more expensive than others. And most likely you are not going to need to use all the possible\nregions. So that's another rule that can help you to limit the surface without restricting too much\nthe kind of activities that developers might want to do. Yeah, that's a really good one.\n\n\nEoin: People might say actually maybe it's worth calling it out. Doesn't that violate the principle of\nleast privilege if you're being so permissive with accounts? Because on cloud security,\nthe important thing is the principle of least privilege, not trying to be over permissive on\npermissions. I think there's maybe talking about two different things. Certainly in production and\nyour test accounts, you need to be enforcing least privilege. In development accounts,\nyou want people to be able to be productive. So you can encourage least privilege there by\nanalyzing cloud trial, by retrospectively looking at things using IAM access analyzer\nand trying to continually refine the IAM policies that people use. Doesn't necessarily mean locking\ndown everything by default. No, that's a very good clarification.\n\n\nLuciano: For instance, I totally like the approach that you can experiment as much as you want in the development account. But as you\ndo that, one principle that I really like to follow and encourage other team to do the same is\nwhen you are building an app, don't give it any IAM permission as a starting point. And then every\ntime you get a failure, because of course you cannot do a specific action, I don't know,\nread or write from an strip bucket, then enable permissions gradually. And at that point,\nit's easier that you end up with a very locked down IAM policy for that particular service.\nAnd you can totally do that in a testing account. So yeah, it's a good point to try to separate\nhow do you define as a developer IAM policies for your services compared to as a developer,\nwhat can you actually do in an AWS account? So what about non-developer accounts then?\n\n\nEoin: What else do you need? Because we focused on, we mentioned production accounts and developer\naccounts. What about testing? Yeah, that's a good point.\n\n\nLuciano: I think we have at least, I've seen two different ways of working and I think they are equally good. One is probably a little bit more\ntraditional. So you basically have, you start from a development account, you build a feature,\neventually you want to ship this feature to production. There is an intermediate step,\nwhich is this feature is going to end up to another account, which is the QA account or\nacceptance test account, whatever you want to call it. But it's basically an account that is dedicated\nfor testing. So before you actually ship it to production, you can observe and use that feature\nin that environment. You can run manual and automated tests to make sure it's actually\ndoing what you want. And then from that environment and account, you can transition to the production\naccount. Another approach could be you skip that QA step entirely and you just go from development\nto production. This is a little bit of a more modern approach. And of course, when you do that,\nyou need to put some boundaries in places to make sure you are not breaking production.\n\n\nAnd that's generally done through the usage of phisher flags and it follows the principle of\ntesting in production. So basically what you will do is from development to production,\nyou ship something that is technically disabled by default, and then individual users can enable\nthat. And then they can start to play around with the phisher and see if it's actually doing what\nit does. And that's a good practice because it removes some of the concern we'll probably discuss\na little bit more later around how is the data going to look like in my QA account versus the\nproduction account. Because you are already in the production account, you don't have that concern.\nSo you are matching your production account straight away without trying to replicate\na similar account in the QA environment.\nAnd I suppose I said if you were to use the QA environment, you would have the concern of,\nokay, I can use CloudFormation or any other infrastructure as code to try to have exactly\nthe same infrastructure and configuration. But when it comes to data, it might not be possible\nto get all the production data to a QA environment because of volumes, because of\ncompliance and security and privacy reasons. So probably you'll need a process that copies some\nof this data and as it's being copied, you might also need to anonymize the data. So that adds\nmore complexity, of course.\n\n\nEoin: Yeah, trying to keep it as close as possible is important because if you have the environments matched one for one, you should match the volume of data and\ntry and simulate the number of transactions and the transaction frequency as well.\nSo I suppose the next topic is how do you set up? Where do you start?\n\n\nLuciano: Because we know it's not easy to provision multiple accounts for different environments. Is there any tool or any best\npractice to get started? You could just start with a console and use that for organizations\nand accounts as a lot of people do.\n\n\nEoin: But I find it's really worthwhile putting some effort into using infrastructure as code for your organizations and accounts. And this is probably the most\npragmatic approach that gives you a lot of stability and control and visibility into what\nchanges you're making. So there's two options there. You could use Terraform because Terraform\nwill allow you to create organization accounts. And there's another product which we use, an open\nsource project called Org Formation. And Org Formation is essentially like a cloud formation\nadditional layer that allows you to manage organizations and accounts and do things across\nlots of different accounts in your organization. So if you need to have every time you spin up an\naccount, you need to put a certain set of resources in place like your config rules or your cloud\ntrail. And it allows you to do that really easy and it doesn't take a lot of code to do that.\nThe project also comes with a lot of great example templates for that. So I'd really recommend either\nof those. I think it's also really good to use single sign-on on AWS because if you're using\nIAM users, not only is it kind of almost a deprecated way of logging onto AWS, you end up\nwith long-term credentials per account, which can be very difficult to manage. So SSO allows you to\nsign on and have assignments between your users in your identity provider, the accounts and the\npermissions. And so there's this triangle and you can configure all those things with infrastructure\nas code. There's some rough edges there because a lot of the AWS APIs don't allow you to control\nall of these things as you wish. And like there's, for example, there's still no API to delete an\naccount. So you can create accounts very easily programmatically, but the last thing you need to\ndo is delete them. So you can't do that very easily programmatically, but deleting them becomes a pain.\n\n\nLuciano: Yeah. The other thing I really like from SSO is that you get, when you go to the web console and\nyou do your logging, your SSO login, you end up in a page where it's very clear what are all the\naccounts that exists and all the roles that you as a user have in every account. So I think just that\nis extremely valuable because you have like literally a dashboard where you can see what you\nhave in your application using the different AWS accounts.\n\n\nEoin: Yeah, I agree. It's a game changer when you're coming from IAM users. There's also this thing\nof account vending machines. And I think if you've got a really large organization and you're doing\nthis frequently and you need a lot of automation, it's something you can look at. And there's a\nlanding zone that AWS provide, which is a solution to kind of automate this. And you can use\ncatalog and a lot of AWS resources to automate the process of creating an account and provisioning it\nwith all these things. You could imagine it could be a massive project for your company. That's the\nonly thing I'd say. So I wouldn't go all in on automating that approach unless you want to really\ndedicate a lot of engineering hours to it. There's nothing that will just work really well out of the\nbox for you. So I think you'll get 90% of what you need with just like org formation or Terraform\nand a process to manage that. And then you can have pull requests against your organization\nfor new accounts and it just works like everything else. So yeah, that's the approach I'd recommend\nfor provisioning new accounts for developers, staging. And then it's something you can have\ntemplates for every new application. You have like a set of dev accounts and a staging account and a\nproduction account and your CI CD. Yeah, I like that approach.\n\n\nLuciano: I think infrastructure as code applies very well also to this particular topic. Okay, so I think that covers everything we had\nfor this episode. I am personally really, really curious to see what kind of way you found to\norganize your accounts and environments. We have seen many, many different configurations with\nour customers and our projects. And I think there are many, many different ways and they are equally\nvalid. So if you do something that is different from what we suggested, please let us know because\nwe are really curious to find out what are your trade-offs and why you decided to do something\ndifferent. Also, this is a topic that is always evolving. Even AWS is constantly pushing new tools\nand new recommendations. So again, if you know any other way that it's equally valid for you,\nplease let us know and let's have a chat. And with that, we'll see you at the next episode. Bye.\n"
    },
    {
      "title": "28. How do you onboard junior devs to AWS?",
      "url": "https://awsbites.com/28-how-do-you-onboard-junior-devs-to-aws/",
      "publish_date": "2022-03-17T00:00:00.000Z",
      "abstract": "Luciano and Eoin discuss their strategies and ideas to help new team members to start embracing cloud computing and get productive with AWS. What are the main concepts to focus on when bootstrapping this journey, how to make a plan and make sure it’s bespoke to the expectation of the new employee. How to do pairing sessions and make sure we can build hands-on experience. Finally we discuss building troubleshooting skills and make sure we put in place a virtuous cycle that can foster continuous learning.\nIn this episode we mentioned the following resources:\n\nOur previous episode about AWS certifications and learning material\n\n",
      "transcript": "Eoin: Today we are going to answer the question, how do you onboard junior devs to AWS?\nSo we're going to chat about what fundamental concepts to focus on first,\nhow to make a plan, pairing and giving hands-on experience,\nlearning troubleshooting skills and creating a virtuous cycle for learning.\nMy name is Eoin and I'm joined by Luciano and this is the AWS Bites podcast.\nLuciano, is there one tried and tested method that works for you when you're\nonboarding junior devs onto AWS?\nYeah, that's a really interesting question.\nSo I think there is a method and probably that's what we're going to try to cover today.\n\n\nLuciano: But I also think that it's very important to recognise that everyone is different to some extent, different people learn in different ways.\nSo don't just try to apply the method to your own So don't just try to apply any method, I guess, not just the one we are about to suggest blindly, but try to always listen, be ready to adapt and also ask for feedback regularly.\nAnd then, of course, the next question is, how do you get the feedback from the users?\n\n\nSo I think that's a really interesting question.\nSo I think there is a method and probably that's what we're going to try to cover today.\nBut I also think that it's very important to recognise that everyone\nis different to some extent, but also try to always listen, be ready to adapt and also ask\nfor feedback regularly, try to make sure that what you're trying to achieve, the way you are\nproposing it is also comfortable enough for the other person.\n\n\nAnd maybe if they have preference to try different things, be ready to support them and adapt\nto whatever is the suggestion.\nOK.\nI would probably start by having a goal that is very well defined and agreed between the two parties.\nWe are trying to, of course, we're not expecting anyone to just come in and learn everything.\nSo we just need to agree on a subset of concepts that make sense for a beginner.\n\n\nAnd just to make an example, you might agree on, OK, we are just going to try to learn about IAM,\nS3, API Gateway, Lambda and SQS.\nAnd that's probably a good enough body of knowledge that somebody can use that and build\nsomething with it.\nAnd of course, that comes a little bit with understanding the basics and the basic tools.\nSo you probably might want to explore something for infrastructure as code.\n\n\nProbably it's not going to be cloud formation from date zero, but probably you want to do\nsomething a little bit simpler like SLS or some.\nBut you definitely need to start exploring the CLI and the SDK.\nSo also these things will get into the mix, I suppose.\nSo probably what I will do at the very beginning is start to explain in general\nall the account system, what is an account in AWS, how do you log in and how IAM works,\nhow do you set up the CLI and then maybe based on the subset of services and knowledge we\nwant to learn, we can discuss about, I don't know, what are those services for, what are\nthe basic topics and how do you use maybe in a very simple way some of the services.\nAnd yeah, it's very common to hear people saying, oh, you need to explain how VPC work\nand networking and all this.\nI think there are like 15 different concepts that you need to understand just to make sense\nof all the networking stuff.\nBut I have a feeling that in 2022, especially with all the serverless stuff that has happened\nin the last few years, you can probably get a long way without really having to fully\nunderstand VPCs.\nSo probably I would leave that as a secondary topic unless you know that in your company,\nyou are actually dealing a lot with VPC and networking at a very low level and that's\nrequired for the job.\nSo yeah, you'll need to learn it in that case.\nSo did I forget, I don't know, anything or would you do something different?\n\n\nEoin: I think that sounds like a really good plan, having that goal and knowing what subset of topics you're going to deal with because you could very easily get completely swamped and\noverwhelmed otherwise.\nAnd I think one thing to be considering is how much time do you leave them, the other\nperson just work on their own versus how much time do you work with them and do you watch\nover their shoulder too much.\n\n\nSo you want to get the balance right.\nYou want to give people enough freedom to learn on their own, but you want to give them\nthe right amount of support as well.\nSo part of that is just pairing to make sure you're explaining the concepts well.\nHaving a pair of sessions where you show them, but they also get the opportunity to drive\nas well and you give them constructive feedback and tips along the way.\n\n\nHands-on experience I think is probably the best way for most people to pick up these\nskills.\nMaybe a good idea would be to propose a project in an area that matches one of their interests.\nSo maybe if there's somebody with front-end development experience, it could be building\nan API that they could talk to from a web app, for example, or if they're a data scientist,\nit could be something in that field with some of the AWS data science services.\n\n\nIt depends also on the context.\nSo if you're in a company where you've got real projects that they can contribute to\non day one, that can be a really good thing too, because it's much more of a real world\ntangible context.\nThe only thing to be careful of there is don't, I suppose, get involved in areas that have\ntoo much complexity around them.\nYou know, if you've got lots of edge cases, lots of integrations, and they need to understand\nthe whole holistic system in order to just understand the one piece they're working on,\nit's very easy to get lost.\n\n\nSo I think you need to find the right project that people can get involved with.\nBut it's always possible that if you've got teams of people working on AWS projects, have\nthe junior devs shadow them and see what it looks like to work on a project.\nAnd they can even mix the two.\nThey can work on a training project at the same time.\nThe main thing is to offer the support when it's needed.\nSo if they're working on a project, follow up regularly with one-to-one sessions to answer\nquestions and find out areas where they need to invest more time and just help them on\na continued basis.\nSo I think it's important to have the review cycle so people need feedback.\nAnd when you look at the work of somebody who's just starting out with AWS, usually\nyou can give them a lot of nudges in the right direction, some tips and tricks that you've\nlearned just from your hard-won experience and some gotchas as well.\nAbsolutely.\nDo you think there's a system you can put in place that allows you to make sure that\nyou don't forget to give that regular feedback and you keep that level of support ongoing\nand not just for the first two weeks?\nYeah, I think this is a very big topic.\n\n\nLuciano: And as with any big topic, it's good to do baby steps and then repeat.\nAnd so I wouldn't do it in any different way in AWS.\nSo probably if you're supporting somebody, you want to have a face, you can create a\nvirtual loop, a virtual loop where you repeat this kind of process over time.\nAnd you probably start by explaining or exploring together some new concepts.\nThen maybe if there is something a little bit more practical, you can show them how\nto do that.\n\n\nAnd then the other way around, you can tell them, now try to do this yourself and you\noversee what they're trying to do and see if they're on the right path.\nAlso, you might just leave them alone to build something for a while and trying to also build\nup that self-confidence that they don't need to have somebody looking at them all the time\nto do something.\nBut even when that happens, make sure that you are ready to offer help and to offer review.\nSo as you said, oh, and that's the best opportunity to try to give some tips and tricks and opportunity\nfor exploring other concepts.\nAnd of course, if you keep repeating this, every time you do this loop, you can add new\nconcepts to the mix and that over time will build a good body of knowledge.\nSo yeah, I don't know if there is, though, any other fundamental skill that we haven't\nmentioned yet.\n\n\nEoin: There is one thing that I find is really always good to focus on from very early on is helping to build troubleshooting skills.\nWe forget that as developers, software engineers, engineers, engineers, engineers, we have to\nin general, you spend a lot of time fixing existing problems and not developing new code.\nAnd sometimes when we think about training people, we're talking about developing new\napplications from scratch all the time.\n\n\nSo it's not necessarily very representative.\nSo think about troubleshooting skills.\nAnd one of the easy patterns you could fall into for any developer is to try to go as\nquickly as possible from the problem you see to getting it fixed.\nAnd the fastest approach isn't necessarily the best one.\nAnd usually, actually, the fastest one is the one that gives you the least opportunity\nfor learning. So you can imagine you see an error message, you Google it, you get the\nStack Overflow answer, you paste it in.\n\n\nYou've really missed a teachable moment, as they say.\nTreat every failure or every problem you encounter as an opportunity to really understand how\nthings work and understand what is this error message telling me?\nHow can I trace this back to what I've been doing, understand it, and then work forwards\nthen towards a solution?\nSo it's really about following the evidence.\nAnd once you've fixed it, then you could maybe have a retrospective and look back at how\nyou got from the problem to the solution.\nJust repeat that and reinforce that learning.\nThat's one that I'm particularly keen on.\nWhat do you think about training and certification?\nAre those useful at this point?\n\n\nLuciano: Yeah, just before we move into that, I really like what you said before about the retrospective.\nAnd I think that's also an opportunity, especially when you're talking about an incident\nor a bug, which is most of the case why you are troubleshooting.\nTo ask the question, what can we do to avoid this from happening again?\nBecause that's another opportunity for, first of all, making sure that you understood the\nproblem, but also you understood it up to a point where you can start to see a little\nbit more long term and try to apply decisions that will make your life easier in the future\nbecause you're not going to have to deal with the same problem over and over again,\nhopefully.\n\n\nSo I really like that part.\nAnd I just wanted to say, yeah, let's also see if it's something that you can proactively\naddress for the future.\nBut going to training and certification, this is a topic that, by the way, we explored quite\nin depth in another episode.\nSo we're going to have a link in the show notes.\nBut I think it's very important to also have, first of all, a library of material that people\ncan use to explore all sorts of different concepts and make sure that this library provides\na list, a basic introduction to things that otherwise might be a little bit more complicated\nto approach in an organic way.\n\n\nOther than that, getting a certification is also something we discussed a lot.\nAnd it can be very useful, especially the basic certifications.\nSo the cloud practitioner and the associate ones, they tend to give you an overview of\nall the important concepts of AWS and understand what are all the different areas.\nSo those might be a little bit tricky and might be a little bit intensive, but definitely\nuseful if somebody is willing to put the effort and try to get a certification.\n\n\nThey should be supported because that definitely is going to boost the learning process in\nmy opinion.\nYeah, we'll put the links in the description.\nAnd hopefully you can tell us if that makes sense to you.\nAnd if you did that kind of journey, how much did it help you to ramp up in AWS?\nAnd one final thing that I want to say is that we mentioned all this process from the\nkind of the perspective of somebody senior that is trying to help somebody that is just\nstarting with AWS.\n\n\nBut I think there is another angle to it, which is as soon as you acquire a little bit\nof knowledge, you should be in a position to teach somebody else that knowledge.\nYou don't need to be a master to basically be able to help somebody else.\nRight?\nAnd I think this is something we can introduce in our virtual cycle if you want by making\nsure that if we have more and more people coming to the company, you don't have to take\nthe most senior engineer to train the most junior engineer.\nBut you can use also your junior engineer that have some degree of knowledge to start\nto teach something to people coming in the company.\nAnd that's a great way for them to, first of all, build confidence and reinforce their\nlearning in general.\nBut also I found as somebody who has done this activity that sometimes when you try\nto teach somebody else, you realize all the gaps in your knowledge, you get very interesting\nquestions.\nAnd these are great prompts for you to revisit your knowledge and try to understand what\nare the gaps and what you need to learn a little bit more in depth.\nYeah, that's really good one.\n\n\nEoin: It definitely, I've been there so many times when you try to explain something you think\nyou know very well to somebody and then you realize you've got to fill a whole number\nof gaps in your knowledge.\nThat's a really good point and maybe a good place to finish.\nSo thanks everybody for listening and watching again on AWS Bites and we'll see you in the\nnext episode.\n"
    },
    {
      "title": "29. Is serverless more secure?",
      "url": "https://awsbites.com/29-is-serverless-more-secure/",
      "publish_date": "2022-03-24T00:00:00.000Z",
      "abstract": "Eoin and Luciano take you through the ways serverless can give you more security out of the box. We cover the tradeoffs between having more security control and the responsibility that comes with this power. There are always new security challenges so we cover some of the common pitfalls with serverless and AWS security in general. Finally, we share some tips to make your serverless deployments more secure.\nIn this episode we mentioned the following resources:\n\nArchitecting Secure Serverless Applications on the AWS Architecture Blog\nAWS IAM Access Analyzer\nThe AWS response to the Log4J2 vulnerability\nFunctionShield\nSnyk for scanning dependencies and containers for compromised packages\n\n",
      "transcript": "Luciano: Is serverless more secure?\nIn today's chat, we're going to answer this question.\nAnd by the end of this episode, you will know\nhow serverless compares with more traditional deployments\nin terms of security.\nWhat are the main security strengths\nof serverless deployments, but also what are some\nof the weaknesses of things to be aware of,\ncommon serverless security challenges, and some tips\nto make your serverless deployments more secure.\nMy name is Luciano, and today I'm joined by Eoin,\nand this is AWS Bites podcast.\nLet's start with one topic that is probably\nthe most commonly discussed topic around serverless\nand security.\nAnd of course, in this case, we are trying to talk\nmore about the context of AWS.\nSo we are probably going to be talking more and more\nabout Lambda.\nSo what do we mean when we say that, for instance,\nit's easier in that context to apply the principle\nof least privilege?\nWhat do you think, Eoin?\nI think this is the first thing that comes up\nwhen people discuss serverless and security, right?\n\n\nEoin: Yeah, for sure.\nSo the idea with least privilege is that if you've got\nvery small units of deployment, very small granular\nfunctions, you can have very small granular policies\nthat only need to do specifically what that function\nneeds to do.\nSo if you imagine like an API and you've got a get method,\na post method, one for listing resources, they can all\nhave the individual policy attached to their Lambda\nexecution role, and then you don't need to have access\nto put item in a DynamoDB table in your read-only\nresource accessors.\nAnd that's a very good way to reduce the attack service,\nright?\nSo if the function is compromised in some way and functions\ncan be compromised, generally you're limiting the blast radius,\nyou're limiting the effect that that attack can have.\nOf course, there are ways to inject vulnerabilities into\nLambda deployments.\nSo what are some of the ways there?\nI guess you're a big Node.js fan on the channel,\nbut one of the things that we've seen from time to time\nis that dependencies can be injected through the Node.js\nmodule system.\n\n\nLuciano: Exactly.\nThat's probably one of the most common attacks, even outside\nthe world of serverless, but of course, applies to serverless\nas well.\nMost likely, when you write a Lambda function, you are just\ngoing to do npm install and get some useful dependency.\nOf course, you need to be careful with that because those\ndependencies can be compromised at some point.\nYou might be installing a version of even a very common\ndependency that might have been compromised in different\nways, and that dependency will be running in the context\nof your Lambda, and it might try to do dangerous things\nthat we'll probably discuss more and more through the\ncourse of this episode.\nThe interesting thing in the context of least privilege\nthere is that, of course, if you have smaller units, you\nwill probably want to install the minimum amount of\nlibraries that you need for every single Lambda.\nSo every Lambda will only keep the libraries that are\nreally needed to perform that particular Lambda task, and\nthat probably helps to reduce the surface once again,\nbecause if one module gets compromised, most likely that's\nnot going to affect your entire application, but only\nthose the subset of Lambdas that actually use that module.\nSo this is another way that I think we can see that the\nprinciple of least privilege doesn't just apply to IAM\npolicies, but also applies to dependencies.\n\n\nEoin: In AWS, you have this shared responsibility model, don't you, where you have a set of things that AWS take\nresponsibility for from the perspective of security, and\nthen the part that you are responsible for.\nIs that significantly different, or how would you quantify\nthat if you're moving from a system where you're doing\nEC2 instances, or maybe something in the middle like\ncontainers on EKS, and then looking at Lambda, how much of\na benefit do you get with that shared responsibility shift?\n\n\nLuciano: Yeah, I think this is a very good point, because let's start with the comparison with EC2.\nWhen you have to provision an EC2, generally, the first\nthing you need to do is decide, okay, which operative\nsystem am I going to use when I build my image, and with\nthat, which version of that operative system, and then at\nthat point, you probably will need to install some custom\nsoftware, even, I don't know, system libraries, things like\nthat.\n\n\nEventually, then you'll get to the point where you install\nyour own code, some sort of configuration in the machine,\nand then at that point, you have something you can actually\nexecute as a compute unit.\nSo there is a lot of things that you need to keep in mind,\nand a lot of places where security will go wrong in a way\nor another, because, of course, you own all these decisions,\nand you need to make sure that you are doing everything\nas secure as possible, of course.\n\n\nWhen you do systems in the context of, I don't know,\nFargate or ECS, so when you're using container, it gets a\nlittle bit better, because you don't worry too much about\nthe machine itself, where things are running.\nYou only worry closer to your code.\nBut again, when you are building a container, you are\nstill starting from that similar concept of operative\nsystem libraries.\nThere is still a layer where it's not just your code, but\nthere is a lot more that you are bringing in, and you need\nto make sure that that layer is also not vulnerable or not\ncompromised.\nAnd also, again, there is a similar thing with external\ndependencies.\nIf you're using third-party containers, those might be\nvulnerable as well, or there might be malicious attacks or\nother worms.\nThere's a whole lot of those.\n\n\nEoin: Yeah, exactly.\nAnd I guess you're thinking about the operating system,\nbut I guess when you have containers and instances, you've\ngot the operating system, but also the disk.\nSo you might have file system security to worry about, and\nthen you have the network in a container or an EC2 instance,\nso you have network security to worry about too.\n\n\nLuciano: Yeah, I guess the point that we are trying to make is that in the context of a Lambda, all these things are...\nYou kind of get a smaller set of options, and therefore you\nalso get a smaller surface.\nAnd most of the things that are happening underneath in the\nunderlying layer that executes your Lambda are managed by\nAWS, and AWS should reasonably take care of keeping security\nunder control.\nAnd I think we had a very good example with the Log4j\nfamous vulnerability when it was a few months ago, where\neveryone was rushing to update their deployments because,\nof course, very, very dangerous vulnerability there.\n\n\nIt could allow pretty much uncontrolled remote execution,\nso probably one of the most dangerous kinds of\nvulnerabilities.\nAnd Log4j is also one of the most common libraries in the\nJava world, so almost any Java system was exposed at that\npoint, or at least potentially could have been exposed.\nSo everyone was rushing overnight to fix all their Java\ndeployments, and it's a huge surface to fix overnight.\n\n\nAnd in the case of AWS, if you were running, for instance,\na Lambda using the Java runtime, AWS immediately, or\nalmost immediately, took care of trying to patch that\nruntime to try to reduce to the minimum the risk for that\nfor that runtime to for people to use that would exploit\nthat vulnerability and create attacks based on that.\nSo this is just an example of having that shared\nresponsibility model.\nIt's something that can help in terms of security.\nRight.\nSo I suppose the next topic I have in mind is another\nadvantage of Lambda, and this is a little bit of a\ncontentious one, because sometimes you talk about short\nexecution times as a negative thing for Lambda.\nIt's more like a blocker, but it's interesting to discuss\nthis because in terms of security can have, I think, even\na positive effect in a way.\nWhat do you think?\nWhat's your opinion on that?\n\n\nEoin: Yeah, and I've heard, I can't remember who it was, but I've\nheard members of the Lambda teams state this when people\nare arguing for shorter execution times, they say, well,\nlook, one of the great benefits of having a short execution\ntime is that the attack window is much shorter.\nSo even if people do get access to that environment, they\nonly have 15 minutes to do the damage or to exfiltrate the\ndata or whatever it is, but it also means, you know, as\nsystems run, they accumulate state or cruft, and this can\nalso have security consequences.\nSo having things that are, don't accumulate this kind of\nstate, let's say, even it could be like some sort of memory\nleak or something that could eventually open up an attack,\nthat 15 minutes is actually a benefit.\nSo I think we should, in a lot of cases, people should try\nand embrace that 15 minutes and say, okay, well, how can I\nsplit the workload so that everything runs in a short\nperiod of time in this kind of stateless way?\nAnd then you get that security benefit as well.\n\n\nLuciano: Absolutely.\nYeah, I agree.\nEven though sometimes I think that there is a little bit of\na double-edged sword in this, because let's say that an\nattacker managed to somehow inject something in a Lambda\nand execute code.\nAgain, we'll be talking a little bit more of some examples.\nWhat's going to happen is that from the perspective of\nsomebody managing this infrastructure, whatever the\nattack is going to do is going to vanish very quickly when\nthe Lambda gets disposed and the next execution is started,\nright?\nSo it can also become harder, I suppose, to see when\nsomething bad is going on, because you don't have, I don't\nknow, any simple way to do, like to run scanners constantly\nover your infrastructure or to detect drift because just\nthings get recreated and deleted all the time.\nSo if an attacker manages to time an attack very well, which\nis going to be hard, of course, for the attacker, but it\nalso means that it can be harder to detect that kind of\nattack.\nSo that's something to be aware of, I guess.\n\n\nEoin: Are we generally talking about like looking at the normal set of attacks that people should be mindful for in any\nexecution environment, but a subset of that?\nOr are there like new ones that emerge?\n\n\nLuciano: I think for sure we can talk about like same type of common attacks and how they change.\nThey will change a little bit in the context of serverless,\nbut most of them are common attacks.\nI don't expect anything like extremely new.\nThey might just have a slightly different variation in the\nway that they are performed and the effects that they can\nhave.\nOf course, the first one that we already mentioned is\neither data filtration or remote code executions and the\ntwo always kind of go together because one is kind of useful\nfor the other.\n\n\nAnd we say that there could be ways for an attacker to be\nable to run arbitrary code in the context of a lambda.\nWe already mentioned the case of dependency that gets\ncompromised.\nThat can also happen with injection.\nSo if your lambda is receiving external input and that\nexternal input is used in an insecure way, that might lead\nto remote code execution as well so that there might be\nseveral different ways for an attacker to be able to run\narbitrary code in the context of your own lambda.\n\n\nSo let's say that that happened in a way or another.\nWhat can happen next?\nWhat can the attacker do?\nAnd the first thing is that I would expect an attacker,\nthe first thing they will do is probably try to do some\nrecon so they will try to see, OK, I am inside an AWS\naccount.\nWhat else can I do?\nWhat's exposed from this point, from this start to the\nend?\nAnd for instance, most likely what they're going to do is\nthey can try to grab the credentials of that particular\nlambda where they are running and they can do that in\ndifferent ways.\nThere is like at that point when you can run arbitrary\ncode, you have access to the credentials.\nSo there's really no environment variables.\nYeah, exactly.\n\n\nEoin: Which is one good reason not to store secrets, additional\nsecrets in environment variables, of course.\nExactly.\n\n\nLuciano: But also the lambda itself will have a policy that gives some permission to the lambdas for the\nlambda.\nSo if those permissions are very wide open, the attacker\ncan start to do list of sorts of resources, try to spin up\nEC2 instances.\nAnd by spinning up EC2 instances, they can create a\nmore persistent footprint in the infrastructure.\nMaybe they can spin up, I don't know, something that\nallows them to do remote control of the infrastructure\nin a more permanent way.\n\n\nThey can steal data because they can access S3.\nSo you need to be very, very careful at that point that\nthe surface of that lambda is as restricted as possible,\nbecause whatever the lambda can do, the attacker will be\nable to do the same things.\nSo it's, again, very important to apply that principle of\nleast privilege.\nAnd another example, if we just want to think about maybe\nthe attacker doesn't really care about using resources in\nyour AWS account that they don't want to run compute and\nbasically steal your money in that indirect way that they\njust run compute that you are going to pay for and they can\ndo their own stuff with it.\n\n\nMaybe they have, exactly, like crypto mining or DDoS endpoints\nor stuff like that.\nMaybe they care more about data.\nSo another interesting thing is that they might try to\nexfiltrate data.\nAnd how can they do that?\nAnd again, we mentioned they could try to exfiltrate\nenvironment variables, secrets and so on, because that can\ngive them other types of access even to other third party\nsystems that you use in your company.\nBut they can also just try to exfiltrate interesting data\nthat you might have in your account from S3, from databases.\nAnd of course, the first way that they are going to try is\nto try to upload some data that they got access to, to some\nremote server.\nSo the next interesting topic is probably network traffic.\nLike, can Lambda give you ways to kind of control the network\ntraffic and limit this kind of attacks?\nSo it can definitely be beneficial as opposed to limit\nthe outbound network traffic.\nBut how is that possible with Lambda?\nIs it easy?\nIs it obvious?\n\n\nEoin: We should talk about that.\nSo maybe we're getting into the area of, OK, what are some\nof the challenges that serverless security can bring that\nyou don't have in traditional security, let's say.\nAnd you already mentioned, like, OK, your 15 minute window\nmight give you a short opportunity to actually spot attacks.\nWhat else do you think is challenging?\nBecause it seems like it's fairly beneficial so far.\n\n\nLuciano: Yeah, I think there are challenges mostly from an engineering perspective, because if you compare something a little bit\nmore monolithic, like a single EC2 instance or even a few\ncontainers, when you look into Lambda and serverless, you\ngenerally have a lot more moving parts.\nThat means that you need to be careful and diligent with a\nlot more smaller things so that the room for mistake is there.\nIt's like it's going to, you're going to have to take care\nand be strict with a lot more things.\n\n\nSo something might slip more easily.\nSo I think in that case, it's good to have processes in the\ncompany and to have a more structured approach.\nFor instance, one thing that I really like to do, even though\nit's a little bit painful, is whenever I write a Lambda, I\nbasically give it zero IAM permissions, except from logging\nand the basic stuffs.\nAnd then I need to use, for instance, DynamoDB.\nI write the code, I make it fail.\n\n\nAnd I see, OK, why did it fail?\nYou don't have permissions to put item, right?\nFor instance, then, OK, do I need permission to put item?\nObviously, yes.\nHow can I limit that permission?\nMaybe I can limit to a subset of resources, not like put item\nasterisk on the table.\nSo I try to do that very strict exercise of, OK, now I need to\ngive some permission, but what's the minimum level of\npermission that I could give?\n\n\nSo that's a very painful way of doing it.\nBut I think it's one way that will help you to make sure that\nyou are at least thinking of what's the minimum amount of\npermission you can give to a Lambda.\nOf course, it's still challenging because over time\nyour Lambda will evolve.\nDifferent people will work on that same Lambda.\nSo sometimes you end up changing even the implementation\nand you might forget to remove permissions that maybe you\ndon't need anymore.\nSo that process is something that needs to be revised even\nevery time you do updates, not just the first time you create\na Lambda.\n\n\nEoin: You might use IAM access analyzer, some of the tooling looking at your analyzing your cloud trail.\nBut yeah, just I guess auditing your policies to make sure\nthey're not overly permissive because you add them right.\nBut then you remove your DynamoDB use.\nYou don't need to have your put item action anymore, so you\nshould remove it.\n\n\nLuciano: Yeah, that's actually a very good tool and we'll put a link in the show description because I don't think many people use\nit enough, I guess.\n\n\nEoin: So do you think security can get in the way then of like the one of the things we talked about when we're talking\nabout serverless deployment is OK, it gives you the ability\nto increase your deployment velocity, your speed of\niteration because you're able to isolate what you're doing\ninto small units.\nYou're relying on less infrastructure, so you have less\nto deploy.\nYou're using a lot more of what's available to you in terms\nof managed databases, API gateways, and so on.\nManaged databases, API gateway, you know you're focusing on\nthe very least minimum amount of business logic you need\nto implement.\nThat's the idea.\nDo you think security might become like a barrier in that\nlike if you've got all these hundreds of IAM roles and\npolicies for all of your Lambda execution roles and your\nstep function execution role that all of this can slow you\ndown?\n\n\nLuciano: I think there are cases where that can happen and it's\ndepending on the kind of process you have in the company\nbecause for instance, if you have an external security team\nby starting I mean not working directly with the development\nteam and maybe they need to sign off everything that is\nrelated to security like for instance they need to review\nmanually every single policy and sign off before you can\ndeploy.\n\n\nAt that point that can become a blocker because you're\nprobably going to create new policies every day and if you\nneed to stop and wait for somebody to approve that it\nmight block basically your work every day and you're not\ngoing to take advantage of that velocity that you could\ngenerally have with serverless deployments.\nSo there is another case where having a good process,\nhaving collaboration and having automation and tooling,\nit's something that might help you in that direction and\nyeah at that point you might still get a good enough level\nof security and still retain that velocity.\nBut I suppose it's tricky to get to that level of maturity\nand really understand how the process can help this way of\ndevelopment and vice versa how that's not going to affect\nsecurity in a bad way.\n\n\nEoin: Yeah you have this idea of like for the over the past five\nto ten years of shifting security left and having\ndevelopment teams take responsibility for security and\nthat happened I guess with like DevOps movement and\ncontainers it became possible I suppose for developers to\ncontrol things at a fine-grained level and to take part of\nthe responsibility for security.\nDoes that increase with serverless?\nDoes it decrease?\nIs it are we now the case where you have to have developer\nteams that have an increased level of security awareness?\nIs that a disadvantage?\nBecause I guess like previously you mentioned you might have\na team that was dedicated to security so it was kind of\nsomebody else's problem.\nNow that might have had its disadvantages because you know\nyou've a dependency on that centralized team but does it\nbecome like an extra skill set that serverless developers\nneed?\n\n\nLuciano: I would probably say that this is something that like all the because the security risks are growing every day and\nthere is a lot more concerns and I think that's a skill that\nevery developer needs to develop anyway to some extent so\nI would be very opinionated on this that way.\nBut yeah I agree that it is also beneficial to try to reduce\nthe gap between teams that are focused on security and\nthat's their core skill and teams that are more focused on\nsoftware engineering and that's their core skill and I've\nseen this new term coming out a few times DevSecOps that\ntries to kind of define I think that idea in a way that\nDevOps so operation, development and security is not three\ndifferent things but it's actually something that needs to\nwork very closely together and it needs to be like one unit,\none skill, one methodology and probably one comprehensive\nset of tools that they facilitate that all of that from\nhappening consistently in a company.\n\n\nEoin: Yeah in this serverless world then the security where you've\ngot that level of skill does it become all about IAM and can\nyou dispense with the network security practices that have\nevolved and improved over decades? Is this more just about\nIAM on AWS?\n\n\nLuciano: I suppose yes and no I would say because I guess if you just ship your Lambdas in the default VPC then you kind of get a\nstandard starting point in terms of network security where\nthings are open to some extent but I mean most of them you\nare just going to be worried about IAM because you take\neverything else for granted but that doesn't necessarily mean\nthat you are secure it's just that you are not thinking about\nsome possibilities that can happen through the network.\n\n\nWith a Lambda you don't get inbound like arbitrary inbound\ntraffic you only get the events that you configured so in that\nsense this is good but then you are still like if there is an\ninjection if there is remote execution that Lambda in a\ndefault VPC can still reach out to any arbitrary server on the\ninternet so that that's something that maybe you want to\nlimit because of course at that point it's a risk.\nSomebody can filtrate data arbitrarily connect to any\narbitrary server so if you want to control that you again are\nin the realm of okay let's do our own custom network security\nlet's do our own VPC and let's configure network access and\nmaybe at that point you can control more what's going on on\nthe outbound traffic and limit exfiltration that way.\n\n\nEoin: Yeah it's a very difficult balance I don't think there's an ideal solution there so if you talk about okay I want to put my\nLambda function in VPC so that I can access an RDS database and\nthat's fine you can just ensure that you've got access to that\nRDS database and they're in the same VPC and there's no\nadditional routing outside that like there's no internet gateway\nno NAT gateway so they can't exfiltrate data outside that\nnetwork but at the same time you might have an existing on-prem\nsystem so you might have a VPC that allows you to route through\nto the company's corporate network so that it can access\nall of their existing on-prem systems and systems running in\nother clouds or whatever it is but you can imagine that in\norder to give network access to one of those systems you may\nhave to give access to the corporate network in general and\nthen think it becomes a little bit more onerous trying to\nrestrict it down to a single IP address or a single host or set\nof hosts because giving an attacker through Lambda or\nthrough EC2 access to your full corporate network is seriously\nrisky and usually your corporate network then access has access\nto the internet and you might have an intrusion detection\nsystem or intrusion monitoring system but suddenly your blast\nradius is seems quite large so it seems like I always kind of\nbelieved you know don't try and avoid VPCs unless you have to\nbecause you know you don't have to worry about the that level\nof that level of access and it's also a little bit more\ncomplexity and you suddenly have to think about network\nsecurity seems less serverless once you start bringing in VPCs\nbut like you say if you don't have a VPC you can't control\ndata exfiltration very carefully because you'll have access to\nthe internet so people can access their own hosts and take\nyour secrets take any data they can pull from your S3 bucket\nand upload it to their host wherever it is on the internet\nthere was a solution to this at one time which I know you've\nencountered Luciano called Function Shield but it looks\nkind of at least I know the company who developed a PureSec\nwere acquired by Panao Total Networks so I think it's not\nmaintained anymore it's been absorbed into some sort of\ncommercial offering the idea behind that was that you could\ninject it into your lambda functions no matter what language\nyou were using and it would make it harder to do disk access\nor network access or do other command execution in your lambda\nenvironment but yeah I'm not sure there anymore but so I don't\nthink there's a valid alternative I'd be interested to\nhear if anybody else has a creative way of solving that\nproblem of network access internet access from a lambda\n\n\nLuciano: without a VPC yeah not sure how it was implemented but that the way you would use it was actually really simple and\ninteresting it will really you import a module and that module\nwould work in Node.js, Python and Java I think so also cross\ncross language and then with that module you just run a\nfunction that says I want to use this policy and this policy\nsays this lambda cannot I don't know execute sub processes or it\ncannot use the temporary file system or it cannot use the\nnetwork and then the library will limit all these things\nfrom happening so that will be an extra level of security for\nyou because these are not things you can easily control\nwith IAM policies they are more behaviors of your code and this\nway you can also control behaviors that you don't really\nneed your code to perform so yeah I'm also interested to see\nif there is any other alternative these days.\n\n\nEoin: Are there also potential for attacks where you know what we know that one\nof the other advantages of serverless is that it can scale\nwith your workload but what if that workload isn't a genuine\nworkload but a malicious kind of denial of service\nworkload what what do you think about that?\n\n\nLuciano: Yeah no there are an interesting few cases that some of them I've even encountered\nmyself but I suppose that the point is that because you have\nunits of computers that just spin up themselves and they can\nspeed up in the order of thousands very very quickly\nthere are a lot of situations where that might go against I\nsuppose your benefits and for instance they can let's say\nit's an attack what can happen for instance that if you have\nsomebody triggering a DDoS attack that DDoS attack might\nspin up a lot of lambdas for you and maybe you don't really\nsee a negative effect in your infrastructure because the\ninfrastructure can actually scale and take that attack but\nyou might see a negative effect in your building because\nsuddenly you are running I don't know maybe an order of\nmagnitude more many lambdas that you generally run so\nprobably your bill will increase proportionally to that\nso that that can be a dangerous side effect of serverless that\nshould be taken under consideration even though it\nmight not be strictly related to security. Another case that I\nhad myself and this is also related to VPC and configuring\nyour own VPC I had a case where I actually did configure very\nbadly a VPC because I started to put lambdas in a subnet where\nthere were also other services I think it was Elasticsearch\nan Elasticsearch cluster and living in the same subnet at\nsome point there was a bug in the code where many lambdas\nwere retrying because of a bug in a processing logic so it\nwas failing and it was retrying and suddenly that generated a\nhuge number of lambdas trying to compete to address that\nevent that would never be fixed because there was a bug in the\ncode so I basically saturated the subnet with lambdas trying\nto do something that I would never be able to do and that\nstopped Elasticsearch from scaling up because the\nElasticsearch nodes couldn't be spun up because they couldn't\nget an IP address in that subnet so that was an interesting\nthing that very convoluted set of events but again the\nreason is that if you don't do your network security\ncorrectly network configuration correctly and you don't\nconsider that lambda can scale massively in a very short\namount of time then you might have this interesting side\neffects where lambda is actually competing with all your\nresources and it might have an impact on them not being able\nto scale as much as you expect so of course then we needed\nto fix the bug in the code we also changed the network\nconfiguration to isolate the lambdas in their own subnet\nrather than keeping them in a shared subnet and we fixed it\nthat way but it was not something obvious it was not\nsomething we anticipated before we we actually encountered\nthe problem so yeah I suppose that the we could probably\nfinish this episode by summarizing what serverless\ndoesn't really protect you against what do you think good idea yeah so yeah first of all we say you don't get protection from injections meaning that yeah it will be\nprobably a little bit more complicated to inject arbitrary\ncommands or I don't know SQL injection XML injection all\nsorts of injections you can think of it will be maybe a\nlittle bit harder because you you have one additional level\nof indirection because you have some sort of input that\ngets converted to one event and then your input is\nencapsulated in that event that will go into lambda but\nthat doesn't really give you any security or perfect\nguarantee that people cannot perform injection attacks then\nyou are still receiving external input the way you process\nthat external input can lead to injection attacks and we\nalso discussed about dependency poisoning so you are using\nthird-party dependencies so those might be poisoned and\nthose might create side effect and security vulnerabilities\nif you are not careful so one suggestion that is to use\ndependency scanners like SNCC is probably one of the most\nfamous but there are a bunch of alternatives make sure you\nhave a process as part of your CI CD or your deployment\nprocess to always keep your dependencies under control and\nmake sure at least you scan them to to find commonly\nvulnerable dependencies and update them another risk is\ndata tampering or data destruction again if code is\nexecuted in your environment malicious attacker can do all\nsorts of different things they can try to aspitrate data they\ncan try to compromise data so that's you don't get many\nguarantees from serverless that that's not gonna happen to\nyou maybe it's again a little bit harder but it can still\nhappen and you need to put boundaries in place and yeah\nremote code execution is absolutely related to that\nbecause at that point an attacker can try to do anything\nnot just as filtrate data but they can run any code they\nwant so you might try to limit the exposure of a lambda but\nif you don't limit that correctly an attacker still can\nhave a very big surface to use and yeah I think that that's\nall I have I think we we can put a bunch of links for more\nin-depth material for instance there is a good OWASP paper\nthat gives you the top 10 vulnerabilities in kind of in a\nmore serverless way so that they reconsider the top 10 OWASP\nfrom a serverless perspective and I think you will find some\ncommon points with what we discussed today but probably\nthere is a lot more material there and then there is an\ninteresting blog article from AWS that gives you a bunch of\nadditional tips on how you can architect secure serverless\napplications so we'll put the link for that in the show notes\nas well. Okay anything else you want to add, Eoin?\n\n\nEoin: Yeah it's never going to be possible for us to cover everything in\nsecurity but I think that's that seems like a pretty\ncomprehensive list but I'm curious to hear what we missed\nif anybody has any ideas for what we missed any other\nsecurity incidents that they've learned.\n\n\nLuciano: Yeah absolutely security is a huge topic and it's always evolving so I'm\nsure that there are a lot of stories that people can share\nand we can all learn from them so definitely please let us\nknow in the comments reach out on Twitter and yeah we'll be\nmore than happy to learn with you and share your learnings so\nplease do that. Okay and with that thank you very much for\nbeing with us today and we'll see you at the next episode.\n"
    },
    {
      "title": "30. What can you do with 10GB of Lambda storage?",
      "url": "https://awsbites.com/30-what-can-you-do-with-10gb-of-lambda-storage/",
      "publish_date": "2022-03-31T00:00:00.000Z",
      "abstract": "AWS Lambda just got a big upgrade in ephemeral storage: you can now have up to 10 GB of storage for your /tmp folder! Before this was limited to “only” 512 Mb… But is this really useful? What can we do now that we couldn’t do before? Also, is this going to have an impact on price? And how does it compare with other storage capabilities that are available in Lambda? Eoin and Luciano are on the case to try to find some answers to these compelling questions, for the greater serverless good!\nIn this episode we mentioned the following resources:\n\nOfficial AWS announcement blog post\nWill Dady on Twitter about 10GB of ephemeral storage now enabling interesting CI/CD use cases\nYan Cui’s post on Lumigo’s blog “Welcome to 10GB of tmp storage with Lambda\nLambda cost comparison with ephemeral storage spreadsheet\n\n",
      "transcript": "Eoin: What can you do with 10 gigabytes of Lambda storage? This is a new feature that was released\nin Lambda and everyone's very hyped about it. So in this episode we're going to give our take and\ntalk about what does it really mean to have 10 gigabytes of ephemeral storage? What can you do\nwith this new capability? And finally we're going to discuss if this is really an advantage or if\nit's just something useful you might use in niche use cases. My name is Eoin, I'm joined by Luciano\nand this is the AWS Bites podcast. Luciano, what is this 10 gigabytes of ephemeral storage? What\ndoes it mean and how is it different?\n\n\nLuciano: Yeah, so one way that I will describe this ephemeral storage is basically if you have a Unix system and you have a TMP folder this is generally where you store\nfiles that are somewhat persistent but yeah they generally have like a long and they don't have a\nlong duration you just use them as a transient storage mechanism and this is something that's\nbeen available in Lambda for since the very beginning but the limit was 512 megabytes now\nthis has been extended up to 10 gigabytes so technically now you can store a lot more\ndata into this particular file system directory. Now what does that mean in the context of Lambda\nis something we're going to discuss more and more throughout this episode but one interesting thing\nthat I want to mention is that because of the characteristics of a Lambda where effectively you\nreceive based on events a Lambda is triggered and you don't really know if for that particular\ninvocation an instance that was already available is going to be reused or if a new instance is\ngoing to be bootstrapped so imagine this as a container is probably the easiest way to\nunderstand this it's like you are running a new container or maybe you already have a container\nthere initializer and going to reuse the same container so what happens to that temporary\nstorage in those two different use cases effectively every time you are running a new\ninstance you are starting with a new blank folder in the temp space if you are reusing an existing\ninstance and you have saved something in that temporary folder you will find again the same\nfiles available there for you to use so one interesting thing is that you could be using this\nstorage across invocations but of course there is no guarantee that your data will actually be there\ndepending on how many Lambdas you are running if they are running all the time or if you have\nspikes of time where nothing happens between one invocation or another and another interesting\nthing that Yan Shui and we're going to mention the article of course in the show notes did some\nexperiments and he realized that there is no cold start overhead so this is another small detail\nthat is interesting to mention as we are talking about bootstrapping new Lambdas with more storage\nso yeah I don't know what do you think in terms of application what is it something that we will do\n\n\nEoin: now that we have this additional capability there's a few things you could think of I guess if you've got video transcoding applications that's one of the examples that that has come up so if you're\nproducing a video you can imagine you might need to have intermediate artifacts you're producing\nso let's say you take some input it could be some images or an existing video that you need to split\ninto frames you might want to process those frames and then stitch them back together and encode them\nwith a video codec if with that intermediate data you might need somewhere to keep it and having 10\ngigabytes of slash temp is going to be pretty useful for that there's also one of the probably\nmore likely use cases for this slash temp is when you're doing ETL or data processing and you have\nintermediate data steps as well so you know in general you probably don't want to be storing\nthings in Lambda so it's more of an optimization for those cases where you really need it\nand you can imagine if you're processing gigabytes of data ideally you would like to just kind of\nstream it in and stream it out and not store anything but sometimes again you need to do\naggregations which require you to store to read in all of the data store it in one format then\nprocess it further and your slash temp might be useful for that I've heard a lot of people say as\nwell machine learning models it'll be useful for that because some machine learning models can be\nquite large you know they can run into gigabytes so having being able to pull them down from s3\nput them in your slash temp and then use that across multiple invocations would be useful\nI'd also kind of challenge that a little bit and say well if your model is part model is almost\npart of your code so it might be more suitable to bundle that into a container image and deploy\nyour Lambda image as a container your Lambda as a container image but if your model changes more\nthan the Lambda does then you might do it the other way and use slash time for it and one of\nthe most I suppose slightly esoteric options was using Lambda for continuous delivery continuous\nintegration and there was a tweet from Will Dady which suggests that maybe AWS step functions and\n10 gigabytes of ephemeral storage in Lambda could be a better option for a continuous build\ncontinuous build performance than using code pipeline with code build I think that's\ndefinitely an interesting one I don't know if I would rush to use it I did try using step\nfunctions for continuous build orchestration before and it's a little bit clunky it's improved\nprobably quite a lot now that you can use AWS SDK from step functions and I would imagine that the\ncold start time for a Lambda function to do a build is going to be significantly less than the\ncode code build container but you still have to then go and implement your git clone and all of\nthat stuff in Lambda if and deal with that the secrets and your access to git and everything so I\nmaybe leave it for someone else to iron out all the kinks before I go trying that option\nso that's those are some of the applications that have come up I guess what do you think Luciano\n\n\nLuciano: is it have I missed anything maybe we can remark again the use case that we kind of inferred to previously about caching because again if you have this storage and maybe you have produced large\nfiles that you might need to produce over and over again there is no guarantee that that file will be\navailable across invocations but you could check if it's there you don't need to to recreate it\nagain you can just use it so in that way could be kind of a soft layer of cache it's not going to be\nthe most reliable but since you have it you can try to use it and maybe it will give you a little\nbit of a boost in your overall computation time across invocations so yeah again I don't know if\nit's the most useful thing but it's there and you can use it and it might give you some small\nadvantages what can we say instead in terms of pricing is this something that as a host is it\n\n\nEoin: something we need to enable or it's just available for everyone you get the so the existing volume of slash temp was 512 megabytes you still get that for free anything above that that you're charged\nfor and the unit prices per gigabyte per second so it's similar it scales linearly just like your\nlambda memory I did a pricing sheet just to compare what it would be like and see\nfor different function sizes how much of an impact would it have if you allocated the maximum 10\ngigabytes of ram and in general my conclusion is it doesn't really make a lot of a difference it's\nit's pretty cheap compared to your memory cost so if you've got a the maximum memory allocated of\n10 gigs and you add also 10 gigs of ephemeral storage it almost makes it's an insignificant\ndifference I would say to your cost because most of your cost is about the memory now if you've\ngot a really frugal function let's say 128 megabytes of ram and you go for the max storage\nwith that then it's going to make around 15 to 20 percent of a cost increase so it's still not\n\n\nLuciano: particularly significant even if you're using a very low memory and high storage nice yeah that's that's interesting I did expect it would be kind of a free feature that you can just use more space\nbut it makes sense because it's still a significant amount of more disk space right so okay let's\nmaybe try to compare how this feature plays against other types of storage that you can use\nwith lambda and I think that the most obvious one is of course S3 so the S3 is probably what you\nwill be using most of the time and one of the big differences of course that S3 is durable and\nreliable so when you store something in S3 it's there and you are pretty much guaranteed it's\ngoing to be there so it's something you can reliably use across lambda invocations\nbut of course every time you need to fit to fetch that data again into the lambda so that\nif it's a big file of course expect that that's going to take some time and that time of course\nbecomes part of your lambda invocation time something you need to pay for something your\nusers are waiting for so that that is an interesting comparison to make another use case is EFS which\nshould be lower latency than a strip for bigger files I think there is again that article\nmaybe it's another article from from Lumigo that shows that there is from five to ten times lower\nlatency with EFS yeah and also it's a little bit more complex to set up because it requires the\nVPC and IOPS optimizations so EFS has pretty much the same characteristics as S3 in terms of\nit is a durable storage it should be reliable but it's a little bit more complex to set up definitely the other two interesting use cases are lambda layers and container images that we already mentioned a little bit but the idea is that another way that you could use to load data\ninto your lambda is you can build your lambda with a container and include the data as part of the\ncontainer data or you could use lambda layers put the data in a layer and then load the layer with\nthe lambda but you need to keep in mind that in those cases the data is immutable so these are\ngood use cases when you have artifacts that need to live with your code maybe I don't know assets\nlike images or whatever you need to use inside your lambda but it's not something you can use to\nwrite into so these are just good use cases for when you need to bring immutable data into the\nlambda and there are different limits lambda layers is limited to 50 megabytes and container\nimages you can do up to 10 gigabytes so for instance you already mentioned that in the case\nfor instance you have a big ml model maybe you can just build the lambda as a container and include\nthe model together with your code because most likely you don't need to change your code for\nthe model as the lambda executes so yeah I don't know in the end what do you feel you will be using\n\n\nEoin: and maybe kind of a summary of what we just said you will use more s3 or other types of storage for sure I think the decision tree for this is with lambda avoid storage if you can and do\neverything in memory because you can get up to 10 gigabytes of ram so if do you need 10 gigabytes\nof ephemeral storage if you can just store everything in memory stream what you need in\nand stream what you need out and if you do need to have any durability stream it in and out from s3\nso the question there just comes down to s3 transfer performance which maybe we can talk\nabout a little bit but again the the rest of the decision tree is if you can't use s3 I would say\nyeah use efs if you do need that shared storage with more guaranteed throughput and more of a\nfile system type model rather than the object store model of s3 and then everything else like\nif it doesn't change that often bundle it into your image container images are it's one of the\nreal benefits of you've been able to use container images is that you can bundle data in it so if\nyou've got some sort of model data something that doesn't change across implications you can bundle\nit in and then slash temp it's almost like the last resort so I don't want to be too negative\nabout this feature but I would say like it's a nice to have for those cases when you need to\ncreate a reasonable amount of ephemeral data if you're we talked about the caching use case you\njust covered it there lee channel and I would say like it's great that you can do caching across\ninstances but adding a caching layer is an extra piece of complexity you need to manage then you\nneed to manage okay your cache capacity you don't want to overfill your cache you need to have a\nejection algorithm for objects in your cache and you know you need to be a kind of monitoring your\ncache hit metrics and that kind of stuff if you're if it's really going to be a proper optimization\nso in general it's much better to for lambda functions to remain stateless because that's\nwhere the beauty and the simplicity ultimately comes from so this is just for some of those edge\ncases where you really need extra disk storage where you need to do kind of random access and\n\n\nLuciano: seeks into the local file system and that exceeds 512 megs yeah you told me that you did a little bit of research in terms of how network speed could affect this decision tree so for instance\nif you really need to load big files and you try to do that from S3 like yeah how the other does\n\n\nEoin: it really play out for you is it better to have something in temp maybe in that case or not yeah yeah this is this is a good question i think because when you say okay well that it's a\noptimization that you can keep this across multiple invocations then the question is okay\nwell how does the data get there in the first place and how much of an optimization are you\ngoing to get so let's say you're going for the maximum 10 gigabytes of storage i did some kind\nof informal benchmarking on lambda and the maximum speed i could get for transferring\na gigabyte of data it was kind of approaching a gigabit per second but not really the average was\nactually around 600 megabits so around two-thirds of a gigabit say you know sometimes it got the\nthe maximum speed got a little bit better than that it was kind of approaching a gigabit per\nsecond but on average it was around two-thirds of a gigabit so if you want to fill your slash temp\non cold start with 10 gigs of data that's going to take you over two minutes at that speed so you\ncan i guess what that says is that by caching it you're going to save two minutes in subsequent\ninvocations but what it also kind of suggests to us is maybe what we want instead of this cache is\njust better network throughput a faster highway to s3 if you like um i know that with ec2 you can\nturn on enhanced networking and get 100 gigabytes of network performance now i haven't benchmarked\nthat to see what if you can get 100 gigabits directly to s3 you know your mileage is going to\nvary but that's a significant difference from the near gigabit performance we can observe from lambda\nso if you can imagine if we could increase the network performance by a factor of say 50\nthat would make a massive difference and the need for caching would suddenly\ndissipate i would definitely pay for some use cases i definitely pay a lot more just to get\n\n\nLuciano: that enhanced networking in lambda yeah that's a very good point and yeah will be interesting to see what other people think about this stuff and if they found particularly interesting use case\nthen maybe we are not seeing right now i remember i i heard somebody saying oh yeah you can load a\nsqlite database into the temp storage and then you can do kind of more dynamic queries and analytics\nfrom that storage which is maybe an interesting use case but i don't know i would like to see\nsome like real application built using these ideas and then you can actually see if there are benefits\nor maybe just a little bit of a stretch to just try to use this new feature\nyeah it sounds all in all that it's more on the niche side than in the let's all adopt this feature kind of uh category would you agree is that a fair characterization yeah right now yes and again maybe it's just that we are not seeing some particularly useful use case but again that\nthat proves that maybe it's a niche type of feature like it's not something that has a\ngenerally available utility that everybody everybody's going to leverage from tomorrow right\nokay so should we remember that we have some resources we are going to link in the show\nepisodes we are going to link also the official announcement which is also surprisingly dry in\nterms of examples so that there was another interesting thing that we picked up on it's\nactually well detailed on how to use it and all but it doesn't give you a lot of examples on when\nthis could be useful and also we're going to mention the original tweet we discussed from\nwill dady and blog post from yan chui atlumigo describing more use cases and how to use it\n\n\nEoin: excellent okay well with that please let us know if you found some really beneficial use cases that we haven't been able to spot and thanks for listening and we'll see you in the next episode\nyou\n"
    },
    {
      "title": "31. CloudFormation or Terraform?",
      "url": "https://awsbites.com/31-cloudformation-or-terraform/",
      "publish_date": "2022-04-07T00:00:00.000Z",
      "abstract": "Should I use CloudFormation or should I use Terraform instead? If you are just starting to write Infrastructure as Code (IaaC) you probably have this question. In this episode, we will discuss in detail how these two amazing pieces of technology compare against each other and what their features, weaknesses and strengths are. We will share our opinions based on our experience with these 2 technologies and guess what, for once we have a bit of clash of opinions! Can you guess who is in the Terraform camp and who is in the CloudFormation camp instead?\nIn this episode we mentioned the following resources:\n\nA tutorial on how to create resources conditionally with CDK (and CloudFormation)\nAn article to understand in depth how to use secrets management with SSM and SecretsManager together with CloudFormation\nBen Kehoe’s tweet about switching from CloudFormation to Terraform\nTerraform null resources\nCloudFormation Macros\nHow to workaround missing CloudFormation features (by Cloudonaut)\nOrg-formation\nHow to create accounts in an org with Terraform\nControl Tower Account Factory for Terraform\nPulumi\nCloudonaut’s comparison of CloudFormation with Terraform\nCloudonaut’s free CloudFormation templates\n\n",
      "transcript": "Luciano: Should I use CloudFormation or should they use Terraform instead?\nIf you are just starting to do infrastructure as code, you probably have this question.\nAnd in today's episode, we'll try to cover these two technologies and highlight some\nsimilarities and differences. We will also try to give you our opinions on which one is best\nand which one you should use, but get ready for some debate because I think here we have\na difference of opinion. My name is Luciano and today I'm joined by Eoin and this is AWS Bites\nPodcast. So let's start by maybe recapping what both CloudFormation and Terraforms are useful\nfor and maybe what infrastructure as code is. What do you think there?\n\n\nEoin: When you're using infrastructure as code, what you're talking about is declaring the state of everything you want to be in AWS and you use some sort of tooling. It could be Terraform or\nCloudFormation and that tooling will perform a set of actions to get you from your current state\ninto the target state that you've declared in code. And so why would you do that? Well,\nwe've covered this in previous episodes, but with infrastructure as code, what you want is to\nmake sure you've got a predictable deployment so you know exactly where you're coming from\nand where you're going to, and you want to get some safety around what's in there and you don't\nwant to have any kind of unpredictability there. So you can do things like code review, so you can\nhave pull requests on your infrastructure as code and traceability, you know, you have a branch with\nyour resource code changes as well as your application code changes. So what it gives you\nis the ability to be able to deploy your infrastructure in multiple environments and make\nsure that it's exactly the same in each one. So given that, I guess, yeah, that's we could\nthat we could probably go into the two tools then. So we're talking today about CloudFormation\nand Terraform. Would you like to talk about Terraform first? Yeah, I can start with Terraform.\n\n\nLuciano: So Terraform is a tool that was not created by AWS, but by another company called HashiCorp.\nAnd basically it's, yeah, it's an open source product. So you can just go on GitHub and look\nat the source code, even contribute yourself. But of course, Terraform being a company as\nalso a commercial offering and specifically in relationship to Terraform, they have something\ncalled Terraform Cloud and Terraform Enterprise, which are tools that allow you to automate,\nI suppose, some of the manual work that you'll need to do yourself around if you're just using\nthat, the bare bone open source Terraform. It supports, of course, infrastructure as code,\nbut not just for AWS, but also for many other cloud providers. And by the way, not just the\nmain ones, like Google Cloud or Azure, there is a very long list and you can even support,\nI don't know, smaller DNS providers. So you can really get very granular. There is a huge variety\nof providers, I guess is the technical terminology supported by Terraform.\n\n\nOne interesting thing, which is very distinctive from other tools like Terraform is that they use,\nit's not really a proprietary language, but it's something that came out of Terraform itself. So\nit's kind of a bespoke language to define infrastructure as code. And this language is\ncalled HCL, which stands for HashiCorp Language\nHCL. And the CLI allows you to, once you have defined your own infrastructure as code using HCL,\nallows you to create a plan, which basically means if I want to apply this particular configuration\nto remote deployment, what's going to happen? And Terraform is basically going to give you a plan of\nthe different things that will change. So this is something very visible that you can see and\ndecide, okay, this looks correct. At that point, you can decide to execute that plan and actually\napply all the changes. Another interesting thing in Terraform is the concept of state.\n\n\nAnd by state, the way I like to think about it is basically the view that Terraform has of your\ncurrent deployment in production or whatever other environment that is. And that is important\nbecause every time you need to reapply something, like you want to change resources, Terraform will\nlook into the last state, the last known state, and use that to decide what to do next. So this\nis another interesting thing because basically Terraform doesn't really go into the infrastructure\nitself and try to assess the current state, but uses the previous representation of the state. So\nvery careful there because if you change things manually in between, Terraform is not going to\nsee your manual changes. It only remembers the last state managed by Terraform. Now, by default,\nthe state is just a JSON file that ends up in your file system when you run Terraform locally.\n\n\nBut of course, this is not ideal because if you're working on a team, you might want to have that\nshared so that everyone deploying will have the same view of the world. So there are ways that\nyou can store that state in S3 or in DynamoDB or in some other shared storage. And this leads\nto another interesting feature of Terraform, which is we can say it's a client server model,\nwhere you are running these changes from a client. It could be like your own development machine,\nit could be a CI CD pipeline. But what happens is that once you try to run the CLI, the CLI itself\nwill do all the changes on your infrastructure by calling AWS APIs directly. At least this is in the\ncase of AWS. Of course, if you use other providers, it will interact with the APIs of those providers.\n\n\nSo this is kind of the idea that Terraform will call the APIs for you as if you were calling your\nAPIs directly from the CLI or maybe if you were running CLI commands against AWS.\nSo just to recap, the main feature is that you can write your infrastructure as code using HCL,\nthen you can plan what happens if you want to apply those changes. You can actually apply the\nchanges or execute the plan. You also have rollbacks. So basically you can decide to go back\nin case something went wrong. But there are also other interesting extensibility features like\nplugins and modules that we can probably discuss a little bit more later. And you can deploy to\nmultiple cloud providers. And finally, you can also query existing resources. So for instance,\nif you want to use things that are already online as part of your definition of infrastructure as\ncode, like I don't know, you want to reference a bucket that was already provisioned before,\nyou can do that as part of the HCL language. There are constructs that allow you to do that.\nI hope that there was a comprehensive view, but yeah, what about CloudFormation at this point?\nYeah, I think that sets it up nicely for a comparison.\n\n\nEoin: So I would describe CloudFormation as the primary difference really is that while Terraform is a tool that operates in that client\nserver mode you described, CloudFormation is an AWS service. So it has a client, but\nthe main features of CloudFormation happen and the AWS managed service. And that's the fun part\nand that's the fundamental difference. And it's quite an important one as well. So what you're\ndoing essentially is you're declaring the state of all of your cloud resources and you're giving\nthe templates to AWS and asking AWS CloudFormation service to apply the differences for you. So it\nall happens then cloud side rather than this client server model. So in terms of how, what\nthe templates look like, it uses JSON or YAML. So it's not the language that CloudFormation gives\nyou out of the box is not as powerful as what you get with HCL, but there's additional tools\nyou can use to overcome some of the limitations of JSON or YAML. So there's a lot of features in\nCloudFormation. It's actually growing even in the last year, we've had a lot of new features,\nbut fundamentally the state in CloudFormation is stored in something called a stack. And that\nstate is something that AWS manages for you. So you don't have to think about where that state is.\n\n\nSo a stack is essentially a collection of resources with its own state.\nAnd there's also something called stack sets, which is essentially the same stack applied across\nmultiple accounts or regions. You also have things like nested stacks. So you can have a hierarchy\nof CloudFormation templates. And when you mentioned Terraform plan, there's a similar idea in Cloud\nformation called a change set. So you can create a change set and that's also created on the cloud\nside. And it's essentially a list of changes that will be applied. And then you can decide whether\nto execute that change set or not. You also mentioned rollbacks. So one of the things I\nlike about CloudFormation is that it has automated rollbacks. So I think about more like when you're\ninteracting with a database, you've got a transaction and within that transaction, you've\ngot a series of changes and a CloudFormation update is very much like a transaction. And if\none of those updates fails, CloudFormation will manage the rollback for you. And for that reason,\nit feels safer using CloudFormation because it's kind of AWS's responsibility to fulfill that\nrollback. Recently, you actually have support for removing or disabling rollbacks in development as\nwell. So that makes the development process a bit handier, a bit faster. And you also mentioned one\nof the things that can happen with any of these tools is that manual changes, people can go in and\nmake changes that aren't reflected in the stored state. And one of the newer features of CloudFormation\nis drift detection. So it'll let you know and track the state of resources against the template.\nSo you can see what's changed compared to that state. That can be useful. It doesn't support all\nthe resource types, but I think there's a growing set. There is support for information as well for\nimporting existing resources. It's not something I would like to have to do very often because it's\na little bit of a laborious process, but it's also a reasonably new feature where if you've got some\nresources that you created manually in the console, you can kind of adopt them into your CloudFormation\nstack. There's a lot of other new features as well, like hooks. So you can execute arbitrary\ncode at different stages in the deployment life cycle. And there's this kind of other stuff as\nwell. Like if you've got an auto scaling group, CloudFormation integrates with that as well and\ncan do rolling deployments. I think one of the kind of last features I'd call out is that\nCloudFormation gives you very good secrets management. So it's well integrated with SSM,\nparameter store for secure secrets and also secrets manager. So you don't have to pass those\naround. They can be imported securely within the cloud side, within the CloudFormation service for\nyou. And again, because it's JSON or YAML, it's fairly flat and declarative and not very dynamic.\nAnd that can be a benefit, but also a drawback. I know in Terraform, you have like loops. You can\ndo count, loops up to account, like a for loop essentially. In CloudFormation, you don't have\nloops, but you do have conditions. So you can decide whether to include something or not based\non the value of a parameter. And that parameter could come from inputs to the template, or it\ncould come from a SSM parameter for you.\n\n\nLuciano: On that one, we have an article that we wrote some time ago with examples that we'll put it in the show description. Yeah, definitely. That's a good chat.\n\n\nEoin: Because it's an AWS service as well, I suppose it's worth calling out some of the integrations\nthat CloudFormation already has with other AWS services. So if you're into code deploy for\ndeploying to EC2 or Lambda, CloudFormation integrates well with that. So you can do rolling\ndeployments there. Of course, it's integrated with IAM. So your CloudFormation actions are going to\nbe done under a role that you can specify and control. And if you want to be able to deploy\nCloudFormation templates, give users the capability to deploy stuff from the console on demand,\nlike if they need a bucket or whatever application you might want to deploy on demand, there's a\nservice called Service Catalog that uses CloudFormation under the hood for that. So those\nare some integrations of note. And of course, you also have the tooling that's built on top of\nCloudFormation. I think I read somewhere recently that like 70% of CloudFormation is deployed using\nthe serverless framework. Don't quote me on that. Something pretty high. Yeah. And AWS SAM is a\nsimilar tool that's also built on CloudFormation. And since we mentioned that with YAML and JSON,\nit's not very dynamic. Of course, you have the CDK, which we have covered in a previous episode\nin a lot of depth. And that's a programmatic imperative way of generating CloudFormation\nin your language of choice. There are some limitations in quotas because it's an AWS service.\nIn terms of numbers, you can put 500 resources in a stack and you can have up to like 2000 stacks,\nwhich should be plenty for a given account. That number of resources was increased a couple of years\nago, maybe even last year, but I've never, I definitely haven't reached the 500 resource limit\nbecause I tend to use small stacks. I prefer things that way. The template size itself can be\n50K, but you can put it up on S3 and then you can use a template size of up to a megabyte.\nWhich considering that's- One of the other important limitations, actually,\njust to mention it quickly, is that with CloudFormation, you cannot modify a resource\nthat isn't within the stack. And that's an important one to be aware of. So if you've\ngot an existing bucket and you wanted to add, like previously it was quite common, you'd want to\ncreate an application, but you wanted it to be a Lambda function to be triggered by a notification\nin S3 bucket. If you were trying to modify the bucket's notification configuration, you couldn't\ndo that in a different stack. So you had all these workarounds in serverless framework that would\ncreate custom resources to fulfill that for you.\n\n\nLuciano: But you can also import something into a stack, right?\n\n\nEoin: Yeah, you could, you can import that, but you couldn't say have a shared bucket and then have lots of different other stacks that create notification configurations in that bucket.\nSo that's it. Should we talk about some of the differences then?\nYeah, let's try to- CloudFormation Terraform, let's try and pick a winner here.\nLet's try to summarize the differences first.\n\n\nLuciano: We already mentioned some of them, but I think it's good to highlight them into a little bit more detail. So probably the first one,\nagain, is that with Terraform, you have this client-side mode where everything is happening\nin the machine that uses the Terraform CLI. So that machine is responsible for calling all the\nAPIs and make sure that all the changes are applied through API calls. While in CloudFormation,\nit's a managed service by AWS, you just submit your YAML or JSON, and then AWS will take care\nof applying all the changes for you. So you could even disconnect the machine at that point,\nall the changes will still go on. So in that sense, probably plus one to CloudFormation for me,\nbecause of course it gives you a little bit more of peace of mind because you don't have to think,\nwhat's going to happen to this machine while the changes are happening? So AWS will take\ncare of all of that for you if you use CloudFormation. Do you agree with that?\nYeah, I agree. It's just a managed service idea, right?\n\n\nEoin: It's taking more of the responsibility away from you, which is always a good thing in my book. I can cite Ben Quijo's tweet on the matter\nthere. I know he's a big fan of CloudFormation and that Cloud-side model. I think that I stole\nthat term, Cloud-side from him. But he mentioned in a tweet there, which we can link in as well\nin the show notes, he said, going from CloudFormation to Terraform because of\nCloudFormation's shortcomings is like getting frustrated with Lambda and going to Kubernetes.\nSure, you can accomplish what you want there, but with a bigger TCO. So your total cost of ownership\nmight be higher because you're adopting a tooling that isn't a managed service from the cloud\nprovider. It's probably an opinionated view, but I would lean towards that side of the argument.\nAbsolutely.\n\n\nLuciano: And if you want something like that, I think you can use one of the commercial offerings from Ashgore. But of course, at that point, you have to pay another provider and set up that\naccount and manage that account. So maybe you have less responsibility at that point, but it comes\nwith the additional cost of paying the provider, but also starting to use all new tools there.\n\n\nEoin: For sure.\nYeah.\n\n\nLuciano: Another interesting thing, and again, this is maybe a little bit opinionated, that Terraform feels a little bit more modular and extensible if you want, because\nthere is a concept of modules, which is literally with the same syntax you use to define,\nI suppose we can call it a stack in Terraform. You can say, this is not a stack I want to\napply right now to an actual deployment, but it's just like a prototype. And I'm going to accept\nsome generic inputs, produce some outputs, and that becomes a module that at that point you can\nimport in different stacks and just provide the different inputs that are expected. And it will\ndo the same things as if you were writing that same code copy-based into your actual ACL code.\n\n\nSo that's a nice feature because basically by using the exact same syntax, there are very\nsmall differences, you get that modularity and it feels like importing functions in a programming\nlanguage and just calling the functions. So that's something I really like from Terraform.\nBut also there are other ways to extend Terraform. There is already a concept of provider. There are\na lot of built-in providers like AWS, Azure, and all sorts of different providers. But of course,\nyou can also create your own if you want to support, I don't know, any provider or any cloud\nservice that is not natively supported. Or if you just want to do custom things to interact even\nwith providers that are supported, but maybe using features that are not currently existing in the\nactual built-in providers. And another interesting thing that I used in the past and I think is not\nthat uncommon is this idea of null resource, which is basically a way to say I'm not really defining\na resource that Terraform itself needs to manage. It's more I want to have a hook in my provisioning\nsteps to say this is kind of a virtual resource and I can define conditions like, I don't know,\nmaybe when something else changes. And then with that condition, you can attach, for instance,\na script or something else. And that way you can create mechanisms to say, okay, maybe before\nevery deployment, if this particular condition happens, run a script that, I don't know, maybe\ntries to get an SSL certificate from somewhere and then use that certificate as part of your stack.\nSo that's another, I suppose, easy enough way that you can create custom hooks into your\nTerraform deployments. In that term, how do we compare extensibility from Terraform to\nCloudFormation?\n\n\nEoin: Yeah, this is an interesting one because it used to be difficult when you had, if you had a gap in the supported resource types in CloudFormation, you were quite limited,\nbut now there's so many options, there are almost too many. So the simplest one is probably custom\nresources where you can fairly quickly create a custom resource and you use AWS Lambda to\nfulfill the creation update or delete of that resource in your account. And that's reasonably\nstraightforward to create. It can be a little bit difficult to troubleshoot, but it's fairly easy\nto get started if you find that there's a gap in functionality or you want to create something\nunique to you in CloudFormation. Now you also have support for CloudFormation modules now.\n\n\nSo you can create either like a single resource or multiple resources. And it's a bit like a CDK\nconstruct, but you're just doing it with declarative CloudFormation. There's a new\nthing called the CloudFormation registry, where you can then register those modules publicly.\nAnd another thing you can put into the CloudFormation registry is a CloudFormation\nresource type. And this is where you're going all full in on creating your own CloudFormation\nresource type. And it's a much more involved process. There is some tooling, there's this\nCloudFormation command line tool that you can use to bootstrap this and to publish it to the\nCloudFormation registry. But it's essentially like you're adding a feature into CloudFormation\nproperly. So it includes validation, progress updates, all of the features you get with any\nCloudFormation resource type. I believe it's the same mechanism that CloudFormation internally\nuses for creating resources. And the difference between it and CloudFormation customer resources\nis that it's not running in your own Lambda, in your own AWS account. It's running in AWS,\nin their managed service. And there's something else called CloudFormation Macros, which allows\nyou to do transformations and templating essentially. So people who have used AWS Sam\nmight be familiar with the serverless transform, which allows you to create a Lambda function with\njust a few lines of code that actually uses the CloudFormation macro feature under the hood.\nAnd there's also another popular one called CloudFormation include, which is just for doing\nincludes in your templates. And that's also using macros. But if I would recommend, if anyone is\ninterested in learning more about creating custom things in CloudFormation and extending\nCloudFormation support where there's missing resources, then the Cloudanaut blog, we've\nmentioned them on the podcast before. You guys know a lot about CloudFormation and they've\ncreated, they've a really good podcast called Three and a Half Ways to Work Around Missing\nCloudFormation Support, which talks about all this stuff and more in depth.\nYeah. So at this point, another topic that comes to mind is what about multi-account deployments?\n\n\nLuciano: Does any of these tools out of the box allow you to start a deployment that actually is\ngoing to deploy resources, not just in one account, but in a few different AWS accounts?\nYeah.\n\n\nEoin: So when you talk about multiple accounts, Luciano, I guess one of the things you think about is AWS organizations and you can create an AWS organization accounts using either CloudFormation\nor, sorry, using Terraform, or you can also create them with the AWS SDK. But what I found is that\nthere's quite a lot of missing support across both of these ecosystems when it comes to\nmultiple accounts. Now, CloudFormation does give you stack sets. We mentioned that already.\n\n\nSo you can deploy the same stack to multiple accounts, but another tool which really fills in\nall of the gaps here is organization formation or org formation. And this is just a really great\nbit of open source tooling that uses CloudFormation syntax, but extends it with lots of really,\nreally great multiple account deployment capability. So it allows you to create your\naccounts, but also decide what accounts you want to, sorry, what stacks you want to deploy into\ndifferent accounts and perform tasks in each of those accounts and manage all of your organizations\ncross account infrastructure as code. Terraform does have some support, so you can create accounts,\nlike I said, but it's not as powerful as org formation. I don't think there's a really a\ngood replacement for org formations, org formations capabilities. There was something called\nControlled Terra Account Factory for Terraform. And I know that AWS and HashiCorp have put a lot\nof effort into that experience to make it easy for people to manage accounts and all the resources\nacross a large organization. But I think it's still fairly new. It's been a long time in\ndevelopment and it's not yet widely adopted. So I haven't used it. I don't have personal experience\nof it. So your mileage may vary with it, but it's probably one to watch.\nYeah, absolutely. I never had to do cross account or multi-account deployments.\n\n\nLuciano: So I've seen these tools, but I never had the first-hand experience with them. So I\nwouldn't be able to compare them or give an opinion on those. Okay.\n\n\nEoin: So one of the nice things we can do actually on that topic is AWS just announced the ability to close an AWS account via an API. So it kind of opens up a lot of new possibilities for people to\ndo kind of ephemeral account deployments with infrastructure as code.\nYeah.\n\n\nLuciano: Or even just to experiment with these features because you don't have to be worried about, I just created a new account just for testing out why I close it and that manual\nprocess. A lot of pain.\nOkay. So just to try to wrap this up. So who is the winner? Let's start with when do you use\nCloudFormesh? I'll leave this to you because you are on the CloudFormesh camp.\nYeah. Yeah.\n\n\nEoin: The managed service aspect of it and the fact that AWS is managing your state for you is a big advantage for me. But of course that advantage only applies if you're only\ntalking about AWS. So I would say use CloudFormesh if you're just talking about deployments to AWS,\nyou're not dealing with multi-cloud deployments or other third-party resources. Then you'll get\nthe benefit of automated rollbacks. You have lots of good tooling like the serverless framework.\n\n\nSo if you're doing serverless applications, you're going to be able to do that.\nIf you're doing serverless applications, I would say embrace cloud formation and one of the tools\nthat allows you to build on top of it like SAM or serverless framework, it makes it a lot, lot\neasier. So I would say my decision tree for infrastructure as code is use CloudFormesh.\nIf it's good tooling, obviously it depends on the organization you're working with and what\nskills people have. These are really important considerations. I'm not going to go in and try\nto convert everybody to CloudFormation if they're already using Terraform. That doesn't make sense.\nBut I would have a bias towards CloudFormation, especially if you're AWS only.\nSo what's the case for Terraform, the channel? Yeah.\n\n\nLuciano: So at this point, I think it's clear that I'm more on the Terraform camp, even though I've been using CloudFormation more and more in the\nlast few years. But I still think that Terraform gives you a little bit of a better user experience\nstill today. Like, yes, it's true that you need to learn a new custom language, but also that\ncustom language, I feel that gives you... It's a lot more expressive and you will not feel stuck\nabout just limitations of trying to express certain concept in JSON or YAML. So I definitely\nlike that. And I also like how clear it is the diff of when you do a plan in Terraform, it's\nvery clear to see what's changing, what not. And also Terraform has a very good documentation and\nvery good ID integration. So you get a lot of auto-complete and it's easy to figure out what\nare the right properties and resources of what you're trying to do. So in general, and of course,\nthis is opinionated, feel free to call me out if you think otherwise, I had a better user experience\nby using Terraform rather than CloudFormation. So that's maybe one data point to keep in mind.\n\n\nBut of course, if you are a company that is already heavily invested in Terraform,\ngo with that. You don't need to change just because you think CloudFormation could be better.\nThey are almost the same to some extent. The only win that I'm going to give to CloudFormation is\nthat with Terraform, you are a little bit on your own in figuring out how to manage deployment,\nmeaning which machine is going to actually do the deployment and where are you going to keep the\nstate so that it's consistent across deployments. And that it's always a little bit of a pain, but\nthere are ways to automate all of that through CI-CD and by keeping the state in shared places\nlike S3, Dynamo or other shared storages. And of course, one final point in favor of Terraform\nis that if you are building applications and those applications need to live in different\ncloud providers, or maybe your application uses resources in different cloud providers,\nTerraform can give you a lot more control there because it supports out of the box a bunch of\ndifferent cloud providers. Now, that doesn't mean that it's doing some magic translation for you.\n\n\nYou still need to explicitly say, I want to use this resource with this provider. So if you use,\nI don't know, an Azure function compared to a Lambda, there isn't any abstraction for you,\nbut you can reuse the same HGL syntax and the same Terraform concept to provision both.\nIt's going to be different code, but you have the same user experience.\nI hope that that summarizes my opinion and maybe to finish off with this episode,\nwhat we can do is give a quick mention to other tools that I personally haven't used,\nbut I've heard them coming up more and more in conversation. And I think the main one is Pulumi,\nwhich is kind of a crossover between CDK and Terraform. And by that, I mean that it's like\nCDK, meaning that you use programming languages to actually define that infrastructure as code.\nSo you get something a lot more dynamic and well integrated with your idea of choice.\nYou don't need to learn a new language, but at the same time, it's multi-cloud. So where CDK\nis targeting only AWS, in quotes, because I think that that's going to be changing in the near\nfuture, but right now is really well-built only for AWS. Pulumi is already aiming to target a\nbunch of different cloud providers. And we already mentioned CDK as an alternative, but also SAM\nand serverless, which are built on top of cloud formation. So sometimes it can be convenient for\nyou to use these higher level abstractions rather than just going straight to cloud formation, which\nmight be a lot more lower level and verbose than using these other tools.\nAnd I think that concludes this episode. Do you have any final remark, Eoin?\n\n\nEoin: I guess I'm really interested to hear what people think of it. And if we've got any\nstrongly held opinions that cite any reasons for using one over the other, we haven't covered here.\nBecause it does tend to be a battle of camps sometimes when you're discussing cloud formation\nversus Terraform. I think the main thing is that you use infrastructure as code. If you're doing,\nno matter what tool you're using, you're already in a good place if you have infrastructure as code.\nAnd if not, it's time to pick one and move forward. Yeah.\n\n\nLuciano: And I'm also really curious to know if there is any other tool, maybe something older that I haven't seen, or maybe something really, really\nnew that we haven't seen yet. So definitely let us know if there is any other tool that you think\nshould be part of this type of conversation and people should consider. So thank you very much\nfor following and please give us a thumbs up, a like, share, whatever, if you are getting value\nfrom this episode. See you the next time.\n"
    },
    {
      "title": "32. What are the benefits of tags?",
      "url": "https://awsbites.com/32-what-are-the-benefits-of-tags/",
      "publish_date": "2022-04-14T00:00:00.000Z",
      "abstract": "What are the benefits of Tags? You have probably seen that you can add tags to almost every resource in AWS… but should you really do it? And if you do it, what are the benefits?\nIn today’s episode Eoin and Luciano cover what tags are, some examples of how to use them and what kind of benefits they can give you and your team. Finally we’ll give you a bunch of ideas on how to build a tagging strategy and get value from tags!\nIn this episode we mentioned the following resources:\n\nResource group tagging\nControlling access to AWS resources using tags\nEnforce tagging via SCP (Service Control Policies)\nUse AWS Config to create rules that can alarm if some resources are not compliant\nExample on how to use CloudTrail to automatically tag newly created resources\nArchived AWS white paper on tagging best practices\n\n",
      "transcript": "Eoin: What are the benefits of tags?\nYou've probably seen that you can add tags\nto almost every resource in AWS,\nbut should you really do it?\nAnd if you do it, what is the point?\nWhat are the benefits?\nIn today's episode,\nwe'll give you a quick overview about what tags are,\ngive you some examples on how to use them\nand what kind of benefits you can get from them.\nFinally, we'll give you a bunch of ideas\non how to build a tagging strategy\nand get value from tags.\nMy name is Eoin, I'm joined by Luciano\nand this is the AWS Bites podcast.\nSo Luciano, maybe tags are one of those things like flossing\nor going to the gym that people should probably know\nthey need to do, but they don't do as much\nas they probably should.\nBut maybe first we could talk about what tags are.\n\n\nLuciano: Yeah, so it's actually a very simple concept.\nYou can imagine tags as just arbitrary key value waves\nthat you can stick to all sorts of different AWS resources.\nNow, to be fair, they are not available\nin every single AWS resource.\nThere are exceptions, but for the most part,\nyou'll be able to use tags for almost every resource.\nSo it's something you should be using\nand we'll discuss why in this episode.\n\n\nI just want to give you maybe some examples\nto make it a little bit more tangible\nand just to give you a feeling\nthat at the end of the day, it's just metadata.\nSo it's something that needs to make sense to you\nand to your team and needs to make sense\nin the context of the application you are building in AWS.\nAnd just to give you an example, you could say,\nfor instance, I don't know,\nwho is the owner of a particular resource\nand that can be the name of the team.\n\n\nOr you can say, rather than calling it owner,\nyou can call it team, for instance, team e-commerce.\nYou can say something like stage or environment\nand maybe say this is a production environment\nrather than a QA environment.\nOr you can use stock name and say, I don't know,\nshopping cart, this is an e-commerce\nand you might have a stock dedicated to the shopping cart\nor even the application name, maybe, I don't know,\nyou are building an e-commerce that sells organic oranges.\nSo your application tag could be organic oranges shop.\nSo yeah, that should give you an idea of what tags are\nand how you could use it at a very high level.\n\n\nEoin: Okay, so that makes sense.\nAnd if you've got the capability to add these kinds of tags\nand you've given a good few examples there,\nwhat kind of use cases would you foresee by having tags?\nWhat would you do?\nWhat's the value of having the pre-production stage\nannotated for your EC2 instances\nor knowing that a Lambda function\nbelongs to your organic oranges shop?\n\n\nLuciano: Yeah, I think that the first thing\nthat's probably a little bit obvious\nis that you have additional information\nattached to your resources.\nSo if you are doing a review of all the resources\nin your account or in your organization,\nor I don't know, if you just want to know\nwhat is this resource being used for\nand you have done this kind of tagging consistently\nacross your organization,\nthen that becomes very useful piece of information.\n\n\nAnd you can also do search, for instance,\nyou can use tools like Tag Editor to search\nacross all your resources,\nthe one matching particular tags,\nor even the opposite,\nyou can search for resources that don't have tags.\nSo it becomes an additional way\nto explore all the resources that you have in your account.\nBut there is probably another use case\nwhich is probably the most used\nor the one for which tags are more commonly used\nand recommended, which is cost allocation.\n\n\nBecause as soon as you do tags,\nyou can use those tags to group cost.\nSo for instance, you could group cost by team\nor application, and at that point,\nyou can use that to basically see\nwhat kind of things you are paying more on\nand I don't know, you can see if there are spikes\non particular application or particular teams.\nSo that's probably the number one recommendation\nthat you will find online\nabout why you should be using tags\nis because they will give you much better visibility\non your cost.\n\n\nAnd you can attach cost to things\nthat make sense for your organization.\nThere are another couple of interesting use cases though.\nSo it's not just about cost and finding resources.\nYou can also use tags for automation and security.\nAnd I want to give you a few other examples.\nFor instance, in terms of security,\nwhat you could do, you could say,\nI'm gonna create an IAM policies\nthat limits people from doing certain things\nif there are certain tags or if there aren't certain tags.\n\n\nFor instance, you could say,\nI am gonna create a policy for an account\nthat says that nobody can stop,\nor at least, I don't know,\nmaybe developers cannot stop EC2 instances\nif they have the tag environment production, for instance.\nAnd that's basically something that gives you,\nusing tags, you get that additional context\nto distinguish between resources\nthat are maybe more sensible in production\nand resources that are less sensible.\nAnd similarly, you can use the same idea for automation.\nSo for instance, you could create, I don't know,\nlet's say a Lambda on a schedule\nthat runs every day at midnight.\nAnd you can say, okay,\nget all the EC2 instances with the tag sandbox true\nand terminate them because maybe those are things\nthat are supposed not to stay around forever\nand that somebody is just experimenting with them.\nSo using that tag,\nyou are automatically creating the capability of,\ndon't worry too much.\nIf you forgot about them,\nthey would be automatically killed for you.\n\n\nEoin: That's really good.\nYeah, so you can use tags\nto essentially audit what you have.\nYou can use it to get control visibility of costs,\nalso enforce security.\nSo I think that's enough motivation\nto actually go ahead and create tags.\nSo how do you actually do it?\nIf you've got like a lot of people out there,\nlots of resources that are untagged, where do you start?\n\n\nLuciano: Yeah, I suppose the easiest way\nand probably every tutorial will show you that way for sure\nis to just go in the web console\nand go around, wander through the different services.\nAnd when you see resources without tags,\njust use the web UI to add the tags.\nBut of course, the bigger your deployments,\nthe more complicated they will be\nto actually find all the resources and do it consistently.\nSo of course, there are other tools\nthat allow you to do this in a more automated\nor more kind of in bulk type of approach\nrather than just doing it one by one.\n\n\nAnd the first thing is you can do it from the CLI.\nSo you can either tag existing resources from the CLI\nor when you create a new resource,\nmost of the commands will allow you\nto also specify the tags you want to attach in line\nwhen you create that resource.\nAnother thing is that if you are creating\nor updating resources through infrastructure as code,\nlike CloudFormation, CDK, Terraform,\nall these tools will also give you the ability\nto add tags to the resources.\n\n\nAlso interesting enough, with CloudFormation,\nCloudFormation will also take the liberty\nof adding a bunch of useful tags out of the box\nfor you to almost every resource.\nFor instance, the stack ID and the stack name,\nwhen you deploy with CloudFormation,\nyou will probably notice that those tags\nwill be applied to all the resources.\nAnother interesting thing is that there is a tool\ncalled AWS Tag Editor.\nIt's an actual service in AWS,\nand that one also give you different abilities to search.\nAnd I think you can also apply tags in bulk.\nI haven't used it that much,\nbut something else you could explore\nand see what are the capabilities.\nThen there is resource group tagging,\nwhich I don't really remember what it does.\nSo if you remember about that, I'll leave that one to you.\n\n\nEoin: Yeah, I think this tag editor you already mentioned,\nit's quite confusing the terminology here,\nbut it's under the umbrella of resource groups,\nresource groups tag editor,\nand then you've got an API with that as well.\nSo I think that's the resource group tagging API.\nAnd it's interesting, depending on the resource type,\nnot every resource type is supported\nby each of these things,\nso you just have to check the documentation.\n\n\nUnfortunately, it's just the case with tags\nthat consistency isn't there across all the services\nin terms of the tagging support.\nSo you just have to check the docs\nand for your costly resources,\nlike EC2 instances, that sort of stuff,\nit's generally very well, very mature,\nbut for some of the other resources, it's just not there.\nYeah.\nAnd I like the idea of using infrastructure as code\nreally to enforce it.\n\n\nI mean, that's, again,\nit sounds like we're kind of beating the drum\nabout infrastructure as code quite regularly on the podcast,\nbut it's one of the real benefits\nof using infrastructure as code\nis that you can define your tags\nin the root of your repository\nand enforce it for all of the resources consistently.\nAnd you know that the tag names are right,\nthe values are correct,\nand the casing, because they're case insensitive,\nand you might want to think about, you know,\nmaking sure they're consistent with casing,\nwith infrastructure as code just makes that a lot easier.\nMm-hmm.\n\n\nLuciano: Absolutely, yeah.\nSo now that we understand a little bit more\nwhat tags are useful for,\nis there a set of tags that you would particularly recommend?\n\n\nEoin: Yeah, I guess the good idea\nis to try and keep the number of tags reasonably small.\nI think you can create up to 50.\nSo the recommended tags name\nis one that probably people will have used quite a lot.\nSo if you've created an EC2 instance in the console,\na lot of people skip that last step\nwhere you have to put in the tags,\nbut if you've put the values in,\nyou'd probably put in name at some point\nbecause that's recognized by the AWS console\nwhen it presents your list of EC2 instances,\nand it will give you the name column populated\nfor all your instances.\n\n\nSo name is an important one that's actually recognized.\nOther things, if you're using it for cost control,\nyou might have like a department or a cost center tag,\nan owner, be that an individual,\nprobably not a great idea to use individuals\nbecause they can change department and application,\nbut you can put in the owner,\nlike the name of the team responsible for a resource.\nAs you already mentioned in your example,\nat the start, you could put in the application\nor service name, and I really liked the idea\nof adding like an environment or a stage tag.\n\n\nSo knowing that it's a development environment,\na test environment, or a production environment,\nis that allows you to enforce security.\nYou gave a good example of like disallowing\nstock terminate instance access, for example,\nan EC2 instance, if it's a production environment.\nIt's also just good for auditing, right?\nAnd also for cost control.\nYou can also say, well, how much of my AWS resource cost\nis coming from production versus pre-production\nand test and development environments?\nSo those, I think, cover it all.\nBut the other one that you mentioned earlier\nwas like version, which I really like.\nLike if you have a Lambda function or an AMI in particular,\nyou could tag it with version numbers so you understand\nwithout having to boot up the AMI,\nand boot an instance based on the AMI,\nyou can understand what version it is\nand even what operating system version, for example.\n\n\nLuciano: Yeah, on environment or stage,\nit might sound a little bit contradictory\nwith something we said in other episodes,\nwhere we recommend to have different accounts\nand structure your organization,\nso you have dedicated account for different environments.\nBut in reality, if you do, for instance,\nconsolidated billing, then you will have like\na high level overview of all your accounts.\nAnd then if you use the tags,\nthen it becomes easier to see how you are looking\nat the different resources,\neven if they live in different accounts,\nbut then you can use tags to group them logically\nand see how they affect cost.\n\n\nEoin: Yeah, I agree.\nAnd we also do things like creating AWS config rules\nand config, I'm forgetting the term,\nbut there's a way you can collect data\nfrom all of your AWS accounts\nand aggregate it into your AWS, say compliance account,\nwhere with AWS config, you can go in\nand write like SQL queries\nto query what resources you have in AWS.\nIt's quite powerful.\nAnd if you can imagine that,\nif you've got things tagged with an environment,\nyou could do a select statement\nthat discovers how many Lambda functions you have\nthat are used in development,\nor how many EC2 instances you've got running in production.\nSo it's quite powerful.\nSo how do you enforce tags then?\nIf you've, we talked a little bit about things\nlike AWS config, but what are the ways that you could say,\nokay, I'm going to try and get my house in order\nand put in tagging policies for my organization.\nHow would you start?\n\n\nLuciano: Yeah, there are a bunch of different ways,\nand I'm sure that there is more\nthat we haven't discovered yet.\nSo don't take this list as a comprehensive one.\nProbably we just scratched the surface there.\nBut the first one that comes to mind is IAM,\nbecause with IAM, you could create policies,\nfor instance, for, I don't know, developers,\nthat says something like, okay,\nyou can create an EC2 instance, but only,\nand this is where you specify a condition in the policy\nthat says only if you are adding tags\nfor environment and cost center, for example.\n\n\nSo that that automatically enforces engineers\nthat have that particular role to do that,\nbecause otherwise they wouldn't be able\nto create the EC2 instance in the first place.\nSo this is actually a very powerful one,\nbecause it's not retroactive.\nSo this is literally preventing resources from being created\nif they don't have the right tags in the first place.\nAnother one is service control policy,\nwhere it's kind of a similar thing,\nbut I suppose from a different perspective.\n\n\nSo service control policies is something you can apply\nto entire accounts in your organization,\nand they can essentially deny certain operation\nif certain conditions apply.\nSo you could think of the same use case\nwhere you want to prevent, for instance,\nengineers from creating EC2 instances\nif they are not tagged consistently with your rules,\nbut you have to think it a little bit backward\nin terms of the policy, because you have to say,\nI'm gonna deny that operation if I don't see this tag.\nSo the policy is kind of, yeah,\nit feels like the opposite policy\nfrom the one we described before,\nbut effectively you can achieve the same result.\nAnd SCP are a little bit more powerful\nbecause then you can apply them\nto an entire sub-tree of organization\nin your organization.\nFor your whole organization, yeah.\n\n\nEoin: Yeah, it's definitely the strictest option, isn't it?\nIt sounds certainly, it's the strictest one I can think of\nbecause it's like a preventative control.\nIt's like, no, you can't even,\nyou can't go any further here.\nYou're missing a tag.\n\n\nLuciano: Exactly, so also use it carefully\nbecause maybe you will end up blocking your engineering,\nyour engineer from doing progress\nbecause yeah, maybe there might be exceptions\nthat you haven't talked about.\nAnd then yeah, people need to go through\nsome sort of a process\nif they have to go around those particular cases.\nSo other approaches that are a little bit more permissive,\nbut they still give you visibility\nwhen something is not compliant with your tagging strategy\nis for instance, using AWS Config,\nyou can create a rule that checks\nthat your resources are actually tagged correctly.\nAnd if you find something that is not compliant,\nthen you can trigger a notification\nand then you can use that notification in a reactive way,\ngo in and check why those resources are not tagged,\nif you need to tag them,\nor maybe you need to create a new rule,\nor maybe you can make an exception in your rules.\nSo this is a little bit of a more permissive approach\nand it's more kind of in that idea,\nyes, let's do compliance,\nbut let's not block engineers from doing progress.\nOther ways that come to mind to you?\n\n\nEoin: Yeah, we mentioned AWS Resource Group.\nSo this is where the Tag Policy Editor came in earlier,\nand there's also ability there to create tag policies.\nSo this is an account level feature,\nbut it's also an AWS organization feature.\nSo within your AWS organizations in your management account,\nyou can turn on tag policies.\nSo there's different kinds of organization level policies.\nYou mentioned service control policies already.\n\n\nThere's another one called tag policies.\nAnd it gives you the ability to create policies\nsaying these tags need to be created.\nYou can specify whether they need to be case sensitive\nor not, whether the exact casing needs to be specified,\nand you can also specify the values that should be used.\nThat's optional.\nIt's quite useful.\nIt's also like a detective control,\nlike a permissive option rather than a preventative control.\n\n\nSo it doesn't stop you from creating resources\nif the policy isn't complied with,\nbut it does allow you to go in and see\nwhich resources are failing tag policy.\nSo it's an alternative option.\nAs with a lot of things in AWS,\nyou kind of have multiple ways of doing the same thing.\nIt's very similar to AWS config rules.\nSo it just depends on what method you want to use.\nIf you're already using AWS config rules\nfor lots of other things,\nit might just make sense to use AWS config\nand detect compliance that way.\n\n\nBut if you're not,\nthen you might just use AWS resource groups instead.\nSo it's probably worth mentioning as well\nthat with AWS config rules,\nyou can also remediate some of the compliance failures\nyou see if one of your resource,\nif a resource gets created,\na config rule spots that it hasn't got a tag,\nthen you can actually use like a Lambda function,\nor you can also use system manager automation\nand go in and actually apply a tag to that.\nSo you can figure out what tag\nmight make most sense for the resource,\nor you could tag it with some sort of temporary value\nor just tag it so that you can come along\nand enforce it better later.\nOkay, that's a pretty good suggestion.\n\n\nLuciano: I suppose the more you can automate the better\nbecause yeah, it's gonna make your life easier\nin the long term.\nOne interesting thing I just want to mention very quickly\nis that there are limits in using tags.\nWe already mentioned that a resource\ncan have at most 50 tags.\nSo this is an interesting one.\nAgain, don't overdo.\nWith tags, 50 is probably more than enough,\nbut I don't know,\nmaybe if you have a very aggressive tagging strategy,\nyou might reach that number.\n\n\nSo it might not be that big as well.\nAlso, every tag key must be unique.\nThat means that you cannot use,\nI don't know, the tag name twice, right?\nYou can only use name once.\nThen there is a maximum key length\nand a maximum value length is 128 characters in Unicode\nfor the key and 256 characters for the value.\nAgain, it should be more than plenty\nfor making sense of your tags,\nbut it's not an endless amount.\nSo don't try to put, I don't know,\nall the metadata as a JSON into one tag\nbecause that's not gonna scale very well.\nAnd also there is an interesting one is that tags,\nthe key cannot start with AWS column.\nThat's kind of a reserved one.\nAnd I'm not really sure if that's something\nthat AWS uses internally, or if you, in some services,\nit will be a little bit more visible to you,\nbut you cannot use AWS column yourself\nas a, for your own custom tags.\n\n\nEoin: Okay, that sounds very sensible.\nAnd are there some kind of best practices?\nIt sounds like there's a lot of options here\nand there's plenty of flexibility.\nSo can we simplify it with some kind of best practices\nand patterns for tagging?\n\n\nLuciano: Yeah, so, well, the first thing that comes to mind\nis definitely try to avoid to store\npersonal identifiable information in tags\nbecause of course those are not meant to be,\nI don't know, encrypted or hidden in any way.\nSo just keep it to the metadata that makes sense\nfrom the service perspective or the resource perspective.\nAnd then the other overall suggestion\nis try to define a strategy and trying to keep it simple.\n\n\nAnd I don't think there is really a bulletproof strategy.\nSo I suppose we can give some hints to people\non how to approach that.\nAnd for instance, you could ask yourself,\nwhich tags are you going to use?\nSo try to come up with a list that says,\nthese are the five or six tags\nthat we want to use consistently.\nThen what are they going to be used for?\nIt's another question you should ask yourself.\nDon't just come up with six random tags,\nbut try to find what is the goal of every single tag.\n\n\nHave a purpose.\nAnd like, I don't know, is it a technical tag?\nIs it related to cost?\nIs it related to security?\nIs it useful for some automation?\nSo try to think that way.\nHow are you going to group tags\nand what are you going to use them for?\nAnd the other thing is that if you're doing\ninfrastructure as code, it's easy to think about,\nokay, I can keep them consistently\nbecause that's going to become part\nof my infrastructure as code.\n\n\nBut ask that anyway.\nLike try to think, okay,\nam I going to make this consistent?\nCan I do something that helps me to keep it consistent?\nSo both proactively and retroactively,\ntry to have ways to simplify the aggression of tags,\nbut also ways to check that your resources\nare actually compliant with your strategy.\nAnd finally, I think about naming conventions,\nlike make sure the casing is correct and used consistently,\nand maybe have a review process\nand you can probably even automate some of that\nusing some of the tools we discussed before.\n\n\nThere are some interesting resources,\none that I would like to add to the show notes\nis an example that's from the AWS blog\nthat shows how you could use CrowdTrail\nto automatically tag resources,\nbased at, for instance, I don't know,\nthe time when it was created,\nwho updated that resource or things like that.\nMaybe it's not the most amazing example\nfor the use that they present,\nbut I think it's cool to see\nthat you can automate some of this stuff.\nSo maybe you can find a different way\nof a different classification for tags, I suppose,\nbecause I don't think tagging the engineer\nthat created the resources extremely useful,\nbut maybe from CloudTrail,\nthere are other things that you can use\nto do some of this tagging automatically.\n\n\nEoin: And there is also- I can imagine that, sorry,\nI was just gonna say that came to mind\nwhen you mentioned that if you've got a CloudTrail event,\nbecause somebody created a resource manually in the console,\nand you've got their, say, SSO identity\nas part of that CloudTrail event,\nthen you could look up what department they're in\nand add the department tag to the resource.\nSo at least when you're doing cost allocation tracking,\nyou've got something to work with, right?\nAnd you don't have to go\nand constantly manually check and remediate.\n\n\nLuciano: Yeah, absolutely.\nThat's a much better example, I think,\nrather than just using the name of the engineer,\nwhich, yeah, as you said before,\npeople might move around in the company\nor they might just go away.\nAnd it's also personally identifiable information anyway.\nSo I would rather prefer their department name\nrather than their specific person name.\nAnother interesting one, which is actually archived,\nbut I think there are still some useful suggestion,\nis a white paper from AWS that was published in,\nI think, in 2018.\nSo maybe that's the reason why it's archived,\nbut we couldn't find a newer version of that.\nSo I think there are still some useful suggestions there.\nSo we'll put the link in the show notes as well.\nAnd I think this is plenty for this episode,\nbut I don't know if you have a final takeaway\nthat we want to use to kind of, yeah,\ngive people something to walk away with.\n\n\nEoin: Yeah, I think it's worthwhile covering this, because I haven't seen a lot of commentary\nabout tags in recent years.\nAnd I think it's something that could be discussed more\nand used more because you have to put a little bit\nof upfront effort in it, but once you've got it,\nthe benefits are there.\nYou can really get a lot of value out of it\nand it'll really help you to just have a better view\nand just sleep easy knowing that you've got more control\nover kind of auditing in terms of compliance,\ncost, and just visibility into your resources.\n\n\nBecause if you don't know what's out there in your accounts,\nit can just get very messy over time.\nSo yeah, I think our, both our takes on this is,\nyeah, use it more, but try to have a strategy,\ntry to be consistent and keep it simple.\nStart with small number of tags that you know you'll use\nand have a tier purpose.\nAnd once you've got that established,\nI think you're going to start benefiting from it\npretty much right away in terms of cost and visibility\ninto your whole estate.\nSo I'm glad we covered this topic.\nLet us know what you think and let us know\nif we missed any other ideas you can consider\nfor tagging AWS resources, any other tools or resources.\nWe'd love to hear from you.\nSo thanks for listening and we'll see you\nin the next episode.\nranch- backdrop-\n"
    },
    {
      "title": "33. What can you do with CloudWatch metrics?",
      "url": "https://awsbites.com/33-what-can-you-do-with-cloudwatch-metrics/",
      "publish_date": "2022-04-21T00:00:00.000Z",
      "abstract": "CloudWatch is the main Observability tool in AWS and it offers a wide range of features: logs, metrics, dashboards, alarms and even events (recently moved into EventBridge).\nIn this episode we are going to focus on CloudWatch metrics. We are going to discuss the characteristics of metrics in CloudWatch: namespaces, dimensions, units and more. What metrics you get out of the box and how to create your own. How to access and explore metrics.\nFinally we will compare CloudWatch to other providers like DataDog, New Relic, Honeycomb and Grafana + Prometheus and try to assess whether CloudWatch is enough or if you need to use other third-party services.\nIn this episode we mentioned the following resources:\n\nAWS Bites EventBridge episode\nHow to send Gzipped requests with boto3 (which uses the PutMetricsData API as an example)\nCloudWatch service quota\nCloudWatch metrics stream for DataDog\n\n",
      "transcript": "Luciano: What can you do with CloudWatch metrics?\nIn today's episode, we are going to discuss\nwhat CloudWatch is, and in particular,\nwe are gonna focus on CloudWatch metrics.\nWe are gonna discuss what are the characteristics\nof metrics like namespaces, dimensions, units,\nand even more.\nAlso, what metrics you can get out of the box\ndirectly from AWS and how you can create\nyour own custom metrics,\nhow to access and explore all these metrics\nthat you have been collecting for your applications.\n\n\nAnd finally, we'll try to compare CloudWatch\nto other providers so that we can assess\nwhether CloudWatch is enough\nor if you need to use any other third-party service.\nMy name is Ruchano, and today I'm joined by Eoin,\nand this is AWS Bites Podcast.\nWhat is CloudWatch?\nWhat are metrics?\nWhat are metrics?\nWhat are metrics?\nSo, CloudWatch is a service with multiple subcategories.\nWe know it's not just about metrics.\n\n\nThere are a number of things\nthat you can do with CloudWatch.\nWe have been discussing many times about,\nfor instance, logs and dashboards or alarms.\nSo let's try to make sense of all these things,\nbut in particular, we want to focus on metrics.\nOne interesting thing that I want to mention\njust because we said in previous episodes\nis that there is also a feature\nrelated to events in CloudWatch\nthat probably if you've been using AWS long enough,\nyou will remember every time\nyou were trying to create a Lambda on schedule,\nit will create like a CloudWatch event for you.\nAll the stuff is now under the umbrella of EventBridge.\nSo we have explored that when we spoke about EventBridge.\nSo check out that episode if you haven't done already.\nSo yeah, again, today we are just going to focus\non the metrics component of CloudWatch.\nSo why don't we start by trying to describe\nwhat a metric is in CloudWatch?\n\n\nEoin: Yeah, so a metric is essentially a time series of data points from your systems.\nAnd in CloudWatch terms, metrics are defined\nby some of the things you mentioned, like a namespace.\nYou have a unit, a value for each metric,\nand then you have dimensions as well.\nSo let's talk about what each of these terms means.\nSo a namespace, you'll always see at the top level.\nSo namespace is a container for all your metrics.\n\n\nFor an example, if you have some of the services metrics\ncoming from AWS itself, like for EC2,\nyou'll have a namespace of AWS forward slash EC2.\nAnd you can create your own metrics\nand give them your own namespace as well.\nNow beyond that, you can have dimensions.\nAnd for every metric, you can have up to 10 dimensions.\nAnd that's essentially a different way\nof categorizing your metric.\nAnd when you store a metric with multiple dimensions,\nCloudWatch is actually storing multiple copies\nof that metric just with different dimensions.\n\n\nSo some examples of that are,\nif you look at the duration metric\nyou get for Lambda functions,\nthat metric is actually stored by function name.\nSo you can create by functioning,\nbut you can also query it by functioning plus version.\nAnd a lot of AWS service metrics are stored\nin multiple different dimensions.\nSo you can query them depending on what you know\nabout what you're trying to query.\nAnd the important thing to be aware of there\nis that each dimension is stored separately.\n\n\nSo it's also priced as an additional metric.\nIf you've got too many dimensions,\nyou try to create dimensions dynamically\nbased on something that changes frequently\nwithin your application,\nthat can result in escalating cost.\nAnd that's one of the things that can catch people out.\nSo try to reduce the number of dimensions\nand keep them constant.\nSo apart from dimensions and namespace,\nyou can also specify the unit.\n\n\nSo when you store a metric in CloudWatch,\nyou can specify that it represents a number of seconds\nor account bytes and percent.\nIt can also change the way the data is stored internally\nand what kind of queries you can do on it,\nbut it's also useful metadata that you can use\nwhen you're creating graphs and dashboards.\nSo those are the main properties of a CloudWatch metric.\nAnd it's important to understand how they're stored\nand how you can query them.\nSo we'll get to that and we can talk in detail\nabout how you can explore these metrics.\nBut maybe we should talk about the different types\nof metrics as well, right?\nSo we mentioned EC2 and Lambda.\nSo you can get those out of the box metrics,\nbut you can also create your own.\nSo how would you categorize those two things Luciano?\n\n\nLuciano: Yeah, so as you said, we have out of the box metrics\nthat are things that you would expect to see\nfor the kind of AWS services you use.\nFor instance, if you spin up an EC2,\nanytime you can see what is the CPU usage,\nyou can see a bunch of things like that\nand same for Lambda, you can see for instance,\nI don't know, average Lambda direction.\nAll of these things are stored in CloudWatch\nas metrics out of the box.\n\n\nYou don't need to do anything to enable them.\nSo anytime you can just go in, check this metrics,\nbuild dashboards, build alarms based on those metrics.\nAnd most importantly, they are given to you for free\nand every service will have a long list of metrics.\nSo just go in the documentation and you will find\nwhat kind of metrics have to be supported,\nwhat do they mean?\nI have to say sometimes the naming is not extremely obvious\nwhat the metric actually means.\n\n\nFor instance, I don't know, if you look at SQS,\nthere are some naming that might be a little bit confusing.\nSo make sure to check the documentation\nbecause there you can find a good description\nof what the metric actually means.\nAgain, the name sometimes is not enough\nto make entirely sense of the metric.\nWhat else can we say there that,\nyeah, so sometimes there are metrics that I wish were there,\nbut they aren't.\n\n\nSo you can kind of fill that gap\nby providing your own custom metrics.\nSo there are of course ways and APIs\nthat you can use to basically say\nwhen I'm recording a certain unit\nand I would like to see these in the future as a metric,\nmaybe because I want to build a dashboard\nor maybe because I want to build custom alarms\nbased on these metrics, you can do that.\nAnd of course it takes a little bit of work,\nbut you can definitely do that.\n\n\nAnd another case where this is useful\nis not just for technical metrics, maybe, I don't know.\nIf you want to keep track of resources\nthat you're spinning up and certain characteristics\nof those resources that are not supported out of the box,\nyou can also use custom metrics for business reasons,\nfor instance, I don't know,\nyou might want to know how many users are logged in\nin your platform at a given moment in time,\nor how many purchases are you generating in,\nI don't know, a unit of time.\nSo you can definitely use CloudWatch also\nfor more business oriented kind of metrics\nand use the custom metrics for that.\n\n\nEoin: I think that's really useful actually.\nAnd sometimes more useful\nthan the custom technical metrics,\nbecause the metrics at a technical level\ncan sometimes create a lot of noise\nbecause there are so many of them.\nBut if you look at in your business,\nwhat's actually important to you,\nand like you say, if you have an e-commerce application\nand you're tracking how many purchases have been made,\nyou generally know that the number of purchases you expect\nin a given hour or a minute is going to be,\nlet's say 20,000.\nAnd you can create an alarm on that metric that says,\nokay, if the number of purchases,\nit drops below a certain threshold,\nor maybe even exceeds a certain threshold,\nthen let me know with an alarm.\nAnd sometimes that's a lot more useful\nthan looking at detailed technical metrics,\nwhich can be very noisy because it's telling you\nthat something actually important to your business\nis being affected here.\nSo like if you suddenly have half the number\nof e-commerce transactions being processed,\nthat's something that you can act on pretty quickly\nand you know what that means for your business.\nSo I think that's a really good one.\n\n\nLuciano: Another example that I have in the past,\nthis was not built on CloudWatch metrics actually,\nbut I think the same example applies.\nYou can build it with CloudWatch metrics,\nbut I was working on an application\nthat had some custom metrics looking at the number\nof logged in users.\nAnd there were good number of users constantly\nthroughout the day,\nbut suddenly we saw that the number dropped to zero\nand that helped us to realize that there was an issue\nwith the login system.\nWhen we didn't have other alarms\nthat could tell us otherwise,\nso having the custom metric was very useful\nbecause we could immediately see\nthat something looked wrong there\nand we could investigate, find the issue\nand fix it as fast as possible.\n\n\nEoin: Really good.\nWhen you mentioned as well these missing service metrics,\none of the things I find is really missing\nfrom AWS metrics is if you had everything,\nAWS billing as we know is very complex,\nbut everything is built based on a certain unit\nof consumption.\nI would really like if CloudWatch metrics out of the box\ngave you everything that was billable as a metric\nso that you could create alarms\nand observe trends in usage.\n\n\nSo for example, a very basic one\nwould be give me a metric on the number\nof running EC2 instances per instance type\nor the number of running containers per ECS cluster.\nBut those things don't exist out of the box\nand I've had many situations where as part of cost control,\nyou end up using event bridge events\nto keep track of when things start and stop\nand then you can increment your own counter,\ncreate running containers metrics\nand then use that to anticipate billing problems\nbefore they occur because billing data is,\nthere's always a lag before you get\nbilling utilization data.\nSo I'd really like if everything that was billable,\nif it was just a hard and fast rule in AWS,\neverything that was billable was also available as a metric\nand that would help you keep an eye on costs much better.\n\n\nLuciano: Yeah, that would be really useful, absolutely.\n\n\nEoin: One for the AWS wishlist.\n\n\nLuciano: Yes, we'll send something that way.\nSo how do you access metrics then?\nLet's say you have custom metrics that you created\nor you just want to look at the metrics\nthat you get out of the box from AWS, how do we access them?\n\n\nEoin: Yeah, so you can use the, there's a, as with everything,\nyou can use the API and the SDK\nto read these metric data.\nMore commonly, this is one of the cases\nwhere you'll jump into the AWS console\nand use the metrics explorer and the metrics dashboard\nto create graphs or yeah, to create graphs\nand look at them in different chart types,\nline graphs, bar graphs, or just numeric values.\nBut it's important to understand\nthat when you're accessing these metrics,\nyou don't access individual data points in the time series,\nyou're always accessing statistics.\n\n\nSo when you store metrics,\nAWS is accumulating all these different statistics for you\nat different levels of granularity\nand that's what you can query.\nYou're not querying individual records.\nSo it's not like you're running queries\nagainst this time series database.\nSo that's something that it's a fundamental concept\nthat's important to understand.\nSo when you store like the account,\nit's going to internally record like the average\nand the minimum and the maximum\nand the sum per minute, for example.\n\n\nBut it also has a lot more kind of in-depth\nstatistics functions.\nSo it'll store the sample count, but also percentiles.\nSo you can query any percentile\nand there's also some more complex one,\nlike trimmed mean and stuff.\nYou can look into the AWS documentation for that.\nSo the user interface is not as slick\nas some of the third-party offerings,\nbut it is practically very useful.\nSo if you understand what you're doing\nand get a bit of familiarity with it,\nit is really, really quite good.\n\n\nYou can also perform some mathematical operations.\nSo you can do metric math\nto combine multiple statistics together\nand do a formula on them.\nSo we mentioned that we're storing statistics, right?\nAnd not individual values.\nSo then it's kind of important to talk about resolution\nand metrics are typically stored\nat one minute level aggregations.\nNow for EC2 metrics,\nit used to be that the default was five minutes.\n\n\nSo for EC2, if you want one minute granularity,\nyou have to enable detailed metrics\nand there's a cost implication of that.\nSo that's important to realize,\nbut you also have the ability for some metrics\nlike custom metrics to specify one second granularity.\nSo those are called high resolution metrics.\nAnd there's an extra cost of that\nbecause you're storing essentially 60 times\nthe volume as you would with minute level aggregations.\n\n\nThere is a recent feature called\nCloudWatch Metrics Insights as well,\nwhich is just another way of accessing those metrics.\nAnd that allows you to write SQL-like queries on metrics\ninstead of just using the UI to build metrics.\nI guess just one point to note about one second granularity\nis these high resolution metrics, they're not very common,\nbut when you go to the dashboard\nand you're building graphs in AWS console,\nthere's a dropdown where you can specify\nthe period of granularity\nyou're trying to present in your graph.\nOne of the options there is one second\nand that option appears whether those metrics\nare available at one second granularity or not,\nbut more often than not,\nit's just going to show you one minute granularity.\n\n\nLuciano: Yeah, and that can be confusing because for instance,\nsometimes you have using the mathematical formula as well,\nyou have graphs that are trying to display,\nI don't know, for instance, limits in Kinesis, right?\nTrue-good limits.\nAnd you get like a red bar that shows you\nhow close you are getting to the limit.\nThat red bar is used,\nis basically built using a formula\nthat uses the resolution.\nAnd if you switch to seconds, it gets pretty confusing\nbecause the bar will be calibrated for the second.\nSo you get a certain level,\nbut while the data that you are still seeing\nis by the minute, so yeah,\nit doesn't immediately make sense and it can be confusing.\nSo just be aware of this particular thing\nand make sure you are looking at the,\nbasically the aggregation unit that you actually expect\nthat at the moment in time.\n\n\nEoin: Yeah, and it's important to be aware\nof the metrics retention then.\nSo different granularities are retained by CloudWatch\nfor different periods of time, isn't that right?\n\n\nLuciano: Yes, that's something actually we can discuss\nin a little bit more detail.\nSo for instance, if you have data points\nfor a period that is less than 60 seconds,\nthey are available for three hours.\nAnd this is also something that sometimes is confusing\nbecause it looks like if you are looking,\nI don't know, for instance,\nyou had a daily run of something\nand then the next day you want to look back at it.\n\n\nAnd depending on the level of what kind of period\ndo you select, basically it looks like you are missing data\nor something was not actually running.\nBut in reality, if you increase the period,\nyou are gonna see data just aggregated\nat a different period.\nSo just be careful with that.\nIf you're looking at the very small periods,\nyou need to look not too way behind in the past,\notherwise the data will be missing.\n\n\nSo it makes sense to look at this data\njust a few hours after the data was recorded.\nSo yeah, let's mention the other ones\nbecause I think it can be interesting\njust to understand this concept better.\nSo if you are looking data points\nwith a period of 60 seconds,\nthey are available for 15 days.\nSo that's generally, I think, a good average.\nLike if you generally look at 60 seconds,\nI think that will work well enough\nfor most of the use cases.\nBut of course, if you want to look even more in the past,\nlike more than 15 days, you can aggregate by five minutes.\nSo in that case, you get retention for 63 days.\nAnd if you even want to look even more than that in the past,\nyou can look at one hour aggregation,\nwhich will be available for 15 months.\n\n\nEoin: That makes sense.\nAnd I guess then it's just a question of understanding\nwhich statistic function you need to select\nwhen you're looking at messages.\nSo if you're looking at duration for a Lambda function,\nwhat do you want to see?\nDo you want to see the average duration per period,\nor do you want to see the maximum duration?\nMaybe for duration, you actually want to look at a percentile.\nThat makes more sense, like the 95th percentile.\n\n\nBut if you're looking at Lambda invocations,\nyou might look at the sum of invocations for a period.\nBut if you're looking at concurrent executions,\nthat doesn't make sense to look at a sum\nbecause concurrent executions is already\nlike a sum in its own right.\nSo you might look at the maximum concurrent executions.\nSo you have to kind of understand the nature\nof these metrics and what they mean\nin order to pick the right statistical function.\nBut the documentation for all of these metrics\nwill kind of help you.\nAnd it'll tell you for each metric,\nkind of which function you should be using to explore them.\n\n\nLuciano: Also, this is something we'll be mentioning probably\nlater on during the episode,\nbut I think similar concept applies\neven if you use third-party alternatives,\nlike I don't know, Datadog.\nOr even if you use your own StatsD and Grafana,\nyou need to know exactly what kind of data\nare you storing, how we structure it,\nand then you'll be using different functions\nto fetch this data and make sense of it.\nSo this is not unique to CloudWatch.\nOkay, so let's say that now we want to create\nsome custom metrics.\nHow do we create one?\nAnd there are a bunch of different ways.\nWhere do we want to start?\n\n\nEoin: Maybe we can start with the most obvious,\nthe fundamental operation there,\nwhich is the PutMetricsData API.\nSo if you want to create a metric,\nyou call the API with your SDK and you can put a metric\nand you'll specify the namespace, the dimensions,\nthe unit, the value.\nI'm not sure if I'm forgetting anything else,\nbut that's fundamentally it.\nThis one is interesting because it has a lot of limits.\n\n\nLuciano: So if you want to use it like one-off, it's probably fine.\nIt's a good way to create one-off metric.\nBut if you want to have a process\nthat is constantly ingesting metric this way,\nit's very easy if you're going to bump into limits.\nWe'll be mentioning an article in the show notes\nthat explore some of these limits,\nhow you can work around some of them.\nYou can also use compression\nif you want to overcome some of the limitations\naround the payload size.\nSo it's an interesting word,\nbut I wouldn't say is the most convenient\nif you really want to start a lot of metrics over time.\n\n\nEoin: Yeah, it's a pain when you have to work\naround all those limits.\nI know you've got the CloudWatch agent as well,\nwhich is one of the older mechanisms for posting metrics,\nparticularly if you're on EC2\nand you want to collect metrics from your EC2 instance,\ncustom metrics and post them.\nSo you can use like StatsD or CollectD\nand the CloudWatch agent can pick those up\nand post them into CloudWatch.\nAnd that also supports something called EMF,\nwhich is Embedded Metrics Format.\nAnd that is one of the newer features.\nWell, maybe it's been around a few years now,\nbut I think it's one of the most useful additions\nto the whole space of CloudWatch metrics.\nDo you want to describe what EMF metrics is\nand why it makes such a difference\nin how it compares to just using metric data?\n\n\nLuciano: Yeah, so I suppose the way we can think about EMF\nis like rather than calling a specific API\nwith a specific structure to store all the information,\nyou're just going to log a JSON structure\nthat contains all that information.\nThen something else will pick up those logs\nand make sure that all that information is translated\ninto CloudWatch and stored for you.\nAnd this is something that works out of the box,\nfor instance, with AWS Lambda.\nSo in Lambda, if you produce logs that are JSON object\nthat conform the EMF specification,\nthen something, some process around Lambda\nwill pick up those logs line\nand you will see the metrics appearing\nin CloudWatch metrics for you.\nAnd they are not priced like put metric data.\nI'm not sure if you still pay something\nor if they are entirely free.\nDo you remember about that?\n\n\nEoin: You still pay for having the metrics.\nSo we could talk about the pricing a little bit,\nbut the metrics, you avoid the cost\nof the put metrics data API\nbecause the API requests are priced separately.\nYou avoid the cost and also the latency\nbecause put metrics data will have a latency\nbecause there's a network request there.\nSo if you can imagine you're performing\nsome time sensitive function,\nevery time you call put metrics data,\nyou've got an HTTP request in there.\nBut if you're just writing to the console\nand that's going to be logged into CloudWatch logs,\nthat's a much more efficient operation.\nSo it gets around all of that.\nIt gets around the cost, it gets around the latency\nand it's just way better for performance\nand you don't have to worry about limits.\n\n\nLuciano: Yeah, that's to me the biggest selling point\nthat it's way friendlier when you,\nbecause you don't have to think as much\nabout all the different types of limits\nand all the different dimensions\nthat will make you hit the limits.\nYou just log these lines\nand you are pretty much guaranteed\nthat eventually your metrics will appear\nin CloudWatch metrics.\n\n\nEoin: Yeah, it's like magic.\nAnd I think it has the side effect then\nof because it becomes so easy to create custom metrics,\nthen people tend to create more\nand you end up with much more insight into your system.\nSo it has that nice side effect.\nThe only drawback I'd say is that the structure\nyou need to create for EMF metrics is quite strange.\nIt's quite unwieldy.\nIf you were to design a nested JSON structure\nfor story metrics, you wouldn't exactly do it this way,\nbut I'm sure there's logic behind it.\nIt's a small price to pay.\nAnd there are libraries.\nAWS do provide libraries for generating this format\nin JavaScript and Java and Python.\nAnd if you're using Python,\nthe AWS Lambda Power Tools makes this really easy.\nIt's really nice, has some really nice support for it.\n\n\nLuciano: Yeah, and I suppose that will also help you\nto avoid mistakes because if you are trying\nto create that object structure yourself,\nmost likely you might do some mistakes\nin particular edge cases using a library,\nall the stuff will be covered for you\nand you just have a simpler interface that you can rely on.\n\n\nEoin: Yeah.\n\n\nLuciano: One thing that I have on this one is that\nthis is not supported, I suppose, everywhere.\nLike if you want to use this on Fargate or EC2,\nyou'll need to figure out different ways\nof making sure that the data is ingested.\nFor instance, we mentioned in EC2, you can use the agent.\nBut if you are using ECS or Fargate,\nI don't know if you can use the agent straight away,\nbut what you could do is you could ingest this data\nsome way like using a Kinesis stream\nand then process it through a Lambda\nthat will actually meet these logs\nand then these logs will be picked up.\nSo there is a little bit of indirection,\nbut you can find other ways to make sure\nthat the EMF format gets ingested.\nI wish that there will be a better support\nacross all the compute services.\n\n\nEoin: I completely agree.\nEspecially with something like Fargate, right,\nwhich is a serverless container,\nit would be nice if it just supported EMF metrics\nout of the box like you get with Lambda.\nBut I think the way you can do it,\nI've seen this done on one of our projects\nwhere you can create a task definition\nthat has two containers.\nOne is your main application container\nand the other one is like a sidecar\nfor the CloudWatch agent.\n\n\nAnd the CloudWatch agent can then pick up the logs\nand create the EMF metrics for you automatically.\nI think there's also another driver.\nSo this is using, normally with a container,\nmost people would use AWS Log driver.\nThere's also a FireLens driver that AWS provide\nand it's not widely used,\nbut I believe it also supports EMF metrics.\nOh, that's interesting. Okay.\nSo it's great for Lambda.\nAnother reason maybe just to use Lambda for everything.\n\n\nBut one of the, actually,\nit seems like we're going on about EMF metrics quite a lot,\nbut maybe there's a good reason for that.\nOne of the great benefits is that while we said\nyou can only access metrics as statistical values\nin the console,\nbear in mind that once you log your metrics\nas individual records like this,\nbecause you want to use EMF,\nyou also get the site benefit that you can now query them\nin your logs as individual data points.\nSo you can actually select individual metrics there\nand you can, I think, actually extend that structure\nto add in other fields like annotations or labels\nthat you might use for querying.\nSo if you find that, okay, your metrics aren't available\nbecause the resolution or the retention\nmeans they've expired, if you retain your logs,\nyou can actually pull them in with Athena\nor you can use CloudWatch Logs Insights\nand you can do really advanced aggregations\nand queries on them there.\nSo it's very powerful.\n\n\nLuciano: Yeah, probably will be a little bit more bespoke\ndata extraction and aggregation.\nAnd maybe it's not as easy to beat graph out of that,\nbut nonetheless, you retain the granularity\nof that single metric information\nand you can use it anytime.\n\n\nEoin: Yeah, yeah, for sure.\n\n\nLuciano: Okay, so should we quickly explore pricing?\nSo we already say that there are metrics\nthat are out of the box available\nand they are pretty much free, if I remember correctly,\nand you get a five minutes frequency.\nYou can have detailed monitoring metrics.\nThis is, I think, only for EC2, right?\n\n\nEoin: EC2, yeah.\n\n\nLuciano: Apply to all, okay?\n\n\nEoin: Yeah, the pricing page is quite confusing on that,\nbut the way I understand it is you only pay\nfor detailed monitoring metrics for one minute\nof frequency for EC2, because you get them out of the box\nfor everything else.\n\n\nLuciano: And then you can do 1 million API requests for free\nto get metric data or get metric widget image,\nwhich I believe is like one way that you can create an image\nfor a dashboard basically, or a graph\nand use it somewhere else.\n\n\nEoin: Yes, your free tier gives you a billion API requests.\nI think get metric data and get metric widget image\nare the ones that don't come under the free tier.\nSo those are just ones to be mindful of.\nSo for most API requests, you're gonna pay like a dollar cent\nfor each 1000 requests, something around that order.\n\n\nLuciano: And then if you want to create your own custom metrics,\nthat is, I suppose, a little bit hard to tell exactly\nhow much it's going to cost you.\nI think we estimate, yeah, we estimated between two cents\nand 30 cents per metric, depending on volume\nand probably depending on the number of dimensions\nthat you're gonna create for every metric.\nSo yeah, that's something that requires a little bit\nof an exercise for you if you really want to be accurate\nabout estimating the cost.\nAnd then there is the usual API requests.\nSo if you do 1000 requests, you're gonna be paying one cent\nper every 1000 requests.\nSo again, another reason not to use the put metric data\nmassively, because probably you're gonna create thousands\nof metrics over a short amount of time,\nso that will affect your cost.\nThere is another feature called metrics streams.\nDo you want to mention what that is\nand how can it be useful?\n\n\nEoin: Yeah, metrics streams is another relatively new addition\nto CloudWatch metrics.\nAnd the idea there is if you've got a third party\napplication or something else where you need to,\nthat you want to use as a sync for CloudWatch metrics,\nthe traditional means of doing that was to kind of pull\non an interval and pull metrics in on some sort\nof pre-configured interval.\nAnd if you've got a third party, like you're using Datadog\nas you mentioned, or using U Relic or something else,\nit often ended up in really significant today.\n\n\nIt's like sometimes you could, people reported, you know,\n15 minutes before they could see their metrics in their APM,\nI think that's what they call it, right?\nSo that's, you know, not very usable for, you know,\nreal time troubleshooting.\nSo the idea with metric streams is that AWS will create\na literally a stream of metrics through either Firehose.\nSo Kinesis, Firehose, and then you can put them in Redshift\nor S3 bucket from there, or you can stream them\nthrough a third party like Datadog.\nSo I'm not sure exactly which third parties are integrating\nwith metric streams, but the idea there is to give you\nfaster time to insight on your metrics.\nSo that's the problem I believe that feature\nwas designed to solve.\n\n\nLuciano: Yeah, and I think that they have also ways to easily integrate that stream from your AWS account into Datadog\nor whatever other service.\nYeah, yeah, yeah, yeah, totally, yeah.\n\n\nEoin: That's a good point actually,\nbecause we're talking all about CloudWatch metrics\nand a lot of people out there probably have used\nCloudWatch metrics from time to time,\nbut actually have a third party solution\nthat's their chosen vendor for performance management.\nWhat do you think?\nShould people be going for the CloudWatch\ninstead of using something like Datadog\nor is there something else you,\nis there a limit beyond which you might say,\nokay, CloudWatch isn't really gonna serve my needs anymore.\nI need to use something more complete.\nWhat's the story there these days?\n\n\nLuciano: Yeah, if you asked me this question a couple of years ago,\nI would probably tell you don't use CloudWatch at all.\nJust do something else.\nAnd honestly, it's not because CloudWatch\ndoesn't have the capabilities that you need.\n\n\nEoin: It's mostly because the UI is, well, it used to be way behind the competitors.\n\n\nLuciano: Now I think that CloudWatch is,\nthe theme is investing a lot\nand trying to catch up with the competitors.\nSo I am seeing a lot of innovation and a lot of improvement,\nand I'm confident that they will get there\nand you will have a very good service\nalso from the UI perspective.\nSo I'm confident it's a good investment\nto try to learn CloudWatch and use it.\nAnd it can probably, for most use cases,\nthe only service you need,\nbut I still think that they are still a little bit behind\nwhat the competition has to offer.\n\n\nFor instance, I've used Datadog\nat a quite decent scale, I think.\nAnd I have to say that most of the UIs\nwere much more intuitive.\nIt was way easier to understand the data.\nAlso, you get a lot more dashboards out of the box,\nso you don't have to configure as many things\nas you need to configure today in CloudWatch.\nSo I suppose your mileage might vary\nif you are starting off\nand you need to just build a few dashboards\nbecause it's a small project.\n\n\nProbably you're not gonna notice any big difference,\nbut if you have multiple teams using multiple products\nand producing a lot of magic dashboards alarms,\nprobably going with something like Datadog will make,\non one side, your life a little bit easier, like as a user.\nOn the other side, you need to make sure\nthat all the integrations are configured correctly.\nYou are not missing any data.\nProbably, yeah, there are features that will require you\nsome fine-tuned integration.\nSo you need to make sure that all the setup is done right,\nbasically, but at that point,\nit's probably gonna be easier for the people in your team\nto avail of that information.\n\n\nEoin: Do you have a similar opinion\nor do you have a different one?\nYeah, I agree.\nIt really comes down to user experience.\nAnd also, if you've got multiple systems outside of AWS\nthat you're monitoring,\nthen it might make sense to have it all\nin kind of unified dashboards,\nunified performance management systems,\nand also your logs, think about that.\nBut I would say that once you get familiar\nwith the user interface in CloudWatch,\nyou can be very productive with it.\n\n\nIt just takes a little bit of investment in time\nbecause you have to overcome\nthe less fluid user experience that you get with it.\nBut like you say, the features are there,\nand you certainly benefit from the fact\nthat you don't have to worry about sending your data\nto a third party and what that entails,\nthe latency involved with it,\nthe separate pricing arrangement.\nIf you want to keep everything under your AWS bill with IAM,\nyou can do quite a lot with CloudWatch metrics,\nand I think it is worth persisting with\nif you don't want to have to bother with another third party.\n\n\nBut at the same time, there's a lot of innovation\nin third parties as well.\nLike if we see the amount of innovation\nthat comes out of tools like Honeycomb,\nthey're really pushing the boundaries\nof what observability means\nand kind of leading the way as well.\nSo I think it is worth exploring the space for sure\nand making your own decisions on it.\nOne thing we didn't cover is, I guess,\nGrafana and Prometheus are very prominent\nin this space as well for metrics, logs,\nand like Grafana particularly for presentation.\nAnd you can actually do both, right?\nSo AWS has managed services for both Grafana and Prometheus,\nand you can use them to visualize,\nyou can use Grafana to visualize your CloudWatch metrics.\nAnd you can build a lot more in terms of useful dashboards\nwith Grafana than you can with CloudWatch.\nSo maybe there's a middle ground\nwhere you can combine these things,\nespecially if you've already got Grafana and Prometheus\nin the organization.\n\n\nLuciano: Yeah, that's a good one.\nOkay, so I think with this,\nwe covered everything we wanted to cover today.\nOf course, we mentioned that you can do a lot more\nwith CloudWatch, you can do dashboards, you can do alarms,\nprobably topics that we will be discussing\nin future episodes.\nSo make sure to subscribe, follow in whatever platform\nyou are using so you can stay in touch with us\nand be up to date when we publish the next episode.\nUntil then, let us know if you have any question\nor if you have any comment, and yeah,\nwe look forward to seeing you in the next episode.\n"
    },
    {
      "title": "34. How to get the most out of CloudWatch Alarms?",
      "url": "https://awsbites.com/34-how-to-get-the-most-out-of-cloudwatch-alarms/",
      "publish_date": "2022-04-28T00:00:00.000Z",
      "abstract": "CloudWatch is a great service for metrics. You get tons of metrics out of the box and you can also create your custom ones. One of the most important things you can do with metrics is to create alarms, so how do we get the most out of CloudWatch alarms?\nIn this episode we share our insights and cover the different types of alarms that exist, how to create an alarm, what to do when an alarm is triggered, a few examples of useful alarms and some of the drawbacks of CloudWatch alarms and how to overcome them.\nIn this episode we mentioned the following resources:\n\nOur previous episode on CloudWatch metrics\nSLIC Watch, a serverless framework plugin that generates sensible alarms and dashboard automatically\n\n",
      "transcript": "Eoin: How do you get the most out of CloudWatch alarms? Last time we covered CloudWatch metrics and with\nCloudWatch metrics you can get out of the box metrics for AWS services and you can create your\nown custom metrics for technical data points as well as business level metrics. But in this\nepisode we're going to discuss what can you do with alarms? So what are the different types of\nalarms? How do you create them? What can you do when your alarm gets triggered? We're going to go\nthrough a few useful examples, types of alarms you can create and we'll also talk about some of the\ndrawbacks of CloudWatch alarms and tips to overcome them. My name is Eoin, I'm joined by Luciano and\nthis is the AWS Bites podcast. Last time that we discussed CloudWatch metrics and one of the most\nimportant things you can do with those metrics is to create alarms and that is our topic for today.\nLuciano what are the different types of alarms you can create with CloudWatch alarms?\n\n\nLuciano: Yeah there are essentially two types of alarms. What we call metric alarm is the first time which is basically\nan alarm based on a single metric and if you want to make it a little bit fancier you can also use\nmathematical expression to basically announce the value that you extrapolate for the metric and do\nsome calculation with it. And another type of alarms is what we can call composite alarms\nand those are effectively what the name says so you take different alarms and you combine them\ntogether and you can build additional alarm on top of that. So as an example you can say if this\nalarm fire and this other alarm fire I want to fire another alarm. What do we want to say first\nhow do metrics work? Maybe we can give an example of how to create one alarm.\n\n\nEoin: Yeah that sounds good so there's a couple of things you need to specify when you create one. First of all you pick your\nmetric. If we're just talking about a simple metric alarm example let's say we want to monitor CPU\nusage on our EC2 instances then we'll pick the metric AWS slash EC2 that's the namespace\nand the metric is CPU utilization. So once you pick the metric then you need to specify your\nthreshold so are you going to put an alarm on 90 utilization for example and you pick then a\ncomparison operator greater than greater or equal to less than and then you can specify the period.\n\n\nSo we know from our last episode on metrics that you have a period of say one minute five minutes\n15 minutes so what do you what period do you want your alarm to be based on and then the statistical\nfunction. Are you looking at the average over that minute the maximum and then you can specify a\nnumber of periods. So what you're doing then is you're saying to cloud watch I want you to look at\nthe average CPU utilization over three minute periods for example then you can specify how\nmany data points of those three will trigger an alarm. So you could say three out of three\nif all of the if the threshold has been breached for three successive data points then alarm or two\nout of three or one out of three or you could just have one data point it's completely up to you and\nhow sensitive you want this alarm to be. What that gives you then is an alarm on CPU utilization if\nit's above or equal 90 percent on average for at least two minutes of the last three minutes say\nand then the last thing you need to be considered is if you want to be alerted. So if you do want\nto be alerted you'll probably need an SNS topic and alerts will be sent to that topic and then\nyou have a lot of options in terms of what you do with those messages otherwise you can just see the\nmessages in the AWS management console or from the CLI. It's not ideal and not very reactive you have\nto be a lot more proactive so you have to get the balance right I guess get the right amount of\nnoise but not not get too noisy. We give one example there right on CPU utilization are there\nany other use cases you can think of that call to mind?\n\n\nLuciano: Yeah just a few that come to mind are for instance if you're monitoring a load balancer you can check the latency and see if that latency\nis going above a certain threshold for a certain amount of time. You can also create billing alerts\nthese are probably some of the most useful ones and some of the ones you should create\nas soon as possible so with those you can monitor if your projected expenditure is going above a\ncertain threshold that probably is going to be like your billing budget or something like that\nand you can use that to stop some services and see maybe if there is a bug that is causing you to\nto spend a lot of money on AWS and react before that bill becomes very very big.\nAnd other ones for instance that I've seen to be very very useful when building APIs\nyou can monitor API gateway and see the number of 500 errors and you can have alarms that check\nmaybe if you have an increased error rate that way and those can be useful to spot bugs for instance.\nSo I think we will yeah we have more examples in in the next sections but I think those are\nvery good example to understand just different kinds of alarms that you can create not just for\napplication errors but also for billing or for latency so it's a range of different things you\ncan do with them.\n\n\nEoin: Yeah it might be worth mentioning that there's we think about alarms as alerts but alarms are always in one of three states so the state you want them to be is in the okay state\nthe state you fear the most is the alarm state and then the third state is actually somewhere in\nbetween that's called insufficient data and that that's the state it gets into if you don't have\nenough metrics for that much to be able to determine which state it's actually in. So often\nwhen you create a metric for the first time if you don't have enough data it'll end up in insufficient\ndata so that's what that's about and any alert so if you're talking about SNS alerts or anything like\nthat you can alert on any state not just the alarm state. So that's a good example of that\njust the alarm state. Yeah that's a alert me when it goes on alarm but also alert me alert me when it\ngoes back into okay.\n\n\nLuciano: That's a very good clarification because the naming can be a little bit confusing I think generally when we talk about alarms we think about the alarm state so when things are\nbad but in AWS CloudWatch the term alarm is just that configuration and then you need to look at\nthe state to understand yes are we in a good situation are we in a bad situation or you can\nas you said you can even look for are we just recovering from a bad situation because you can\ncreate alarms that tell you you sorry you can create notifications that tell you okay you were\nbad before you were in an alarm state before and now we are finally back into okay state and if\nyou use things like chatbots we'll mention some of them later on that can be very useful to see\nbecause maybe if you have a noisy configuration sometimes you realize okay this thing triggered\nan event I'm seeing an issue but then immediately afterwards I see that the issue resolved itself\nand that means you don't really need to do anything maybe you can tweak a little bit your\nconfiguration to reduce the noise but you don't have like an immediate need to fix anything.\n\n\nEoin: What do you think is the best way to actually get notified then if you want to react in a reasonable\ntime frame and you want to also avoid noise what's the best tooling that you can apply for\nnotifications?\n\n\nLuciano: Yeah so we there are different ways that you can basically be notified when when the state of an alarm changes and probably the most common that I used to see in the past\nis not necessarily the one I would recommend is sns to an email so you get you configure your\nalarm and you say every time this condition happens send an sns event and then from that\nsns event you can dispatch the event somewhere else so the most common one I've seen in the\npast was email but I am not a big fan of that one because I remember a few years ago when I\nwas trying to use this you will literally get a big wall of text on an email and it's not the\nmost intuitive way of understanding when you have to go in an emergency and try to fix something\nlike it takes a while for you to process even what's just written there in the email and\nmake sense of it so there are more sophisticated ways today and one is like totally custom you can\njust send the sns to a lambda and then you can do whatever you want with it and that really depends\non how you structure your teams and your operations for instance if you use tools like pager duty you\ncould use the lambda to send the event to pager duty and then manage the incident that way or you\ncan build the lambda to integrate with other systems that you might want to use another\ninteresting way if you for instance want to use a chat platform because maybe you do that kind of\noperation based on slack or teams or something else there is of course you can do it with a\nlambda you can do it yourself and build the integration that way but there is a tool from\naws called aws chatbot so you can send the alarm from sns to aws chatbot and aws chatbot has already\nbuilt in a lot of nice things like you get a very good user experience when you integrate it with\nslack or chime you get very nice preview of the message describing what's what the incident is\nabout the only issues that as far as i know right now microsoft teams is not supported yeah and and\ni've been working in many companies using microsoft teams as the primary system so in that case you\n\n\nEoin: are a little bit on your own to find other tools or to write your own integration yeah i really like the chatbot experience actually it's pretty easy to set up once you've got your topic chatbot\nis pretty slick for setting up and writing your own integration into slack and has become a little\nbit more involved you have to create kind of create an app now you can't just fire data at\na webhook at least they don't want you to so aws chatbot has kind of solved all that for you and\ngives you a really nice message so what do we do after we get notified what can we do with that in janitor that's a good question so you can have all these i suppose manual intervention is going to be completely dependent on the kind of alarm you're dealing with you know um but there's also some\nautomated interventions so apart from sns there's there's some other actions you can take that are\nactually like automated remediation and you could like you say you can use sns to lambda or you\ncould target you know systems manager automation for remediation there's so much you could do there\nbut they're out of the box you if you're talking about ec2 ec2 metric alarms can also trigger ec2\nactions so you could say if your cpu utilization is getting too high then reboot the instance or\nterminate the instance and you know if you've got some legacy application with a memory leak and\nthat's your only option i guess that's um one kind of path you could take um you can also trigger an\nauto-scaling action and this is probably where a lot of people may have used alarms in the past\nmaybe without even realizing it because uh alarms and auto-scaling kind of go well together\nso you're scaling essentially in response to the observation of a metric breaching a\nthreshold so that could be an ec2 auto-scaling group or and that could be you know based on\nnumber of requests or some of your load balancer metrics but it could also be an ecs service auto\nscaling an example of that before and i think we've raised this one a few times is if you've\ngot a pool of workers doing some jobs batch processing say in an ecs cluster then you could\nscale based on the number of messages in a queue like an sqs queue that they're pulling their\nactual jobs from and that's that's a good way to handle it because you could have you know a\nstandard default maybe a five or so workers working away by default but then if a large\nvolume comes into the queue you might want to scale up to a certain limit and alarms are really\ngood for that i don't use a lot of the many features in systems manager but i know as well\nthere's there's some things you can do there if you're using some of the incident management\nfeatures like you can create an ops item if you're using opcenter and you can also create\nincidents and systems manager so there's um there's quite a lot you can do um\nbut i suppose one of the challenges with alarms and one of the reasons people probably shy away\nfrom them is because people have experience with very noisy alarms and it can be very\ndifficult to actually know what's going along going on when the default state is that things\nare always an alarm and then people just stop trusting the value of them so composite alarms\nare is something you mentioned at the start do you want to talk about maybe some examples for\n\n\nLuciano: you to use composite alarms and how they compare to just the simple single metric alarms sure yeah before that i like what you said because i there is a quote that i really like that is when\neverything is an emergency nothing is an emergency right so i think that we can say the same with with\nalarms like if if the alarm system is always noisy you stop being concerned right that becomes your\nnorm and you are not really trying to react anymore and fix things so that that's something\nto be aware i i would also caveat that at the beginning when you start to set up your alarms\nit's probably a good idea to be to try to be a little bit noisy so you can find actually the\nthreshold that work for you so initially probably there is a little bit of a tuning phase where you\ntry to find what your threshold should be like for you to actually be effective anyway back to\ncomposite alarms i i think composite alarm are actually relevant to this topic because they can\nbe one tool that you can use to make things a little bit less noisy and one example i have is\nthat it's very common for instance when you build apis with api gateway and lambda to have individual\nalarms both on for instance on lambda failures but also 500 errors on api gateway and if you have both\nwhat happens when a lambda failed you get a 500 on api gateway so you are basically alarming twice\nand one way that you can remove a little bit of noise is that of course you need to create both\nalarms but then you are going to fire an alarm event on sns using a composite alarm so that\nbasically means that you take the two alarms together you combine them with a boolean expression\nsaying if either one or the other for instance fire then fire the other composite alarm and then\nyou only receive a notification for the composite alarm so this this way you are nice effectively\nflipping uh two alarms but being notified only on one of them um yeah i don't know if you have\nother examples when composite alarms can be useful but i think they are just a nice way to\nreduce noise maybe you can also build more complicated rules maybe in some cases you can\nuse that to combine different metrics and then maybe only when you see a certain combination\nof matrix happening then it really makes sense for you to allow this is maybe another example\n\n\nEoin: where composite alarms can be useful yeah yeah it's very flexible in that way one of the other points about i suppose the usability so noise is one factor that's important for usability another\none is that we've frequently talked about separating lots of different services and\napplications and environments into different accounts so the question then becomes how do\nyou keep an eye on alarms and metrics across multiple accounts and multiple regions even\nand it is possible to do this without having to open dozens of tabs in your web browser dozens of\nand dozens of containers or whatever way you can manage different accounts because cross-region\nmetrics you get out of the box so you can switch you can view alarms and metrics from the console\nalready but if you're talking about cross account you just have to do a little bit of setup so for\nevery account that you want to share metrics from you just need to say okay i'm going to share it\nwith this central monitoring account and give the account number and then on the monitoring account\nside you just say okay i'm going to pull in metrics from these 20 accounts across my organization or\nall accounts across my organization and then you can start to look at things from one dashboard\nand from you know a single pane of glass essentially without having to log out and log in or switch tabs\nand that makes it a lot easier so i think that's yeah worth mentioning in this multi-account world\nso maybe it's worthwhile if we talk about some more useful use cases for alarms right to see\nif we can give some inspiration for people who are maybe not using alarms extensively and can\nstart getting cracking and making their lives easier and make maybe reduce the operational\noverhead that's what they're for after all right especially if you can predict errors before they\ncan happen where will we start we talked about business metrics in the last episode so we we've\ngiven examples about API Gateway EC2 what kind of alarms could you create on business metrics that\n\n\nLuciano: might be useful yeah i think the the ones we mentioned in a previous episode were for instance the number of sales per day again our favorite e-commerce example like some i suppose in some\ne-commerce is that the amount of sales tends to be quite predictable so you could create a custom\nmetric and then an alarm to see when that value goes a little bit outside your expected amounts\neither lower or way higher in both cases maybe you need to do some action so it's worth to be\nalarmed as soon as you see that that event happening and another one that i used in the\npast and was very beneficial to me is to monitor the number of people that are logged in into\nan application like in some application that can be predictable enough i would say so you can\ndefine rules or even sometimes you can just say let me know when this value is zero for a long\ntime and this is actually the one i used in the past and that one helped me to figure out an issue\nthat was present in the login system actually was more in the session system than the login system\nbut anyway having that alarm was good for me to see that people were not able of logging in for\na long period of time so i could figure out there was something wrong investigate find the issue fix\nit and bring everything back online so these are examples of business metrics that you can leverage\nto build alarms and then be more reactive and prevent incident for happening for a long time\nbefore you realize and you can fix them again we mentioned API Gateway and Lambda for more\ntechnical ones that you can use to to capture specific bugs so when the code is actually\nthrowing errors similarly you can yeah if it's timing out you can consider that\nan error the problem with all these ones we just described is that they tend to be reactive so it\nmeans the the issue happens first then you're probably giving a disservice to your users because\nof these issues and then you try to to rush and say okay let's fix this because our users are\ncurrently i don't know seeing unexpected results and we don't want that to happen anymore but\nthe question could be okay can we do something in advance can we predict when something is about to\nhappen and maybe fix it before it's too late before it's impacting the users and there are\ntwo things that you can do there are one thing is that you can use for instance in the case of\ntimeouts right Lambda timeouts you could create an alarm that looks for your your Lambdas getting\ncloser and closer to that timeout so for instance you could say trigger this alarm if you get 90\nif your Lambda are taking 90 of the time that has been allocated for that Lambda so if you are\nclose enough to the timeout but you are not eating it yet and this is something that for instance it\ncan happen if you have n plus one queries in a Lambda right as your database grows the time for\nexecuting that query will increase it will get longer and longer so initially you will have\nplenty of time to stay within the timeout but over time maybe you will see that that time increasing\nand getting closer and closer to the timeout so having an alarm there can tell you that this is\nbecoming a problem before your users will start to see an error so that that's something that can be\nvery very useful and another thing you can do i'm not sure if we mention it already but there is a\nanomaly detection based alarms that you can use to basically see if your current metrics are going\noutside the norm yeah so you can use that for instance again for Lambda directions if suddenly\nyou see your Lambdas taking much longer not necessarily getting close to the timeout but just\n\n\nEoin: being taking much more time unusual behavior exact yeah that's good yeah yeah i think you you kind of for that one you specify like uh it's like a standard deviation so you specify a band how how\nhow much outside the average you want to uh to be your anomaly detection band and that that can\n\n\nLuciano: trigger it on and one last example i have is for asynchronous computation uh for instance i don't know when you have a queue and a pool of workers you could monitor um the i think it's called\napproximate age of all that message in in sqs and something that is like the iterator age in kinesis\nso if you're using kinesis you have a similar concept there but that's something you can monitor\nto see if your workers are doing their job fast enough so that you don't keep accumulating messages\nbecause of course if you accumulate more messages than you can process in a reasonable amount of time\nlike eventually you're not going to be able to to process these messages fast enough anymore\nso your users will wait more and more before they can get results so that those are other good\nthings to to explore and maybe have some alarms to make sure things are being done efficiently enough\n\n\nEoin: yeah and if you're into like continuous deployment and continuous delivery one of the other things that just springs to mind is that you can tie your alarms into your deployment process as well\nand this is particularly useful if you're doing like canary deployments blue-green deployments\nwhere you need to monitor the health of the application if you're shifting traffic or\npercentage of traffic from one deployment to another and cloud watch alarms are ideal for\nthat because you can get programmatic access to the state of the alarm and it's also well integrated\ninto code deploy and cloud formation so they that those tools will actually integrate into your\nalarms start shifting traffic over to a new container a new instance and if the alarm starts\nto fire it'll say okay let's back off here and shift back to the old instance so it gives you\ndeployment safety as well maybe that's something we can go into in depth in a another future episode\nare there any kind of drawbacks with cloud watch alarms what's the what's the first user experience\nlike what do you think for people getting started with cloud watch alarms and how could it be\n\n\nLuciano: improved yeah i don't know if i would call it a drawback but one thing that i wish was a little bit better with cloud watch is that you don't really get anything out of the box like you get\nall these amazing features you can do a lot of things but literally you start with a blank slate\nand it's on you to define which things to look for and create all the alarms and if you haven't\ndone it before it might be a little bit stressful to just come up with different use cases and make\nsure things are finely tuned and for that AWS just gives you like the the well-architected framework\nfor instance but gives you documentation that can give you ideas on where to start and what to look\nfor but again it's all new to do the artwork of configuring everything and sometimes you you\ncreate for instance application serverless application using maybe the serverless framework\nit feels a little bit unnecessary that you have already defined everything with infrastructure\nas code why not apply those best practices by taking that information that is already\navailable in a sus code seeing for instance that you are using lambdas you're using sqs why not\ncreating all the out of the box default alarms based on your infrastructure and i wish that\nthat was available and we have been spending a lot of time ourselves on this problem and because of\nthat we ended up creating a serverless framework plugin that does exactly that so it looks into your\ninfrastructure as code so your yaml file your serverless yaml and if it sees for instance\nthat you're using a lambda it will provision for you a bunch of alarms that make sense for lambda\nsimilarly if you're using step functions api gateway dynamo db kinesis sqs and what we did\nis we are going to give you with that plugin you just initialize it you get a bunch of defaults\nbut of course you can configure it a little bit more if you want to change the default thresholds\nor if you want to enable or disable specific alarms that are a little bit more advanced\n\n\nEoin: that's a good one and i guess one and it creates dashboards as well so it also gives you kind of a little bit of a better user experience in terms of how those graphs of metrics are organized\ni don't think i don't think it's probably it's worthwhile to have a dedicated episode on cloud\nwatch dashboards since they're really just like a representation of your metrics and they can also\nshow alarms as well but it's another feature that plug-in so when you deploy a serverless stack\nyou'll get a dashboard for that stack so maybe it's a good idea to wrap up just uh talk about\ncost and you get 10 metric alarms for free which which is nice of them but beyond that it's uh\nit can get expensive if you've got a lot of them right so i guess it your mileage will always vary\ndepending on your context and usage it you're talking about 10 cents per alarm metric um\nnow if you're using high resolution alarms high resolution alarms are alarms based on high\nresolution metric so those one second metrics we talked about in the last episode um so those are\n30 dollar cents and if you have a composite alarms for some reason they're 50 cents for a composite\nalarm and i don't know why the composite alarm implementation is so complex that it justifies that\ncost it seems like a pretty expensive boolean expression evaluation but that's the cost you\ndidn't get so if you've got you know tens dozens of alarms that's probably for most organizations\nrevenue generating organizations is probably okay but if you're starting out and trying to keep\neverything free tier it's definitely something to keep an eye on so at this point i think we've\ncovered now from the last two episodes metrics fairly in depth and what you can do with those\nin terms of creating alarms next next one i think we can talk about logs log aggregation and how in\nparticular you can now use cloud watch logs to get a lot more and maybe avoid having to use a\ncentralized log aggregation service uh this logs is probably my favorite topic of the three so i'm\nlooking forward to that one but if you haven't checked out the previous episode on cloud watch\nmetrics check it out it's episode number 33 and let us know what you think and like and subscribe\nand we'll see you in the next one all about logs you\n"
    },
    {
      "title": "35. How can you become a Logs Ninja with CloudWatch?",
      "url": "https://awsbites.com/35-how-can-you-become-a-logs-ninja-with-cloudwatch/",
      "publish_date": "2022-05-05T00:00:00.000Z",
      "abstract": "In the age of distributed systems we produce tons and tons of logs. This is especially true for AWS when using CloudWatch logs. So how do we make sense of all these logs and how can we find useful information in them?\nIn this episode we talk all about logs on AWS and we discuss the main concepts in CloudWatch for logs like Log Groups and Log Streams. We discuss how you can consume logs and how this used to be a big pain point with AWS CloudWatch logs and how now things are a lot better thanks to a relatively new feature called Log Insights.\nFinally we discuss some best practices that you should consider when thinking about logs for your distributed cloud applications.\nIn this episode we mentioned the following resources:\n\nOur previous episode on CloudWatch alarms\nAnalyzing log data with CloudWatch Logs Insights\nCloudWatch logs insights query syntax\nPino logger for Node.js\n\n",
      "transcript": "Luciano: In the age of distributed systems, we produce tons and tons of logs.\nThis is especially true for AWS when you use CloudWatch logs.\nSo how do we make sense of all these logs and how can we find useful information in them?\nMy name is Luciano and today I'm joined by Eoin and this is AWS Bites podcast.\nSo if you use AWS, chances are that we have been using CloudWatch and you have probably seen that\nCloudWatch is a service that allows you to do a bunch of different things. In previous episodes,\nwe have been talking about metrics and alarms and today we're going to focus on logs, which is the\nnext big topics when you deal with CloudWatch. So an interesting thing is that for a long time,\nat least in my career, I was using third-party tools because my opinion, take it with a pinch\nof salt, CloudWatch logs was a little bit underwhelming in terms of functionality,\nin terms of ability to actually make sense and use the log data. I don't know Eoin,\nif you share the same opinion. Definitely, especially when you want to read them.\n\n\nEoin: It was pretty good for storing logs, but the main challenge then is how do you get useful information\nout of them? And for a long time, there was really nothing you could do there.\n\n\nLuciano: Yeah, I think now this has changed a lot, especially in the last couple of years. So\nin this episode, we're going to try to give you a few insights on how you can get the most out of\nCloudWatch logs and maybe you don't need to use a third-party tool for logs anymore. So do we want\nto start maybe by describing what are the main concepts when it comes to CloudWatch logs?\nYeah, I can give that a go.\n\n\nEoin: With a lot of log aggregators, so you talk about third-party log aggregators, and when you send all your logs, people used to, it was pretty common that you'd\nhave log groups in CloudWatch, you'd send everything into Elasticsearch, for example, with\nan ELK stack or Elastic stack where you funnel all your logs into one place. And then you've got a\nbig stream of all your logs from all your microservices, all your distributed applications.\n\n\nSo it's just one thing, but CloudWatch, it's quite fragmented the way they're stored. So\nthere's a two-level structure. You've got these log groups, and that's like the primary folder\nstructure for logs from a given application usually, or a given Lambda function or a given\ncontainer. So that's a log group. Imagine that like a folder structure. And then within the log\ngroup, you've got log streams. So the number of streams you have kind of depends on the service,\nbut your log stream is like a file within that folder. When you're looking for logs,\nbecause you've got multiple files, you've got multiple folders with streams and log groups,\nyou don't know where to look necessarily if you've just got those resources.\n\n\nOne other thing that it might be worthwhile mentioning for log groups is that some services\nallow you to log to log groups. For example, you can log step functions, state executions and state\nchanges to a log group. You have to make sure that the log group starts with a certain prefix.\nAnd this is something that isn't very clear in the documentation. So for Lambda, your function,\nyour log group name should start with slash AWS slash Lambda. And for step functions,\nthey should start with AWS slash states. And then for event bridge, it should be AWS slash events.\nSometimes it lets you use something that has a different naming convention, but it doesn't tell\nyou why your logs aren't being written. So that's one thing I'd call out just in case it helps save\npeople some time. So then you've got log streams and log groups. So the question is, how do you\nview them? So maybe we can talk about what the case used to be like a couple of years ago, and\nyou didn't really have a lot of options. What would you use, Ligiana?\nYeah, mostly AWS logs tailing from the CLI.\n\n\nLuciano: That used to be one way, but to be honest, I was more in the camp, less in just everything in a big Elasticsearch cluster, because there\nwe can just keep BANA and that's a much nicer experience to use and find information that way.\nSo I think that was my preferred way to operate when I was building any sensible application.\n\n\nEoin: For something like the serverless framework, when you deploy a Lambda function, you can use serverless framework to tell the logs in the console. That works pretty well.\nAWS CLI has AWS logs tail command, which also works. And that allows you to,\nsaves you from going through the console and clicking through individual streams,\nlooking for logs, which is just too painful. But I was always kind of, I always found it a little\nbit of an effort to create that Elasticsearch cluster just so you could store your logs. It\nseems like too much infrastructure for what you were trying to do. That kind of led me to try\ndifferent things like log entries or Splunk or many of the other solutions out there. There are,\nthere's probably a hundred of them at this stage. And they're all pretty good in terms of user\ninterface. One of the things that stops people from using them sometimes I think is, well,\nthere are probably two things actually. One is that people don't always feel comfortable with\nputting all of their logs in a third party. Now that depends on what kind of logs you have and\nwhat kind of data is in those logs. And the other thing is that sometimes there's just a propagation\ndelay from the time logs go into CloudWatch logs and then into your third party vendor before you\ncan query them. And when you're in a production troubleshooting scenario, seconds matter.\nAbsolutely.\nWhen it comes to the time for logs. So we were always on the lookout for ways you could\nimprove the experience with CloudWatch logs. So maybe before we get into the game-changing feature\nthat enabled that, should we talk about some of the other features that might be less commonly\nused like metrics filters? It's not something I use very often, but it is pretty useful. Do you\nwant to talk about that one?\nI'll try my best. There's also not a feature that I've been using a lot.\n\n\nLuciano: So let me know if you think I'm missing anything important. But the idea is that you can basically define a filter that is\nlike an expression that is constantly analyzing logs coming in and trying to see if your filter\nmatches the log lines. And then you can create metrics better than it. For instance, an example\ncould be count the number of 404 errors just by looking at access logs, right? Doesn't have to be\nan API gateway. Maybe you want to count from an NGINX instance that you are running on an EC2 or\na container. You could create your own custom metric filter and have metrics this way.\nOkay. So that's different to the last time we were talking about embedded metrics format, right?\n\n\nEoin: But this is not the same thing.\nIt's not the same thing. Yeah, exactly.\n\n\nLuciano: Embedded metrics format is basically when you create log lines that are a JSON object effectively. Like you are printing a JSON object that follows a\nspecific structure that AWS recognizes. And then AWS will use that particular structure to create\nmetrics for you. So you have in that structure all the information that is needed to actually\npush a metric into CloudWatch metrics. And that is integrated well with Lambda. And we spoke about\nideas that you can basically use that to integrate it with containers. Or you can use the CloudWatch\nagent, for instance, in an EC2 instance to ingest it from this kind of log ingest metrics into\nCloudWatch. Yeah, there is a little bit of, I guess, terminology confusion there between filter\nand EMF metrics. But at the end of the day, the idea is that you can use logs to also create\nmetrics for still in CloudWatch, but from logs, you also create custom metrics. As we think more\nabout now that I have logs and I understand the structure of the logs, how can we\nuse these logs in different types of processing logic?\n\n\nEoin: So you got the option to create subscriptions with CloudWatch log groups. So if you want to programmatically process them, send them\nsomewhere else, you can create subscriptions. It used to be that you could only have one subscription\nfor a log group, and then they increased that limit to two. So you can have two subscriptions.\nI think I've heard of people having that limit raised further still, so that might be worth a try.\n\n\nBut the idea is that you can subscribe and then you can send all your logs to Lambda or to Kinesis\nor to Kinesis Firehose. So Lambda is a good way you can process logs directly, but if you want to\nbatch things before you process them, it's a good idea to put them into a Kinesis data stream first.\nThen you can have lots of log entries in one message and process them in Lambda from that point.\nBut you can also subscribe directly into Kinesis Firehose and Firehose will...\n\n\nIf you want to put your logs into an S3 bucket, the Firehose approach is a good way to do it.\nBut Firehose can also go to Elasticsearch. So that's one way of going into Elasticsearch.\nAnd you can use Firehose to go to Splunk as well.\nSo there's lots of options there. Depending on what you've got in your logs, you can use them\nto create metrics. Like you say, you can use EMF metrics, but if you have your own metrics format,\nand people used to do this before EMF metrics existed, people would have a...\nUse the StatsD format for metrics or custom format, and then you could use Lambda\nto create the metrics with the CloudWatch API.\nThat's the story when it comes to integrating with other services.\nWe mentioned the game-changing feature for CloudWatch logs then.\nSo a year or two ago, probably two years ago, the CloudWatch logs insights feature was announced.\nFor me, this ended up being a big deal. What do you think?\n\n\nLuciano: Yeah, I consider you a CloudWatch log insights ninja, because every time I have to search something CloudWatch, I struggle with the syntax,\nbecause of course it's a new syntax and you need to know all the different features.\nAnd Eoin already mastered all of that, so he's my reference.\nBut yeah, other than that, I think it's a pretty powerful thing,\nbecause it really gives you effectively a query language that you can use to go\nand search for stuff, not just in a log stream, but you can even look for things\nin multiple log streams together. So it's very powerful, for instance,\nwhen you're trying to troubleshoot a problem,\non a particular distributed system, maybe you have all sorts of different components,\nmultiple lambdas, containers running, and you know more or less what's the time\nwhere you are looking for particular evidence of an issue or try to understand the problem.\nYou can construct queries using all sorts of different properties.\nWe'll talk more about how to have structured logs.\nAnd with CloudWatch logs insight, you literally have all the power to do that from\na web interface and you get the results and you can use that to dig deeper\nuntil you find the results you're looking for or the evidence you're looking for.\n\n\nEoin: You mentioned that the syntax you use for querying with CloudWatch logs insights, and I really like it actually. Maybe for a lot of people it would be nice if you could\nuse the Lucene syntax that people are familiar with from Elasticsearch and Kibana,\nbut I never really got to grips with that fully. So I actually prefer this syntax\nand it's pretty easy to remember once you have got to grips with some of the things you can do.\n\n\nSo it's essentially like a Unix pipe format where you create different,\nit's like a pipeline for filtering and searching your logs. So you can say filter\na given field for a given string or a numeric value or Boolean operator or regex.\nAnd then you can extract fields, you can parse strings and actually,\neven if it's unstructured text, you can parse numbers, strings, or whatever from\nan arbitrary piece of text, again using regular expressions. And then you can do statistics.\n\n\nSo you can almost treat it like a metrics query engine then. So you can extract numbers and do,\nit's almost like SQL in that case. So you could find the number of users\nwho have logged into your system in five second buckets. And then you can do\nfive second buckets using this syntax. And then you can sort and you get the results back in the\nAWS console. You can use the API for this as well, but the API I find is a little bit cumbersome for\nDog Insights. But if you have some programmatic cases, you can give that a try as well.\nSo I really like it actually. I only wish there were a few extra features that would really\nmake this as usable as all the third party systems out there. So I use this every day. I use this\nmultiple times a day across multiple different applications, but there are some limits and those\nlimits get to me. Yeah.\n\n\nLuciano: One example that comes to mind when we just want to present a case where this was useful is in a project I worked on recently where in particular workflows, we write\na lot of files to S3. We needed to troubleshoot something related to the creation of these files.\nAnd one of the things we do, we have a common utility library that allows us to write all\nthese files in S3 in a consistent way. And this library also happens to write logs in a somewhat\nstructured way. There is always a line saying something like creating a file called blah in\nthis bucket called blah and the size of the file is this. And what we did, that was the easiest way\nto find the results we were looking for was literally, okay, let's go to CloudWatch Insights.\nWe know we produce all these logs consistently. We know that the time span we are trying to\nunderstand this particular problem. So let's just set CloudWatch logs inside this time span. Let's\njust create a parse expression to find these particular lines and extrapolate the number\nof kilobytes I think we were looking for. So we basically managed to build a query that way\nand figure out, okay, we are creating this many files in this amount of time and this is the size\nof files that we are creating. So this is just a random example of the kind of things you could do.\nAnd it's very flexible because in that case, we didn't even have structured logs, but we were able\nto extrapolate information from text effectively by using the parse functionality.\nThat's nice. We talked about these metric logs before.\n\n\nEoin: So we talked about EMF metrics and then other less structured ones or not JSON structured like StatsD format where you start with monitoring\npipe, the metric name pipe, the unit pipe value. So you could use CloudWatch logs insights to parse\nthat, extracted all the fields, extracted the metric, the value of the unit, and then say,\ngive me the maximum metric value for a five minute value and group by the time segment, but also\nthe user's country code. And that kind of stuff works really well. You can do with EMF metrics.\nAnd if we go back to our metrics episode, we were talking about how CloudWatch metrics\nonly gives you like one minute aggregations. Usually you can't get any finer grain than\nthat unless you have high cardinality custom metrics. But once you've got those metrics\nin your logs, you can query and do really powerful things with this interface, CloudWatch logs insights.\nSo some of the limitations... Sorry, go ahead, Lucio.\n\n\nLuciano: Before we go into the limitation, another useful feature is the fact that you can save the queries persistently. And one of the things we did with one of our customers is that we have an operational\nplaybook for some of the alarms that we have created. And when those alarms fire, we go in\nkind of an incident mode where we need to try to troubleshoot and resolve the particular problem.\nAnd we have in this playbook links to the CloudWatch logs inside page that point specifically\nto particular queries where we have effectively some placeholders that we can fill to try to find\nthe answers on what's actually going on for that incident. So that's another very useful feature.\nIt's something that you can do also, for instance, in Kibana. So it's not like an innovation in the\nmarket, but again, it kind of remarks the point that now that the reason for using a third party\nsystem is becoming less and less relevant, right? Because you can do all these things natively in\nAWS. Yeah, I agree. That's a useful one.\n\n\nEoin: And another useful feature worth mentioning is that you can export the data. So you can export it right now. Interestingly enough, exporting it\nlog lines, you can export to CSV or Markdown. And I don't know why these are the first two chosen\noptions. A text file would have been the immediate first option I'd prefer because I end up like\ndownloading the CSV and turning the CSV into a text file. So maybe this brings us onto some of\nthe gaps and the limitations of CloudWatch logs insights. So in terms of limitations, one of the\nones people cite often saying that it's kind of a barrier for the adoption is the limit of 20 log\ngroups. As you mentioned, it's good that you can query across multiple log groups in a distributed\nsystem, but why is the limit 20? I personally find that it doesn't cause me a problem often because\n20 log groups is generally quite a broad area. But if you compare it to something like Cabana where\nall of your logs are in one place, then it's a difference, right? So you have to be more selective.\nSo it would be good to see the limit raised on that. And it would also be good if you could kind\nof save those groups of logs. So you can run multiple... You could have say a collection of\nlog groups that related to part of your application. And then you can run different\nqueries against that collection of log groups. So another kind of safe feature.\nAnd the other limitation that you can run into sometimes is that the results you're limited to\n10,000 log entries, 9,999 to be precise. But I mean, that's okay if you're browsing in the\nconsole, that's more than you generally want to read. But if you wanted to programmatically\nextract a large volume, then once you hit that limit, you end up having to try and use time\nranges to extend it to another page. There's not an inbuilt support for pagination across\nvolumes greater than 10,000, which is a pity. Yeah.\n\n\nLuciano: And that's, I think, something a little bit confusing when combined with the export data functionality, because it's not obvious that you\nare exporting only that much as a maximum bound. So sometimes you feel like, okay, this is just\ngenerating a big file. It gave me all the data. Now be careful, you might have missed some data.\nIf you hit that limit, it's not going to go more than that particular limit.\n\n\nEoin: So in the user interface, one of the things I miss is when you find some error, for example, often what you want to do and what you can do with some of the third parties is find nearby\nrelated logs. So if you can imagine you're searching across 20 log groups and you found\nan error, then you got a stack trace, but you're only seeing the errors at that point. So immediately\nyou want to say, okay, well, I want to see all the other messages related to that request in that\nLambda function five seconds before and five seconds afterwards. And the only way to do that\nright now is to find the request ID and put it into a new query and to continually start editing\nyour query to do drill downs from an error back to related logs and then into another log stream.\nSo it would be nice to have a little bit more of a user experience improvement where you could\nclick on a request and find your via related logs very quickly, for example.\nYeah, that would be very useful. I think I had a few occasions where I wished I had that feature.\n\n\nLuciano: One thing that I want to remark is that because we mentioned a few times that this is something\nthat can give you capabilities that are very close to metrics because you can get this kind\nof information. But one thing to really understand is that the model of storing this information\nand receiving the data is entirely different when you are using CloudWatch logs inside\nand when you're using metrics. Metrics are already highly optimized for quick retrieval\nand quick aggregation, just the way that this data is stored. Logs are effectively not. You\ncan imagine them as being text files somewhere. And every time you run a query, you are literally\nscanning through all these text files. I know that AWS is probably very, very smart because\nyou get results in pretty reasonable time. So probably they parallelize this data in some very\nefficient way. But nonetheless, you are scanning through large amounts of data. And actually,\ninteresting enough, from the UI, you can see how much data you are actually scanning for every\nsingle query you run. And the reason why I wanted to remark this point, which is maybe not so\nobvious, because of course, this has a direct impact on cost because of course, every query\nis not necessarily cheap depending on how much data you are scanning.\n\n\nEoin: Yeah, it's actually one of the few areas in the AWS console where you can see the billing units update in real time because the volume of data scanned is being updated multiple times.\nMultiple times per second as the query is running.\n\n\nLuciano: Yeah, one thing actually I found myself doing very often is when I'm building a new query, of course, I'm not confident in my log insight skills as you probably are. So it takes me a while,\na little bit of trial and error before I fine tune my query to do what I want to do.\nSo I just run a very generic query, but then I try to stop it as soon as I see some results.\nBecause of course, it doesn't make sense to keep it going for a large amount of time if I know this\nis not going to be my final query. So I kind of progress in small increments and I found it very\nnice that you can stop the queries before you actually keep scanning gigabytes and gigabytes\nof logs. So that's a neat trick and it's good to see that indicator going up that reminds you,\nyou maybe don't want to pay for this query if you don't have your final query right there.\n\n\nEoin: Yeah, and the queries can run for up to 15 minutes, right? So that gives an idea of the\nvolumes it can process, but also the potential cost. So maybe that's a good segue into the\npricing topic. I know that you can scan up to five gigabytes of data for free in the free tier.\nAfter that, we're looking at 57 cents for ingestion of data into CloudWatch logs, right?\nSo this is, sorry, this is storage, right? So storage and ingestion, over 50 cents to ingest,\nand then 3 cents per gigabyte per for storing your logs. So you can compare that to your third\nparty log aggregator and see how the cost compares. Then when it comes to log insights queries,\nthere's a price per gigabyte scanned, which is like just over half a cent per gigabyte.\nSo you can imagine terabytes that starts to escalate. It kind of scared me a little bit when\nI saw this first and when I started running queries at log insights for the first time,\nthe fact that all of a sudden you start running queries, you can run into big bills.\nIt hasn't materialized in any kind of bill shock yet. I found that the cost, especially compared\nto the value when you're troubleshooting and looking for insights with issues,\nI found this to be a good value for money feature personally. It's one of the areas where I wouldn't\ngripe too much about the pricing.\n\n\nLuciano: Yeah, especially if you already have done your work in terms of structuring metrics and alarms, or you already have other indicators for what you're looking for,\nyou can probably just be very selective, for instance, with the time ranges.\nAnd that will limit the amount of logs you're going to be scanning every time you run a query.\nYeah, yeah, absolutely. Yeah. And I would say compare it to the third party options out there.\n\n\nEoin: Some of them may offer a much cheaper option depending on your usage, because it would be down\nto volume ingested, volume storage, the retention, and then also other third party options might be\nrelated to the number of users you have on the system as well. So there's different dimensions\nto consider. And here might as well vary.\n\n\nLuciano: Yeah, maybe one final topic before we wrap up this episode is to discuss, I don't know, suggestions or tips that we might have in terms of\nwhen you produce your logs, is there anything you might do that will make your life easier\nin the future? Right? Do you have any recommendation in that sense?\nI would always start with using structured JSON logs.\n\n\nEoin: And I think this has been a major improvement when it comes to being able to run queries. It means you don't have to parse\nlogs yourself. So previously there used to be kind of a trade-off between human readable logs\nand structured logs. I think now people tend to favor structured logs because it's much easier\nto query and parse and programmatically interpret. And if you need to present them for human\nreadability, you can process them. What do you think is that? Would you also agree that structured\nlogs are the way to go? I would agree.\n\n\nLuciano: And for a long time, I've seen those Apache Access Log format standards, which I think is exactly what you described. It was a good compromise\nbetween readability, but also something that you can easily parse. But of course, that comes with\nthe problem that once you have the kind of standard, the standard is very limited. There\nis only so many fields that the standard is giving you. While in real life, if you want to go outside\nthe domain, for instance, of access logs, you probably want to log a lot more details in terms\nof fields and attributes and things that you care about for troubleshooting. So going with JSON is\nkind of an obvious strategy at that point, because we have total freedom of logging all sorts of\ndifferent fields that might make sense for your application. And then, as you said, CloudWatch logs\ninside will already have parse every single field for you. And those fields are already available\nfor you to query. So that's a great way. And I've seen even web frameworks starting to go in this\ndirection. For instance, if you use loggers like Pino in the Node.js land, Pino will automatically\ngive you JSON logging, but also has a bunch of utilities that allow you to pre-deprint those\nJSON logs. So they are kind of coming at the problem from the two different angles of let's\nmake it useful and easy to process. But at the same time, if you need to read those logs, you\nhave all the tooling that allows you to do that. Yeah, I love Pino. It's a really good logger.\n\n\nEoin: And I've used it a lot for things like Lambda functions. So I know that Pino allows you to say\nhave nested loggers and to put contextual data into your logs as well. So what kind of\nadditional data helps you to get better results when you start querying later?\nYeah, I think definitely given that in AWS, we are building more and more distributed systems.\n\n\nLuciano: One thing that you would definitely need to make sure you have is a correlation like this.\nSo for every log, if that log is something that you can consider part of a transaction in a way\nor a request from a user and you have a unique ID for that particular transaction or request,\nmake sure that it is propagated through all the different components that are trying to satisfy\nthat particular request. Because at that point, if you have a correlation ID for something that\nwent wrong, maybe a specific request failed, you can easily query and get a unified view of all\nthe logs with that particular correlation ID. And that's literally just one filter looking at one\nfield where you know correlation ID equal a specific value. So that's something I found\nextremely useful, especially recently to troubleshoot particular issues.\nIt takes a little bit of diligence to make sure that you have that information is propagated\ncorrectly everywhere. But as you say, if you use loggers like Pino, they have built-in features\nthat can help you to make sure that information is correctly propagated everywhere. Other things\nis similarly you can have trace IDs if you're using tracing like X-ray or maybe if you use\nopen tracing, you can also put that trace information in your logs and that can help you\nout to correlate logs with traces. I remember for instance, one thing I really liked from using\ndata log in the past is that they push you to do that. And also when you look for traces,\nfor every single trace, you see like a window with all the logs that have the same trace ID.\nAnd sometimes that can be very useful. So hopefully we'll have something similar eventually\nin CloudWatch. I don't know if it's already possible in some other way, but.\n\n\nEoin: I know that the service lens part of the AWS console is a going in this direction, but I still, I haven't really played with it very extensively. So I know the idea is to show you all\nthese things at the same time, but I don't know if it's up to the level of some of the third parties\nout there with a really slick responsive user interface for that. I still tend to do that.\nDo things manually and dive from one tab to the other to correlate things.\nYeah.\n\n\nLuciano: One last thing to be aware about is that of course, when you log in a structured way, you might be very tempted to just log entire objects without even thinking like,\nwhat are you actually printing? Right. Because it's like, okay, just, if something bad happens,\nchances are, I want to see these entire objects. I think it's a reasonable way of thinking about\nlogs, but be careful with that mindset because you might be end up logging sensitive information.\nWhat happens if it's, I don't know, a payment lambda, and you might end up logging credit\ncard details or personal information about the user. So there are ways, and again, it comes to\ndifferent libraries. There are ways that you can anonymize this data before it gets logged.\nSo just, I don't have like a silver bullet type of solution for you, but just be aware of the\nproblem and check out different libraries and what kind of support they give you and try to see if\nthey can be applied to your use case.\n\n\nEoin: Completely agree with investing some time in X-Ray and trying to get the trace IDs in there. Cause I really like the way X-Ray really kind of\ncovers all the services now and gives you a very, very good view. Not only is it good for\ntroubleshooting, but it's also in distributed world when you've got lots of tiny components\nrunning together and event-based communication with each other, it's sometimes very difficult\nto visualize your architecture and keep your architecture diagrams up to date. But X-Ray is\nactually good for that too, because it will almost, the diagrams emerge from your traces\nand then you can get very good performance data as well as troubleshooting for errors in X-Ray too.\nAnd then if you link it to your logs, you've got a very, I would say high level of maturity when\nit comes to troubleshooting.\n\n\nLuciano: Yeah, I think the ideal goal is that if you imagine, like you are giving a user-facing experience, like somebody calling an API or loading a webpage, if you could\nkind of leave the same experience that you gave to your user, to your observability stack, I think\nthat's the ultimate dream where you can literally say, okay, this is what was happening. And this\nwas the speed and this was the components involved. And these are the logs for the component.\n\n\nAnd then you see that the entire architecture respond into that request and everything that\nhappened. I think we are getting close. I think these days we have all the tooling, maybe takes\na little bit of effort in configuring everything correctly, but I think that this kind of\nobservability dream is not a dream anymore. It's something that it is achievable with a little bit\nof effort. So definitely something to look forward, especially for heavy user-facing products where\nyou really want to make sure you are always giving the best possible user experience.\nExactly. And it is part of the well-architected framework to get this level of observability.\n\n\nEoin: And it's probably the days of looking in a single access dot log with grep and awk are well behind\nus with a distributed serverless architecture or microservices architecture. So to address some of\nthe complexity that these modern architectures give you, you have to fill that gap with good\nobservability. So it is worth investing the time and probably upfront actually. So if you're\nstarting a new project, adding X-ray, structured logs, metrics, the tools, the plugins we talked\nabout in the previous episode, they're all there and it's pretty low barrier to entry to get going.\nIt takes more time if you're retrofitting it to an application you've already been running in\nproduction for a year or more. Yeah, I totally agree with that.\n\n\nLuciano: So yeah, I think with this, we have covered probably a lot. So this is probably a good point to try to summarize this\nepisode and then finish up. I don't know if you're feeling that you are now closer to be a CloudWatch,\nLog Insights Ninja or Log Ninja in general. Probably not, but nonetheless, I hope that we\ngave you a lot of good suggestions and that you can get value out of them. And by the way, as with\nany other episodes of this podcast, if you think that there are better ways to do the things that\nwe are recommending, definitely let us know in the chat or comments in whatever platform you're\nusing or reach out to us on Twitter, because we'd be more than interested in having a conversation.\nWe are sure we can always learn a thing or two from our audience. So thank you very much for\nbeing with us. We definitely recommend you to check out the metrics episode if we haven't done\nit already and we'll see you at the next episode. Bye.\n"
    },
    {
      "title": "36. What’s new for JavaScript developers on AWS?",
      "url": "https://awsbites.com/36-what-s-new-for-javascript-developers-on-aws/",
      "publish_date": "2022-05-12T00:00:00.000Z",
      "abstract": "There are lots of options for programming languages on AWS these days but one of the most popular ones remains JavaScript. In this episode of AWS Bites we discuss what it’s like to develop with JavaScript, Node.js and TypeScript on AWS and what’s new in this field.\nWe explore why you would choose JavaScript and what are the trade-offs that come with this choice. We present some of the main features of the all-new AWS SDK v3 for JavaScript. We discuss runtime support and tooling for AWS Lambda and finally some interesting developments in the JavaScript ecosystem for the cloud and AWS.\nIn this episode we mentioned the following resources:\n\nOur previous episode on What language to use for lambda\nAI as a Service by Eoin Shanaghy and Peter Elger (book)\nNode.js Design Patterns by Mario Casciaro and Luciano Mammino (book)\nAWS SDK for JavaScript v3 high level concepts (including command based model)\nAWS SDK for JavaScript v3 paginators using Async Iterators\nMocking support for the AWS SDK for JavaScript v3\nVarious interesting benchmarks on different Lambda runtimes\nVarious interesting benchmarks on different Lambda runtimes, by Aleksandr Filichkin\nVarious interesting benchmarks on different Lambda runtimes, by Jignesh Solanki\nSupport for ESM modules in AWS Lambda (Node.js 14)\nThe Middy Framework (middleware pattern for AWS Lambda)\nLambda Power Tools library for TypeScript\nYan Cui’s article on performance improvements with bundling\nZX project (scripting with JavaScript) by Google\n\n",
      "transcript": "Eoin: There are lots of options for programming languages on AWS these days,\nbut one of the most popular ones remains JavaScript.\nToday, we're going to discuss what it's like to develop with JavaScript\nand Node.js on AWS and what's new in this field.\nSo we're going to talk about why you'd use JavaScript\nand what are the trade-offs,\nwhat are the features of the all-new AWS SDK version 3.\nWe'll talk about runtime support and tooling for Lambda,\nand we'll talk about all the new developments in the ecosystem.\nMy name is Eoin, and I'm joined by Luciano,\nand this is AWS Bites.\nLuciano, I think you're well-placed to talk about this as a Node.js expert.\nYou have your book, the Node.js Design Patterns book,\nwhich is really popular.\nIt's also very up-to-date on all the latest things you can do with Node.\nAnd not only that, you're also the author of the MIDI framework\nfor Node.js on Lambda.\n\n\nLuciano: Yeah, but I think... Thank you for doing all that advertisement for me.\nAlways appreciated.\nBut I think you also deserve a little bit of advertisement\nbecause you are the co-author of this book, AI as a Service,\nwhich is effectively a book that talks about serverless on AWS.\nBut I was particularly impressed by the choice of using JavaScript\nas basically the main language for all the code examples.\nSo I think you are also heavily invested in the JavaScript ecosystem\nwhen it comes to AWS.\nSo maybe I would like to start by getting your opinion\non why do you think that, yeah, it's actually a good choice,\nor not maybe, to use JavaScript on AWS.\n\n\nEoin: Yeah, well, I mean, both of us use JavaScript pretty heavily on AWS,\nbut we're also fairly well used to using Python and other languages.\nI guess the thing about JavaScript is that it's well supported\nand widely used in all the tools and tutorials,\nso it's a pretty safe choice,\nespecially for the serverless type of development.\nIt's also, of course, the fact that full-stack JavaScript\nif you're doing front-end work as well,\nit means you have a more reasonable skills requirement\nand you can do JavaScript all the way down.\nBut of course, there are trade-offs.\nGenerally, I think JavaScript gives you a pretty good balance\nbetween performance and speed of iteration as a developer.\nAnd it's also just a really good fit for AWS Lambda.\nMaybe we could talk about that a little bit later.\nBut given that we're trying to focus on a lot of the new developments\nin the ecosystem and on AWS,\nI know that you've done a lot of work recently with the new AWS SDK,\nwhich is version 3, and it's a complete rewrite.\nSo what's all of that about and what's new for everybody?\n\n\nLuciano: Maybe a lot of work is an overstatement, but I did give it a spin and I quite like it.\nSo I'd like to share a few things that I was positively impressed by.\nWell, first of all is that you get very good support\nfor both JavaScript and TypeScript,\nmeaning that all the functions and methods that you use have good types.\nSo if you use TypeScript, you get that advantage,\nauto-completion, type checking and so on.\n\n\nBut of course, you can use it also in plain JavaScript\nand everything works as expected.\nOne interesting change, and this is pretty big,\nI think people will see the difference\nin just writing even the simplest integration, I don't know,\nwriting a file to S3 or sending a message to a queue,\nis that now the entire API is using a pattern called the command pattern.\nSo you don't... you instantiate a client, let's say an S3 client,\nand then rather than saying client.putObject, for instance,\nif you want to write to S3,\nnow you have to create a put object command\nand then you can send that command through the client.\n\n\nAnd this is, I guess, a little bit unusual at first\nwhen you come from the version of the API,\nbut there are some advantages.\nFirst of all, that you can import only the commands\nthat you actually need to use.\nAnd we'll talk a little bit more about tree-shaking,\nbut that's something advantageous\nif you want to bundle your code and do tree-shaking.\nAnd also you get good typing for every single command.\n\n\nSo it's basically an easier way to just define the kind of action\nthat you want to use and understand how to use them.\nAnd we'll put links in the show notes\nif you want to find the documentation\nand understand more how that changes the way you write your code.\nOther interesting things, and these, to be fair,\nto some extent were available also in V2,\nbut I think that they have improved the level of support in V3,\nis that the JavaScript SDK in general tends to be very idiomatic,\nlike rather than just being... I don't know,\nyou don't get the feeling that you are using a pound-for-pine conversion\nof a Java client or something like that.\n\n\nYou really get the feeling that that library\nwas natively written for JavaScript,\neven though if you look closely into the code,\nmost of the code is automatically generated\nby more high-level definition of the AWS APIs.\nBut still, I think the developers put a lot of effort\nto make sure that the code generated is actually as idiomatic\nas it could be for JavaScript developers.\nAnd two things that I really like in that sense\nare support for Node.js streams in a bunch of places.\n\n\nThe best example to me is S3,\nbecause very often you need to read or write files\nthat could be big enough in the order of gigabytes.\nAnd if you've done any Node.js, you know that you have an odd limit\nwhen it comes to two gigabytes.\nYou cannot put all the stuff in memory in one go.\nSo you need to find other ways to deal with a lot of data.\nAnd the canonical way is to use Node.js streams.\nSo when you do a strip put object,\nthe body of that request could be a stream.\n\n\nSo you could be generating data at runtime,\nor you could even process data at runtime.\nA very good example is you're trying to write something to S3\nand you maybe want to compress it and maybe even encrypt it as well.\nYou could do all of that in a streaming fashion\nand as you write, do all the processing to compress and encrypt.\nAnd similarly, when you read from S3, you can read as a stream.\nSo that allows you to, as soon as you consume\nthe first few bytes of the file,\nyou can already start to process them, send them somewhere else,\nbuild pipelines based on the data you're fetching.\n\n\nSo that can also give you a good boost in performance\nbecause you're not waiting to load the entire file\nbefore starting the processing.\nAnother interesting one is support for async iterators,\nwhich comes very handy, especially when it comes to pagination.\nAnd there is an entire API for pagination.\nAnd I don't know, the classic example could be DynamoDB, right?\nYou want to scan a table, you're going to be doing that in pages, I suppose.\n\n\nYou build a paginator and it gets the first few records.\nThen if you want more, you go to the next page and you keep going that way.\nThere is one construct when you use async iterators,\nwhich is the for-await loop.\nSo if you've never seen it, it looks like for-await const page\nof an object, maybe a paginator.\nAnd that basically creates a loop for you,\nwhere inside the loop you have this concept of a page.\n\n\nBut the loop, even though it's an asynchronous loop, it's kind of blocking in the sense that it's not going to continue the iteration until the next page is available.\nSo it's managing all that asynchronicity for you.\nAnd your code looks synchronous, even though you are still\ntaking advantage of the asynchronous model of JavaScript.\nSo that's something that I really like and also allows you to handle\nasynchronous error with try-catch blocks,\nso makes also error handling much easier.\n\n\nAnd one last note that I have about the SDK v3\nis that there is built-in support for mocking,\nwhich comes very convenient when you want to do unit tests.\nNow, built-in is a little bit of an overstatement.\nIt's an additional library that you need to install,\nbut that library is maintained by AWS,\nand I'm quite sure they tried to keep it in sync\nwith the evolution of the SDK.\nBut the cool thing is that once you import this library,\nyou basically get a set of mock objects\nthat you can import for every single client,\nand then you can easily specify in your test.\n\n\nIf your test, for instance, if your code depends on,\nlet's say, I don't know, SQS, you can say, OK,\ncreate a mock SQS client and simulate\nthis particular behavior.\nMaybe it fails to write, maybe returns a specific response.\nSo all this stuff is very useful,\nand before, with the previous version of the client,\nyou were a little bit on your own\nto recreate all these types in your test\nand simulate those behavior.\n\n\nOh, yeah, there is one more thing that I almost forgot.\nThere is another additional feature,\nwhich is something we saw in Python before,\nbut in Python, it's not well-documented,\nI have to say.\nBut it's basically the ability to create custom logic that\ncan run before or after the client actually sends\nthe HTTP request to AWS.\nSo in the SDK v3, they actually formalized a little bit\nthis thing into the middleware pattern.\n\n\nSo basically, you can write code that wraps around,\nlet's say, the HTTP request going to AWS,\nand you can use that to either enhance the request going\nto AWS or manipulate the response coming back to AWS.\nSo you can add behaviors like, I don't know,\nadd compression to a message that is going to AWS,\nand that can be convenient, for instance.\nSomething we mentioned in a previous episode,\nwhen you call CloudWatch put metric data,\nyou have very restrictive payload size limits.\nSo in that case, you can leverage compression\nto go a little bit over those limits\nto be able to push more data while staying\nbetween the boundaries.\nSo that's something really cool.\nI haven't used it yet, but I had a look at the documentation,\nand it could be useful in cases like this.\nYeah, that's a big spiel about the SDK v3.\nMaybe we should talk about serverless and Lambda in general.\n\n\nEoin: For Lambda, yeah, I think we mentioned\nthat Node.js is a good choice for runtime on Lambda.\nIt's been around for a very long time.\nIt's been supported from the very beginning with Node.js.\nSo it's, I would say, a first-class citizen,\nalthough you have lots of options these days,\nas well as custom runtimes.\nIt's also one of the most performant runtimes,\nespecially for scripting languages.\nSo you'll get good cold start times.\n\n\nAnd there's lots of useful benchmarks out there\non Lambda cold start times in general\nfor lots of different languages.\nI think, you know, languages like Go and Rust\nare almost leading the way in terms of compiled languages,\nbut you get very good performance,\nand I have to say I'm pretty pleased with it in general.\nAnd there's lots of ways you can optimize it as well\nwhen it comes to squeezing those last few tens\nof milliseconds out of your cold start times.\n\n\nSo we can link, actually, in the show notes\nto some really good benchmarks that you can take a look at.\nSome of the things that you've mentioned there,\nthere's some new language features.\nLike we have ESM modules now.\nSo we've moved away from common JS modules\nto ESM modules with Node.js,\nand they've been supported since Node 14.\nI think there was a little bit of a bumpy road\nin terms of supporting them when Node.js 14 support\nwas added to Lambda, but it is possible now.\n\n\nAnd you can also do things like top level await.\nSo I think async await is very widely adopted\nand almost de facto at this stage\nfor asynchronous programming in Node.js,\nbut one of the things in Lambda handler\nyou'd like to do often is fetch some\nmaybe configuration parameters from SSM parameter store\noutside of your handler logic\nso that it's done in the cold start phase\nand you don't incur the penalty for each event.\n\n\nAnd that's something you can now do with async await\nwith, you know, if you're using the AWS SDK\noutside of your handler functions, that's pretty nice.\nI also believe that support for Node.js\nis this is just like hot off the press.\nIt's either on its way.\nI don't think it's been officially announced by Node.js,\nbut I can see that, you know,\nVercel have added their functions built on Lambda\nand it's serverless framework has added a support\nand that's in mainline now as well.\nSo you can actually deploy server Node.js 16 functions\nas of today.\nWe've also got lots of, I suppose,\necosystem support within Lambda as well.\nSo we mentioned that one of the things you did\na number of years ago and has been really growing\nat pace since is create the MIDI framework,\nwhich is a middleware engine for AWS Lambda written in Node.js.\nDo you want to give us a brief update,\na brief explainer for people who haven't encountered MIDI?\nWhat kind of problems does it solve?\nWhy is it useful in Lambda?\n\n\nLuciano: Yeah, absolutely.\nSo when I started to write my very first few Lambdas,\nI realized that because you have this such small unit\nof code, like you think more in functions\nrather than services, most of that,\nwhen you're actually writing the code,\nwhat tends to happen is that you have a little bit\nof boilerplate always before and after\nyour main business logic for that Lambda function.\nSo that boilerplate could be for instance,\ndoing authentication, doing validation of the input,\nmaybe fetching configuration from SSM\nand then eventually you get to run your actual\nbusiness logic and then you might be ready\nto produce a response,\nbut you might want to do a few additional things\nbefore the response is sent back to the user,\nmaybe normalize that response.\n\n\nCan make sense if you're using the HTTP integration\nwith API gateway or you want to have error handling\nin a more generalized way.\nSo what I realized after probably the first year\nthat I was writing Lambdas for a project is,\nokay, I have all the Lambdas where it's very hard to tell\nwhat's the actual business logic\nand what's the boilerplate around it.\nAnd I was told that the whole promise around Lambdas\nwas to focus more on the business logic.\n\n\nSo I was a little bit disappointed in that sense.\nAnd with the team I was working with at the time,\nwe figured out that we would use the middleware pattern\nto try to isolate a little bit more all this boilerplate\ncode into their own function and then keep your handler\nas pure as possible, so just the business logic,\nand then use the handler as a function where you could\nattach the boilerplate behavior that you wanted.\nBut basically the function will read as everything is clean,\neverything has been already done for you,\njust do the business logic,\nand then you attach everything else in another place.\nAnd that also makes these behaviors,\nthese additional behaviors or boilerplate if you want,\nmore isolated and therefore testable, reusable, and so on.\nSo that's kind of the use case for MIDI,\nand MIDI is basically a framework that allows you\nto do this thing in an easy way,\njust focus on the business logic\nand attach the additional behavior afterwards.\n\n\nEoin: Yeah, I remember being in that situation\nwhere you had all this boilerplate to parse your HTTP body\nand then create the status code\nand the headers on the way out.\nIt was really nice to be able to just add in MIDI\nand solve a lot of those problems with the middlewares\nthat you can add in from the community.\nI also know that this is part of something new,\nwhich is the Lambda Power Tools for TypeScript.\nAnd we might've mentioned the Lambda Power Tools for Python\na few times on previous episodes.\nI believe the TypeScript version is due to come out\nin its stable version very soon, version 1.0.\nIt has been available as a preview beta for some time.\nThat builds on MIDI and also allows you to do lots of stuff\nlike logging metrics and traces as well\nfor your Node.js Lambdas.\nSo we might put a link to that in the show notes as well\nand people can check it out.\n\n\nLuciano: Yeah, absolutely.\nAnd I think it's another very useful utility\nwhen you care about making sure that your Lambdas\nare basically production ready.\nLibraries like Power Tools can make that process much easier\nbecause they have baked in a lot of defaults\nthat you don't have to rebuild on your own basically.\n\n\nEoin: Yeah.\nAnd speaking of TypeScript,\nbecause that's called Lambda Power Tools for TypeScript,\neven though it works with JavaScript as well,\nthere may be a question people will have is,\nshould I use JavaScript or TypeScript\nwhen I'm building Node.js functions on Lambda?\nAnd even within JavaScript,\ndo we just deploy vanilla raw JavaScript\nto our Lambda functions\nor should we be using some sort of bundling\nlike esbuild or webpack to optimize the functions\nin the deployment package?\nWhat do you think?\n\n\nLuciano: Yeah, this is very opinionated,\nbecause I think there are three types of people\nwhen it comes to TypeScript,\nthree types of JavaScript developer, I would like to say.\nOne that is like always use TypeScript.\nThe other one is like, no, no, TypeScript is a bad idea.\nAlways use poor JavaScript.\nAnd then there is the camp in between like, yes,\nuse TypeScript, but not always.\nI am a little bit in that kind of moderate ground\nwhere I like TypeScript, but with moderation.\n\n\nI think the really good use case for TypeScript\nis when you do a lot of complex data manipulation.\nSo if you put the effort in defining all the types,\nthen you definitely get the advantage\nthat every time you are manipulating the data,\nthe type checker can prevent you from doing mistakes.\nAnd this is something that happens all the time,\nlike a type of undefined,\nand you need to do all these checks.\n\n\nYou cannot call undefined functions.\nAll this kind of problems will go away\nin the sense that the TypeScript compiler,\nif you did a good job with your typings,\nwill tell you before you actually run your code\nthat there might be something wrong.\nSo definitely use TypeScript\nwhen you are in this kind of situation\ndoing a lot of heavy data transformation\nwhere you have complex types,\nwhich I think in Lambda happens a lot\nbecause most of the time you are writing glue code.\nSo often you need to kind of map types from one system\ninto type for another system.\n\n\nEoin: Yeah, I've seen that.\nAnd sometimes it's useful to have,\nif you're doing HTTP proxy integration,\nhaving the types for the event that you know you'll receive,\nthat can be very useful,\nbut I guess you can take advantage of those\neven if you're using JavaScript.\nI'm somewhat conflicted when it comes to\nusing TypeScript in Lambda\nbecause I often think Lambda functions\nshould be small and simple as possible.\n\n\nAnd if you need to have that level of typing support,\nmaybe you have too much complexity,\nbut that can't be a hard and fast rule\nbecause it's not always that simple\nand you can benefit from those typings in some cases.\nBut even with bundling, I often think\nbundling adds an extra bit of development tooling complexity.\nSo if you want to reduce your package size,\nyou can definitely benefit from bundling,\nbut that means now you've got a minified JavaScript running\nand you need to make sure that your stack traces\nare readable from a developer when you're troubleshooting.\n\n\nSo you need to ensure that you've got source map support\nin there and all of that stuff,\nas well as the fact that bundling adds\na little bit of a latency to your build.\nSo it's a trade-off, right?\nThere was a really good benchmark exercise\nthat Yan Shui did about AWS Lambda performance\nand the trade-offs with adding bundling support\nand how that benefits your cold start time.\nSo there is a benefit there.\n\n\nIt's just a question of what's the penalty\nyou incur at build time as a developer.\nHow does that matter to you?\nOften it depends on how fast\nyour developer machine is even.\nSo there's lots of different factors to weigh up there.\nAt this point, should we talk about\nsome of the other things, parts of the ecosystem\nthat are relevant for JavaScript and Node.js developers?\nI think we mentioned one of the statistics recently\nabout 70% of CloudFormation being deployed\nby the serverless framework.\nSo serverless framework is pretty ubiquitous\nwhen it comes to building and deploying Lambdas.\nIt's written in Node.js itself.\nHave you found as well that writing Node.js functions\nin serverless framework is always\na little bit better supported\nor feels more like a first class citizen\ncompared to writing other languages\nand deploying them with the serverless framework?\nI think so.\n\n\nLuciano: And I mean, the first reason is because\nI think the documentation is probably\nthe first one to come out every day.\nThere's a new feature, I think,\nthat the first thing that they publish\nis the JavaScript version.\nSo maybe that's one reason, but I suppose because\nthe tool itself is written in JavaScript.\nProbably the majority of developers in the team\nknow JavaScript better than other languages.\n\n\nEoin: Yeah, and from a packaging perspective, it kind of integrates seamlessly with NPM\nand your Node modules directory\nand you don't need additional tooling\nto package up your zip.\nExactly.\nI guess that makes sense.\nOne of the other tools, of course,\nthat's well supported with JavaScript and TypeScript\nis CDK.\nSo we've covered that CDK in a previous episode\nin reasonable detail.\nAnd although it supports a lot of languages,\nI think it's a little bit more user friendly\nor developer friendly when you're using JavaScript\nand TypeScript.\nBecause I believe the way it's written\nwith the JSII library is that it's essentially\na TypeScript implementation with bindings\nfor other languages.\nSo I think it's, yeah, it's a good option.\nI really actually like one of the areas\nwhere I really enjoy using TypeScript\nis when I'm using CDK\nbecause the tooling integration is so seamless.\n\n\nLuciano: Yeah, there is another tool\nthat I would like to give it a shout out\neven though it's not exclusively something\nthat you will use in AWS.\nIt's kind of an interesting project from Google.\nIt's called ZX.\nAnd the idea is that sometimes writing bash scripts\nis tedious or at least I would agree with that statement.\nMaybe not everybody agrees with it.\nSo the idea is like, okay,\nbecause JavaScript and Node.js\nare becoming more and more ubiquitous\nand you probably use them\nin different parts of your application.\n\n\nWhat if you could write scripts that are JavaScript\nwhereas in the past you would have been using bash?\nSo the idea is how do we make that easier for people?\nBecause of course you could have done that all the time,\nbecause at the end of the day, those are just scripts\nand you could do whatever you want with those scripts.\nBut I suppose the advantage of bash\nis that it's very easy to run all their commands,\nall their processes with what we have in the system\ncalling all their executables.\n\n\nSo what ZX did,\nand this is probably the primary features,\nthey made very easy for you to do that\nin a JavaScript language effectively.\nSo you can use dollar and backticks.\nSo they are basically exploiting the feature\nof template literal strings.\nAnd what they say is, okay,\nif you tag a literal string with the dollar,\nthen we are gonna execute the string for you\nas a command in the system and give you back a promise\nthat tracks the execution of that command.\nSo that way you can easily write a script in Node.js\neffectively, but also call other processes\nwith like one liner.\nSo that seems to be kind of a decent replacement for bash.\nAnd I suppose in the context of doing automation\nand deploying projects to the cloud,\nthat could be something convenient\nif you really want to be like 100% JavaScript end to end.\n\n\nEoin: And that's pretty nice.\nHow does that work then with,\nbecause one of the primary features of Node.js\nis it's asynchronous nature\nand you have either callbacks or promises and await.\nHow does that fit into ZX?\n\n\nLuciano: So basically you,\nit's like you have top level await out of the box.\nActually you effectively have top level await\nout of the box.\nSo you, every time you call a command\nwith this kind of dollar backtick syntax,\nyou need to of course await that command.\nIf you care about our result\nbefore you continue your execution,\nyou could also use that for instance with promise all,\nif you want to kind of do a bunch of things concurrently.\nSo it just makes it very nice to,\nif you are used to of course the JavaScript patterns,\nbecause if you are more used to bash than to JavaScript,\nI would probably recommend stick to bash.\nBut if you're, exactly.\nBut if you're more used to JavaScript,\nyou get something that is as close as possible\nto the experience of writing bash\nwhile keeping the JavaScript syntax.\n\n\nEoin: That's really good.\nOkay, well, maybe that's a good point to wrap up.\nIt might be useful if people have other suggestions\naround great tips, tooling, improvements in JavaScript\nthat are relevant for AWS developers.\nLet us know in the comments.\nAnd you can also reach out to us on Twitter\nto either Luciano or myself.\nOur links are all in the show notes.\nAnd maybe the last thing I do\nis just point people back to episode four.\nWe discussed, I suppose the trade-offs\nof using all the different languages for AWS Lambda,\nincluding JavaScript, but all the other options as well.\nSo feel free to check that out\nand we'll see you in the next episode.\nbe.\nThanks for watching!\n"
    },
    {
      "title": "37. How do you migrate a monolith to AWS without the drama?",
      "url": "https://awsbites.com/37-how-do-you-migrate-a-monolith-to-aws-without-the-drama/",
      "publish_date": "2022-05-19T00:00:00.000Z",
      "abstract": "Migrating monoliths to the cloud can be a scary, expensive and time consuming and time consuming activity. Many companies try to avoid it even if it could be beneficial for them, just because they think it would require too much work and be too risky. But there are interesting compromises and incremental approaches that can be used to simplify and de-risk this kind of migration. The idea is that you don’t necessarily have to dramatically re-engineer your application to move it to the cloud (and start to take advantage of it).\nIn this episode, based on an InfoQ article that we recently published, we discuss a fictional use case where a company with a monolithic application managed to move to the cloud with a minimum amount of change. The move to the cloud has brought more scalability and resilience for the company to move forward and expand. But it also brings new challenges and opportunities. We will discuss all of this in more detail and by the end of this episode you should have a checklist for migrating monoliths to the cloud with minimal effort.\nIn this episode we mentioned the following resources:\n\nInfoQ article “A Recipe to Migrate and Scale Monoliths in the Cloud”\nOur previous episode about other cloud migration strategies\nOur previous episode about the difference between CloudFormation and Terraform for infrastructure as code\nOur previous episode about the pros and cons of CDK for infrastructure as code\n\n",
      "transcript": "Luciano: Migrating monoliths to the cloud can be scary, expensive, and time-consuming, but you don't have to massively re-engineer your application to do that.\nToday, we are going to present a case study and a potential strategy to move a monolith to AWS with minimal drama.\nWe will discuss how a typical on-premise, three-tier web application can be migrated to AWS and made scalable and resilient.\nWe will discuss some of the steps that you can take to make that happen.\nAnd finally, we will present some of the new challenges, but also opportunities that come once you shift an application to the cloud.\nMy name is Luciano, and today I'm joined by Eoin, and this is AWS Bites podcast.\nLuciano, this is based on an article you wrote that's now available on InfoQ.\n\n\nEoin: It's called a recipe to migrate and scale monoliths in the cloud.\nWe'll put a link to that article in the show notes.\nI think it's really good because it's a very clear process on how you think about this kind of migration.\nThere's also a really good case study that gives a context for all the steps that follow, and there's a really good checklist in it.\nMaybe we can start with that case study.\nYou talk about a fictitious legal company.\nWhat's the story with that? What's the context?\nYeah, it's a fictitious company, but a reality that kind of company and the kind of project reflects the reality of many, many projects that I've seen in my career.\n\n\nLuciano: And even projects that we are seeing every day at Forteorum.\nSo I think it represents very well a good class of solutions that are still out there and that can benefit from moving to the cloud.\nIn this particular case, just to set the stage, we can imagine that we have this startup that operates in the legal space.\nThey've built a CMS for legal practices.\nSo you can imagine that they offer this product to legal practices, and what they can do with it is that every practice can upload their legal documents.\n\n\nAnd there is like a search index mechanism that happens behind the scenes, and then people logging in in the system, they can use keywords to find documents that have been uploaded before.\nSo it's effectively a way to make legal documents easily searchable within the context of a legal practice.\nAnd we can assume that the current solution that exists today, like let's call it the MVP for this startup, is something built on premise in a very standard fashion.\nIt's like a three tier web application where you have a front end, a back end, and a database.\nAnd we can imagine just for reference that, I don't know, the technology can be Python.\nSo maybe they're using Django as a web framework and the database can be a relational database, let's say Postgres, just to mention one technology as a reference.\nSo that's the system that we are operating in right now.\nYep, it sounds very familiar.\n\n\nEoin: And I suppose that brings the question, so why do they have a problem? Why do they need to migrate? And what's the background story there? What are they actually trying to solve by migrating to the cloud?\n\n\nLuciano: So right now, the whole application is hosted on premise on one machine. So everything is running in this like one monolithic server.\nAnd that has been working fine for the MVP. But of course, we know that it's not something that scales long term. And right now, this company is starting to grow a little bit because they released this MVP.\nThey are working with sales to get new customers. And it turns out that they have been very lucky. They got quite a big legal practice that wants to try out this platform.\n\n\nSo what's happening is that suddenly they have a bunch of new users using the system all at the same time.\nAnd that's creating a lot of additional stress to the servers. So there is too much load on one machine.\nThe whole application is sometimes slow and unresponsive, sometimes even unavailable.\nAnd the other thing is that this is a system that stores files. So right now, everything is monolithic in one server.\n\n\nSo there is literally a bunch of files that are being accumulated in the file system.\nSo it has been happening a few times that the file system was totally full and somebody had to manually allocate more space, more disks.\nAnd while that was happening, the whole application was unresponsive and it was effectively a downtime and an incident that needed to be managed by somebody.\nSo the customers were a little bit disappointed with all of that.\n\n\nAnd similarly, you can imagine there is stress also in the database because if everything is in the same machine, everything is competing for resources.\nSo as soon as something is stressed, everything else doesn't have the necessary resources to work optimally.\nSo all of that is basically one single big point of failure.\nIf anything fails for any reason, the whole application is failing, going down, being unreachable and customers cannot use the application.\nThey cannot search for their file and they cannot ultimately do their job.\nSo the prompt that we got from this scenario is, okay, but if I move everything into the cloud, everything is going to be better.\nBut at the same time, the feeling is that if you move to the cloud, it's a very big and scary investment that might take a lot of money and time.\nSo it's like, how do we find a trade-off there that makes everyone happy?\nYeah, I guess that's an important question. We covered this before in one of the previous episodes on how do you migrate to the cloud.\n\n\nEoin: There's a lot of different options. It can be very overwhelming.\nSo I suppose you have to bear in mind what are the skills, how many people, how much cloud awareness do you have, as well as real world problems.\nWhat are they going to solve? How much time do they have?\nAnd ultimately, really, they've got to get this out there in time for customers to achieve success with it, not impact existing customers and scale with their growth.\nSo what do you suggest?\nYeah, so my suggestion would be that trying to reach the best outcome with the minimum amount of investment in terms of time, money, effort.\n\n\nLuciano: So an idea could be, can we find an architecture that is not dramatically different from the current one, but that at the same time allows us to move everything to the cloud and make it more resilient and scalable, which are the main problems that we are facing right now.\nThe system doesn't scale, and if there is any crash, everything burns, basically.\nSo that's kind of the line that I would like to keep here, so that the challenge is literally, how do we make that happen?\nAnd at the same time, we are working with a small team, so how do we minimize also the amount of information overload on that team that will need to actually do the work and learn all the new concepts that come with the cloud?\nYeah, I guess it's a difficult thing to resist the temptation to adopt all of the new tools and toys you get with the cloud and to try and simplify.\n\n\nEoin: So what do you think is a reasonable approach that solves all the problems, gets some of the advantages of the cloud, achieves the business goals of scaling for the new customers, but doesn't overwhelm the team with a whole lot of new learnings and distractions?\nSo the architecture that I had in mind is something actually quite common when it comes to cloud architectures, especially if we look at the very beginning of the cloud, it's like more traditional three-tier application cloud version, if you want.\n\n\nLuciano: And basically, the idea is that you have an application load balancer, which is kind of the entry point to the entire architecture, so it's where we receive the requests from the users.\nThen that load balancer is going to forward all these requests, not to just one machine, but at this point you can use as many machines as you need, so you have this kind of pool of EC2 instances, and they all run the exact same copy of the application code.\n\n\nSo it's literally just taking the monolith and multiplying it n times, where n is going to be a factor of the traffic that you get and how much resources do you need to run the application.\nAnd of course, another big problem that we mentioned is files, and those files cannot live in a file system. Well, I suppose they could, but it's more ideal once you are in the cloud to use something like S3, which has been literally built with that goal of making it easier to host files in a safe and distributed way.\n\n\nSo definitely, we should try to leverage S3, and if you have used S3, you know it's not dramatically complicated. It's a reasonable change to make in the architecture.\nAnd we can also discuss some tricks to make it easier at the beginning if you don't have time to kind of adopt the SDK and do a lot of code changes.\nAnd then, for instance, another big problem that comes with having multiple instances is that you cannot have a local state anymore. You need to, for instance, if you have users that log in, you need to manage their sessions, and this session cannot live in one machine.\n\n\nAnd again, it could if you use TikiSessions, but that's not the best way of doing it.\nSo the best thing to do is to use a session storage. Maybe something like Redis can be used to host all that data. So connecting all the instances to Redis is another part of the architecture.\nAnd finally, the database. We mentioned that the recurrent solution is essentially a process running in line in the same machine.\n\n\nWhat we want to do is to ideally remove all of that from inside the machine and have it independent and scalable and resilient on its own.\nAnd there is a perfect service for that in AWS, which is RDS, that being a managed service allows you to get a Postgres database running, make it distributed.\nYou can have read replica. You can have all the features you need, just a click away from you. You don't need to manually write the scripts to provision all that stuff yourself.\nOkay, I like that. I mean, it's, I guess, a sane approach to this problem, right? It's not overwhelming the team with new things like serverless architecture and containers.\n\n\nEoin: And so it keeps a lot of the skills in their comfort zone, right? And it minimizes the amount of new cloud technologies they have to adopt and sets them up pretty well for the future.\nSo I hope a lot of people would kind of copy this model, especially when you're working with a team that's compromised in terms of the amount of time they have to adopt new skills.\nAnd no, this is a good first step, I think. So maybe we can talk about the steps to actually make this happen.\nSo now we've got the target architecture in mind. We need a roadmap, right, to get there. So where do we start? What are the first things we need to start preparing?\n\n\nLuciano: Yeah, I will definitely start by, of course, creating an AWS account. So let's create the target environment.\nAnd one thing that I will try to do straight away, and this is probably a little bit of a burden to the team if it's something that they haven't done before, but I consider it almost necessary if you want to be successful in the cloud, is to start to adopt infrastructure as code.\nSo everything you do in the cloud is not something you do manually by going to the web console and clicking around.\n\n\nOf course, you can do that while you're learning, but when you're building production-ready solutions, you should use infrastructure as code.\nSo this is a step where the team needs to maybe invest a little bit of time and play a little bit around with it and learn the basic concept.\nAnd of course, they can select whatever tool feels more natural to them. We have another episode dedicated to that, but CDK, Platform, Terraform, Pulumi, there are many tools out there.\n\n\nWhatever feels more natural, they are all good enough for the goal that we want to achieve.\nAnd then finally, the other thing we need to do as something that is needed to set the stage is to create a network where the whole application will be deployed.\nSo that can be also a little bit of a learning curve if the team doesn't have experience with building visual networks in the cloud.\nAnd in particular, with AWS, there are some concepts that you need to learn. What is a VPC? What are availability zones? What are public and private subnets?\n\n\nAnd how to configure all of that. If you use CDK, maybe you can get some defaults, but we spoke in another episode how that can also be dangerous because you might end up not really understanding what's going on in the architecture and maybe provisioning things that you don't really need and end up with an expensive setup, like NAT gateways and all this stuff.\nSo yeah, this is probably another point where the team needs to spend a little bit of time, learn at least the basics, do a few experiments, and once they are comfortable, they can start to use that learning from the infrastructure as code to provision the VPC.\nAnd at that point, we have an AWS account, minimal understanding of infrastructure as code, and a virtual private network that we can use to host the entire application.\n\n\nEoin: I think those points you just made give a good outline of why you don't want to burden a team with too much when you're migrating to the cloud, because even with this simplified, sane approach, you already have an AWS account and possibly AWS organization fundamentals to understand.\nYou have infrastructure as code to understand. And the basics of AWS networking, like what's a private subnet? What's a public subnet? What's an internet gateway and a NAT gateway?\n\n\nWhat are the pricing impacts and security impacts of all of these components? So there's enough there in terms of good, solid AWS foundations to understand.\nI think it's probably enough for the first dive into AWS.\nSo with those fundamentals in place, I think with migrations in general, data is key and data retention and avoiding data loss is important. So data is probably a good topic for the next phase of this journey.\n\n\nWhat do we have to think about? You mentioned file storage. I think moving from an on-premise disk or an on-premise NAS to S3 is one of the lowest overhead parts of this and one of the biggest benefits because you can suddenly stop worrying about disks filling up.\nAnd it's one of the biggest wins, I think. So is that where you'd start with the file migration?\nProbably, yes. I think in general, as you said, if you can show the customer that all the data is already in the new environment and all the data gets replicated automatically or as automatic as possible to the new environment, that gives a lot of confidence boost.\n\n\nLuciano: Because as you said, the data is king and that's the main concern. Like maybe I'm not too concerned about being offline for a few hours while I migrate, but I'm definitely going to be concerned if I'm going to lose some data.\nSo if we can reassure a customer, a company that that's not going to be the case, that there are ways to actually keep the data in sync as we move through two different systems, I think that that's literally a big win and we should aim to that.\n\n\nSo I agree that this is a good next step to address to build more and more confidence that we are going in the right direction.\nSo yeah, talking about S3, the easiest thing that I could think of is, okay, let's start by creating an S3 bucket and let's make sure that every new file that gets created in the old system is also created in S3.\nSo that might require code changes, but there are tricks there. I mentioned that before. For instance, you can use virtual file system like Fuses 3 and things like that to keep the code as unchanged as possible because the code is good at reading and writing files from the file system.\n\n\nWith a virtual file system, you will only have like a different virtual folder that you use to read and write and that virtual file system will take the burden of actually using the AWS APIs to actually read and write into S3.\nI don't necessarily recommend that because there are problems that come with that solution, but at the same time, if you don't want to change the code too much because you don't have the time, it's something else you need to learn, it's new dependencies that you need to bring into the application.\n\n\nAnd maybe at that moment in time, it's not easy to do that. That can be a solution right now to just start to see the data popping into the S3 bucket.\nThen another thing you can do once you have new data being written also to S3 is to just go into the current machine, the current monolith, and do an S3 sync from the CLI and that will copy all the existing files over into the bucket as well.\nSo at that point, you have all the new data coming in, but you also copied all the historic data. So at that point, you have S3 perfectly in line.\n\n\nThe next problem is the database data. And that's also a big one because if you have a relational database, how do you keep it in sync with another copy of the relational database, right?\nThen it's going to be running in AWS. We mentioned you can use RDS. So the next thing you should do is just go to RDS and create a cluster for your Postgres.\nAnd then how do you actually bring the data from the current system to this new RDS cluster?\n\n\nAnd there is actually a service dedicated to that. It's called Database Migration Service.\nAnd one of the things that it does other than just helping you to migrate all your schema and copying the data, but it can also work in the original system, so in the on-premise system, and make sure that every time there is new data in that on-premise database, this data is also replicated to the RDS database.\nSo this way, again, we are creating that system that allows not just to copy the data once, but also to keep copying new data as it arrives, which gives us confidence that we can take all the time that is needed for this migration without having to put the system offline.\nSo that the old system can still work and new data will be replicated, and we can switch over to the new system whenever we feel ready.\n\n\nEoin: That sounds like a good pragmatic set of decisions there. I think you also have the option of manually migrating your database data.\nBut maybe that's a little bit more difficult than it was with S3 where you can use the AWS CLI to do an S3 sync.\nSimilarly, you could probably go a step further and migrate your S3 data using Storage Gateway and have more of a pattern like you have with the Database Migration Service.\nBut S3 is probably just a little bit simpler to migrate because you don't have to think about all the transactional updates happening and file systems are a little bit simpler to reason about.\nSo you've got options there, but you don't have to go all in and choose Storage Gateway, which has lots of options and its own set of complexities.\n\n\nLuciano: Yeah, and then the last thing is to provision Redis, and you can do that in a managed way on AWS using something like Elasticash, for instance.\n\n\nEoin: And the good thing about Redis is that it tends to be quite schema-less, so you don't need to really worry too much right now about, I don't know, how are you going to structure the data in Redis.\n\n\nLuciano: So just spinning up the cluster is probably enough for you right now to get started.\nOkay, so Redis, I suppose the important thing is to size it correctly, make sure you have enough memory, and it's going to work for your performance, but assuming, it depends on what you're using it for, and that probably brings us to the application and how the application leverages Redis.\n\n\nEoin: And I think we've talked about preparation, we've got our data migrations started. So this is everything in the right order so far, I think.\nProbably a good time to start thinking about compute and the application itself. So is it just a lift and shift? Do we need to make much change there?\nI would say almost, but there is like a big mindset shift, I think, when it comes to this kind of architecture.\n\n\nLuciano: And the reason why is because in the initial state, we have only one machine. So you can imagine that machine to be like totally stateful.\nEverything that happens, connections, sessions, are all managed. They could be managed in memory in that one machine.\nThe problem is that as soon as you have multiple machines, even just two machines, the load balancer will route traffic to them in kind of a round robin fashion.\n\n\nSo it's not guaranteed that a user sending a request the first time will end up in the same machine when they send a request the second time.\nThey might be bouncing between two or more machines. So if the state is not somehow available to all the machines, that becomes a problem because a user might log in into one machine, then send that request to another machine.\nAnd basically the second machine doesn't have any clue about that particular session.\n\n\nSo the problem is how do we keep all the instances as stateless as possible?\nWhich means we need to put the state somewhere else that is shared. And that's why we created the Redis cluster.\nAnd for this particular application, I expect that the main kind of state that we need to keep track of is just user sessions.\nSo we can kind of simplify it that way. We already say that files will be copied in S3 so that that kind of decouples as well the statefulness of the application into something a little bit more stateless.\n\n\nBut there is another interesting thing to bring in mind that is you cannot, you could probably do it, but you shouldn't do it, that you can SSH into one of the machines to do operational stuff.\nAnd operational stuff could be, I don't know, tail logs because you're trying to troubleshoot something or even just install updates or do code changes because you're trying to fix or update something.\nThat doesn't make any sense anymore because you, first of all, if you're looking for logs, you have no guarantee that the logs are being produced.\n\n\nThe logs you are looking for are being produced in the machine that you just connected to. It might be any other of the machines or maybe that original machines that where you saw a potential bug doesn't even exist anymore.\nBecause you have to think these machines are dynamically, they could be configured to dynamically appear and created and destroyed to be elastically scalable.\nSo that that concept of I'm just going to SSH to do operation. I think it's a big no no when you move to this kind of architectures.\nSo what is the solution? The solution is to use images, machine images like AMIs to provision your instances. So you make sure every instance is literally the same.\nEverything is stateless. So we said we move all the state outside the instance.\nBut also you'll need to start adopting observability tools for things like logs and metrics.\nAnd that makes also all this information in a way stateless, meaning that it's moved outside the instance itself.\nThat sounds good. And I guess people can make their own judgment as to whether they need an auto scaling group.\n\n\nEoin: You might also just decide to bring up a number of instances, like three instances and multiple AZs.\nAnd if you know your traffic is never going to exceed the compute amounts of three instances and you're just doing it for high availability.\nThat's completely OK, too. You can decide to adopt an auto scaling group at a later stage.\nAbsolutely. So we talked about some of the networking fundamentals, public and private subnets.\nYou've mentioned the application and we've got auto scaling. We talked about multiple AZs.\nWhat are the other, I suppose, front facing networking considerations that we need to take?\nSo we're starting to wire our application closer to our user. What are the parts that we need to think about there?\n\n\nLuciano: Yeah, one thing that we didn't mention is HTTPS, which, of course, it's going to be a critical thing for a system like this, where users are logging in and there is sensitive information being uploaded. So we definitely need to have HTTPS.\nThe good news is that in AWS, there are ways to make that somewhat simple and managed,\nbecause you can use services like ACM to create the certificates and manage the lifecycle of the certificate.\n\n\nAnd then a certificate with ACM can just be attached to the load balancer and the load balancer can deal with all the SSL termination.\nSo it becomes kind of from the user to the load balancer is HTTPS and everything else you don't necessarily have to keep doing HTTPS unless you want to, of course.\nSo the things that we need to configure is create a certificate with ACM, attach the certificate to the load balancer.\n\n\nAnd of course, when you create the certificate, there are different ways to validate that certificate.\nYou need to prove that you have control over the domain and you can do that either by email or with DNS records.\nSo depending on how you are set up there, you might pick whatever way is most suitable to you.\nAnd finally, if you want to do auto scaling, you need to make sure that your application has a kind of an ELT check endpoint that the load balancer can use to verify that when a new instance is brought up, it's actually ready to receive requests.\nAnd also if the instance crashes for whatever reason, the load balancer can recognize and remove it from the pool of EC2 instances.\nAnd with that, you also need to configure the targets and auto scaling groups.\nSo there is a little bit of extra configuration. Also, what are the scaling rules?\nDo you want to scale based on, I don't know, average CPU or number of connections? Things you can decide based on what are your expectations in terms of incoming traffic.\nOK, so that sounds like it'll set people up for a seamless switchover as long as they understand exactly what they expect in terms of what domains they're using.\n\n\nEoin: They need to think about are they using the same domain, different domain, but the important thing is to be able to test your old system and your live system, make sure they're both working and then seamlessly switch over with the no deployment steps really just to use DNS.\nThat's always the safest way to do things.\nSo there's at that point, right, we've got our application up and running in the cloud. Users can start using it right away.\nExisting users should have noticed no difference, maybe just a dramatic increase in performance and stability.\nAnd we know that we're scaled for future growth as well. So in terms of thinking about the team, people who actually have to do this work and support it, and we don't want them to lose too many sleepless nights.\nSo what are the things that teams need to learn? What are the fundamentals? We talked about some of them there. Maybe we can summarize.\n\n\nLuciano: Yeah, we definitely mentioned infrastructure as code as being one of the most important investments, I suppose, because if you do that right at the beginning, it's going to pay off big time as you deploy the application the first time, but then especially when you want to do changes in the future and update the application.\nSo that's definitely one, and it can be a big one, I suppose. If you've never done it before, it can be a little bit overwhelming. So this is probably the one thing I would recommend to really spend your time and make sure you feel comfortable with it.\n\n\nThe other one is AWS networking. You don't have to become an expert, but at least understand the basics, what are the different concepts and need to be comfortable thinking that you are not just running a server in the public internet or on premise and somehow with a public IP.\nBut you literally have your own virtual network where there are different things running inside, they are connected with each other, and then how do you expose that to public facing internet?\n\n\nSo just make sure you understand all the basics there and how the different AWS concepts allow you to implement that kind of architecture.\nAnd another thing we didn't mention, but it's probably important, is to understand AWS permission. So get yourself comfortable with IAM because, of course, we'll need to have instances that are able to read and write to S3.\nSo to the very minimum, you need to be able to define the IAM policies that allow that.\n\n\nBut of course, as soon as you learn IAM, that can be beneficial in general in AWS to make sure that every time you are integrating different services, all the policies are configured correctly.\nAnd also that's important for users logging into AWS, what kind of permissions do they get? So something to learn anyway as soon as you need to start managing that AWS account.\nAnd finally, how to create AMIs. There are different ways and different tools, but of course, it's something that you need to do because this is how you change, well, how you create the code in the first place that goes into every machine, but also how do you change it every time you want to do a new release.\n\n\nEoin: So I think that's a good summary of all the skills you need, and there's enough there. And if you could focus on those basics, I think after a success like this and with those skills, you've got a team that's really well set up to grow on AWS really well.\nSo what's next? Once that's in place, what should the team thinking about in terms of, okay, now that we're there in AWS, where do we go from here? What are the improvements we can make? What new opportunities does this open up for us?\nYeah, I think there will be in general some new challenges, but also new opportunities once the new system is running in the cloud.\n\n\nLuciano: We mentioned already that there will be challenges in terms of observability because again, you have a lot of things happening in different systems. How do you make sense of if there is an issue, like where the issue will even be? Like where do you start looking?\nWhere do you find evidence about that issue? Where do you collect more information to be able to troubleshoot and solve the issue? And all of that comes with the topic of observability and learning how to do that in the cloud and all the tooling.\n\n\nIt's another skill that the team will need to start developing. And that probably requires a lot of code changes, making sure that all the information is logged correctly or metrics are being created, alarms are set.\nAnd then you also need to develop operational skills. How do you react to incidents? Who is going to be available? What are they going to do to address problems?\nThings that maybe you were doing to some extent with the monolithic system, but now they get to a different degree of complexity just because you have more moving parts.\n\n\nAnd then similar topics are testing. How do you do testing now? Because it's not just one system. How do you make sure that all the new different parts of the system work end to end?\nAnd with that, you can also start to think about building and deployment. Can we automate some of that stuff, even just the building part?\nBut if you can even get to a point where you do full CI-CD, that's kind of even better goal to have. And again, this is a little bit of both of a challenge and an opportunity.\n\n\nBut there are also other opportunities there that are very interesting because the goal that we hopefully achieved at this point is that we have an architecture that can scale and be more resilient to failure.\nThere is not a single point of failure anymore. And if things fail, you can have systems in place that will automatically spin up new instances and the system can auto-ill up to some extent.\n\n\nThe interesting thing is that at this point, as soon as your product grows, you have more customers, you need to develop new features, you can start to think about two options there.\nOne, you can start to think about microservices so you can start to break down the existing application into individual services and then give different teams different responsibilities.\nBut also you can approach that way of thinking in a more, I suppose, safe way, which is you don't necessarily have to do full monolith to microservice migration.\n\n\nYou can think, okay, if we need to develop a new feature, how can we build that one feature in a way that is decoupled from the existing monolith?\nAnd that's something that you can do in AWS for instance, you can use ABA gateway and then Lambda as a backend and then tell the load balancer this particular feature, I don't know, slash search maybe, goes into the ABA gateway and then it's managed by Lambdas rather than being managed by the monolith application.\nSo that gives you ways to experiment and get more comfortable with different tools that are available in AWS before you actually dramatically change the entire application.\nAnd similarly, you can experiment with SQS for instance, and Lambda to offload some of the usual things like, I don't know, sending emails, notifications, processing data in the background.\nSo you can also leverage additional tools and do that as soon as you see an opportunity to do it with very small and tactical changes.\nThis is great. Yeah, I think there's a number of opportunities.\n\n\nEoin: It really is a good appetizer for people who are thinking about taking this approach and I think the whole order of things and doing things simply in a managed way and then opening up these opportunities for later is good.\nYou're not taking on too much too soon. If you want to learn more about the details of this particular strategy, there's a lot of detail in that really great InfoQ article.\nThe link is in the show notes below.\nBut if you want to know about all the different ways, Episode 18, How Do You Move to the Cloud, we're going to link to that and we'd really love your thoughts and other alternative ideas on migration strategies because there's a lot of them out there.\nSo let us know what you think and we'll see you next time.\nBye.\n"
    },
    {
      "title": "38. How do you choose the right compute service on AWS?",
      "url": "https://awsbites.com/38-how-do-you-choose-the-right-compute-service-on-aws/",
      "publish_date": "2022-05-26T00:00:00.000Z",
      "abstract": "When it comes to choosing compute services on AWS, there are a lot of options, including EC2, ECS, Lambda, EKS… New ones keep emerging all the time! Selecting the right one for each application is no longer an easy choice. In this episode we discuss why you need compute services and what kinds of problems should be offloaded to something else entirely. We suggest how you can develop a methodology to make the selection process easier and less biased within your company. We discuss at a high level what are some of the different compute options available in AWS and finally we provide a few different options example use cases and describe how we picked the compute service for each.\nIn this episode we mentioned the following resources:\n\nInfoQ article “A Recipe to Migrate and Scale Monoliths in the Cloud”\nOur previous episode about migrating monoliths to the cloud\nArticle on choosing the right compute service\n\n",
      "transcript": "Eoin: When it comes to choosing compute services on AWS,\nthere are a lot of options including EC2, ECS, Lambda, EKS,\nand new ones keep emerging all the time.\nSo selecting the right one for each application\nis no longer an easy choice.\nSo today we want to talk about\nwhy do you need compute services\nand what kind of problems\nshould you actually be offloading\nto something else entirely?\nWe're gonna talk about creating a methodology\nto make the selection process easier and less biased.\n\n\nWe'll talk about the different compute options available\nin AWS and we'll give some example use cases\nand how you make the right choice in each case.\nMy name is Eoin, I'm joined by Luciano\nand this is the AWS Bites podcast.\nIn the previous episode,\nwe talked about migrating a monolith to AWS.\nAnd in that case, we talked about considering data,\ncompute and networking.\nFor compute in that case, we chose EC2\nbecause it made sense for the team.\nThis time we want to talk about different options for compute\nand how to have a system for choosing one over the other\nin different use cases.\nBut we should probably start with the basics.\nIt might seem obvious Luciano,\nbut what do we mean when we say compute?\n\n\nLuciano: Yeah, I think it's actually not obvious at all.\nI think historically I will describe compute\nas you get a virtual machine\nand you can do whatever you want with it.\nAnd that's used to be literally everything\nfrom your application code,\nlike maybe a web server or even a database\nor even an event bus and everything,\nincluding services and business logic,\nwe're living together in this one virtual machine.\n\n\nSo I would describe that as a compute layer of its own.\nBut I don't know,\nif we want to be a little bit more specific,\nmaybe we can try to isolate the everything else\nand focus more on actual business logic.\nSo what do we mean in general by business logic?\nAnd there are different use cases that we should consider.\nFor instance, I don't know,\nif you have to run specific algorithms\nor if you have differentiating,\nparts of your businesses require custom code to be run\nto actually make something happen.\n\n\nThis is definitely one category that we can describe\nas something that can be fulfilled with compute.\nOther common things are when you need to run,\nfor instance, control flow or some sort of orchestration.\nSo you basically have some data maybe coming in\nand you need to decide what to do with that data\nand then produce some output based on rules of some sort.\nThat's definitely another category\nwhere you can have a compute layer\nthat is dedicated to that particular kind of use case.\n\n\nAnd very similarly, there is a concept of integration.\nSo maybe you are trying to connect to different systems,\nmaybe using different types of APIs.\nAnd again, that might be something event-based.\nSo an event comes in, you need to deal with that event\nmaybe by sending the data to another system\nand this way you can connect multiple parts\nof your application.\nAnd finally, another interesting use case is data access,\nwhich is the idea that you might have a data layer\nand you need to run specific code\nto allow the access to that data layer.\n\n\nAnd you can imagine, I don't know,\nif you have a web application,\nmaybe you have an ORM layer that allows you\nto execute query against the database.\nThat can be transactional or not,\nor you can have other maybe background processes\nwhere you use them to do like data gathering,\ndata mining, or even data processing.\nLike for instance, if you have big data workloads\nand you need to enrich data, manipulate data.\n\n\nSo in all these cases, you can find, I suppose,\nsome degree of compute.\nAnd the idea is again,\nthat your business will have certain needs\nand to express all these needs,\nyou probably need to write some code\nand this code needs to run somewhere to satisfy those needs.\nThe interesting thing is that I have a feeling\nthat in modern architectures,\nespecially if we think more and more\nabout the concept of serverless\nor the concept of low code architectures,\nit feels like there is a goal of trying to reduce\nthat level of custom compute as much as possible.\nAnd the alternative is to use services that are managed\nand given to you by some third-party provider\nlike AWS or other cloud vendors.\nI don't know if you agree with this view,\nbut this is like what I feel about where the world is going.\n\n\nEoin: That's definitely the direction of travel.\nYeah, this whole, if you ask what does serverless mean,\nit's not about functions or Lambda really,\nit's about removing compute as much as possible.\nAnd do you even have people using this term,\nservice, serverless service full applications now\nwhere you're talking about composing really\njust doing integrations\nand composing these third-party services together\nand trying to remove that code.\n\n\nI think it's a good practice actually.\nSo in an AWS context, maybe some examples of that\nwould be using step functions for orchestration\ninstead of imperative reams of code\nand trying to figure out how do you handle errors?\nHow do you handle the delays?\nHow do you handle back off and retry\nand circuit breakers and all that stuff?\nSo now you don't need Lambda for a lot of those things\nand step functions even.\n\n\nSo you can do direct SDK integrations\nand talk to SQS or EventBridge or S3\ndirectly from step function states.\nSo you can see the potential to remove\ntraditional imperative code running in a function\nor on a container is getting better all the time.\nThen you also have other examples\nlike when you're integrating things together,\nyou can use event services like EventBridge, SQS,\nSNS, Kinesis, and you can combine them\nwith API Gateway or AppSync.\nSo you can integrate APIs inbound\nand also third-party webhooks together\nwithout having to write a lot of custom logic.\n\n\nLuciano: Yeah, it's interesting that some of the services\nyou mentioned, like in the last few years,\nwe have seen more and more features\nthat actually looks like they're trying\nto reduce the amount of custom code,\nlike the opportunity to filter certain type of events\nor to remap the structure of an event\nbefore it's forwarded somewhere else.\nLike you can do today all this stuff\nwith just configuration rather than writing\nyour own custom code that takes the data as an input,\nchange the data, and send it back somewhere else.\n\n\nEoin: I was just about to mention AppSync and API Gateway\nand directly integrating with DynamoDB\nor other backend services or APIs.\nAnd exactly, you could do that.\nYou can use input transformers for some services\nor with AppSync and API Gateway,\nyou can use a lot of VTL velocity language\nto do those mappings.\nAnd it really becomes then a choice of,\ndo you want to use VTL or do you want to use Lambda\nfor that translation there?\n\n\nAnd in that case, you're just using Lambda\nas a slightly more powerful data transformation there\nin some cases using the language of your choice.\nSo it's less compute and more data transformation.\nYeah, this kind of architecture can be very beneficial.\nIt can remove a lot of effort.\nIt can push a lot more responsibility to AWS\nor someone else, but it does require like specific skills.\nWe've mentioned this before.\nThere is a mindset shift.\nIt's not as suitable for everybody.\nAnd you also might have existing software\nthat relies on specific containers, sorry,\nspecific frameworks running in containers or on instances.\nSo we can't pretend that this is the way everybody\nshould be going right now.\nSo it's not always a simple choice\nand it depends on your context.\nSo I think what we'd recommend is having\na technology selection methodology to help your team\nor your organization choose a good solution\ndepending on each context.\n\n\nLuciano: Yeah, I think I agree because it's,\nI think there are two extreme scenarios.\nOne is that you always stick with what you know,\nand then there is the kind of no evolution.\nAnd then eventually you will have a huge technical depth\nand it's very hard to move on from there.\nThe other opposite extreme is when you always try\nto the newest shiny things, just because you want to have fun\nand you want to try the new things.\n\n\nAnd maybe you end up in situations\nwhere you rely on technology that is gonna fade away\nbecause maybe there wasn't enough adoption\nas was initially expected, or maybe you run into cases\nwhere there isn't a lot of documentation and community\nand use cases and examples.\nSo you are left on your own to really reinvent the wheel\nwith this new technology.\nSo I think those are two very extreme use cases.\nSo we need to find, I suppose, a balance\nbetween those two extreme cases and having like a methodology\nthat you can rely on and is somewhat objective,\nI think can be very beneficial for a team\nto find the right trade off for them.\nYeah, I think we can maybe mention a few pillars there.\nI don't know if we will come up with a very like\nstep-by-step methodology, but I think we can give people\ninsights on how to build their own methodology,\nmaybe better than some common ideas.\n\n\nEoin: We can all be very emotional\nwhen making technology decisions.\nAnd when you have a group of large people\nwith different perspectives,\nit can become sometimes even bitter or suboptimal\nor you end up with some regrets or feelings\nof somebody else forced their way in technology selection.\nBut you also just want to, for your own benefit,\nremove your own biases.\nSo I think having the idea of having a methodology\nis great just for removing that and allowing everybody\nto move on together with a sense of shared consensus\nand that you've made a good decision for everybody.\n\n\nLuciano: Yeah, I actually hate when in some companies\nthere is a phrase that comes up that is,\nwe are a company, technology X, like no matter what,\nwe always use technology X, like we intentionally committed\nto use this technology forever.\nAnd I think that can have its own benefits\nbecause maybe you structure training\nand hiring around that technology,\nbut at the same time in the long run\ncan become very dangerous because it kind of blocks\nany opportunity for evolving the combat itself,\nthe technology, maybe leveraging some new opportunities\nthat are available in the market.\nSo that's another case where you can be emotional\nin a dangerous way and maybe lose opportunities\nto do something different that can be more beneficial\nfor your business.\nSo where do we start?\nWhat kind of suggestions can we give to people\nfor building this methodology?\n\n\nEoin: So I've done this a few different times\nand different times I kind of adjust the selection criteria,\nbut the one I always include is try to put in place\na measure of simplicity.\nAnd this can be a difficult one,\nbut it's really the number one factor.\nAnd the idea is that you don't end up\nwith lots of unnecessary complexity that's hard to foresee.\nSo you think about shifting more of the responsibility\nto your vendor and away from your team.\n\n\nSo there's different like operational complexity,\ndifficult troubleshooting, but it's up to you\nto kind of define what the criteria for simplicity are.\nBut simplicity is a really big one, right?\nBecause any small piece of complexity\nwill be compounded over time as you go into production\nand the rubber hits the road.\nSo simplicity is always a good one to try and score.\nAnother one is like a lot more practical and quantitative,\nwhich is performance and scalability.\nSo I find this is something that people either overthink\nin the beginning, like overestimating\nthe performance characteristics they need\nor else they dismiss it entirely until it's too late.\nThere's never a happy ground.\nBut you can also, you can very quickly\njust put some numbers together on what your usage\nis expected to be like based on historical data\nor market data or whatever your growth projections are,\nput in some performance criteria\nand just use that for your, to make sure,\ndoes the service I'm choosing fit within that?\nAnd then it makes it a very easy decision.\n\n\nLuciano: Yeah, it needs to be more of a sanity check, I suppose,\nrather than, I don't know, artificial goals\nthat you want to achieve just to prove\nthat that technology is the best one.\n\n\nEoin: Yeah, for sure.\nAnd remember as well, that this is something\nyou can revisit over time.\nBecause you expect your architecture to evolve\nand your compute options can change as well over time.\nAnd this is another reason to go with the simple option,\nbecause simplicity often implies what I think\nWerner Fokkel's once called evolvability,\nwhich is that if you don't have a lot,\nif it's a simple service,\nyou don't have a lot of upfront investment in it.\n\n\nSo you can actually back out and reverse the decision\nvery quickly as well, and it doesn't hurt you very much.\nSo a couple of other criteria you could put\nin your methodology are like reliability,\nresilience and security.\nAgain, going with managed services should help you there,\nbut you should try and have a measure of that.\nSome of the less quantitative ones\nare like developer experience.\nSo that's a really important one.\nWe often think about how,\nlook at how things will work in production,\nbut we forget that most of the time developers\nare working on the development environment,\nnot the production environment.\nSo how easy it is to get your development environment\nup and running, onboarding new developers,\nall of that is really important.\nDo you have any other ideas for some other factors\nwe can include in this selection methodology?\n\n\nLuciano: Yeah, one that I will suggest is like skills related to,\nlike how does what you know already in your team\nhelps you to adopt a specific compute layer, right?\nDo you need to learn something entirely new\nor the model that is being offered\nwith that particular compute layer\nactually fits very well your current skills?\nSo this is something that you need to keep in mind,\nbecause of course, if you need to learn a lot,\nthat means time invested in giving people,\nI don't know, space to learn and experiment,\nand also to fail, because probably you're gonna be\ndoing something wrong at the beginning\nbefore you learn all the patterns\nand all the right ways of using the technology.\n\n\nSo all the stuff is something you need to account for\nif you're going to learn something new.\nSo it might not always be desirable to do that\nif you maybe have very strict timelines,\nor maybe if you don't necessarily need\nthe characteristic of a specific compute layer.\nSo that's a good trade off to keep in mind.\nThe other one is cost, because of course,\nit's another dimension that is always worth considering.\n\n\nDifferent services have very different cost factors,\neven different cost formulas if you want.\nSo depending on how you intend to use that service,\nit might be much more convenient for you to use\none or another compute service.\nSo keep in mind, again, maybe take some figure,\nuse it on your expected use cases, historical data,\nwhatever you can think of in terms of giving you\na guarantee that you know what's gonna be\nthe trajectory of your usage,\nand try to put some data in and figure out,\nokay, how much more expensive is this solution\ngoing to be compared to this other one,\nand try to evaluate on that metric as well.\n\n\nEoin: These criteria are beginning to sound like\nlist of pillars from the well-architected framework,\nwith like some of the softer ones added in,\nlike developer experience and skills suitability\nfor your team.\n\n\nLuciano: Yeah, the other one I wanted to mention,\nyou kind of mentioned it already to some extent,\nis the ease of deployment, which is more like\nhow much effort is gonna require you to actually\nput something in production, and then once it's\nin production, to keep it running,\nand every time you need to do updates,\nmake sure you are in a position that it's easy enough\nfor you to update the system.\nSo that's another metric that is definitely worth considering\ndepending on the kind of application you're trying to build.\n\n\nEoin: Yeah, so with all those factors then,\nI think you can, the idea is we're taking something\nthat's very subjective process,\nand we're trying to make it a little bit quantitative.\nSo for each of those criteria,\nyou can give every one of your computer options a score,\nlike from one to five, and you can get everybody\non the team to give a score,\nand if there's widely diverging opinions,\nyou can discuss that and form a consensus, right?\n\n\nBut the idea is to try and be as objective as possible.\nWhat I found is that by employing this process,\nyou end up with quite a good shared consensus on the team,\nand much more of an understanding of different perspectives.\nAnd once you go forward with those decisions,\nyou actually end up with, well, you know,\nwe've got some very close scores here,\nbut let's pick this one, and we also have optionality.\nYou know, we can switch down the line\nif we architect this in the right way,\nand we know exactly what we're dealing with\nbecause we've put a lot of effort into understanding\nexactly what the pros and cons are.\nSo it tends to remove all of that emotional baggage\nthat comes with, you know,\nopinions-based technology selection.\n\n\nLuciano: Yeah, and I think it makes also\nevery team member more involved,\nor at least they feel more involved in the decision,\nwhich will make a big difference\nrather than having a decision being imposed from above,\nand maybe you end up not agreeing with it,\nand then because of that, you actually kind of resist\nin trying to adopt the technology as much as possible\nor as best as you can.\nI think when you have this process in place,\nit's much easier to get everyone on board too,\neven if it's an experiment,\nbut at least to go ahead with the experiment\nand very honestly try to see how it plays out\nin the context of the team.\nBut I have one question at this point\nbecause we are defining this process,\nand it feels like once you do this experiment,\neventually you have one result, which is like,\nokay, for our particular need,\nthis is the technology of choice.\nDoes it mean that you end up picking only one,\nor is there room for different parts of the application\npicking different layers of compute?\n\n\nEoin: Yeah, that's a good point.\nIt's definitely not the idea that you say,\nokay, now we go with Fargate for everything,\nor now we go with Lambda for everything.\nIt's a bit like the way the database selection practice\nhas evolved in a lot of companies now.\nPreviously, you would pick one database\nthat you would put at the heart of every system,\nprobably ended up growing very large.\nNowadays, people are more used to the polyglot database idea\nwhere you have different data stores for different purposes.\nYou might have NoSQL for some data\nand different relational databases for others,\nmaybe a data lake as well.\nSo with compute, it's the same thing I would suggest.\nSo for different applications or use cases,\ndifferent processes,\nyou can apply this process independently.\nSo you end up with a mix of compute types,\nand that's completely okay because you're picking options\nthat are optimized for the particular application\nand scalability.\n\n\nLuciano: Yeah, I like that because I think the analogy\nfits really well today is very well accepted\nthat you're not gonna use one database for everything.\nI mean, if you do that,\nyou probably gonna end up in a suboptimal place.\nSo I think it works also for compute\nthat if you try to find the parts\nthat fits best the particular problem,\nthen probably you end up in a position\nwhere things are glued together in a better way\nand work better than just like one generic thing\ntrying to do everything.\n\n\nEoin: So should we talk about the compute options\nthat are actually available on AWS now?\n\n\nLuciano: Yeah, I think the most obvious is EC2\nis probably the one that has been around the longest, right?\nAnd it's the most generic.\nYou just been a virtual machines\nand you're free to do whatever you want with them.\nProbably also the one that requires more like maintenance\nand management on your side.\nSo it's worth thinking it that way.\nIt's like, yeah, the most generic, the most,\nI don't know usable for the largest variety of use cases,\nbut also the one that would require a lot of investment\nin terms of non-differentiating things\nlike which operative system do I use?\nHow do I keep it up to date?\nSecurity and all that kind of stuff.\nAll of that is it's on you.\nSo keep that in mind.\nAnd I think we can move down the line\nof more and more managed and we from EC2,\nwe can go to kind of containers.\nSo we have Fargate, ECS, EKS are all compute options\nthat you can use where you start to offload a lot more\nto AWS the amount of operational stuff that yeah,\nAWS will take care on for you\nrather than you having to do it yourself.\n\n\nEoin: Yeah, I think it's interesting to assign a simplicity score\nmaybe in your head too, for each of those options.\nCause I would have certain view on that,\nbut it's a good thing to explore with your team.\nExactly how much complexity you're getting into\nand the difference for different container options\nyou can end up with very different scorings.\n\n\nLuciano: Absolutely and we already mentioned this in the previous\nepisode where in that particular use case,\nwe decided to go with EC2 rather than using containers,\njust because the team didn't have any previous knowledge\nabout containers, not because containers\nwouldn't have been beneficial for them.\nIt's more how much load do we put into the team\nto learn new things.\nAnd we decided at that point in time,\njust switching to AWS was already enough cognitive overload.\nSo that could be maybe a next transition\nwhen the team is confident with AWS,\nthey can start to get more confident with containers\nand then switch to ECS, EKS or Fargate\nand start to upload more and more to AWS.\nSo I think you need to also keep in mind as we said,\nthe kind of knowledge bias and decide how much more knowledge\ndo you want to gain in one go rather than\nsticking with what you know.\n\n\nEoin: That makes a lot of sense.\nYeah, in that case, I suppose you just put an increased\nweight on the skills match than on the simplicity factor,\nwhich is completely okay as long as that's an agreed\napproach by everybody.\n\n\nLuciano: Yeah, and if we keep moving into more and more managed,\nI think that the next ones are uprunner, light sale\nor Beanstalk, which is probably the most traditional\nolder version of light sale and uprunner,\nbut also things like SageMaker or AWS batch,\nwhich are a little bit more specialized.\nBut again, you kind of,\nyou still can run your own custom computers\njust in a more constrained fashion\nand you offload a lot more into AWS.\n\n\nEoin: Yeah, AWS batch can be very useful in some cases.\nIt runs on top of ECS really,\nbut it can make the process a lot simpler as you say, yeah.\n\n\nLuciano: Then what do we have?\nI think we can also consider code build as part of compute.\nMaybe it's a little bit of a stretch,\nbut at the end of the day,\nyou are in control of what kind of code\nis used to build something.\nSo it can become a little bit more general purpose\nthan you might want to think about it.\nSo yeah, you can have your own totally custom build processes\nand write them in any language or any technology\nthat you really want to.\nAnd you just run it on that code build compute layer.\n\n\nEoin: Quick and dirty approach,\nbut it works really well in some cases, yeah, for sure.\n\n\nLuciano: And then we have, I guess,\nAWS Glue spoken of that one or EMR,\nwhere you are using this kind of bespoke compute layer\nmore for data management and data processing.\nAnd finally, the last one is Lambda,\nwhich is probably the most generic,\nbut also very lightweight in terms of,\nlike it's as pure compute as it gets.\nAnd you just write one function with your business logic\nand that's it, you generally don't have to worry\nabout almost anything else.\nDid I miss anything?\nIs there any other service worth mentioning?\n\n\nEoin: There are probably others you could shoehorn in there,\nbut I think that's a pretty good picture.\nI think somebody said there's 19 different ways\nto run containers on AWS.\nBut look, I think we've covered enough\nand people get the general idea.\nMaybe we should start to conclude by talking about\nsome of those example case studies\nand real world scenarios.\nSo you already alluded to the legal,\nEGLE application from your article\nand from the last episode,\nwhich we'll link in the show notes.\n\n\nLuciano: Yes, yeah, on that one,\nI think the most relevant thing was\nthere were a bunch of different trade-offs\nthat were made in that case study\nthat are very relevant, not just to the technology itself,\nbut also the team and the stage of the project.\nSo I think we recommend you to watch that episode\nor read the article to kind of get the full context,\nbecause there we go with EC2,\nwhich seems a little bit of a counterintuitive choice\nbased on what we just said so far.\nSo I think it's interesting to get the full context\nto understand why that was the suggestion\nfor that particular use case.\n\n\nEoin: Another one I'd like to throw in here is a couple of machine learning cases.\nIn a few different projects, I've come across examples\nwhere people had a pre-trained model,\nlike an image recognition model or something like that,\nwhich has come from a team of data scientists\nworking on GPU instances they had access to\nin some data center,\nand then they want to productionize it on AWS,\nand they want it to be able to scale quickly\nand work elastically,\ndepending on the number of images that come in,\nand that can be very variable.\n\n\nAnd I've actually gone through the process\nof trying this on different platforms\nand surprised by where I eventually landed,\nbecause now my default kind of scoring methodology on this\nwould lead me actually to AWS Lambda,\nwhich you wouldn't expect to be the ideal platform\nfor machine learning,\nbut with the meta memory you have\nand the container support now,\nand the fact that you don't really often need GPUs\nfor inference when you're actually just running the model,\nLambda scales really well,\nso you can have results in seconds or less,\nbut you can also scale to thousands in an instant,\nwhich you can't really do with SageMaker.\nAnd SageMaker ended up having a lot more complexity,\nit was less common,\nso there wasn't as much documentation\nand tutorials out there,\nand the interface was a little bit non-traditional,\nyou know, it was just a bit more bespoke.\nSo I ended up running more applications like that on Lambda\nand orchestrating it with step functions and API gateway,\nand I found that's really nice solution in general,\nif your model works in Lambda.\n\n\nLuciano: Yeah, hopefully we will also get GPUs on Lambda soon.\n\n\nEoin: Yeah.\n\n\nLuciano: At some point, but I agree with you,\nit's probably a niche use case for now,\nand you can live without it\nand still reap the benefits of the simplicity of Lambda.\nOkay, is there any other example that we want to present?\n\n\nEoin: So there's one that I thought would be worth throwing in\nlast, which is just in terms of batch processing.\nWe mentioned AWS batch in already,\nbut there's a use case,\nwhich is similar to projects we've worked on in the past,\nand if you consider like a financial company,\nlike a pensions provider,\nand they have a lot of clients\nwith different portfolios of pensions.\nSo, you know, when you're choosing a pension,\nyou have to say how risk averse you are,\nwhat your risk level is,\nand your pensions provider are supposed to purchase assets\nand invest your money in something\nthat affects that risk level.\n\n\nSo if you can imagine that that pensions company\nwill have to calculate, okay,\nfor all of the assets we invest in,\nwhat is the risk level and how does that change over time?\nAnd how does that affect individual clients\nand their portfolios?\nSo on a frequent basis,\nthat pensions company will want to calculate risk level\nbased on all the data they get in.\nSo it could be stock market data, data about their clients,\ncontracts from their various different stakeholders\nthey deal with,\nand they need to run some sort of statistical model\nto calculate that risk.\n\n\nThe workload, the compute option there\nwill depend really on the volume of data,\nthe number of deals you have to calculate\nand the performance requirement, you know,\nhow quickly do you need to get access to it?\nSo traditionally, I think a lot of people\nwould go to some sort of high performance\ncompute infrastructure, like big instances\nwith very rapid networking between them,\nshared state, message passing infrastructure.\n\n\nBut in terms of the scoring methodology,\nwe've discussed this kind of scenario,\nbut again, lead me towards either something like Fargate\nor indeed Lambda.\nIn the context of Lambda,\nwhat would actually forces you to do\nis divide all this work into small pieces\nthat can run on Lambda functions.\nAnd then you try to make Lambda\ngive you as much concurrency as possible\nso that you can get all of these little pieces\nexecuted as quickly as possible.\n\n\nAnd they're scaled in thousands of jobs in parallel.\nJust if you look at the performance factor\nin the scoring chart, it takes care of that,\nbut you also get reliability, security\nand higher availability out of the box.\nAnd then your jobs, because they're running in Lambda,\nthey're also stateless.\nSo for a developer experience,\nit means that if you've got an individual job failure\nand you need to troubleshoot it,\nyou can just run that job in isolation\nand it doesn't rely on some clusters shared state\nin order to be able to troubleshoot it.\nSo there's lots of options there\nand there's no one right answer in any of these cases.\nBut I think this was just an example I wanted to bring up\nbecause it's like something that traditionally\nyou would have solved with high performance compute cluster,\nbut now you can do it with just very scalable commodity\ncloud computing with Lambda or Fargate containers.\n\n\nLuciano: Yeah, I think what you just said there at the end in terms of, because if you embrace Lambda,\nyou're kind of forced to go down that path\nthat you need to think, okay,\nhow do we break this problem down\ninto more kind of manageable pieces that run concurrently?\nI think it also forces you from an architecture perspective\nto build something that is probably closer\nto the idea of microservices,\nwhere you have small components that can be executed\nand tested and developed autonomously.\nAnd then you kind of orchestrate all of them together.\nSo there could also be advantages in terms of,\nI don't know, the simplicity for the team\nto work on an individual component\nother than just troubleshooting at runtime,\nbut even building it from day zero to production.\nI think you can divide a lot more of the work\nand bring everyone on board more easily\non the different components,\nrather than having one big monolithic thing\nthat could be very hard to develop and test\nand run locally.\n\n\nEoin: And I think we're seeing more and more of this,\npeople realizing that you can use this commodity\nfunction as a service or containers\nto do like high performance computing\nor even scientific computing at scale.\nWe're seeing more and more examples of that.\nSo I hope these examples should explain why it's useful\nfirst in having a system for scoring each of the options,\nand then how you would apply that scoring system\nfor different use cases in order\nto make technology decisions.\nWe do have an article already on this\non choosing AWS compute services.\nAnd it has a methodology just like this.\nAnd you can see the scores we've given to various services.\nSo there's a link to this article in the show notes.\nAnd in upcoming episodes,\nactually we're going to dive deeper on specific applications\nthat we've worked on at 4th Erem\nand show how this methodology played out in reality.\nSo until then we recommend that you check out\nthe previous episode on migrating a monolithic legal CMS\napplication to AWS without the drama.\nAnd we'll see you next time.\nSmridge cataly falsehoods\n"
    },
    {
      "title": "39. How do you build a cross-account event backbone with EventBridge?",
      "url": "https://awsbites.com/39-how-do-you-build-a-cross-account-event-backbone-with-eventbridge/",
      "publish_date": "2022-06-03T00:00:00.000Z",
      "abstract": "When it comes to building and deploying microservice applications on AWS, there are 2 emerging best practices: use a separate AWS account per application (and environment) and decouple communication between separate systems using events (instead of point-to-point communication). Can we use these two best practices together? Yes, but we will need to find a way to pass messages between AWS accounts! In this episode we discuss how to do that using EventBridge as a cross-account event backbone! We discuss why these 2 suggestions are well established best practices, what are the pros and cons that they bring to the table, what an event backbone is and why EventBridge is a great service to implement one. Finally, we will discuss a case study and an example implementation of this pattern in the context of an e-commerce application built with a microservices architecture.\nIn this episode we mentioned the following resources:\n\nArticle “How to use EventBridge as a Cross-Account Event Backbone”\nRepository with example code\nArticle “What can you do with EventBridge?” (fourTheorem blog)\nFor great ideas on structuring event payloads, take a read of Sheen Brisals' post on the Lego Engineering blog\nArticle “What do you need to know about SNS?” (fourTheorem blog) which includes a comparison of SNS and EventBridge\nAWS Bites Episode 23: “What’s the big deal with EventBridge?”\nAWS Community Day talk by Luc van Donkersgoed “Event-Driven Architecture at PostNL Scale”\n\n",
      "transcript": "Luciano: There are two emerging best practices when it comes to building and deploying microservices applications on AWS.\nThe first one is to use separate AWS accounts per application and per environment.\nAnd the second one is to decouple communication between separate systems using events instead of point-to-point communication.\nBut what if we want to use both of these best practices together?\nWe will need to find a way to pass messages between AWS accounts, basically.\nSo in this episode, we will discuss how to do that, specifically using Event Breach as a cross-account event backbone.\nWe will also learn what are the advantages of this approach, and we will discuss a case study and examples,\nand we will see an implementation of this pattern.\nMy name is Luciano, and today I'm joined by Eoin, and this is AWS Bites podcast.\nSo let's start with a quick recap. We just say that it is a good practice to have separate AWS accounts per application. Why is that?\n\n\nEoin: The goal is to keep accounts fairly small and clean and not have too much clutter in them.\nThat's one of the things. But there's also some technical limitations as well.\nFrom a security perspective, you might want to actually define policies at an account level and make sure that an account is restricted\nand what you can do with it, depending on the team who's using that account.\nSo you can use things like service control policies to restrict at a high level what's going to happen within an account or a set of accounts.\n\n\nThe other thing is around quotas, so the limits in different services that you can get.\nNormally, those are applied at account level.\nSo if you've got lots of different workloads mixed in one account, you want to avoid the situation where one workload can starve another of available containers or EC2 instances or whatever else you need to rely on.\nThere's also, I suppose, a trade off with this multi-account approach that's worth stating.\nSo the potential disadvantage is that with more accounts, you'll have more account management overhead.\nSo I think we discussed some of that in previous episodes, but it does require investing in some sort of tooling like org formation to make your life easier when deploying resources across these different accounts.\nYeah, that makes a lot of sense. I think we also mentioned a few times, for instance, the example of total number of concurred Lambdas.\n\n\nLuciano: So if you have workloads that tend to spin up thousands and thousands of Lambdas, they will effectively compete with each other and you'll be hit in the quota.\nAnd yeah, by separating accounts, I suppose you can kind of prevent all of that from happening.\nBut we also say that it's a good idea to decouple communication between systems using events.\nSo basically that kind of asynchronous communication.\nWhat do we really mean by that and how is that really something that gives us an advantage or gives us a better architecture, I guess?\nYeah, I mean, this is another trade off, but there's a lot of people talking about event driven architecture these days and the benefits of it.\n\n\nEoin: There is a couple of things that it solves as well as introducing a few challenges.\nBut some of the things it solves are the fact that if you've got synchronous point to point communication between systems, you end up with two types of coupling.\nTypically, when you do that. So one is location coupling where one system needs to know the address of the other.\nSo you can't really easily change that address. The other one is temporal coupling.\n\n\nWhen you have a synchronous request, the request processor needs to be available for at the same time that the request is being sent and the sender is being is actually waiting on the result.\nSo the idea with asynchronous communication is to reduce both of these things, to allow teams to be able to maintain and scale these applications separately and minimize the amount of coordination they need to do.\n\n\nSo you define your event communication mechanism and try to define some sort of a schema or a way in which those messages can be structured, but avoiding too much of a tightly bound contract and certainly an address in the middle.\nAnd what you end up with then is a system where you can scale each component independently. We talked about that in some of our previous episodes.\nBut it also allows teams just to develop in isolation and not be blocked by each other.\n\n\nSo it solves those problems, but it also introduces some challenges. And you still have a need from time to time where you will say, OK, well, we've got two separate applications in our organization.\nLet's define a RESTful API with some JSON schemas and some Swagger open API documentation.\nAnd that's our interface. And that's still OK, but you just have to be prepared to solve the coupling concerns in a different way then and just be prepared to version those APIs and evolve them and communicate if you're going to change them, etc.\nYeah, I like that you mentioned that it is a trade off. And I hope that with the examples we are going to be discussing in a moment, it will be more apparent what kind of trade off we are going to be talking about and where it becomes really an advantage and when it's not really worth doing.\n\n\nLuciano: All this kind of stuff. But we mentioned that the solution to this problem, or at least one of the possible implementation for this particular idea, is to use an event backbone.\nWhat is really an event backbone? How do we define it?\nThe event backbone terminology you often hear about in the context of Apache Kafka. And the idea is really that in the past you had service oriented architectures, the solar systems that led to enterprise service buses.\n\n\nEoin: And the vision of the enterprise service bus was that you would be able to integrate all of the disparate systems in your organization through one piece of foundational technology that everyone could access.\nAnd this could work in some cases, but again, enterprise service bus, because the goal was often to include a lot of integration and routing logic and sometimes some business logic in there, that it could become a big single point of failure in your system.\nAnd you end up just introducing the coupling in one location.\nSo the idea with an event backbone is to simplify that ideal and have a simple piece of very scalable technology that can be used for high level communication of events throughout the business. And you try to avoid any business logic there and just instead focus on reliability, durability of messages, and ensure that it scales very well so you don't have to worry about single points of failure in your system.\n\n\nLuciano: Yeah, actually, funny story. I once built a customer service bus in a company, mostly inspired by Apache Camel. And I do remember how painful it was to provide all the building blocks, like if statements, switch conditions, mapping of data to allow people to actually be able to put all that business logic into the enterprise service bus.\nI definitely agree that an ESP is something like really stateful and powerful, but that comes, of course, with a cost that all that integration is like, all the logic lives in the integration, which maybe not always a good idea.\nSo I guess the next question I have is, is this something we should do all the time for any kind of distributed application? Or maybe there are some examples where it could be worth it and other cases where it's much more clear that it's not really worth the effort of setting up something like that.\nDo we have many examples worth discussing?\n\n\nEoin: Yeah, I guess it's probably worthwhile saying that we're talking about a specific level of event-based communication here. We're not talking about having one event technology that every single event-centered consumer uses across the whole organization.\nIt's really about high-level inter-system communication. So communication in separate domains in the business, in separate applications, maybe even between services or microservices if they're sufficiently complex. But it's up to you really to pick where you would use this, but I would avoid saying this is a one-size-fits-all solution for all eventing needs.\n\n\nOkay, so you can still have an event backbone and then within each team's application they can use different event technologies for internal communication that don't necessarily have anything to do with the event backbone.\nSo maybe some examples. If we've got, say, a video streaming service, right? We know many, but imagine that in their content ingestion workflow they've got two separate, completely distinct applications. One that's involved in ingesting all the video files and transcoding them and making them available.\n\n\nAnd another one is there to sort out the catalog so people can browse, see what to watch next, see details about programs. So you can imagine that it's pretty important as a business rule that a title should not appear in the catalog until all the videos have been processed.\nSo maybe the ingestion service will eventually emit an event onto this event backbone and the catalog application would subscribe to a number of events across the system and once it knows everything is ready and the licensing of the title is in effect and all of the video files have been transcoded and published out to the CDN, then it knows it can set the catalog's entry to published.\n\n\nSo that's one example.\nMaybe another one is in the kind of business application we talked about more in recent episodes. If you're performing lots of calculations like batch processing and producing a lot of data in a database, maybe at the end of that process you might publish an event to say all this data is ready, and then you have a downstream reporting application which is, again, completely distinct in terms of the application and its deployment, but is related in some way because it's consuming from the data that is generated by that first system.\nSo I think a backbone is another good fit for that kind of high level intersystem business communication.\nYeah, I suppose in general if you also expect your application to evolve a lot and get more kind of moving parts around it, probably there is another advantage there because in the case of the analytics you just mentioned, maybe eventually you want to have another process that knows when these files are published and at that point you can just create a new application, listen for the same event, and you don't have to change anything else.\n\n\nLuciano: So the source application emitting events doesn't even know which other applications are listening for those events.\nYep, that's a good call, and you might want to actually have just a standard in place that when you finish the business process you emit life cycle events about that.\n\n\nEoin: You know, when you start a process, when it's completed, when it's failed is also important. That might also be an interesting fact for some other systems to know about.\nSo we mentioned that EventBridge is one of the possible services that we could use in AWS to build something like this, but why specifically EventBridge and not any of the other messaging services?\n\n\nLuciano: And we have been talking about messaging services a lot, so I'm kind of curious why EventBridge is specifically a good choice for this use case rather than any of the others.\nYeah, I think if you go back to those episodes, there's a couple of things that EventBridge really shines at, and the main one is simplicity, because there's very little to set up and I think that makes it a very good candidate.\n\n\nEoin: But also in real terms, it has a very large feature set. So one of the important ones in this context, since we're talking about separate applications and separate accounts, EventBridge has really good cross account support.\nIt could be better, actually, so maybe we'll get onto that in a while, how it could be improved, but it is really good. And if you're building all these applications in AWS, and you're thinking, okay, how do I figure out the networking for all these applications to talk to my centralized event backbone?\n\n\nAll of this complexity is taken away with EventBridge, really. It's also massively scalable. There's a very low investment for teams to get up and running with it.\nIt integrates really well with lots of other AWS services. And it's also, that means you can adapt it to more complex use cases. So if you have needs where you need durability, you can add in SQS on the consumer side, and that makes it very, very easy to adapt.\n\n\nYou can also decide that for specific cases, you could add in Kinesis, for example, or SNS. So it's the kind of investment that you can adapt really easily. You're not stuck with it.\nYou could swap it out in very specific cases if you need something slightly different. And we covered a lot of those different advantages in the comparison episode when we talked about all the different event services.\n\n\nSo I think it's a really good one to start with. You could also build on Kafka. We mentioned Kafka already, especially if you already have a lot of Kafka skills in-house and you've got a team who can work with that.\nOr you could build on SNS. SNS is actually another good service that you could use to build a backbone. But I think EventBridge will go into some of the details on how it works maybe. But I think it's definitely a good place to start if you're not sure.\nYeah, I agree. EventBridge feels like the simplest and the most flexible option at the same time, which is really interesting. So if you don't really need that kind of extreme latency performance that you get with maybe something like SNS, probably you're going to be fine for most of the use cases.\n\n\nLuciano: So now we talked about why it's good to have separated AWS accounts, why it's good to have something like an event backbone, why EventBridge can be a good solution to implement all of that. Can we maybe propose an actual somewhat realistic example and describe a little bit more in detail how we could implement all of this stuff?\n\n\nEoin: Yeah, let's do that. And there is an article actually, there's a blog post that details this architecture and the source code as well. So we'll link to that in the show notes if people want to explore it in more detail and deploy it for themselves and see the events flowing in front of their eyes.\nSo let's go back to our typical example.\nI will caveat that it's not just an article, there is an entire repository attached to it so you get all the source code from like application code, infrastructure as code. So definitely try to have a look and deploy it yourself and play around with it.\n\n\nLuciano: So we like ecommerce examples on AWS Bites. So let's talk about our canonical ecommerce example where we've got an, in this example we tried to simplify it as much as possible so you've got an order service and a delivery service.\n\n\nEoin: So one service deals with orders and more user facing. The other one is dealing with more fulfillment. And in our example code. These are very very simple applications.\nBut remember that we said for an event backbone we're really talking about very, you know, sufficiently complex ones that would warrant their own teams and accounts to maintain them.\nAnd you can imagine for a major ecommerce vendor, your order service and your delivery service probably will have one or more teams maintaining each of them, they will be running in separate AWS accounts for each environment.\n\n\nSo one of the workflows you can imagine here is that when an order is created. We want the delivery service to be able to react to that and handle it in some way.\nAnd correspondingly, when the deliveries are fulfilled, we might want to update the state of the order to reflect that.\nSo each of those applications doesn't really have to know that the other order system exists, but they do understand that there is a concept of an order, a concept of a delivery, and each of them has a lifecycle where they transition between different states.\n\n\nSo you would deploy those to separate AWS accounts. But in this example application we actually have three AWS accounts because we've decided to deploy the backbone itself as an event bridge global bus, that's what we're calling it, the global bus into its own account.\nSo it's again, it's its own piece of functionality. And then each of these services, the order service and the delivery service will publish events to this global bus.\n\n\nThere's probably a couple of questions that might be coming up in people's minds as they talk through this.\nSo you might think, well, why do the services have to communicate with this global bus? Do they have to reach across an account in order to do anything?\nWell, there's a couple of, I suppose, fundamental principles with event bridge that you have to be aware of. And one is that if you want to have cross account rules, you can really only create rules that target another bus.\n\n\nYou can't have a cross account rule that will target a Lambda function in another account or an SQS queue in another account.\nSo if you want to, I suppose, propagate events from one account to another, you need to use, go from one bus to another bus, and then you can create the rules within an account to trigger resources in that same account.\nSo what we're basically doing is building a routing mechanism here. So each application will have its own local bus.\nAnd there's a global bus that will essentially make sure that the right events are routed through to each local bus.\nSo it might take a little bit of time to kind of see how this all plays out. But if you have a look at the article and the repo, there's a diagram and there's the source code that will show it.\nIt's not a lot of AWS resources. It just means that you kind of have to understand what the limitations are here.\nSo if I'm getting this correct, we basically have three accounts, one for the global bus, where the only thing that lives there is this global bus.\n\n\nLuciano: And then in the other two accounts, they are dedicated individually for specific services. And in every account, you have all the service related stuff plus an event breach bus.\nAnd basically when you want to dispatch a global event, you go directly to the global account. But when you want to listen for a global event, you listen from your local event bus.\nExactly.\nIs that correct?\nThat's how it works. Yeah.\n\n\nEoin: Perfect.\n\n\nLuciano: And I suppose it takes a lot of fun to configure all the policies so that the accounts are effectively allowed to read and write from all the different places, right?\nYeah. Because it's cross-account access, you need to authorize that interaction on both sides.\n\n\nEoin: The code example there hopefully gives you a template that you can use to do this in your own application. It's a CDK application.\nYou could do it in CloudFormation or Terraform just as easily, but you just need to make sure that you have the policy in place every time you add a new application with its own account.\nSo I suppose one of the questions that might come up is also, so we talked about these three accounts, the global bus, I guess, makes sense, and the local bus makes sense for each account.\n\n\nBut you have to publish to the global bus and then you kind of react to rules on the local bus. So that might seem like it's an inconsistency.\nThe reason for that is that you might say, okay, well, let's just publish to our local bus and react to events from our local bus.\nAnd we don't have to know about the global bus at all and then let the routing happen behind the scenes.\nBut there is actually a restriction in event bridge. That means you can't go and have a rule that goes from bus A to B to C.\nYou can only go from A to B basically. So you can only have one cross account event bridge rule to another bridge, to another bus.\nYou can't just keep hopping to another bus and then another bus. So that's why we basically say you publish the global bus and then that event can be distributed to all the local buses.\nAnd that's how you react to them. So you could check out the architecture diagram and it might provide a bit of clarity on how all this works.\nBut that's just how we set it up from the beginning to make sure that this will work and scale to many, many applications.\nYeah.\n\n\nLuciano: The other thing I like about this is that you need to like the architecture kind of forces you to think what is a global event, like something you want to dispatch for other services and systems to consume, as opposed to something that is maybe a local event that makes sense only in the context of a specific service.\nAnd you don't necessarily want to publish outside of your service. So that's another thing that I think this architecture forces you to think a little bit more about.\nSo I have another question. Is there any, I don't know, best practice or pitfall or something else to be aware when we implement and start to do something like this?\nYeah, I guess so. And I remember when we were talking about EventBridge specifically in the EventBridge episode, we talked about how do you troubleshoot? How do you get observability into what happens?\n\n\nEoin: How do you find out where your missing event has gone? So one of the things this application example has is logging of all events on every bus.\nSo creating an event bus is easier. And as we know from that episode, you don't have to create one at all. You get a default one with every account.\nBut in this case, we were deciding to create specifically named buses in every account, just so it provides clarity and isolation.\n\n\nWe're also providing a log group, CloudWatch log group with every bus. And we're logging all events that come into a bus into the log group.\nSo if your rule isn't working, you can look in the log, see where the event is, see if what the pattern is like.\nYou can go into the AWS console and use the tool there for testing your event, checking the match.\nSo I think that's a really good practice and I'd recommend that. But there's plenty more.\nIf you're deploying this in an enterprise context, you'll also need to think about the structure of events and schemas as well.\nAnd whether you want to enforce schema validation on events. I think there's a couple of resources that are worth pointing to on this.\nMaybe we'll cover them at the end. But there are some really good talks and blogs out there that give good advice on how to construct those events\nand how to do validation and enforce more strictness if you need to.\nAll right. That sounds awesome.\n\n\nLuciano: So I think at this point, we're just going to give people more of these resources to check out if you want to know more about EventBridge in general and other similar services.\nSo the first one is that we have a blog post on the FortierM blog. You'll find the link in the description.\nThat basically is kind of a summary of all the main features and all the things you need to know to actually use EventBridge.\n\n\nAnd then we have another blog post by Shin Breezels about how basically you could structure your payloads so that you get the best out of EventBridge.\nWe also have another blog post about SNS. So this will give you a very good comparison between SNS and EventBridge.\nThis is in the FortierM blog. And finally, actually other two resources.\nAnother one is a very good talk by Luc van Dorkins, I think, by PostNL.\nAnd it explains how they use EventBridge and event-driven architectures to scale all that system.\nAnd if all of that sounds confusing, the next step that you should be doing is going check out episode 23 of AWS Bites podcast,\nwhere we give you all the details about EventBridge. So we'll leave you with that.\nAnd thank you for following and we'll see you at the next one.\n"
    },
    {
      "title": "40. What do you need to know about IAM?",
      "url": "https://awsbites.com/40-what-do-you-need-to-know-about-iam/",
      "publish_date": "2022-06-10T00:00:00.000Z",
      "abstract": "Identity Access Management, also known as IAM, can be an intimidating service when getting started with AWS. But IAM is also one of those core services that you can’t really avoid. In this episode we try to distill down everything that you need to know to understand IAM and start to use it proficiently. We cover what IAM is, why it is so important, how authentication and authorization work, what policy documents are and how to write them, how a user or an application get credentials to interact with AWS and finally many examples, tips and tricks.\nIn this episode we mentioned the following resources:\n\nIAM access analyzer\nOur previous episode on how to manage AWS credentials\nUnderstanding how IAM Policies work (AWS Docs)\nPolicy simulator\nPolicy evaluation logic (AWS Docs)\nHow the Signature v4 (sigv4) algorithm works (AWS Docs)\nActions, resources, and condition keys for AWS services\nVideo IAM Concepts by beabetterdev\nRe:Invent session “Getting started with AWS identity” by Becky Weiss\nRe:Invent session “Become an IAM Policy Master in 60 Minutes or Less” by Brigid Johnson\n\n",
      "transcript": "Eoin: Identity and access management, also known as IAM,\nis one of those services that you cannot escape when learning AWS.\nIt can be a bit intimidating to learn it at first.\nSo today we are going to try to distill down\nwhat you really need to know about IAM.\nAnd by the end of this episode, you will know what IAM is\nand why it's so important,\nhow authentication and authorization works with IAM,\nhow policies work, how users and applications can get credentials,\nand a few general interesting tips and tricks.\nMy name is Eoin, I'm joined by Luciano,\nand this is the AWS Bites Podcast.\nIAM\nIAM\nIAM\nIAM\nSo this is a video for starters,\nbut also for those who are struggling,\nlike a lot of people, I feel,\nwith making sense of the various concepts of IAM.\nHow do we start with this one?\nWhat is IAM? How would you just explain it to a beginner?\n\n\nLuciano: Yeah, I guess the simplest way to describe IAM is just define the acronym.\nSo it means Identity and Access Management,\nwhich basically tells us that it's a service in AWS\nthat allows us to define identities and access\nto different resources.\nIn another way of saying it,\nit's basically a way to apply fine-grained permissions\nto different types of services and resources.\nAnd it is very important\nbecause it's not like an isolated service.\nIt really has ramification in everything you do with AWS.\nSo you really need to understand how it works\nbecause probably in your day-to-day work with AWS,\nyou need to define permissions.\nYou need to understand what's the language\nbehind those permissions, the policy language,\nand how to read and write different policies.\nSo all the stuff means that you basically need\nto be very familiar with IAM to be proficient with AWS.\nSo, yeah, I guess maybe we can explain next\nhow does it work at a very high level.\nWhat do you think?\n\n\nEoin: Yeah, let's try that.\nSo the main thing with IAM is that you're trying to grant\naccess to resources for specific principles of some sort.\nSo a principle is going to be a user or a service.\nSo maybe you can think about the formula\nas being made up of who.\nSo what's the user or the service?\nWho's trying to access something?\nAnd what level of access you're giving them?\nSo what are the actions that you want this user or service\nto be able to perform?\n\n\nAnd what are the resources associated with this action?\nSo who are you giving access to?\nWhat can they access and what actions can they perform?\nSo if we break that down a little bit, right,\nwe're talking about resources.\nSo what is a resource?\nAWS has hundreds of services and each of those services\ngenerally allows you to create resources.\nSo we're talking about an S3 bucket is a resource,\na Lambda function is a resource,\nan API gateway, API is a resource.\n\n\nThey have other things like a load balancer,\nan EC2 instance or a CloudWatch log group.\nEverything is a resource, right?\nAnd you can define access to those resources\nor to within specific subsets of those resources\nin some cases, actually.\nThen we can talk about the principle, right?\nSo the principle, as we mentioned, could be a user,\nbut it could also be an AWS service itself.\nSo people may be familiar with an IAM user.\nWhen you, IAM user is one of the older principles there.\nSo you can create users in IAM and use that essentially\nas a way for people to log on to AWS and do stuff.\nAnd you can log on by the username and password\non the console, or you can use keys.\nWhat about services?\nSo what, can you think of some good examples\nthat would be illustrative of what a non-human interaction\nwith IAM would be?\n\n\nLuciano: Yeah, for instance, I don't know,\nif you are creating an application that runs on an EC2,\nfor instance, that application probably needs to access\nall the resources in AWS, maybe needs to send a message\nto an SQS queue, maybe needs to write files to S3.\nSo that application will need somehow to be authorized\nto perform those actions.\nAnd yeah, it's important to understand that by default\nin AWS, everything is blocked by default.\nSo if we don't explicitly say that that application\nis authorized to perform those actions on those resources,\nyou will get a permission error when running the application\nin an AWS environment.\nSo that's, I think, a good example and applies\nto every compute layer, even Lambda or ECS,\nthey would need to have some sort of policy applied to them\nthat guarantees them that they can perform certain actions\nagainst AWS APIs and resources.\n\n\nEoin: Okay, that's interesting that you mentioned\nthat everything is, you don't have access by default,\nyou're denied by default.\nSo it becomes a little bit challenging, I guess,\nyou have to understand how do you grant access\nto this one specific thing for the specific principle\nunder specific conditions, maybe.\nAnd that can be hard to get right and probably explains\nwhy a lot of people like to use wildcards for everything\nand grant access, because it's just easier.\nBut of course, maybe something people can take\nfrom this episode is that it is possible to be specific.\nIt's just a question of understanding how.\nSo where do we start?\nHow do you define those relationships between users,\nactions and resources?\n\n\nLuciano: Yeah, I think the main concept to explore\nis this concept of policy.\nWe already mentioned it a few times.\nSo what does it mean?\nA policy is basically a document that is generally written\nin JSON format.\nAnd this document contains like a description\nof a particular permission or set of permissions.\nSo let's try to describe what are the main parts\nof a policy.\nGenerally a policy contains one or more statements.\n\n\nSo in a policy, you could have multiple statements,\nbut yeah, so imagine that a policy starts with an array\nof things in a way, right?\nAnd in this array is literally where you try to define\nthose relationships between the different concepts\nthat we defined so far.\nSo there is going to be resources, actions\nand other things.\nSo let's go in order and describe all of them.\nGenerally the first thing that you see\nis something called effect.\n\n\nSo there is a property inside a statement called effect\nand the value of this property can be either allow or deny.\nNow this is interesting because most of the times\nyou will need to write policies with an effect allow\nbecause you are trying to allow access to something.\nRarely you will need to write explicit denies\nfor specific actions.\nSo I don't want to go too much into detail right now,\nbut imagine that you need to explicitly say,\nI want to allow something to happen\nor I want to deny something to happen.\n\n\nMost of the time you're just gonna write allow effects.\nThen we have another section called action\nand action is actually interesting\nbecause it can be either just a string\nor an array of strings.\nAnd the idea is that you can put together\nin a single statement, multiple actions.\nAnd an action is a string that identifies a specific action\nthat you could perform against a set of resources.\nAnd they are generally name spaced for instance,\nby service, I think is most commonly the case.\n\n\nAn example can be S3 create bucket.\nAnd an interesting thing is that you can specify\nwildcards there.\nSo for instance, you could say S3 column asterisk,\nthat means any action available in the S3 service,\nI am for instance allowing it if the effect is allow.\nYou can also wildcard more specifically,\nlike you can narrow down a specific subset of S3 actions.\nFor instance, you could say S3 column create asterisk,\nthat basically means all the action names\nthat starts with S3 column create,\nlike S3 create bucket or S3 create access point,\nor I don't even know if there are others.\n\n\nBut yeah, this is generally common for instance,\nto try to distinguish read and write actions.\nSometimes you can classify all the read actions\nwith like a prefix and then an asterisk\nand similarly with the write actions.\nIt can be a little bit dangerous\nbecause sometimes you end up allowing more permissions\nthat you really want to.\nSo just word of conscious,\ndon't try to use asterisk as less as possible,\nunless you are really sure that you are describing\nthe very limited subset of actions\nthat you are interested into.\n\n\nAnd then the last interesting part of a policy is resource.\nAnd again, resource is an array or a string.\nSo you can have either one single string\nor multiple strings\nbecause you can group together multiple resources.\nHow do you define a resource?\nLike what is the content of the string\nthat identifies a resource?\nAnd you generally do that using an ARN,\nwhich basically means Amazon Resource Name.\nAnd you can imagine that as a unique ID\nfor every single resource that exists in an AWS account.\n\n\nThe interesting thing is that this unique ID\nare not like pseudo random values,\nlike I don't know, increment that integers or UUIDs,\nbut actually they have a very well-defined structure.\nSo you can follow like a namespace\nand then you can have an account region and so on.\nSo basically it's like a tree\nand you can also use wildcards at some point to say,\ntake all the sub tree there.\nAnd the common example is when you want to allow,\nI don't know, for instance,\nread access to a set of files in S3,\nyou can do that either by prefix\nbecause you can say, I don't know,\nthe name, the ARN of the packet\nslash a certain prefix and then asterisk,\nor I don't know,\nif you want to give access to the entire packet,\nall the files in the packet,\nyou just say the ARN of the bucket slash asterisk.\nSo it's actually a really interesting approach from AWS\nto give you this ARNs in a way\nthat you can easily define expressions\nto describe an entire subset of resources.\nAnd then I think there is a more advanced use case\nabout condition.\nDo you want to talk about that, Eoin?\n\n\nEoin: Yeah, that's probably worth talking about\nbecause I think conditions\nare becoming more and more common.\nThey were less so in the past,\nbut they're becoming more expressive.\nSo conditions allow you to have more fine grained access\nfor specific resources.\nSo you can grant access to S3 to create a bucket,\nfor example,\nbut you can also restrict it to certain conditions.\nFor example, you might only want to have a condition\nwhich allows people to create objects in S3,\nbut only if they have a.lutiano extension.\n\n\nThat's one example.\nThere's also other ones like based on the tag\nand the request or the tag of the actual principle,\nyou can kind of use it to enforce access control\nbased on the user's principle.\nAnd every specific service has a different set of conditions\nthat they support,\nas well as there being global conditions\nthat are supported as well.\nSo other examples are,\nif the source IP address is within a certain CIDR block\nor you're coming from a certain VPC,\nthere's lots and lots of different conditions supported\nand that's really growing, like I said.\nSo one of the things we can link to in the show notes\nis a document I really, really like,\nand I have bookmarked,\nwhich outlines all the actions, statements, resources,\nand conditions that are supported\nfor all the different AWS services.\nSo it's definitely a go-to\nwhen you're trying to develop policies.\n\n\nLuciano: Very complicated issue,\nbut you can be extremely specific.\n\n\nEoin: Yeah, and it sometimes feels like a real burden\nhaving to do this for people,\nbut it's really powerful as well\nbecause it allows you to enforce really good security.\nMm-hmm.\nWhere do people start then as a user\nwhen they're trying to get up and running with AWS,\nmaybe in their own account\nor in an account they've been given in their organization?\n\n\nLuciano: Yeah, that's a very good question.\nSo I suppose as a user,\nif you're just starting to learn AWS,\nit's probably okay to have an admin role\nand just use that to log into the web console\nand play around and try to create different things,\ndestroy them and see what happens.\nBut of course, that's not acceptable for a production account.\nLike if you have a production account\nwhere you are deploying production applications,\ntry to limit as much as possible write access for humans.\n\n\nLimit that to read access\nbecause of course you need people to be able to log in\nand check if everything is working correctly,\nread the logs and all these kind of operational tasks,\nbut try to limit the amount of write that everyone can do.\nAnd a much better approach in that case\nwould be to have an account that is dedicated to deployment\ninto production accounts.\nYou provision CI-CD pipelines in those accounts\nand those pipelines will do all the changes\nin the production account for you.\n\n\nAnd the idea is that you reduce as much as possible\nlike uncontrolled write access from people\nand you try to do that in a programmatic and auditable way.\nSo you try to have processes that will do that\nwith a very well-defined, I don't know,\npipeline or set of steps\nand all the steps will give you all the trails\nthat you can use in the future to make sure\nthat everything is happening as expected.\nThis is kind of the best practice.\nSo don't be afraid to just use an admin user\nfor your own testing stuff,\nplay around and try to learn as much as possible.\nBut when you start to move to production,\nthen having an admin account there is kind of a big no-no.\nSo try to be careful at that point.\nAnd I think we can approach the same topic\nalso for applications.\nSo are there, I don't know, best practices,\nhow do you start to define permissions\nfor applications instead?\n\n\nEoin: Yeah, ideally you want anything that's important\nin your system not to be triggered by a user,\nbut to be automatically triggered by some code running\nin EC2, Lambda or a container maybe.\nSo the idea there is that you will create a role,\nan IAM role and you attach policies to it.\nSame kind of policies we've already described.\nSo you're giving permissions to that programmatic piece\nof code running there to write to the bucket\nor to put a message on the queue.\n\n\nAnd you try and be restrictive\nand you would then associate that role\nwith the resource in some way.\nSo with an EC2, you have like an instance role\nor in Lambda, you have an execution role\nand lots of AWS services have a place\nwhere you can link the role that that service\nis going to use.\nAnd when you create that role, you have to say,\nI'm going to trust EC2 to assume this role.\nAnd the idea then is that you try to keep that minimal\nas possible because if access is ever compromised\nto that compute environment, you want to minimize\nthat blast radius, minimize what people can do.\nSo there's a couple of other things there.\nMaybe it's worth mentioning the identity\nand access management access analyzer tool\nthat you can use and it can analyze your roles for you\nand let you know if you've over assigned privileges\nthat aren't being used.\nThat's a really good one.\nIs there anything that people should be aware of here\nwhen they're developing code, creating a role for it?\nSome people are probably used to running that code\nalso locally.\nWhen they run it locally,\nthey're not necessarily using the same privileges.\nWhat kind of practice would you recommend\nto make sure that people have a good understanding\nof what to watch out for there?\n\n\nLuciano: Yeah, that's a very good one.\nI think we'll explain a little bit more later\nin this episode, how the whole credential system\nworks and maybe at that point, more gaps will be filled\nand everything will be more clear.\nBut basically one thing that happens\nwhen you are testing something locally,\nwhen you're running, for instance,\na Lambda function somehow locally,\nis that generally you have authenticated yourself\nthrough the AWS CLI using your own personal credentials.\n\n\nSo as a user, you probably have in a development account,\nadmin credentials or very extensive set\nof permissions anyway.\nSo you are basically running your local environment\nwith those credentials.\nSo what can happen is that you have a false sense\nof security because you see your application being able\nto do a bunch of different things and you think,\nokay, this application is ready to go\nand be shipped somewhere.\n\n\nThen you deploy that application to some environment\nand as soon as you run it, you bump into a permission issue.\nAnd there you have to realize that there is a disconnect\nbetween when you run things locally\nand that local environment inherits\nyour own personal permissions as opposed\nto when you run the same application\nin an AWS environment.\nAnd in that environment,\nthe environment is not necessarily inheriting\nyour own permissions.\n\n\nIt's probably gonna have a more restrictive set\nof permissions that maybe you haven't even defined.\nSo the permission set is literally as small as possible\nfor that particular application.\nSo yeah, it's important to understand how to basically go\nand define those application permissions\nand make sure that they actually work\nfor your production use cases.\nI was hearing, I think you mentioned it to me a few days ago\nthat on Twitter, somebody was mentioning\nthat that might be an interesting feature\nfor tools like SAM to be able to actually run\na local simulation with real credentials.\nThe ones you are specifying for the function, because that's the tricky bit.\nEven if you are specifying a role, for instance,\nwhen you use SAM for a particular Lambda function,\nthen when you run that function locally,\nit's still using your own personal credentials\nand not the roles that you have defined.\nSo even if you define the roles,\nit can be very misleading, that experience.\nYou might think, okay, it's using these roles\nthat maybe you think you define correctly,\nthen you ship to production.\nThere was an error, but you haven't tested that locally.\nYou only realize in production.\nSo that's another thing that might bite you,\nespecially at the beginning.\nAnd it's a little bit tricky to realize\nwhat's actually going on there.\n\n\nEoin: Yeah, that's a good point.\nI think we've talked about policies\nbeing attached to roles.\nIt might be worth just covering briefly where policies fit\nin with the various other pieces that they can be attached\nto.\nSo we said we can attach them to IAM users\nand also IAM user groups.\nNow, a lot of people say don't use IAM users\nin groups before.\nAnd we did cover this a little bit in episode 12,\nwhen we talked about how to manage your AWS credentials,\nlink will be below.\n\n\nAnd the alternative to that is to use AWS single sign-on.\nWe're not going to go into that in much detail now,\nagain, refer to that previous episode,\nbut you can also attach your policy statements,\nput them into these SSL permission sets,\nand they kind of get converted into IAM roles\nunder the hood.\nBut you also have specific services\nthat can have policies attached.\nSo these are sometimes called resource policies,\nbut an example of a resource policy is an S3 bucket policy.\n\n\nAnd that allows you to have specific policies\nthat are attached to the resource itself.\nAnd that policy gets combined with all the other policies\nthat come into play, like your identity policy.\nAnd the two policies together will work\nto determine the permissions.\nWe've already covered then the EC2 case\nand the ECS case where applications\nwill generally assume a role\nand will inherit policies and permissions from that role.\nSo it's probably, maybe we can briefly cover\nthe interesting technical details\nabout how AWS recognizes who you are,\nwhat your principle is,\nwhether you're using a user or an EC2 instance,\ntakes that, what it knows about you,\nand converts that into some sort of allow or deny\nfor every API call or SDK call you make.\n\n\nLuciano: Yeah, I think that's a very good way\nof understanding a little bit more how things work\nand therefore make the right assumptions\nwhen you are building an application for AWS.\nSo there are generally two phases.\nThe first phase is authentication\nand the second phase is authorization.\nAnd authentication is generally,\nAWS needs to understand who you are\nas a person or as a service.\nAnd then authorization is based on the principle\nthat was identified during the authentication phase.\n\n\nIs this principle authorized to perform a specific action\non a certain resource?\nAnd in a certain context,\nif you put also the condition set\nthat you can have in a policy, right?\nSo all these rules needs to be evaluated,\nbut the rules are evaluated only after\nthat it's clear to AWS who the user actually is.\nSo it's very important to distinguish these two phases.\nNow, how do you define what's the user?\n\n\nIt changes a little bit,\ndepending if you are doing a logging on the web console.\nSo it's kind of a manual user\nthat is in the web console doing things.\nOr if you're doing something programmatically\nthrough either the CLI or the SDK,\nwhich bot are using underneath the AWS APIs.\nSo when you are logging in manually,\nyou generally provide your credentials\nthrough username and password.\nOr if you use federated login,\nyou probably go through a login flow,\na federated login flow that involves, I don't know,\nyour Google identity provider or Azure ID, whatever.\n\n\nInstead, when you're doing something programmatically,\nyou probably have seen that there is this concept of keys\nto identify credentials.\nAnd keys are generally a couple of access key\nand secret access key,\naccess key ID and secret access key, actually.\nAnd this is generally what you need to use\nwhen you authenticate your local CLI,\nbut also what you could use when you do API calls\nor use the SDK.\n\n\nNow, it's very interesting.\nThere is something I really like,\nand I'm maybe a little bit nerdy about this stuff,\nabout the authentication protocol\nthat is actually used underneath when calling APIs on AWS,\nbecause it's a signature-based protocol.\nAnd I really like this kind of protocols.\nBut the idea is that basically\nmost of the applications when you write,\nwhen you need to have some sort of authentication,\nthere is a flow that is like,\nokay, I'm gonna send my credentials somewhere.\n\n\nI'm gonna get a session token or a cookie or whatever.\nAnd then I keep using that session ID\nto communicate with a backend, right?\nAnd that's a little bit annoying\nbecause you have these two distinct phases.\nI need to get my authentication first with my token,\nand then with the token, I can do whatever I want to do.\nIn AWS, you don't need to do all of that\nbecause you already have your secret access key\nand your key ID.\n\n\nAnd the way that it works is basically\nevery single request is signed using your secret.\nSo you never really send the secret to AWS.\nYou only use it to sign every single request.\nSo this way you can just fire the request straight away\nwithout having to create a session first.\nAnd this is documented in the SIGV4,\nwhich stands for signature v4 protocol.\nSo it's actually well-documented and you can read it.\n\n\nYou can even implement it yourself if you really want to.\nBut I just find it very interesting\nbecause it's something that I always liked from AWS.\nAnd sometimes I even implemented myself in my own APIs\nwith my own authentication.\nNot necessarily like 100%,\nbut you can take some principles from this approach\nand use them yourself.\nSo we're gonna have a link to the protocol\nin the show notes.\nBut there is another interesting way,\nmaybe Eoin you want to talk a little bit more about that.\nLike if you are an application,\ndon't just copy paste those credentials into the application.\nBut yeah, how do you do it in that case?\n\n\nEoin: Yeah, that's a good point.\nYou want to avoid generally copying,\npasting credentials everywhere.\nSo there's lots of tools you can use\nand you can look into it more\nto avoid having to copy paste credentials,\nparticularly with SSO.\nBut if you're talking about your application, right?\nSo your EC2 instance or your ECS container, for example,\nyou never have to provide environment variables\nfor a process running in that EC2 instance,\nlike your secret access key and access key ID.\n\n\nYou don't have to put those into environment variables.\nYou do not want to do that.\nInstead, there are mechanisms already in place.\nWe talked about the role that you can attach\nto your EC2 instance and the role you can attach\nto containers and Lambda functions.\nAWS already has a mechanism built in\nthat will seamlessly under the hood,\nallow that instance to get authorized access\nto the resources in that role's policy.\n\n\nSo if you've got an instance role attached\nto your EC2 instance, AWS has something called\nthe instance metadata service.\nThere's actually two versions of it,\nversion one and version two.\nYou should use version two because it's the secure one.\nBut it allows you, under the hood, it will, the SDK,\nif you make an SDK call to put an object in a bucket,\nit will use that metadata service to check\nthe instance role and authorize the principal,\nauthorize that code to perform that action or not.\n\n\nSo you don't, there's a basic, essentially a little HTTP\nendpoint available inside of EC2 and also ECS\nthat performs that for you under the hood.\nYou can look into the details of that if you're curious,\nbut otherwise it will just work.\nThe AWS SDKs are already designed to check for credentials\nin a specific order, and it'll vary depending\non which language you use, but generally it's looking\nat environment variables, then your credentials file,\nand then these metadata services.\n\n\nSo that's something to bear in mind,\navoid you putting secrets in environment variables\nas always.\nAnother one to, another thing to be aware of is the STS.\nWhen you're dealing with IAM, from time to time,\nyou'll see the session token service or STS pop up.\nIt's used in a lot of different places, but very briefly,\nif you've got a federated identity, like sign on\nwith Google or maybe SAML sign on with Active Directory,\nyou can use the session token service to exchange\nyour federated identity for some AWS credentials.\n\n\nAnd that's gonna be really useful.\nIt's used in SSO, but it's also used in Cognito\nfor providing access to the front end to gain direct access\nto services on the backend.\nYou can also use it very effectively if you want to,\nif you have a user who's authenticated in one account,\nthey can use it to get access to another account.\nSo the account you're trying to get access to can grant\ncross account access essentially for you to assume a role\nin that destination account.\nAnd it means that you don't have to create users\nand have credentials for every account you need to work in,\nwhich would be very difficult to manage\nand ultimately not very secure.\nSo STS allows you to do an assume role action.\nAnd if you're somebody who ends up working\nwith multiple accounts, you'll end up using that explicitly\nor implicitly one way or the other.\nLike even in the AWS console, when you switch role\nthat will use an STS assume role under the hood.\nSo it's great because it avoids you having to create users\neverywhere and manage lots of credentials.\n\n\nLuciano: Yeah, absolutely.\nDo we have any final tip maybe before?\n\n\nEoin: I think, yeah, I think at this point, right,\nthere's so many resources that are really powerful, right?\nBut one thing to mention is that we've got lots\nof different types of policies.\nWe talked about the identity based policies.\nWe talked about resource policies.\nYou also have something called a permissions boundary,\nwhich is like another set of policies\nthat can be attached to your role\nthat restrict further what that role can do.\n\n\nOrganizations sometimes use this\nto be more, to provide more guardrails.\nAnd then you also have the service control policy,\nwhich is like the organization level control\nthat says within this account or this set of accounts,\nyou cannot do these things.\nIt's something, it's the kind of policy actually\nwhere you would encounter deny quite often.\nAnd all of these things work together.\nIt's generally like an intersection\nof all of the different policies,\nbut there's a really good document\ncalled policy evaluation logic on the AWS docs,\nwhich we can link to.\n\n\nAnd it's got a nice diagram of the whole flow,\nhow at the start it results in an allow or deny.\nWe've got some other really useful references\nin the show notes.\nI want to call out a few videos though, finally.\nThere's quite a good one on IAM core concepts.\nIf you want to just explore the fundamentals there,\nit's on, it's a Be a Better Dev YouTube channel.\nAnd we've also got two really good ones,\nwhich I tend to share quite frequently.\n\n\nOne from Becky Weiss at AWS about AWS identity\nand another one from Bridget Johnson,\nwhich is a really good one called\nbecoming an IAM policy master in 60 minutes or less.\nI think if you understand everything in those videos,\nyou will truly be in the top 5% of IAM users in the world,\nbecause there's just so many useful tips in them.\nSo thanks very much for joining us\nand for sharing and liking these episodes.\nPlease send us your feedback\nand let us know what else you'd like to learn\nabout security or any other topic.\nAnd we'll see you in the next episode.\n[\"Dance of the Sugar Plum Fairy\"]\n"
    },
    {
      "title": "41. How can Middy make writing Lambda functions easier?",
      "url": "https://awsbites.com/41-how-can-middy-make-writing-lambda-functions-easier/",
      "publish_date": "2022-06-17T00:00:00.000Z",
      "abstract": "Lambda functions are small units of code that achieve a specific purpose. It’s always a good idea to keep your code short, clean and simple. And yet, sometimes you find yourself writing lots of boilerplate code in every function to do common things, like parsing events, validation, loading parameters and lots more.\nThe Middy.js framework was designed to help you keep Node.js Lambda function code simple, letting you focus on the business logic and clearing away duplication and boilerplate. By the end of this episode, you will know: How Middy.js works with JavaScript and TypeScript, how to perform validation, event parsing and parameter loading, and how you can even write and use your own Middy middleware. Finally you’ll get to know who is using Middy and how you could contribute to the Middy ecosystem.\nIn this episode we mentioned the following resources:\n\nMiddy Website and documentation\nHow to get started with middy (official docs)\nMiddy official middlewares (official docs)\nHow to write your own middlewares (official docs)\nMiddy integrations (official docs)\nInterview with Taco Bell in an episode of Real World Serverless where they mention how they use middy\nOpen source projects using Middy\nLambda Power Tools for TypeScript Middy Integration\n\n",
      "transcript": "Luciano: Lambda functions are small units of code that achieve a very specific purpose.\nIt's always a good idea to keep your code short, clean and simple,\nand yet sometimes you find yourself writing a lot of boilerplate code in every function to do common\nthings like parsing events, validation, loading parameters and a lot more.\nThe Middy.js framework was designed to help you keep Lambda function code simple,\nletting you focus on the business logic and clearing away duplication and boilerplate.\nBy the end of this episode, you will know how Middy.js works with JavaScript and TypeScript,\nhow to perform validation, event parsing, parameters loading,\nhow can you even write and use your own middlewares with Middy.\nAnd you're going to learn a little bit about the history of Middy, who is using Middy right now,\nand how is the community evolving around Middy.\nMy name is Luciano and today I'm joined by Eoin and this is AWS Bites podcast.\nLuciano, I know that you created Middy.\n\n\nEoin: I remember hearing about it all the way back when it launched in 2017, 2018. How did it come about and why did you start it?\nYeah, it's an interesting story. I'm going to try to summarize it for a minute.\n\n\nLuciano: But basically I was working with this company, which was a spinoff of USB,\nwhich in Ireland is one of the main electricity providers.\nAnd we were building an innovative project around energy trading.\nAnd we decided to build it entirely serverless, which I think it was very brave at the time.\nThis was around, I think 2016 was actually the year.\nAnd so it was the very beginning of serverless. Lambda was still quite new.\n\n\nThere wasn't really a lot of documentation out there and case studies.\nBut we were really excited about this idea because the project was like a startup and we wanted to\nkeep different components very simple and then build on top of those and evolve it that way.\nAnd funny enough, our assumption was that if we use Lambda, then our code is going to be very\nsimple and we're going to focus strictly on the business logic because everything else is done for\nyou by Lambda, by the runtime itself. That was kind of our initial assumption.\n\n\nAnd then we realized very quickly after we wrote the first prototype that in reality,\nour Lambdas were far from simple. There was so much boilerplate in every single Lambda\nand there was so much inconsistency because we were literally copy pasting this boilerplate\naround and then we were not keeping it in sync. And at that point, we realized that\nthere must have been a better way to manage all our code base and avoid all this duplication\nand make sure that our function would be, I suppose, as focused with the business logic as possible.\n\n\nSo basically, we kind of explored some of the ideas that we have seen in frameworks like\nExpress.js or other Node.js frameworks. And we thought, OK, in those frameworks,\nyou literally have the same problems, even though you build more monolithic applications.\nYou still have to do a bunch of things like validation, authentication, and response\nrealization, all these kind of concerns that generally go around the actual business logic\nthat you want to implement in a particular endpoint. So we kind of took inspiration from\nthe way that frameworks like Express solved that problem by basically using middlewares and trying\nto push all this concern outside your controllers or Lambdas or whatever you want to call your unit\nof business logic for an endpoint. And we tried to apply that same principle to our Lambdas.\nAnd at that point, we realized, OK, in a Lambda, you don't really have something like Express,\neven though at the time there were already ways to put Express in a Lambda, but we didn't feel\nthat was the right way of doing it. And we also wanted something a little bit more generic that\nwe could use even for non-HTTP related Lambdas. So we basically ended up implementing our own\nmiddleware engine for specifically built for Lambda. And then we used that and then helped\nus a lot to simplify our code and remove all this duplication. All this generic boilerplate code\nbecame a unit that we could easily write once, make it testable, reuse it, keep evolving it,\nand then consistently use it everywhere else. And that was it. We basically used it for about one\nyear and we were very happy with it. And eventually we decided to open source it. And that was\nbasically how Midi came to be. OK, very good.\n\n\nEoin: And I guess so you and your team and this startup were the original contributors. Have you managed to grow much of a community around it? Are there\nother maintainers now?\n\n\nLuciano: Yeah, that's an interesting story because just shortly after we open sourced this project, then the company I was working with effectively stopped. It was a startup,\na very experimental project, kind of a spin-off board just to experiment a particular idea.\nBut then eventually they decided not to go ahead with that idea. So the whole project ended and\neveryone found a different path working for other companies. So the main core group kind of dissolved\nat that point and everyone was doing something else and people were not really interested in\ncontinuing working on Midi because they didn't really have a use case anymore. So because I'm\nvery passionate about open source and I felt like there is something there that is worth continuing\nand some people were starting to use it and they were very happy. They actually found it was\nsolving the same problems for them that we saw. So we realized, okay, there is value in the community\nfor something like this. So what I did, even though I moved to a company where I was not doing\nthat much serverless anymore, I kept maintaining Midi for about another year. And meanwhile,\nthere was a little bit of community that organically formed around the open source project,\nlike people that just were coming randomly asking for help or maybe submitting PRs and contributing\nin all sorts of different ways, writing documentation as well. And among these people,\nspecifically, there was Will Farrell, who kind of was one of the main contributors and he was\nhelping a lot in making sure that Midi was like a serious project, not just something done and\nleft on GitHub and people might just copy paste things. Yeah. He was literally putting a lot of\neffort in making sure that it was always up to date, documentation was clear, there were examples\nand also adding more and more middlewares because Midi is not just the runtime, but there are also\na bunch of built-in middlewares that you can just use and configure. So eventually I decided I was\nnot having enough time and focus to continue being the main maintainer of Midi. So I asked if somebody\nwanted to step in and take over and then Will decided to do that. And that was, I think, around\n2019 and 2020, but I think we kind of officialized that in 2020 with the V1 release. Okay.\n\n\nEoin: So I remember using the pre-V1 release and you could use Midi, you installed one package, it came,\nI think, with a bunch of in-built middlewares and you can also write your own. How has it changed\nsince then? Because I know that you've had another milestone release recently.\n\n\nLuciano: Yeah, I think the first big change was that when we started working on Midi, it was still the time where everything\nwas callbacks, even like writing a lambda, it was like the signature was your function, then event,\ncontext and callback. And actually Midi did support already a way to use callbacks,\nto not use callbacks, but to use async-await and promises. But at the time, async-await wasn't even\navailable in different version of Node.js that were mainstream. So basically the way it was\nworking was kind of using Babel, you needed to transpire your code and then it was just giving\nyou an interface. But then at the end of the day, your lambda was still being exposed to the lambda\nruntime as a callback-based function. So it was kind of an abstraction layer, it was a little bit\nmessy. And I think that's something that we kept doing throughout all the version 0.x. And then\nwith version 1, I think that the ecosystem was mature enough to start to use async-await\nconsistently. So then it was, we decided to go with version 1 because we kind of cleaned up\nall that mess and make it much more integrated with the ecosystem, like basic, real async,\nand not just like a simulation of all of that through transpilers. So that was the first big\nmilestone. And also in that milestone, we also, initially Midi was like very monolithic. It's just\nlike one package and you get everything, the core middleware engine, but also, I don't know,\nI think there were something in the order of 10 or 12 different middlewares and all their own\ndependencies, different middlewares might have different dependencies. So it was like a very,\nit wasn't like really a small package. Like you really needed to have a strong use case to\nmake sense for you to import that package and include it in your lambdas.\nSo we decided, okay, if we break this down into smaller units and we do like a monorepo\nwhere every unit is published independently, then people can just install the core and then only the\nmiddlewares that they really need to use. And this way we can offer an API that is much more\nlightweight and it's not going to affect your Lambda runtime because you are importing only\nthe code that you actually need. So this was another big change from version 0 to version 1.\nEverything became like monorepo. We started to adopt the atmid namespace on npm, and then you\ninstall atmid decor and then you decide to install all the other middlewares independently.\nSo that was, yeah, I suppose the story of experimenting with version 0.x and getting to\na state with version 1 where we felt, okay, this is really something that people can use and have\na good experience. Okay, nice. So you mentioned all these different middlewares then.\n\n\nEoin: So what are some of the common things you can do with Middy? Maybe we could talk about some of the canonical\nexamples with these core middlewares.\n\n\nLuciano: Yeah, so there is actually a page in the documentation, and we'll drop a link in the show notes, that basically showcases all the official middlewares.\nSo we have this concept of community-maintained middlewares, but also official middlewares.\nAnd the difference is that we recognize that there are a bunch of use cases that are so common that\nit's worth to have those use cases solved within Middy. And every time we do a new release, we make\nsure that all these middlewares are maintained and they work well with the new changes that we might\nhave introduced in the new version. So that's why we have this list of official middlewares,\nand we basically maintain them together with the core engine. But then, of course, there is an\nactive community and people are creating all sorts of middlewares that are useful to them. So in the\nwebsite, you can also find a list of community-maintained middlewares, and they are not necessarily\nalways up to date or tested together with the core, but we kind of got a selection of the ones\nthat we think are reasonably well written and you might use without too many issues.\n\n\nSo the ones that are in the core, I'm just going to mention a few. We kind of group them in different\ngroups. There are ones that are related to basically like handling input, I don't know,\ndoing validation with the input or certain events in AWS are a little bit flaky. There are\ncertain gotchas that are not obvious, like I don't know, certain strings are encoded in ways that you\nmight not expect. And we have also middlewares that will normalize your JSON for you, basically\ngiving you a cleaner JSON and you don't have to think... For instance, the one use case that I\nthink is worth mentioning just to explain this better is S3 events. So when you have a file in\nS3, if that file contains... the path of the file contains certain characters, you will receive\nan object as an event that says the key of this file is a string, but that string encodes the\nspecial characters in a certain way. And that might be actually, has been for me, source of\nbugs in the past because I never realized that that string was encoded until I actually had the\ncase where it was using special characters. And then my lambda would explode because I\nwould just take the string as this and use it without realizing that I needed to decode it\nfirst. So we have a normalizer that will take care of, make sure that if there is any special\ncharacter, when you get your event, it's already converted to a proper clean string that you can\njust use. I think the example is if you have a space, rather than getting a space, you get a\npercentage 20 or something like that, or a plus. I'm not really sure, but it's one of those gotchas\nthat, yeah, you don't expect. So this is one class where you can kind of simplify handling inputs\nand validation and make sure that the events are clean enough so that you can just use the data\nwithout having to do additional conversion. Then there is also parsing stuff. For instance,\nif you are building, I don't know, an API that receives data from a form, you might want to use\nthe proper algorithm to decode that form encoded input. Or if it's a JSON, you don't want to do\nJSON parse manually. Maybe you just want to have the body already parsed as an object. Or I don't\nknow, if it's XML, because you are implementing an API that needs to receive XML, there is a parser\nfor that as well. And then there is also something similar for responses. So if you're building an\nAPI that needs to send a response in a certain format, like again, JSON or XML or YAML, whatever,\nyou can have your own serializers and do that. And the best part is that there is also a content\nnegotiation middleware, where if you want to build an API that can receive different types of inputs\nand response in different formats, it follows the HTTP specification to negotiate, OK, I am\nreceiving an XML and I expect to receive back an XML, your lambda business logic remains completely\nabstract from all of that. It just needs to receive an object and produce an object back.\nAnd then this middleware takes care of the serializing and re-serializing\nrequests and response respectively. So basically you have all these middlewares to try to focus\nmore and more on the business logic and leave all these extra concerns to the middleware layers.\nThat sounds really great.\n\n\nEoin: So let's say if I've got a set of lambda functions and I've been doing serverless for a few months or maybe even years, but I realize that every time I'm doing JSON.parse\non the body and I have to construct a response that has the status code and the encoded result,\nand I'm thinking, OK, this is causing bugs. There's duplication everywhere. I want to clean\nthis all up. How do you get started then with Middy? What's the process?\nYes, so I will say that again, I'm going to point on the documentation.\n\n\nLuciano: There is like a getting started section, which gives you examples and so on. But I think the main thing you should do is\njust do npm install at Middy slash core and that gives you just the middleware engine.\nJust the middleware engine. And at that point you need to decide, OK, what am I doing? Am I building\nan API? Do I need to parse JSON? If I need to do that, I can install the Middy at Middy slash\nHTTP JSON body parser. And similarly, you can install a bunch of middlewares that you think\nyou're going to need, like validation, error handling and so on. And then the way that Middy\nchanges your way of writing lambda is actually very subtle. Like it's not very, it doesn't force\nyou to change your coding style too much because you are still writing your handler in the same way.\n\n\nYou are still writing the same signature of a function. The only difference is that for every\nhandler that you write, you need to, let's say something we call midify the handler,\nwhich basically means take that handler and wrap it with this middleware layer, middleware runtime\nengine. So this is literally a function that you import from core that is called Middy. So you just\nneed to say, call Middy, pass the handler inside and you are basically getting a new instance of\nthe same function handler, which has, let's say, additional superpowers. And this superpowers is\nthat you can use the.use syntax to basically specify which middlewares do you want to attach.\n\n\nSo the idea is that you write your handler, you don't worry too much about all these extra\nconcerns in your handler. So you assume that the data coming into your handler is already clean\nand ready to be used and that you don't need to do anything extra to send back a response,\njust provide an object. You midify this handler and then you attach all the middlewares that\nyou need to actually do all the pre-processing and post-processing of the request and response.\n\n\nThere are slightly variations in syntaxes that you can use today because we try to listen to\nfeedback and figure out ways that could be simpler in different use cases. So if you look at the\ndocumentation, you can find that you can use other things, not just.use, you can use.before,.after,\n.error because we have different use cases and if you're writing something very, very simple,\nyou don't necessarily need to write or use fully-fledged middlewares, you can find shortcuts.\nSo I'm going to let people check the documentation for more details about that, but\nin broad strokes, write your handler, keep it simple, midify it and then.use all the\nmiddlewares that you want to use.\n\n\nEoin: And then you can start deleting all that boilerplate code you had before, which is nice. One of my favorite things in software development, deleting code you don't\nneed anymore. So given that you got started, maybe are there any kind of interesting examples of\npeople out there using Middy in production or like open source projects that are building on top of\nMiddy? Yeah, that's a good question.\n\n\nLuciano: So it's something that we are trying to collect more use cases and hopefully we'll be able to showcase them on the website. We haven't done that in a formal way yet,\nbut we have been very happy about mentions that we got in the public from actually pretty big name.\nLike we had a conference, I think it was one of the serverless days a few years ago,\nwhere Lego mentioned that they were using Middy internally for some of their own APIs built on\ntop of Lambda. Then we also had recently, I think it was in the last re-invent, if I'm not wrong,\nTaco Bells that also mentioned Middy in their own presentation at re-invent as one of the things\nthat they use for serverless. And I think the best one is the fact that the upcoming TypeScript\npower tools for Lambda also supports Middy. So of course, it's not the only way you can use\npower tool, but if you are already using Middy, they make it easier for you to add all the extra\nfunctionality that they are providing with power tools. So I think that's an interesting validation\nalso from AWS that they think Middy is actually solving a problem for the Lambda ecosystem in\nNode.js. And I recently noticed that there is a repository called AWS Solutions, open source from\nAWS, where there are also a bunch of examples that also use Middy and this is AWS providing examples\non how to use Lambda and they suggest to use Middy. So that's also another very good validation that\nthe project makes sense and it's actually solving a real problem for people. That's great.\n\n\nEoin: I knew Middy was useful, but I didn't know that it was powering tacos. So that's... Yeah.\n\n\nLuciano: That was actually the comment I got from Will when we shared this news that we realized that it was\nmentioned, it was like, oh, it's amazing to see that this open source project is helping people\nto have more tacos. Making the world a better place. Okay.\n\n\nEoin: So let's say you're up and running and using some of these really good official middlewares on some of the third party middlewares\nout there. What about writing your own middleware? Is that something that people would commonly need\nto do? And how would you set about that task? Yeah, that's a very good question.\n\n\nLuciano: So again, there is an intersection in the documentation with examples and so on, and we'll be linking that in\nthe show notes. But I will say that for simple use cases, you generally don't need to do that\nbecause probably the default middlewares are going to cover most of the needs. But there are cases\nwhere, I don't know, maybe you're doing something very custom. For instance, you have your own\nauthentication mechanism, right? So you'll need to validate credentials in a way that is not a\ncanonical way of doing it. Maybe it's not using JWT, maybe it's not using Cognito. So you have\nyour own mechanism, you need to use your own libraries to do that. And of course, this is one\nof those concerns that you don't want to copy and paste into every single Lambda, or even you don't\nwant to call a function, remember to call a function inside your Lambda handler every time and\nmanage the error. And you probably want to just say, use validation somewhere, so use\nauthentication somewhere, and then keep your Lambda code as clean as possible. So this is one\nuse case where you could decide, okay, I'm just going to use Middy, write my own authentication\nmiddleware, and then for every handler, I'm going to attach that particular middleware where I want\nto use the authentication feature. And there are different ways you can write a middleware. The\nsimplest one is literally just write one function, and this one function needs to have a very\nspecific signature that, by the way, is the same signature as a Lambda function, you just receive\nan event and context. Or you can just call it request, that is kind of an object that we use\nthat contains both the event and the context and give you extra functionality. And basically,\nthe only thing you need to do is rather than saying.use on the mid-defined handler, you say\n.before and you pass this function inside it. And then that function can basically either return\nor throw exception to try to handle the different use cases where you want to stop the execution\nearly with a success, or you want to fail because maybe the authentication is not valid, or if\nbasically you don't do anything in that function, you assume that everything was fine, the\nauthentication was okay, and at that point when that function completes, it's going to run for you,\nyour own handler. And that's one use case. Sometimes you want to have actions that happen\nbefore your actual handler runs, after your handler runs, and also in case of error, you want\nto have specific logic to be executed because maybe you need to clean up something. In those\ncases, it's worth to write a fully-fledged middleware where the syntax is very similar,\nbut it's just that you have an object that contains a before function, an after function,\nand an on error function, and you define the behaviors that want to happen in those three\ndifferent phases. That sounds really good.\n\n\nEoin: And just as you were talking about authentication, I was thinking about the case where commonly you have with a multi-tenanted application,\nthe need in your Lambda function to assume a specific role that is scoped down for a tenant\nor for a specific user, for example, that restricts them to specific key prefixes on S3.\nWe talked about that in the previous episode, for example. And that's the kind of thing that\nyou would be doing for every function, and you need to make sure that you're doing it for every\nfunction and that people are using that scoped down policy. So that seems like an ideal application\nfor a custom middleware that could be invoked before your handler and ensure that your context\nis decorated with a session that you can use to make calls out to AWS for DynamoDB in S3.\nYou mentioned that the documentation, and I've seen the documentation, is really good\nand improving all the time. Is there anything specifically we need to point, or are there any\ntutorials out there that people have contributed that will help people to get started?\nYeah, that's a good question. I think we have some links in the documentation.\n\n\nLuciano: There was one recently by Serverless Stack, I think, where they show how to use Middy with Serverless Stack.\nAnd I think a good pointer, and this is something that we want to expand more in our own\nofficial documentation, is that Middy integrates very well with basically all the tools, because\nit's not an opinionated take on how you deploy your code. It's more helping you to write the code\nwith a different style that promotes focusing on the business logic and keeping\nevery other concern outside the business logic. Because of that, you can use Middy with Terraform,\nwith Serverless Framework, with Serverless Stack, with, I don't know, SDK, CloudFormation,\neverything you are currently using. It just changes the way you use your code in the way\nthat any other library would affect your code. But it doesn't affect anything else outside the code.\n\n\nSo we want to have a section called integration in the documentation where we already started that.\nBut it's still pretty much a to-do. There are different pages, but if you open most of them,\nit's like, please help us to fill this guide. But we really want to highlight the fact that\nMiddy plays well with most of the other tools, so it's not really going to force you down a particular part.\nSo maybe that's something for the audience. If people are actually already using Middy\nand they want to contribute, it would be nice to get some help in writing some of this documentation.\nI want to just give a final shout out to Will, who has been maintaining Middy in an excellent way\nfor, I think, more than two years at this point. Because I felt like I took my distance from the\nproject more and more in the last years, and this project wouldn't be today at this level if Will\nwasn't there putting a lot of effort every day into maintaining it. So I just want to say again,\nthank you to Will for making effectively the project available to everyone today.\nGood shout. Yeah, that's great. Okay, thanks everyone for listening. We'll see you next time.\n"
    },
    {
      "title": "42. How do you containerise and run your API with Fargate?",
      "url": "https://awsbites.com/42-how-do-you-containerise-and-run-your-api-with-fargate/",
      "publish_date": "2022-06-24T00:00:00.000Z",
      "abstract": "We recently talked about migrating a monolithic application to AWS, using EC2, load balancers, S3 and RDS. In this episode we want to talk about a slightly different setup, where we are going for containers instead of EC2 and we want to deploy them in Fargate. In this We are going to cover all the components you will need in your architecture, the reasons to choose Fargate over any alternatives and discuss some CDK tricks to get started in a quick way (and the pitfalls that might come with them).\nIn this episode, we mentioned the following resources:\n\nCDK ECS Patterns\nHow to fine tune the health checks to speed up the deployment process\nPrevious Episode &quot;37. How do you migrate a monolith to AWS without the drama?&quot;\n\n",
      "transcript": "Eoin: A couple of episodes back, we talked about the process of migrating a monolithic application\nto AWS using EC2 load balancers, S3 and RDS.\nToday we want to talk about a slightly different approach where we're going to use containers\ninstead of EC2 and we want to deploy them into Fargate.\nSo we're going to cover all the components you need in that architecture, why you choose\nFargate over some of the alternatives and some CDK tricks to help you get started faster.\n\n\nMy name is Eoin, I'm here with Luciano and this is the AWS Bites podcast.\nIn episode 37, we talked about migrating a monolithic application to AWS.\nAnd in that case, we talked about how you'd choose EC2 because adopting containers was\na step too far for the team.\nThe team was already having to learn a lot of new skills approaching AWS for the first\ntime.\nBut what about if we do have an appetite to move to containers and you've already got\nsome of those skills?\nSo we're going to talk about that example where we take something like an API backend\nwritten in Python that can run in a container.\nWhat are the simplest ways of getting it to run in a scalable and a reliable way using\ncontainers when you're moving into the cloud?\nSo there's a lot of ways to run containers in AWS.\nWhy would we go for Fargate, Luciano?\n\n\nLuciano: Yeah, I think another one would be AppRunner, which is probably the simpler that I've seen\nso far, or at least that's the way it's presented.\nBut it's still very new and that probably deserves its own dedicated episode when we\nhave some more time to actually play with it and see how it feels like.\nSo Fargate so far seems kind of the default choice to me because, well, I had some experience\nwith it and it's basically built on top of ECS.\n\n\nSo all the concepts are the same if you're familiar with ECS, which stands for Elastic\nContainer Service.\nAnd just to summarize what are the main reasons, it's basically very simple to set up.\nIt doesn't require you to manage instances as in EC2 instances.\nIt's kind of serverless that way.\nYou just say, run this container for me and it will figure out some hidden instance where\nto run it for you.\nIt supports autoscaling and also integrates very well with Elastic Load Balancers, but\nalso with CodeDeploy.\nSo you get autoscalability through Elastic Load Balancer and through multiple containers\nrunning in a cluster.\nBut also you can fine-tune your pipeline with CodeDeploy to actually build and deploy your\ncontainers.\nYou mentioned, though, that it's interesting to go through all the different components\nthat an architecture like this actually will require under the hood.\nShould we describe what are those components?\nYeah.\n\n\nEoin: I mean, I went through a lot of detail in episode 37 and a lot of them will be the same\nhere.\nIt's just the compute layer that we're switching out from EC2.\nWe're going for Fargate ECS instead.\nSo the VPC will be similar.\nYou've got a public and private subnet.\nYou've got the NAT gateway, your internet gateway, so that you've got outbound internet\naccess.\nYou've got your routing tables, the VPC security groups.\n\n\nSo that's your networking foundations really.\nAnd then you'll have an application load balancer on top.\nThe difference between our EC2 approach and the Fargate approach is that the targets in\nyour target groups within the application load balancer will be different.\nAnd yeah, we'll assume, again, we're using HTTP ES, so we'll have a Route 53 hosted zone\nfor the DNS and we'll have a certificate using certificates manager.\n\n\nSo that's the similarity.\nAnd then the different parts are around ECS and Fargate.\nAnd when you're working with Fargate, there's a few different resource types you need to\ncreate.\nSo you've got your task definition, which is like the defining the container image and\nall of the container configuration like environment variables, what ports are you exposing, how\nmuch memory and CPU does your container need, and any volume mappings as well, volume mounts.\n\n\nSo that's your task definition.\nYou'll also have the ECS cluster itself, which is kind of like a boundary that all of your\nFargate services will run in.\nAnd that's where you'd basically just specify the VPC.\nSo you don't have to configure any EC2 instances because it's Fargate.\nAll of that is taken to care of for you.\nSo then the last and probably the most important thing you need to create is the Fargate service\nitself.\n\n\nAnd that's where you specify, okay, how many of those tasks that I've outlined in my task\ndefinition, how many of them do you want to run?\nHow do you scale it?\nAnd the Fargate service is the bit that integrates well with the other pieces.\nSo it integrates well with our application load balancer.\nSo when you start a task, it will register the IP address of that container in the target\ngroup so that traffic can start to be directed to that container.\n\n\nIt also will maintain a desired level of healthy containers.\nSo you can specify in your service what the minimum healthy percentage is, how many containers\nyou're desiring to run, your desired count, and your maximum count as well.\nAnd then you can specify your auto scaling configuration.\nAnd this is what makes it, I suppose, very advantageous moving to this container based\napproach because based on whatever criteria you specify, you can choose to scale up and\ndown those containers.\n\n\nSo that could be based on a schedule.\nIf you know that all your traffic happens on, I don't know, Monday to Friday at 9 AM,\nor if you're based, you can base it on the API request count, but like auto scaling groups\nwith EC2, you can also base it on any metric.\nSo the CPU of your containers, memory utilization of your containers, or actually any other\nmetric.\nIt could be a custom metric even that you're generating within the containers themselves.\nSo there's lots of different triggers you could use to say, this is when to scale up,\nthis is when to scale down, and there's a lot of advanced configuration there.\nSo there's quite a lot there, right?\n\n\n\nLuciano: But- Yeah, one interesting detail that maybe it's worth mentioning for people that are more\nskilled with, I don't know, something like Kubernetes, is that a task is probably a little\nbit closer to the concept of pod in the Kubernetes world.\nBecause it's not necessarily like one-to-one with a container image or a container definition,\nwhatever you want to call it, but it's more, it could even contain multiple containers,\nlike the idea of you could run a main application and a sidecar container.\nSo that is just an interesting thing that I wasn't really aware at the beginning.\nThe task is kind of a higher level concept than just one container, but it could be multiple\ncontainers that need to run together.\n\n\nEoin: That's true.\nAnd if you want to run the CloudWatch agent, for example, which you normally you run EC2,\nbut sometimes you won't need to run it with containers if you want to get the EMF metrics\nout and stuff.\nThat's one application for that, where you would run an application with the CloudWatch\nagent as a sidecar.\nSo this is, for people who are used to ECS, this is probably okay.\nBut if you're thinking, okay, I was expecting to hear about a simple way to get started\nwith containers on AWS.\nAnd if you've never heard of any of this before, it probably doesn't sound very simple.\nSo what do you think?\nWould you recommend any kind of templates or tutorials out there?\nWhat's the best way to get started easily?\n\n\nLuciano: Yeah, there is one particular way that I used.\nThis was about one year ago, I think.\nSo my view of this thing might be a little bit obsolete right now, but I'm going to try\nto describe the experience I got about one year ago anyway, and you can challenge me\nif you had a more recent experience.\nSo there is something as part of CDK, it's like a set of patterns that are maintained\nas higher level constructs by AWS itself.\n\n\nSo you just install them from AWS and this ECS patterns with CDK, what they do is they\nbasically allow you to define how, basically where the source code of your application\nis and more or less few configuration toggles that you can just play around with to say,\nokay, do you need volumes?\nDo you want to associate a domain name?\nIs it going to use a load balancer?\nAnd it literally, you end up writing 10 lines of code and it's code as in configuration\ncode, you copy paste and you change a few things.\n\n\nAnd later you just do CDK deploy, it takes maybe around 10 minutes and it will provision\nalmost everything for you.\nAnd it will, the magic thing is that you basically just connect to that domain that you specified\nand your application is working, which feels a little bit magic that you with 10 lines\nof code, if you're used to AWS where it takes you to really deep dive to do anything, it\nfeels like, okay, this is still a different experience that you're not used to when dealing\nwith AWS.\n\n\nAnd this can have of course, pros and cons because on one side you get started very,\nvery quickly, but it might give you a false sense of confidence that basically you know\nwhat you're doing while in reality there is a lot of stuff that is being hidden from you.\nAnd I think if you are building a serious application, eventually you might want to\nknow what's happening behind the scenes.\nAnd when I actually did that about one year ago, I was surprised because I was trying\nto run, it was kind of a microservice project with, I think it was something like about\nfive application runnings on different domains, but those applications would be related with\neach other.\n\n\nEach user will log in as one experience and jump into different domains, depending which\nparts of the application the user was trying to use.\nSo it was like, okay, we deploy them together as part of the same cluster.\nAnd I realized, and they will be using five different subdomains on the same main domain.\nAnd I realized at some point that this thing was provisioning five different load balancers\nrather than just creating rules on the same load balancer and divided the traffic that\nway.\n\n\nMaybe someday it could be fine tuned if you're willing to spend more time and starting at\nthe configuration, maybe there are ways to actually reuse the same load balancer.\nBut it was something that I only figured out deep down the road when I was starting to\nlook back at all the resources like, okay, this is going to be expensive and for no reason\nbecause you are provisioning five load balancers when you might use just one and route the\ntraffic in a more, I don't know, efficient way, I guess.\nSo that's the caveat.\nJust be careful that with CDK, that is kind of a golden rule anyway, that when you use\nhigher level constructs, they can do so much stuff that you're not aware.\nSo it's always good to kind of have a look at some point and make sure you really understand\nwhat's going on.\nAnd probably there are opportunities to fine tune a few things for your actual use case.\n\n\nEoin: I've had another similar kind of a shock with the CDK patterns because the first one I ever\nused was not the application load balancer one, but there's another one that is more\ndesigned for background processing.\nIt's called the queue processing Fargate service.\nAnd you can specify an SQS queue and it will create all the infrastructure you need to\nscale up and down the number of workers based on the number of messages in the queue.\n\n\nAnd it was really easy to get started with.\nThat's sometimes a very misleading sense of security, like you say, because it was not\nlater that I realized it had created the NAT gateway as well because it created the whole\nVPC.\nYou don't want to necessarily create a VPC for every single thing you deploy, right?\nYou probably want to think about your VPC design a little bit more carefully.\nAnd you can specify your own VPC in these services, but it's always definitely worth\na while to do a CDK synth before you deploy and actually look at all the resources that\nare being deployed.\n\n\nBecause in our case, we ended up with S3 traffic going through that NAT gateway where we would\nhave just preferred a VPC endpoint.\nAnd we ended up with a, it wasn't a massive bit of bill shock, but some unexpected cost\nthere for sure.\nSo I think it's a really good thing, right?\nBecause I've been using them recently as well with this API load balancer service.\nAnd I think I'm still impressed when you can create those 10 lines of code, wire it through\nto route 53.\n\n\nYou don't have to configure the certificate resource, the load balancer resource, all\nthe VPC resources, it's all done for you.\nSo it really, and you can just as well point it at your Docker file.\nYou don't even have to push a container anywhere and CDK will manage all of that for you.\nSo it's really good for getting started, but then also think about, okay, now that I've\ngot started, how do I want to keep going?\n\n\nDo I want to continue using this pattern or is this just like a learning experience where\nI can see all the things that's generated and then I'm going to pick and choose the\npieces, understand them and kind of replicate that in a more with lower level constructs\nin CDK or with CloudFormation or Terraform.\nSo I think CDK is sometimes a neat trick for getting started and figuring out how everything\nshould fit together, but you don't necessarily have to keep going with it.\n\n\nMaybe it's worthwhile talking about the deployment then because with containers, you have to\nthink about, okay, you've got the repository, you need to deploy that.\nThen you've got the cluster itself and the service, and then you've got your load balancer\nin front.\nWhat happens when I've got a new version of the image and I want to deploy it?\nWhat happens to my current connections?\nWhat happens to existing users?\nWhat happens if I deploy a container that's got a bug in it?\nHow do we manage this?\nWhat do you, how will we break this down?\nHow does the first CDK pattern manage it actually?\nDo you want to talk about that?\nYeah, I can try to explain what I remember from my previous experience.\n\n\nLuciano: I hope it's still up to date and relevant, but I think in broad strokes it should be\nstill the way it works today.\nAnd it's very convenient.\nAs you said, you just literally in your CDK code, you literally say, this is where my\nDocker file is.\nYou literally give it a relative path.\nSo you probably have your Docker file in the same folder where you have your CDK code and\nmagically it's going to do a lot of stuff for you.\n\n\nAnd that magically means that when you do CDK deploy, basically it's going to, the same\nCDK deploy process is going to spin up a Docker build process in the background.\nIt's going to wait for the container to build.\nAnd then in the bootstrap phase, CDKs also create an ECR repository for you.\nSo basically at that point, it's built the container, finishes to build the container\nalready as a repository.\n\n\nSo it's going to push a new version of that image.\nAnd then at that point you have everything ready to start a new deployment because your\nimage is up and you can just say, okay, now I want to switch over all my containers to\nthe new version.\nAnd that's interesting as well because we know that there might be a lot of complexity\nthere, but you don't get to see it with CDK.\nAnd if I understand correctly what's going on, it does kind of a blue green deployment\nwhere it's basically provisioning a new version of the container as a new service.\n\n\nI don't know if it's the right terminology, but basically you get a copy of the previous\nversion that is still running and the new version that starts to spin up.\nIt doesn't kill the old version until the new version is up and running and all the\nhealth check pass.\nAnd it registered that as a new target in the load balancer.\nThen it starts to drain the connection from the previous version, move the connection\nto the new version and eventually start to destroy all the old containers.\n\n\nAnd of course there are health checks.\nSo if your new version of the container cannot really run and receive connection correctly,\nit will kind of roll back.\nIt will just kill the new containers and say, okay, this deployment failed.\nIt wasn't able to pass the health checks.\nThe only issue I had with this process at the time, and again, I was trying to run five\ndifferent applications, is that the steps were very sequential.\n\n\nSo I had to go through five, not parallel, but sequential times building the same thing.\nSo okay, building container one takes a few minutes.\nThen provision container one takes up to 15, 30 minutes, depending on the case, because\nyeah, all the time of draining connections and uploading everything, health checks and\nso on.\nAnd then it goes with the second container, third container, fourth container.\nSo it might take a long time to do that.\nAnd there is actually an article we're going to link in the show notes that gives you ideas\non how you can tweak the configuration of health checks to speed up the process.\nSo if you have containers that can come up very fast and they can prove that they are\nhealthy very, very quickly, you can kind of fine tune some of these configurations.\nAnd then you have much shorter times to tell, okay, this container is already good to go,\nstart to swap them out.\n\n\nEoin: Yeah, I think that's a good way to, I guess, get started with the deployments.\nYou can use that workflow.\nIf you have to wait so long, it's going to be a little bit awkward.\nAnd I'd also recommend kind of thinking about your long-term deployment strategy, because\nwe mentioned at the start that code deploy integrates well with Fargate.\nAnd that's another way to manage shifting traffic over to a new version of the container\nimage.\n\n\nSo you could think of your CDK stack as managing the infrastructure for this service, then\nthe container image updates could be done with code deploy.\nAnd that's a really nice set of features, I think, what code deploy gives you, because\nit allows you to trigger code deploy deployment, and it requires two target groups with your\nload balancer.\nSo you could do like a blue-green deployment strategy, in which case it will start creating\na new target group and starting new versions of the task, but putting them in the different\ntarget group.\n\n\nAnd then it can use the traffic shifting features of the load balancers.\nSo it can start the waiting of the traffics between the two target groups can be adjusted\nover time.\nSo it can start shifting some of the requests over to the new version of the image.\nAnd then there's all sorts of advanced health checks you can do.\nSo you can just use your load balancer health check, but you can also put in like hooks\ninto the deployment so that it will check that all of the expected business logic is\nworking as well as you want it to, or you can check for alarms, and then you can gradually\nshift traffic over to the new version.\n\n\nAnd if anything is detected, any kind of failed health check or alarm, it will revert back\nto the old version.\nSo you can get much safer deployment strategies and it decouples the, I guess, the creation\nof the cluster and the service and all that stuff from the deployment of the software\non it.\nSo I think it's definitely worthwhile.\nAnd you can integrate it into CodePipeline, or you can integrate it into whatever else\nyou're using to deploy, be it like GitLab or Bitbucket or GitHub actions, say.\nGiven that we've talked about everything from the foundations to setting up all of the resources\nwith CDK patterns and then deployment with CodePipeline, that's probably a good place\nto wrap it up.\nWhat we'll do is we can give a link to that ECS pattern in the show notes, and we'll also\ngive a link to episode 37, where we talked about migrating to the cloud with EC2.\nThank you very much for listening.\nWe'll see you in the next episode.\nHave a great day.\nWe'll see you in the next OneOG video and, of course, soon in grassroots media.\nyou\n"
    },
    {
      "title": "43. When is it OK to cheat on AWS?",
      "url": "https://awsbites.com/43-when-is-it-ok-to-cheat-on-aws/",
      "publish_date": "2022-07-01T00:00:00.000Z",
      "abstract": "We do love AWS, but sometimes we have to admit that it’s not always a silver bullet. There are definitely use cases where it might be worth considering alternatives to AWS.\nIn this episode we will discuss some of these use cases and try to highlight what are the advantages that other platforms or services can have over AWS in very specific circumstances. First of all we clarify why we like AWS and why (and when) it’s worth sticking with it. Then, we discuss what are some of the reasons why it might be worth considering alternatives to AWS. At this point we go into the specifics and talk about authentication services (Auth0), search services (ElasticSearch, Algolia), CDN Services (GitHub Pages, Netlify, Vercel, CloudFlare, Fastly, Akamai), Databases (MongoDB Atlas, Digital Ocean managed databases, IBM Compose, CloudFlare R4 and D1, Upstash, Confluent Kafka), Headless CMS services (ContentFul, Storyful, AirTable, Google Spreadsheet), Virtual Machine services (Digital Ocean, Linode).\nIn this episode, we mentioned the following resources:\n\nEpisode 3. ​​How do you deploy a static website on AWS?\nAuth0\nAmazon OpenSearch\nElastic Cloud\nAlgolia\nVercel\nNetlify\nCloudFlare R2\nMongoDB Atlas\nDigital Ocean managed database\nCompose (now IBM Cloud Databases)\nUpstash\nConfluent\nAirTable\nLinode\n\n",
      "transcript": "Luciano: We do love AWS, but we have to admit that it's not always a silver bullet.\nThere are definitely use cases where it might be worth considering alternatives.\nToday we will discuss some of these use cases and we will try to highlight what are the advantages\nthat other platforms or services can have over AWS in very specific circumstances.\nMy name is Luciano and today I'm joined by Eoin and this is AWS Bites podcast.\nSo Eoin, why don't we start by asking ourselves why we would prefer to go all in on AWS for everything?\n\n\nEoin: That seems to be kind of the default, that you try and look for the AWS solution first,\nbut you don't want to get distracted by that single focus and maybe be a little bit more open-minded.\nBut the reason you do it is because generally the way the AWS services are delivered, they're well integrated with each other.\nYou also get like unified billing and in general people have a level of trust in AWS.\nIf you pick a vendor as important as your primary public cloud vendor, you want to go all in on them and you want to trust them\nand you believe that they're going to stick around and they're not going to retire services,\nthey're not going to be acquired by some bigger company who then retires the services.\nSo that's a good reason to go all in on AWS.\nBut like I said, you probably shouldn't always, so why would you want to go for third-party services instead in some cases?\n\n\nLuciano: Yeah, that's a good point. I would also add that AWS generally gives you very good levels of scalability.\nYou have this peace of mind that if your project actually grows a lot, you can find a very good part of scaling up with AWS.\nBut I mean, realistically, that's not always a realistic expectation, right?\nSometimes you have very modest projects and it's okay to deal with them so you don't have to think about that level of scalability.\n\n\nSo other cases, I guess, is when you have very specific needs and there might be tools,\nthe companies that are focusing on that one particular need and they're providing very, very good service.\nAnd I think we'll discuss some of these examples later on during this episode.\nSo basically, in those cases, you might end up having a much better user experience, probably even price and feature set,\nthan just going with some of the many AWS services.\n\n\nIn other cases, it's more about the billing, I guess.\nIt's more about how easy it is for me to predict cost in a specific service rather than in AWS.\nSome offering, for instance, we'll talk about virtual machines.\nIf you just need one virtual machine, the cost unit is generally easier if you go with something like Digital Ocean or Linod,\nrather than thinking in terms of AWS where you have so many different parameters that is a little bit harder to predict the cost in advance.\n\n\nAnd in general, we'll talk about simplicity because, of course, we know we have been talking a lot about this, actually, in previous episodes,\nthat AWS has a decent barrier to entry.\nYou need to learn a bunch of concepts before you can start to deploy anything serious on AWS.\nSo if you are just looking for, again, a simpler way to just deploy things in the cloud and launch products,\nother platforms might give you better access to the resources and more simple user experience in general.\nSo I guess, yeah, let's start by picking some examples.\nI suppose my first one, because it's also a topic we have been discussing before, is what about authentication?\nWould you go with Cognito or are there alternatives that you would prefer in some cases?\nYeah, I would definitely use either Cognito or likely something like Auth0.\n\n\nEoin: I think these are the two which compete very well with each other.\nCognito, maybe we should do a deep dive on it in a future episode because there is a lot of features there.\nUnfortunately, it suffers from a couple of drawbacks, like poor naming of some of the concepts in there,\nwhich make it very confusing, difficult to get to grips with the documentation.\nThere are some feature gaps as well.\nSo from a developer experience point of view, it suffers, unfortunately.\n\n\nAuth0, on the other hand, has a really nice developer experience and a really good onboarding.\nSo if you want to integrate authentication, authorization, like sign up, log in,\nlog in with social identity providers, Auth0 will allow you to do that very quickly.\nAnd the whole developer experience in general is going to be much smoother.\nThe other thing about Auth0 is that it solves those kind of small company startup problems,\nlike how do I add sign up and log into my application in an easy way?\n\n\nBut it also solves equally well the enterprise integration scenarios like SAML integration\nand OpenID Connect for enterprise as well.\nSo there's a lot of feature supported there.\nSo the only advantage is, and as with all of these things, you just have to compare the pricing models,\nI think, with the user-based pricing model with Auth0.\nI believe it's still quite expensive when you get to large numbers of users.\n\n\nCognito is pretty inexpensive for large numbers of users by comparison.\nI suppose the other thing you need to think about is just do you need deep integration with IAM?\nSo if you're using Cognito, you get good integration to API Gateway authorizers and AppSync authorizers.\nYou won't get that out of the box, and you just have to use a custom authorizer with Auth0 or any other third party.\nSo that's the authentication, authorization scenario.\nWhat else? What else is a topic where people...\nI think Auth0 is probably the number one thing that people would reach for outside of AWS.\nWhat else is pretty popular for people who are...\nThere is an option in AWS, but it's not people's number one choice.\n\n\nLuciano: I guess we can talk about search because it's one of those use cases where eventually for any reasonable application, you'll need some search functionality, and that's where you start to ask yourself,\nokay, how do I build this in AWS, and then you realize it can be much more involved than you actually wanted it to be,\nand then you might want to start thinking about alternatives outside AWS.\nBut if we just stick to AWS, probably the default is Amazon Open Search, so Amazon version of Elasticsearch.\n\n\nAnd while that is good, it's still a little bit annoying that you still need to think in terms of servers.\nYou need to provision a cluster and find the servers, and there is not really a serverless model,\nwhich is something that I guess is going to go into my own personal wish list for AWS,\nbecause of course most of the time you just want an API or something that allows you to index some data\nand then search over the indices.\n\n\nAnd with Open Search, of course, you need to do a lot more before you can access to that level of API.\nAnd there is an Elastic Cloud as an alternative, so the cloud from Elasticsearch creators,\nbut that one is also very similar, meaning that it's based on instances, and you need to provision those instances.\nSo I guess maybe the user experience can be a little bit better.\nI actually don't know because I haven't tried it.\n\n\nBut again, you need to go outside AWS, create a whole account, and manage the billing\nfor something that eventually is still based on instances, and it doesn't seem to give you a lot of advantages.\nSo yeah, I guess if you want to zoom in a little bit more on different types of use cases,\nmaybe we can also talk about other alternative products, because I suppose one use case is about search.\nSo you really just want to index data and then search over it.\n\n\nOther very common use case for Elasticsearch is about managing logs, distributed logs, and centralizing them somehow.\nAnd for those use cases, I've used Elasticsearch, and I think it works really, really well, especially when combined with Kibana.\nBut again, it's a little bit of a messy setup, and it takes a little bit of time before you get it right.\nSo in those cases, if you want to avoid all of that, you can use third-party services focused at 100% on the logs use cases.\n\n\nAnd these are like Loggly, Logs, DotIO, Splunk, Datadog, Onacom, and so on.\nInstead, equivalent if you want to think about pure product search, or documentation search, document search in a more general way,\nAlgolia is probably the go-to service where you just want to give me an account, give me an API, I don't want to think about servers.\nAnd it gives you a very nice developer experience, you can get a very good documentation.\n\n\nThere are also cases where you can get the service for free, like for instance if it's an open-source project, you can easily request Algolia.\nThere is like a web form, you need to feel like, can you support us by giving us a certain level of access for free?\nAnd at least in my experience, it's been pretty easy to go through the process, and they seem to be very open to support open-source projects.\n\n\nAnd the API is generally very nice, and you also get pre-made scripts or widgets that you can just copy in your application,\nand everything magically works, at least to a certain degree.\nThe only thing is that if you use it at scale, because you really have successfully commerce with thousands of products, I think it might get expensive very quickly, based on what I've seen on the pricing.\nBut that's always kind of the trade-off between go and do all the work of setting up infrastructure,\nand then maybe you get a little bit of economy of scale there, rather than just using a software as a service,\nwhich might be very cheap at the beginning, but then the scale curve grows very steeply,\nand you might end up paying a lot when you are reaching those levels of growth.\nYeah, so I think that covers search and logs pretty well.\nWhat about something else very common? I'm thinking, hosting files like CDNs or websites.\n\n\nEoin: Yeah, this is actually something we talked about in one of the previous episodes, I think back near the beginning, we talked about how to deploy a static website on AWS, and we talked specifically about CDNs.\nI know we mentioned things like GitHub Pages and Vercel and Netlify as ways of getting a static website up and running very quickly on CDN.\nNetlify is actually the site that we use for hosting the AWS bytes website, awsbytes.com, funnily enough.\n\n\nAnd then you also have, I guess, not just static websites, but sometimes you've got other CDN needs,\nlike for video or documents or images, so there's lots of different use cases out there that can leverage a CMS.\nSo CloudFront is pretty good. I think CloudFront is a great option, and I'd use it quite a lot, but it's not suitable for every scenario.\nWe mentioned before that it might be slow to update. Configuration is complex enough.\n\n\nSo this is an area where there are quite a few alternatives.\nCloudflare comes up quite often, I think, especially in the kind of small to medium business space.\nThey've got also a lot of other products around it, like for protection and firewall, that kind of stuff.\nAnd then you have things like Fastly, which is known for being particularly fast.\nAnd then you've got the big older enterprise players like Akamai, which have been around forever, it seems.\n\n\nSo there's a lot of alternatives out there in the CDN space, and some of them are pretty fast to get up and running with,\nespecially the more modern services. And yeah, I think it's one area where it really depends on your use case,\nbut you've got good options out there.\nSo if we think back then to data, let's maybe think about databases.\nYou've got a lot of options in AWS, really, with RDS, right? You've got DynamoDB, which has grown in popularity in a phenomenal way.\nThen you have things like Aurora Serverless, which are kind of serverless, not quite there yet,\na little bit debatable in whether you should use them or not. So what else is out there that people should be looking at?\nI guess looking outside AWS, the main first contender to DynamoDB is probably MongoDB.\n\n\nLuciano: And there is MongoDB Atlas, which is kind of, I suppose, a good alternative to DynamoDB, if you like,\ndocument-based databases like Dynamo or NoSQL databases.\nAnd the good news is that recently there is a serverless option for MongoDB Atlas,\nso probably much more lightweight in terms of configuring the whole cluster and scaling it and so on.\nI haven't used it yet, but it seems to be an interesting alternative.\n\n\nIf you're looking, if you like to use MongoDB and if you're looking for the kind of model where it's like,\njust give me a database and scale it for me and I'm going to pay as I go.\nAnother interesting one is that I noticed that recently enough, I think it was either this year or at some point last year,\nDigitalOcean launched kind of an alternative to RDS.\nIt looks very similar, even though it covers Postgres and MySQL as kind of relational databases,\nbut it also covers Redis and MongoDB as other classes of databases that in AWS they will go kind of somewhere else,\nin a different category of databases.\n\n\nAnd this one is interesting because if you like DigitalOcean and if you use it,\nyou probably know that they spend a lot of time making sure that the developer experience is good\nand it's very easy to see, to understand how to provision anything and get things up and running.\nYou get lots of documentation, lots of tutorials,\nbut the UI is generally very driven to kind of a workflow where it's like step one, step two, step three,\nand at the end you have everything up and running as you would expect.\nAnd I saw a small demo there showing how to provision a Postgres database and it seems like really simple to get started with.\nSo if you are already using DigitalOcean for something else and you just want a database that is more managed\nthan provisioning everything yourself, that's probably a good alternative to use.\nAnd I remember you mentioned to me some time ago, Eoin, about something I think from IBM called Compose,\nwhich is probably in a similar space. Do you want to talk about that?\nIt actually was since I've used Compose, but when I did use it, I was really pleased with how easy it was to get up and running.\n\n\nEoin: I was using it for MongoDB at the time. So it's also not target for hosting MongoDB.\nThey also host lots of other databases like Postgres, MySQL.\nI think they also have things like Redis and Kafka. I'm not sure about Kafka, but they've got quite a few different options.\nSo it's one to check out. They were acquired by IBM, like you say, so they're an IBM service now.\nAnd yeah, I just thought it was really seamless to get a MongoDB instance up and running.\n\n\nSo it might be something else to look at.\nMaybe another one to mention finally on databases is Cloudflare.\nWe might talk about Cloudflare seems to come up with new products very frequently these days.\nAnd one of the recent ones was they gave early access to their new relational database called D1.\nAnd I think the idea here is that if you're using a SQL, if you want SQL database and you're using Cloudflare workers,\nthis is the solution that they are proposing. And it's an interesting one, right?\n\n\nIt's a bit like Cloudflare workers. It was a different approach to serverless functions.\nThis is a different approach to serverless databases as well, because it's actually built on top of SQLite as a database engine.\nSo yeah, it would be very interesting to watch that. I haven't used it yet.\nI think it's still in early access, so you can sign up to the wait list.\nI'll wait to see what other people say first and take my guidance from them.\n\n\nSo maybe slightly related, since we mentioned Kafka and Redis there a little bit,\nwe covered this when we talked about event services.\nBut you've also got things like Upstash for serverless Redis, which is really nice,\nand Confluent Cloud as well for Kafka and Upstash are doing Kafka as well.\nSo those are definitely ones to check out if you're interested in Kafka and Redis.\nSince we were talking about databases, maybe it's a good idea to talk a little bit about the future direction of this.\n\n\nAnd I think this is something we've alluded to once or twice.\nWe've got databases out there, but you still end up building what seems like the same crud code for every time you build an application.\nEverything seems to seem a little bit like a CMS at its heart.\nAnd I think we've mentioned things like Contentful, Firebase for headless CMSs, or even backend as a service.\nI always feel like this is the way the industry is going and should be going,\nbut it's maybe not getting there quite as fast as I expected.\n\n\nI thought that maybe by 2022 more of us would be building systems on top of vendors that would provide just an API as a service,\nthat we just give it our data, they give us an API back,\nand we don't have to worry too much about all the data access wiring and the query patterns and the optimization and the building indexes and normalization.\nBut it still seems like we're still there, right?\nEspecially if you adopt DynamoDB, it seems like people are actually getting deeper into the weeds now,\nand you have to gain a high level of proficiency with things like single table design to use it properly.\nBut I think eventually we have to move away from that and start treating APIs as managed services,\nand maybe just build a definer schema and we get a GraphQL API and we don't know how it's stored anymore.\nWhat do you think? What are the alternatives out there? Is there anything that you can use today?\nI think you make a very good point.\n\n\nLuciano: I agree that there was an explosion at some point a few years ago, I think about three or four years ago, of this kind of headless CMS.\nAnd there was a lot of buzz around the jump stack and building static websites off of data managed by somebody else,\nmaybe in your team that is focused on content, and they will use this kind of CMS, static headless CMS UI,\nwhile then you have developers using a build process to take all this data and build new versions of the website.\n\n\nThat seems to be the most likely future for building marketing web pages or product pages.\nI agree with you that it doesn't feel like it has been adopted as much as we would have expected a few years ago.\nThat might change because I think there is still a big push for these companies,\nand I think the market is starting to realize that there are many advantages in keeping your front end very simple and statically rendered,\nwhile you still want to retain all the capabilities of managing data and changing things very quickly.\n\n\nSo I am still convinced that this is a good solution going forward.\nBut there is an interesting, I think, maybe it's an edge case, I'm not really sure,\nbut I think there are many cases where you want to build just a small website for even sometimes even for just a limited amount of time.\nI'm thinking like, I don't know, event websites where, I don't know, for instance, like a conference,\nwhere you want to showcase that this event is happening and there will be, let's say, speakers and people attending.\n\n\nSo there is some data to manage that and most likely want to use some sort of database to manage all the data as you keep building the whole event.\nBut at the same time, the website is going to be very simple so you can take advantage of statically rendering it and also do something like Netlify or Cell and so on.\nAnd yeah, because most likely you're not going to have a very complex dataset,\nI have seen cases and actually built something myself very successfully where you can use very simple data alternatives that are not really meant to be databases,\nlike, I don't know, a Google spreadsheet.\n\n\nAnd because they offer APIs, you can just ask everything there in one page.\nMost people would be comfortable enough with like inputting data there.\nAnd then you can easily build an API, even just at build time, like if you have a static website, at build time you can just fetch the data through the API and use that as a mini database.\nAnd if you don't like Google spreadsheet, because of course the APIs are not the most straightforward for this particular use case,\nI think Airtable does kind of a better job in giving you slightly better APIs.\n\n\nAnd also what I really like by Airtable is that when you open the document, you can really watch it in real time when new data comes in.\nLike let's say that you also have a read-write pattern from the front end, maybe, I don't know, you're asking people, do you want to attend this event?\nThey can click a button and say yes and give you, I don't know, an email address or other information.\nYou can actually see in real time the data appearing in Airtable and there is like a nice highlight effect.\nSo it actually looks really nice then to use that as a back end to watch things changing.\nAnd recently I had somebody mention that Notion is starting to put out APIs as well and they seem to be fairly good.\nAnd because more and more people are using Notion as their own personal database for everything, I think that might become another alternative for simple cases to just use the data there, use the data through an API to build websites and experiences based on top of that data.\nI like that. Yeah. I mean, I think Airtable was kind of designed almost as like an easy to use database with a spreadsheet kind of layer on top, wasn't it?\n\n\nEoin: So I think it's a perfectly viable solution and it's good to see that going. I think maybe a few years ago there were a lot of companies building on top of Parse and Parse, it didn't end well because it ended up getting acquired and retired.\nSo a lot of people were left kind of hanging there. So maybe that's one of the reasons why this kind of thing didn't take off as it should.\nPeople felt a little bit wounded. But I hope, I definitely hope there's options there in the future.\n\n\nMaybe if we take it back to basics, at the start you mentioned a couple of things about the idea of getting into AWS and all the things you need to understand if you just want to get an EC2 instance up and running.\nYou can definitely do it quickly, but there's probably a lot you should be understanding, especially in terms of security and billing.\nIt's probably worth mentioning, we've covered DigitalOcean already. They've got a user experience that helps you get up and running pretty quickly there.\nAnd there's other alternatives out there like Linode. And the idea with any of those specialists kind of providing simple instances, very well providers, is that you get more predictable cost generally.\nAnd it's very easy to see what you're getting, to understand it, and there's not a lot of complexity with it because it's just, they're essentially just providing those basic elements for you and they're not trying to do 200 other services along with it like AWS.\nYeah, absolutely. I actually used Linode to host the website for my first startup. This was like seven years ago, I think now.\n\n\nLuciano: And at the time I didn't really know much about, OK, how do you run your own servers? How do you host products on a server and put them online?\nI have to admit, I was quite scared about doing all of that. We were looking for like a sysadmin to help us.\nBut because we were a startup with a small budget, we couldn't afford that. And eventually we were like, OK, let's just eat the bullet and try to do it ourselves and see where we end up.\n\n\nAnd I was actually very pleased. And this was already like seven years ago. So I assume now the experience is like much, much better.\nHow easy it was to really understand what were the building blocks? How do you start to create an instance? How do you provision software on that instance?\nThen you have a very nice dashboard that allows you to see what are the performances that any bottleneck should increase something.\n\n\nIs disk read and writing doing OK? Is network doing OK? And I really enjoyed that experience.\nAnd I think at the end that the server was actually doing quite well, even in terms of performance.\nAnd it was relatively cheap because we did go with a kind of a medium instance and one instance was good enough for that particular startup.\nSo it was very nice also for us started with a very small budget to have very predictable costs.\nI think we were paying something like fifty dollars a month. So that was giving us a lot of confidence that that solution would be good enough for us in that period of time.\nI think it's great to have that level of competition there. It's important that AWS doesn't completely run away with the market.\n\n\nEoin: And maybe on that topic, we do see some services aiming to come in and replace or give a viable alternative to specific AWS services and being API compatible.\nSo one example of that is, again, Cloudflare, who introduced with a lot of fanfare, which is a replacement for S3.\nSo it's an API compatible drop in replacement. And one of the standard features of that was its pricing.\nSo the pricing, especially for data ingestion, sorry, for data extract, was vastly superior to Amazon's.\n\n\nAmazon actually followed up with a significant change to the free tier for S3.\nBut they've also got workers. So as an alternative, perhaps, although they're slightly different to Lambda functions, you've got Cloudflare workers and they've got a database storage there as well.\nSo workers KV, it's called, key value store. So it might be kind of an alternative to DynamoDB global tables.\nI think it's great to see that competition, especially if they're pushing AWS on price or features or performance, because it keeps them on their toes and it keeps everybody innovating, I hope.\nSo what do you think there? Would you be keen to adopt or to instead of S3, do you think, or other services that are just drop in replacements for AWS services?\nYeah, that's a good question. I have to admit that my experience with Cloudflare is kind of a little bit conflicted because there are things that I really like, like how easy it is to set up the CDN.\n\n\nLuciano: For instance, when you're building a static website hosted in GitHub pages, you can literally set up the integration with Cloudflare if you want that extra level of CDN and more configuration in terms of DNS and so on in literally minutes.\nAnd it works really well. And most of the time it's going to be free because for most use cases, you don't even need to go and pay for the other, for the kind of the custom enterprise plan.\n\n\nBut when it comes to these other new services, I found the last time I tried to use them was probably six months ago.\nSo I hope now has improved. But I found that UI was a little bit confusing. It was a little bit complex to understand.\nOkay, what if I want to invite people to help me with this project? And then do I need to switch to from a personal plan to an enterprise plan?\nAnd then it seems that if you want to do that, you can. But there is no easy migration of your plan, or at least there wasn't.\nSo I think they still need to get the whole developer experience in terms of transforming Cloudflare from a CDN company to a more like cloud offering company.\nI think there is work to do there, but definitely I love to see all the new products that they are pushing and to see that they are quite different from their territory.\nSo they're trying to innovate in the market and also that they are competing on price.\nSo I really love to see what is going to come out from Cloudflare in the near future.\nAnd I think it's going to become a more and more prominent competitor in the cloud space.\n\n\nEoin: If other people out there have seen third party services which they use instead of something we maybe you take for granted in AWS, we'd love to hear about it because we're definitely keen to explore other options, especially if it helps us achieve that kind of serverless vision where you're just offloading more and more effort to a cloud provider and just getting started in a very, very simple way.\nThat was great to talk through those. We'd love to hear your ideas. Thanks for listening.\nContinue to listen to AWS Bites and follow us and subscribe to the YouTube channel and share the podcast with your friends and rate it and everything.\nWe really appreciate having you here with us and we'll see you in the next episode. Goodbye.\n"
    },
    {
      "title": "44. Do you use CodePipeline or GitHub Actions?",
      "url": "https://awsbites.com/44-do-you-use-codepipeline-or-github-actions/",
      "publish_date": "2022-07-08T00:00:00.000Z",
      "abstract": "Automated, Continuous Build and Continuous Delivery are must-haves when building modern applications on AWS. To achieve this, you have numerous options, including third party providers like GitHub Actions and Circle CI, and the AWS services, CodePipeline and CodeBuild. In this episode we focus on GitHub Actions and we compare it with the native AWS features offered by services like CodePipeline and Code Build. In particular we discuss what CodePipeline offers and how to set it up, what the tradeoffs are and when to choose one over the other. We also discuss when you should look outside AWS to a third-party provider and highlight when GitHub Actions can be a great fit for your AWS CI/CD needs!\nIn this episode, we mentioned the following resources:\n\nExample pipeline for a serverless mono repo using CDK is available in SLIC Starter\n50+ official actions provided by GitHub themselves\nHow to configure OIDC integrations with AWS and other services like GitHub\nGitHub Actions billing details\nWorkshop illustrating how to create CodeBuild and CodePipeline resources using CDK\nPaul Swail’s article “Why I switched from AWS CodePipeline to GitHub Actions”\nA tutorial article by AWS showing how to authenticate and use GitHub actions to build &amp; deploy a web app to an EC2 instance\nOther examples of when it is OK to ditch AWS services for third party (previous podcast episode)\n\n",
      "transcript": "Eoin: Automated continuous build and continuous delivery are must-haves when you're building modern applications on AWS.\nTo achieve this you've got numerous options including third-party providers like GitHub Actions and CircleCI\nand the AWS services CodePipeline and CodeBuild.\nThis is our topic for today so you're going to hear about what CodePipeline offers and how to set it up,\nwhat are the trade-offs and when to choose one over the other,\nand when you should look outside AWS to a third-party provider for continuous deployment.\nWe'll also talk about the rise of GitHub Actions and when to choose it and all the features it offers.\nMy name is Eoin, I'm joined by Luciano and this is the AWS Bites Podcast.\nOkay Luciano, regardless of the service we're trying to pick for continuous integration, continuous deployment, continuous build,\nwhat are we trying to achieve? Maybe we should talk about that first.\nYeah, I think there are a number of use cases when it comes to discussing topics such as CI and CD.\n\n\nLuciano: And so let's try to come up with a list of the different things that you might want to do with this kind of technology.\nSo first of all you might want to do something we can define as automated build.\nFor instance you have a repository with all your source code, you do things on the repository, I don't know,\nyou open a PR, you merge that PR or you just push a commit.\nYou might want something to happen as a reaction to that event\nand that reaction is probably building your code and doing something with it,\nI don't know, running tests, making sure that your code is okay and conform to certain standards that you define.\n\n\nThen another aspect is releasing a specific artifact.\nSo for this is maybe something you would do for instance when you create a tag on your repository,\nyou can say I don't know this is version 1.5 and then at that point you want to kind of\narchive that particular version and package it in such a way that it can be easily deployed to a particular environment.\nThen there is the deploying itself where it's basically you take one artifact and you all the code inside it,\nsomehow you deploy that to an environment that might mean, I don't know, if it's defining containers,\nprobably those containers the artifact is going to be an image in a registry and you might want to publish\nand run a task for instance on ECS using that image or maybe it's a serverless function so you might want to deploy that function and run it.\n\n\nOr sometimes it can be even something more complex, it can be, I don't know, an entire cloud formation stack\nand you want to apply all the changes in that stack. So it's really up to you to define the\ngranularity but the concept is you build, you create an artifact and you try to deploy that artifact.\nAnd there are other things to keep in mind, for instance we want to be, most likely we want to be\nusing different environments and different applications so how do you manage maybe different\nAWS accounts, one per environment, how do you manage multiple applications running in AWS accounts?\n\n\nWe spoke before should you use multiple accounts for multiple applications, multiple accounts for\nmultiple environments, so you might have a very large matrix of combinations there and your CI CD\nneeds to be able to interact with all of those. And then there are other two aspects that I would\nclassify as observability and security. So on one side you want to know exactly what's going on, so\nif you are doing a deployment what are the different steps and if something goes wrong at which step\ndid it go wrong and you should be able to see logs and react to whatever is going wrong. And finally\nin terms of security that's kind of a very broad term but in general we want to make sure that our\nCI CD doesn't get compromised, doesn't become kind of an attack surface for people to just, what if\nthey can steal credentials for instance and then they can impersonate your CI CD and do all the\nthings that your CI CD can do. So there needs to be also a certain level of concern around making\nsure that your CI CD infrastructure is as secure as possible because generally that layer has a lot\nof permissions because it's literally spinning up new infrastructure, changing existing infrastructure\nand so on. So yeah I think that covers more or less what is the need but speaking of AWS,\nhow do we do all this stuff because I know there are so many different services to do all these\ndifferent things and I often get confused about which service does what. So should we try to do\nErica?\n\n\nEoin: Yeah in the AWS console if you go to developer tools you see four or five different services and the main ones there are CodeCommit and that's kind of their alternative to GitHub and\nBitbucket and the like. So we're not really going to cover that here since we're assuming people are\nusing something like Bitbucket or GitHub. We've also got CodeDeploy and this is a special\nservice for deploying to EC2 or ECS or Lambda. We talked about it a little bit before you can do\nblue-green deployments with it. That can be used regardless of whether you used CodePipeline or a\nthird-party service. So let's also park that to one side. The other two main services are CodeBuild\nand CodePipeline. So those are really I guess what we're going to focus on today. CodeBuild is like\nthe basic building block and it compares them a little bit to what CircleCI or Travis or other\nsimilar services offer in that you can declare a YAML file called a build spec and that allows you\nto declaratively write all your build steps and CodeBuild will execute them for you. Because it's\nan AWS service you always need to kind of create a resource for that to work so you also before that\nworks you have to create a CodeBuild project and associate it with a source like GitHub or Bitbucket.\n\n\nThen you can pick a container image for your project to run in and you give it an IAM role\nto use. You can configure the size of the instance. All in all I think CodeBuild is a pretty good\nservice as a basic build execution environment but it's I would say no frills. It doesn't provide a\nparticularly good UI or anything. CodePipeline then is like a continuous deployment or continuous\ndelivery workflow service so it allows you to chain multiple stages and actions together.\n\n\nSo you can use it together with CodeBuild and it will allow you to orchestrate multiple steps.\nSo in CodePipeline you have this concept of stages so each phase of your workflow is called\na stage and each stage can have multiple actions and actions can be run in parallel or in sequence.\nThere's lots of different actions you can do. So you've got source actions and then you've got\nso source actions can come from GitHub, it can come from S3, ECR, CodeCommit and then for\nactually doing stuff based on that source you can run a CodeBuild job, you can even run Django,\nyou can even run Jenkins and there's also lots of third-party providers as well including lots of\nproviders for running specific tests. So it's well integrated into lots of AWS services but also\nthird-party services and it then allows you to deploy with CodeDeploy out to ECS, you can deploy\nout to S3, Elastic Beanstalk and as well as that if you really want to do custom stuff you can\ninvoke Lambda from CodePipeline or Step Function. I've spent actually a lot of time creating\npipelines based on CodePipeline and CodeBuild and that might be a little bit of a hint as to where\nthese services fit because you do end up spending a lot of time. We do have an open source project\ncalled SlickStarter which is like a serverless template project that you can use for exploring\nlots of different things you need to implement when you are creating a serverless project. One\nof those is continuous deployment and in there there's a CICD folder that has a CDK project that\ncreates a CodePipeline and CodeBuild with all of the build phases, integration tests and multiple\nstages to multiple environments. It supports cross-account deployment and yeah that's probably\na good example because to achieve something like that you do need to spend quite a lot of time but\nlet's maybe talk about the advantages there first. So with CodePipeline you know everything is well\nintegrated on AWS that's always going to be the advantage of picking the AWS native option. You\ncan also maintain the pipeline code using the same infrastructure as code be it Terraform or\nCloudFormation or CDK as you do for the rest of the application so you can manage all those\nchanges together and the way you do it is it's fairly consistent right everything is an AWS\nresource. It also kind of scales well right you've got the elasticity of AWS with your\nwith your CodeBuild jobs and you know it's it's you can get notifications as well so you get\nSNS notifications on your code pipeline you can integrate that to Slack using AWS chatbot.\n\n\nIt's obviously quite well integrated into the AWS ecosystem but having used it quite a lot on\nmultiple projects I still think there's lots of areas where CodePipeline and CodeBuild could be\nimproved right so the main disadvantages I'd say would be like there's a steep learning curve right\ncompared to the alternatives you you kind of have to design and deploy the CodeBuild projects\nunderstand how the services work it's never as easy as you will think you will always underestimate\nthe amount of time you need to create these things. Unsurprisingly the user experience for\nboth services is not as great as the alternatives you don't get like a nice single pane of glass for\nall your pipelines with expandable sections that you can quickly go into and go out of.\n\n\nCodePipeline's overview for the execution is pretty good overview but it doesn't show you\nlike multiple workflows very well if you want to drill drill down deeper you end up clicking\nacross to CodeBuild and then the user experience is just a big log. There can also be a performance\nproblem I'd say so if you've got multiple CodePipeline stages because you're trying to\nbreak it up into lots of distinct steps the transition step between each one can be a little\nbit slow now they did improve the performance of this but you still use S3 as an intermediate\nstorage between your stages so there's always a push to S3 at the end of stage and a pull from\nS3 at the end of the next stage and if you've got you know a substantial amount of data source code\nbeing passed around which is quite common these days that can really slow down your execution.\n\n\nSo that's a bit of a problem right because build and deployment speed is important it is really\nimportant for developer productivity I would say and we should always be trying to get that\ndeployment time down as low as possible. I have seen people overcome that problem by like just\ngetting rid of CodePipeline altogether and just using CodeBuild but the problem there is that\nwhen you're just using CodeBuild you lose that structured workflow everything is running one job\nyou do have distinct phases but you don't get any visualization of that really it's just\nthey're all just steps that are logged out to a log file. The last kind of disadvantage I'd say\nis that source providers be it from GitHub or elsewhere are quite clunky to set up with CodeBuild\nand CodePipeline. Setting up authentication there's different ways of doing it in CodeBuild\nand CodePipeline they also have a reasonably new thing called connections which is a little bit\nbetter but you still can't have triggers in CodePipeline from multiple branches.\nEverything else out there allows you to specify like a glob pattern for your branches\nto trigger from PRs. CodeBuild allows you to do that but CodePipeline does not. People end up then\nusing a CodeBuild job at the start which uses a wildcard pattern on your branches and then this\ntriggers the CodePipeline and it's just not as seamless as you would want. It seems like we've\ntalked a lot about the shortcomings now of them. How does the alternative compare? Do you want to\ngo through what GitHub actions is like in comparison?\n\n\nLuciano: Yeah let's talk specifically about GitHub actions because seems to be kind of the main contender in the market even outside AWS\njust because even in open source everyone is hosting projects or almost everyone at least is\nhosting projects on GitHub and everyone is starting to take more and more advantage of the built-in\nGitHub actions to do all the automations around their open source projects so it kind of makes\nsense to also use all that knowledge to try to deploy applications in all sorts of different\nenvironments including AWS and the experience is actually fairly simple in my opinion. I've been\nusing this extensively for open source not as much with AWS but for open source I have a very good\ngrasp on what is the process to kind of create a workflow and make it run and it always starts with\na YAML file that is generally created in the root folder of your repository in a actually it's not\nin the root folder there is a special folder called.github and in that folder you can create\nanother subfolder called workflows and then every YAML file that you define in there is kind of\nautomatically becomes a pipeline or a workflow if you wish that will be executed depending on the\ncondition that you specify inside of your YAML file so this is kind of what makes that integration\nalmost seamless because you don't really have to go and call APIs or click around the UI to enable\na specific workflow you just create a file and as long that file exists and it's well formatted\nyour workflow exists and will be executed according to what you specified inside the file\nin terms of AWS of course there is a step to integrate the two systems together so GitHub\nneeds to be aware of a particular role that needs to be assumed in order to have permissions to do\nall sorts of different actions and this is something that can be done using OIDC provider\nand used to be a little bit more cumbersome in the past but I think now is a little bit more\nsimplified so maybe we can go in details in another dedicated episode but yeah basically\nyou create an OIDC provider for GitHub actions and at that point GitHub actions is able to\nassume a role and all the permissions related to that role so in terms of why is this better we\nalready mentioned that looks a little bit easier to define our workflow and make it run because\nit's just creating a YAML file with a quite simple syntax but there are a lot more advantages and\nfirst of all that is already like if you are already using GitHub as a source your repositories\nare in GitHub that that's literally it like you don't need to create another source or another\nconnection or you just create files in the same repository and that's it so that that integration\nis very seamless and the other thing is that it's very easy to have conditionals for instance if a\nparticular step is something that maybe you want to run only in the main branch or maybe you want\nto run if that commit was a tag you can have all sorts of conditions actually the language is very\nflexible that is very very easy to do there is literally an if attribute in the YAML statements\nand that if attribute has its own expression language and it is quite simple but at the same\ntime powerful enough for most use cases and another thing that i really like that is also\nvery simple maybe in comparison with code build and code pipeline is the matrix feature so for\ninstance a common use case this is more maybe when you are building a library you might want to run\nthe unit tests against different versions of your runtime let's say it's an OJS project probably you\nwant to run the tests against i don't know node 14, node 16, node 18 just to make sure that people\nusing different versions of Node.js can use that particular library without problems this is\nextremely easy to do with GitHub actions because you literally have to define a property that says\ni have these three attributes that are variations of my pipeline and the attributes will be\nnode 16, 18 and maybe 14 and then you can use these variables inside your action for instance\nyou're probably going to have a setup step that says configure the version of Node.js and use that\nvariable and at that point um GitHub action is going to take all the variations that you specify\nfor every type of attribute for instance Node.js versions you might have also a imperative system\nit's going to do a matrix with all of them and basically it's going to execute all the variations\nfor you and the UI is actually very sleek in making you see all the different variations that\nare running by default they are executed in parallel so the amount of configuration is very\nminimal and the result is quite powerful the other thing that is very nice is this idea of\nthird-party actions that is basically for most cases you might want to do something that is\nvery common for instance setup Node.js or authenticate against AWS like in in other\nCI systems you need to write your own bash script using specific CLI utilities to do certain things\nand that's always either a copy paste which is a little bit annoying or something that you need\nto figure out every single time and then you end up copy pasting from your previous repository\nusing GitHub third-party actions basically what you do is like you are importing a module and then\nyou say do this thing and use this particular configuration for instance the the action called\nsetup Node.js is either you say reuse this third-party action which is provided by GitHub\nitself and say you only need to specify the version of Node.js that you want there are of\ncourse other parameters but it's literally import this module initialize it with this configuration\nI don't want to know exactly what's happening behind the scene but I know that it's going to\nsolve this particular use case that might also be a little bit of a problem because you might start\nto think oh what about supply chain attacks what if I'm using an action that maybe is not\ntrustworthy and people can use that as an attack vector this is definitely a concern I'm not sure\nof course but the good news is that GitHub has 50 official actions that they maintain themselves\nand these are the most commonly used ones for instance setup node setup java all the basic\nbuilding blocks that you might find on all sorts of different programming languages and runtimes\nbut they also do this thing called verified by GitHub where the most commonly used actions they\nactually audit them to certain extent I'm not really sure to what degree but they will tell you\nwe kind of spend some time and this is trustworthy so if you see that badge you might be a little bit\nmore confident that it's not gonna create security problems for you I still recommend you verify the\nsource code because all these actions are actually open source so it's actually another repository\non GitHub so you can literally read all the code they generally run as containers so what happens\nis that they will pull the code from that repository and run it as part of your workflow\nso you can literally see exactly what's going to happen and you can also tag specific commits on\nthat repository if you really want to be sure you are running a specific version that you have been\nauditing so this is just as a suggestion if you if you want to be really cautious about importing\nthird-party source code into your pipelines what else I think oh yeah there is another interesting\npoint regarding self-hosted runners so in general when you use GitHub actions the pipeline is\nrunning on GitHub infrastructure and of course that comes with a cost that maybe will detail a\nlittle bit later but you can if you don't want to run your code your pipelines in GitHub self-host\nin GitHub runners you can self-host the runners yourself and there is like an agent that you can\ninstall in anywhere where you want to run your code it might even be I don't know a raspberry pi\nconnected to the internet if you really want to and at that point you just need to register the\nworkers with a particular project and GitHub will dispatch the the workload to to this\nmanaged runners that you manage yourself\num yeah I think that's all we have so maybe in terms of disadvantages let's see what we can say\nuh in comparison with CodePipeline of course so yeah we said that there are a lot of things that\nare simplified because just the user experience of GitHub actions is is different from what you\nget with CodePipeline but at the same time you need to do this additional step of making sure\nthat credentials are actually set up correctly and that you configure this OIDC provider to allow\nGitHub actions to authenticate against your AWS accounts yeah um if you use multiple environments\nthat comes with a host so also that's something to keep in mind some people complain that\nis not the most reliable service it has been going down a few times and even if you even if you use\nyour own workers that if the let's call it the data plane I don't know if it's the most correct\nterminology but the orchestration plane whatever you want to call it if that one goes down your\nworkers on your own infrastructure are not getting triggered anyway so that that's also another case\nwhere you are not 100% in control and also self-hosted runners are they have some quirks\nI heard people complaining about they are also not reliable in different ways and they seem to\nbe a little bit different from what you get in the hosted the managed GitHub runners in in a sense\nthat there are some subtle differences in behaviors and it's not obvious when they appear so your\nmileage might vary but be careful if you use the self-hosted runners make sure you test them\n\n\n\nEoin: because it might not be 100% the same experience you get with the GitHub runners yeah it's probably a good idea I suppose just to try and use the managed runners where possible unless for some\n\n\n\nLuciano: compliance reason you need to keep those that build running on premise and then we can also talk very quickly about pricing so if you're if you're doing an open source project this is\nactually the best part it's totally free like if you're building a library and this library is\nyour repository is public you can build as much as you want and it's literally free\nso this is actually really nice because it gives you an opportunity to experiment with\nGitHub actions without having to worry too much and it's funny that you can even do like scheduled\nexecutions for instance for the AWS bytes website what we do is every Friday at midnight if we have\na new episode coming that will be released that day the website automatically build itself and\nit will show the new episode and this is something we do entirely for free because the whole website\nis open source in a public repository so GitHub gives us all that service for free so that can be\nvery nice in different ways but of course if you are building a startup you are not going to\npublish probably all your source code publicly right so what happens when you need something\nprivate something more enterprisey you actually the pricing is really interesting because on\nGitHub it's like you have a bunch of different services like repositories\nGitHub actions I think maybe even copilots now as part of all the different services that they offer\nand you don't buy them like individually it's kind of a one plan where you pay seats for developers\nand you get access to a certain amount of features for all the services and in the case of\nGitHub actions you get 3 000 minutes per month I think per seat and that seat is 45 dollars per year\nso my understanding is that if you need more than 3 000 minutes of build time this is of course\nusing their own workers the GitHub workers you probably have to buy more seats so buying more\nseats will give access to more developers but this is going to give you more build minutes yeah I'm\n\n\n\nEoin: not sure I'm a big fan of that just because I mean why why is the number of builds going to be tied to the number of developers it seems a little bit like those those things aren't necessarily going\nto scale linearly I mean I would joke like everybody knows every time you add a member to\nyour team your productivity goes down anyway because you have so much more coordination to do\nso maybe it should be the opposite anyway I digress yeah on one side I appreciate that they\n\n\n\nLuciano: are trying to keep it simple and you don't have to worry about too many dimensions and how they can affect your pricing but at the same time it's probably true that you might have specific use\ncases where I don't know maybe you end up paying so many seats just because you need more build time\nbut you don't really have as many developers right so yeah there are kind of implicit dimensions and\nI guess if you happen to be in the standard use case you're probably fine but if you deviate from\nthat standard use case I don't know maybe your pricing is not going to make that much sense\nanymore and then what else can we say there is a thing called GitHub engine that is not going to\nsay there is a thing called GitHub enterprise that is 231 dollars a year I think this is per\norganization right not per user it's also per user per year okay and that one gives you environment\n\n\n\nEoin: production which I'm not really sure what that is yeah so that means if you want to have you know rules conditions that specify under what conditions can you deploy to production so you don't allow\neverybody to create a bill that can trigger release to production those kind of conditions\nthen you need GitHub enterprise for that which is you know it's it's something that I guess a lot of\npeople might want and it's kind of unusual that you would need to go from 45 to 44 dollars a year\nto 231 dollars a year just to get that so I get your mileage is going to vary like pricing could\nbe very work out really well for you with GitHub but it could also get expensive if you've got\n\n\n\nLuciano: \n\n\nEoin: long running builds with you know small startup so in comparison to all of that how does code pipeline and code build work what is the price in there I think I think it's a little bit I guess just more linear in terms of the number of pipelines and build jobs you have so code pipeline\nis one of the simplest AWS pricing sheets out there it's a dollar per month per pipeline and\nthat's it whether it runs or not you get one free on the free tier and code build then it depends on\nthe instance size so you can configure different instance sizes the kind of standard one is general\none medium that's a cent a minute dollar cent per minute for Linux you can also do Windows\nbuilds are more expensive you can go down as far as the smallest arm instance which is like a third\nof a cent per minute and if you want a really massive GPU on its own it's like 65 cents per\nminute so if it's but it's just based on the number of minutes you execute and there's there\nare some quotas but you can get the quotas increased so I would say that it's one of the\nadvantages of code build is actually it scales pretty well I have had cases on especially on\nnew accounts where code build jobs can sometimes take a while to provision and I guess this is\nsomething that will kind of come up now and again as AWS add more and more infrastructure and as\nmore people run code build jobs but I have found that sometimes even recently that it can you can\n\n\n\nLuciano: end up waiting and prefer provisioning stuff so that's something to be mindful of yeah I was about to say that this is another kind of trade-off that with GitHub actions if you use the managed\nrunners you don't really know on what kind of hardware you are running your code so if you\nneed specific things like a GPU because I don't know if you're doing training models whatever\nyou you're not necessarily going to get a fine-tuned experience there but if you either\nhost yourself the runners then you can use whatever hardware you want but in code build\nthat's a lot more kind of obvious that you're going to pay for the compute you actually use\n\n\n\nEoin: but at the same time you can customize that compute as much as you want yeah that's a good point maybe it's a good point to talk about when to choose one or the over the other in summary so\nI think maybe people have already made up their own mind based on what we discussed in the pros\nand cons I'd say like use code build code build and code pipeline if you have a good understanding\nof those services already and want this AWS service integrations also maybe if you're all\nin on CDK CDK actually has something called CDK pipelines which allows you to create all these\nthings very simply for you with a self-updating pipeline we'll link in the show notes to a CDK\nworkshop which is really good it talks about how you do that but in general I'd say if you should\nuse GitHub actions if you want to reduce the amount of time developers spend on maintaining\nthe pipelines because it's just a lower barrier to entry and it's not so steep a learning curve\nand if you don't need those specific AWS integrations code pipeline offers\nit's I would see that more and more people are going to choose GitHub actions to do this in the\nfuture so it would become the well-traveled path and code pipeline code build maybe you know it's\nstill very widely used because it's just part of all the AWS services and it's reasonably well\ndocumented in terms of third-party community resources but not it will not be it's not the\njuggernaut that GitHub is so you won't you just won't find the same level of support after that\nyeah we're interested in everyone else's opinion also there's plenty of other third-party services\nout there circle CI I've used in the past as well which has also been really easy to set up\nand comparable I think in to GitHub actions in a lot of ways in terms of resources if people are\nlooking for other places to to go one of the reasons why we were inspired to talk about this\ntopic today is because Paul Swale released a really good article a couple of weeks ago\ncalled why I switched from AWS code pipeline to GitHub actions it's really excellent article and\nwell worth a read we've also linked to a tutorial showing how you can set up authentication between\nGitHub actions and building and deploying a web app to EC2 we'll also link to our previous episode\nwhich is on the same theme when to use other alternatives to AWS services so please check\nout that last episode if you haven't heard it already so thanks for listening and we'll see you\nnext time\n"
    },
    {
      "title": "45. What’s the magic of OIDC identity providers?",
      "url": "https://awsbites.com/45-what-s-the-magic-of-oidc-identity-providers/",
      "publish_date": "2022-07-15T00:00:00.000Z",
      "abstract": "If you are thinking of using an external CICD tool to deploy to AWS you are probably wondering how to securely connect your pipelines to your AWS account.\nYou could create a user for your CICD tool of choice and copy some hard coded credentials into it, but, let’s face it: this doesn’t feel like the right - or at least the most secure - approach!\nIn the previous episode we discussed how AWS and GitHub solved this problem by using OIDC identity providers and this seems to be a good solution to the problem.\nIn this episode of AWS Bites we will try to demystify the secrets of OIDC identity providers and explain how they work and what’s the trust model between AWS and an OIDC provider like GitHub actions. We will also explain all the steps required to integrate AWS with GitHub, how JWT works in this particular scenario and other use cases where you could use OIDC providers.\nIn this episode, we mentioned the following resources:\n\nGitHub docs explaining how to integrate with AWS as an OIDC provider\nArticle “What’s in a JWT”\njwtinfo, CLI tool to inspect JWT\nAWS action to assume a role from a GitHub Pipeline\nGreat post by Elias Brange detailing how to setup GitHub OIDC integration for AWS\nPrevious episode on why you should consider GitHub Actions rather than AWS CodePipeline\n\n",
      "transcript": "spk_0: If you're thinking of using an external CI/CD tool to deploy to AWS, you're probably wondering\nhow to securely connect your pipelines to an AWS account. You could create a user for your\nCI/CD tool of choice and copy some hard-coded credentials into it. But let's face it,\nthis one doesn't feel really the right way, or at least not the most secure approach.\nIn the previous episode, we discussed how AWS and GitHub solved this problem by using OIDC\nidentity providers, and this seems to be a much better approach to this particular problem,\nor at least a much more secure approach. My name is Luciano, and today I'm joined by Eoin.\nIn this episode of AWS Bites, we'll try to demystify the secrets of OIDC identity providers\nand understand how they really work under the hood.\nLet's start by summarizing that use case again. We have some process running outside of AWS.\n\n\nspk_1: For instance, we have a pipeline running on GitHub actions. This process needs to interact with\nresources on AWS, like making API calls to AWS to create resources, so it needs some sort of\nauthentication. A classic way of doing that would be to create a user in IAM, create a role\nassociated with that user with the right permissions, and then you generate an access key,\nsecret access key, and those are long-lived credentials. At this point, you could put those\ncredentials into your pipeline. A lot of people might have done this before. You put it in some\nsecret store, like GitHub actions secret store, and use those long-lived credentials to interact\nwith AWS with the CLI or one of the SDKs. Now, the problem with that approach, as we may know by now,\nis that long-lived credentials might easily be leaked, and then it's very hard to detect that\nand really work against that sort of attack. So you can allow an attacker to impersonate your\npipeline and execute malicious code. Of course, pipelines can tend to have very extensive\npermissions because they have to be able to create and delete important resources in your account\nand update your code, so this is a really dangerous use case. So the better alternative\nhere is to use an OpenID Connect identity provider, so more specifically, configuring\nAWS to trust GitHub as an identity provider using the OIDC protocol. Luciano, do you feel like you\ncould describe how OIDC works in broad strokes?\n\n\nspk_0: Yeah, I'll try my best to do that, but before I do that, there is a link that we are going to have in the show notes, which is actually the GitHub\ndocumentation that explains really well how all of that works and there are illustrations. So if\neverything we say today is not 100% clear, we really recommend to check out this particular\narticle. So the first thing to clarify is that there are two main entities here, and we are\ntalking about AWS itself and GitHub, so we need to figure out how to make them talk to each other and\nhow to create this kind of trust relationship. And in the OpenID Connect lingo, there are two\npieces of terminology, identity provider and service provider. In this case, GitHub is the\nidentity provider and AWS is the service provider. So GitHub is kind of the one providing users,\nwhile AWS is the one providing a specific service to the user. And this is where I was a little bit\nconfused at first, because I don't think this is the most intuitive use case to understand this\ndifference. In fact, GitHub doesn't really have a concept of users for AWS itself. GitHub doesn't\nreally have a database, let's say, where there is a mapping between particular user names and\nparticular roles. All this stuff actually still lives in AWS. So we'll see how this can be a\nlittle bit confusing and hopefully we'll try to explain that a little bit better.\nAnd another thing to keep in mind to really understand why there is this little bit of a\nblurry definition between who is providing the users and who is providing the services\nis because this is not like a user-facing integration, but it's more of a service-to-service\nintegration. So in a way, we are connecting two services and the definition of a user there\nis not the canonical one, I would say. So in reality, the way we could see that is that AWS\nis providing a particular role and AWS is basically trusting GitHub to generate some sort\nof credential that will allow GitHub to assume that role. And we'll try to explain better how\nall of that works. So yeah, the first thing that needs to happen in this particular scenario is\nthat we'll need to tell AWS to trust GitHub. So we need to create a trust relationship. And once\ntrust is established, again, GitHub can just say, okay, there is a token that proves that I am the\nthing you trusted before, now give me access to this particular role. And that access is safer\nthan the permanent credential scenario because that kind of access is using temporary credentials,\nusing STS. So those credential will be short-lived and the chance of leaking them is much, I guess,\nis much harder to, even if they are leaked, is much harder to take advantage of these credentials,\nor at least not long-term. Okay. That sounds really good.\n\n\nspk_1: So you've got the identity provider, you've got AWS as a service provider. We've mentioned STS and short-lived credentials,\nI think, a few times in various different episodes. So how do you start and how do you\ncreate the trust relationship between the identity provider and AWS as a service provider? What are\nthe steps there? Yeah, this is something that I've done only manually.\n\n\nspk_0: I don't know if there is a way to actually automate that through Terraform or something else. Probably there is, I'm going to\nguess, but if you want to do it manually, it's kind of one-off type of thing for most, at least\nfor creating that first trust, then you can probably automate the creation of roles. But just\nto do that, what you can do, you can just go to the IAM portal and there is a section there called\nidentity providers. And if you go in there, you can create a new identity provider. And once you\ngo in that interface, it allows you to select different kinds of identity providers and one\nof which is OIDC, OpenID Connect identity provider. And it gives you kind of a form and you need to\nfill that form with certain kind of information that allows AWS to recognize GitHub actions as an\nidentity provider. And the first thing that you need to provide is a URL. This is the OIDC URL.\n\n\nAnd this is actually an interesting thing. I don't know if anyone is familiar with OIDC.\nIt's kind of an extension of OAuth2 that also specifies in a much stricter way how the URL\nstructure should be made, how the tokens should be created. While OAuth2 was much more liberal and\nevery OAuth2 provider could be implemented in a much different way, in OIDC, you literally just\nneed to know that one URL and everything else is standardized. So that's why we can here afford to\nspecify only one particular URL. The other field that we need to populate in this form is something\ncalled audience. And audience, I don't think it's extremely important here because I think GitHub\naction can customize that audience on demand if you want to. But the standard convention that you\nfind in the documentation is to set that to sts.amazonaws.com. And this is basically a value\nthat will be available in your tokens and that you need to check to make sure that GitHub generated\nthe token for the right application, in this case integration with AWS. And then the last thing that\nyou need to do is to pass the time print of the TLS certificate. This is not really something that you\nneed to copy paste, you just need to click a button in the UI and AWS will download the time\nprint of the TLS certificate of the connection to that URL that we specified as the OIDC identity\nprovider. And this is important because we need to make sure that in the future when AWS connects\nagain to GitHub actions, it's still connecting to the same server, so to speak, that the trust is\ngiven by the TLS certificate. So if that TLS certificate changes, most likely we want to\nrevisit the trust relationship and make sure we are still talking with the right provider at the\nother end of the line. Okay, so that's interesting to know.\n\n\nspk_1: So this trust relationship can expire, so you need to have some process in place to make sure you're keeping on top of that and make sure\nyou renew before these things expire potentially. I did actually just check there if it was possible\nto create all of this in CloudFormation and Terraform, and there's a Terraform plugin or\nTerraform resource for this, and there's also a CloudFormation OIDC provider resource. So it\nlooks like everything that you just said is also possible there. I think you just have to figure\nout then how are you going to get the thumb prints into your infrastructure as code template if you're\ngoing to hard code those or do something maybe more dynamic. So you've now got this trust\nrelationship. So that's step one. How do we link that? What's the next step in linking that through\nto permissions in AWS?\n\n\nspk_0: So the next step is to create a trust policy in AWS, and that trust policy needs to have certain particular fields to make sure that you are kind of locking down\nthe security as much as possible. So you're not basically allowing anyone or anything to assume\nthe role. You just want GitHub action and maybe a specific workflow even to assume that particular\nrole. So I suppose at this point you should have in mind exactly the kind of pipeline you're going\nto build in GitHub and what kind of permission that pipeline would require, and you create this\ntrust policy where you specify that the principal for the trust policy is the ARN of the AM identity\nprovider that we just created. Then the action is STS assume role with web identity, and then we can\nspecify a bunch of conditions. We want to check that the audience is actually the one we specified,\nSTS, amazon.aws.com, but also if we want to lock down the role to a particular GitHub action\nworkflow, we can also specify another condition saying that the subject, which is going to be a\nfield in the token specifying exactly the workflow that triggered that particular execution, matches\nexactly your expectation. So let's say that you have, I don't know, a project called e-commerce,\nand that project has a particular repository and a particular workflow in GitHub action called,\nI don't know, build and publish. You will have a way to say assume this role only if the pipeline\nwas the e-commerce and the workflow was build and deploy. So you can create a condition to limit\nthat kind of thing. At that point you have this trust policy and you can attach... that allows you\nto assume a role and that role can have specific permissions like, I don't know, you'll be able to\ncreate a bucket, you'll be able to deploy a lambda and all the things that you need to do for\ndeploying your application. We mentioned...\n\n\nspk_1: you've mentioned the token here and you mentioned things like the audience, the subject. Should we talk about the technology underpinning this, which\npeople may have covered before, various different authentication authorization flows? What is a JWT?\nWhat do we need to know about it in this context?\n\n\nspk_0: Yeah, one of the things that we mentioned before is that OIDC standardizes also the format of a token. A token can really be anything,\nlike any string that you can verify and make sure it is actually trustworthy because, I don't know,\nmaybe you can do an API call and get that that token is reliable from the API call or the token\nitself is somehow signed and you can trust that that signature gives you a guarantee that somebody\ntrustworthy generated that token. In the case of OIDC, this is kind of the choice. They went for\nsigned tokens and the technology of choice is JWT JSON web token. I have written an article a couple\nof years ago with some illustration that tried to describe in brief what's the structure and how\nthey are generated, how they are validated. We'll have a link to that article if you want to go\ndeeper, but the summary of that is that a JWT token is a string made of three parts separated\nby a dot and those three parts are a header, a payload, and a signature. They are all encoded\nin base64url and if you basically split the three parts and do a base64url decode, the header and\nthe payload are actually two JSON objects, JSON encoded objects, and the payload can contain\nproperties that are generally called claims and those properties are... they can be whatever you\nwant, but there are some standards. For instance, audience is one of those, AUD, and generally\nrepresents the particular application for which the token was generated. So if you have an\nidentity provider that can generate tokens for multiple applications, you can use the audience\nto make sure that you are receiving a token that is meant to be used in a particular application.\n\n\nThen there are other claims like time validity. Don't use this token before a certain date,\ndon't use this token after a certain date, or there are information about the issuer.\nFor instance, which identity provider created that token. And again, if you have an application\nthat accepts tokens from multiple identity providers, that's an important information because\nit also tells you how to check the signature for that particular token.\n\n\nAnd if you want to check the signature, you also need to know which key was used to sign the token,\nso the ID of the key is another field that you will generally find in the payload. And in the\ncase of GitHub Actions, there is also subject which is, in that case, will contain a reference\nto the workflow that generated the token. So, other information that you can use and is the\ninformation we can use in our roles to limit the fact that only the particular workflow can assume\na particular role. One interesting thing is that you might wonder how the signature thing works\nbecause it's a little bit magic if you never really looked under the hood, but it works by\nusing... Actually, JWT is a little bit open. You can use both symmetric and asymmetric encryption,\nso you could have either just a shared key to sign the token or you can have public and private key.\nOf course, in the case of OIDC, you want to have public and private key because you don't want to\nshare a secret key between GitHub and AWS because then it means everyone will know that secret key\nand everyone will be able to create signed tokens. Instead, when you use a model with\nasymmetric keys, you will have a public key that allows you to validate tokens and that can be...\nAnyone can read that. And the private key is only known to GitHub in this case and it means that\nonly GitHub will be able to sign these tokens. So, in reality, you almost never want to use the\nsymmetric key these days. You always go for public and private keys.\n\n\nspk_1: Given that we've got this trust relationship and we've got the role created and we've set the trust policy in the role so that it\ncan only be assumed by principles coming from this identity provider, what's the next... How do we get\nthat identity provider being GitHub and specifically ours pipelines to get credentials? So, to assume\na role or whatever it is that allows us to enter the AWS world and make API calls.\nYeah.\n\n\nspk_0: So, in this part, I think there is a little bit of speculation because some parts are well known and well described and other ones, we can only assume what AWS is doing to actually validate\nthe token based on the OIDC standard. So, I'm going to try to come up with a narrative, but it\nmight not be 100% truthful to what AWS actually does. But basically, the point is that at some\npoint we start a workflow in GitHub Action. And GitHub is kind of an event based in that sense,\nsaying that every time there is a new workflow, it's going to generate a token for that particular\nworkflow. And with that token, in your workflow, you might decide to use it or not. But of course,\nif you're going to interact with AWS, you might want to use that token and exchange it for AWS\ntemporary credentials. And that's something that can be done either manually, if you want to write\nall that code with a CLI or an SDK, or if you want to make your life easier, there is an action that\nis provided by AWS and you can just import that action into your workflow and configure it to\nassume the particular role that you have in your AWS account. And what happens behind the scene in\nthat action is that it's basically fetching the token generated by the GitHub workflow\nand then making an STS call, assume role with web identity and pass that token to AWS.\n\n\nNow, this is where it gets a little bit speculative because we'll need to imagine what\nAWS will do to actually trust that particular token. Because the token, as we understood this\nlike a string, where you can do some decoding and get some information out of it, and then there is\na signature that gives some sort of trust that it was generated by a trusted source. So what AWS\nshould do, in my opinion at least, is that first of all, it needs to check if the token is well\nformed. So is it a valid JWT? Can we decode it? And then are there three parts, a header, a payload,\nand a signature? Can we read the claims inside the payload? And then when we read the claims,\nis this token issued by an identity provider that we recognize? So this particular account did have\na connection, a trusted relationship with this particular identity provider. If yes, then at that\npoint it needs to check the audience. Like, do we recognize the application for which this token\nwas created? And in our example, we said we will just use the generic STS, AWS, something, I don't\neven remember. But you can keep that generic or you can customize it if you have different\napplications. And GitHub Actions can actually change that value for you when you create the\ntoken. So that value is actually a little bit of a placeholder that you can configure. Either keep\nit standard if you have one particular use case, or you can customize it by application.\nAnd then the next phase is, okay, once we have validated that the token is correct, that the\ninformation in the token looks good and we understand it, we need to make sure that that\ntoken is authentic. So it wasn't forged by a third party, but it needs to come really from GitHub\nAction. And the way that I assume AWS is going to verify that is by using OIDC. So it's going to see\nin the claims what is the key ID. It knows what's the URL of the token, what's the name of the\npublic key URL endpoint from the OIDC protocol. And it's going to use that to download that\nparticular key, the public key. And then at that point, it can actually check, okay, was it really\nthis key, the one that signed the token? So there is kind of a double trust there. One is given by\nthe fact that we created this trust relationship with that particular URL of the OIDC provider.\nAnd the other one is given by the fact that AWS can download a public key from that URL,\nand that public key actually can verify that the token was signed by that particular OIDC provider.\nAnd at that point, if everything is good, STS will do its own thing. It will create temporary\ncredentials, and it will return these temporary credentials that can be used to interact with AWS\nand will have the policy that is given to the particular role for those credentials.\nOkay. What form, what does it, what do those credentials look like? What form do they take?\n\n\nspk_1: If I understand correctly, this is like the usual when you assume a role with STS.\n\n\nspk_0: So my understanding is that it will be like an access key, a secret access key, and also it will have\na session token and an expiration field. Okay.\nSo yeah, temporary credentials that are linked to a particular role.\n\n\nspk_1: Okay. Yeah. So I guess this is kind of familiar in some ways if you've used SSO or some of the\nCognito flows where you're exchanging some third-party identity provider, you've got some\ncredentials and you're exchanging it for temporary credentials, you get the exact same thing.\nOkay. That sounds a little bit clearer now. How would you use this outside of GitHub? Is this\nreally limited to GitHub for now? What if you've got other CD providers? What other things would\nyou use OIDC providers for?\n\n\nspk_0: Yeah, this is something that got my curiosity because I was thinking, okay, how does AWS trust GitHub? And of course they made that generic. So if you can understand\nhow that connection works, then you can create your own sources for allowing on a certain event\nto assume a particular role and you can kind of delegate to this particular source the idea of\ngiven that there is a trust relationship, I trusted when a particular event happens,\nthen I can assume a particular role with temporary credentials.\n\n\nSo I don't know if there are interesting examples outside the CI/CD world,\nbut for instance, if you have an on-premise Jenkins and you have your own OIDC provider,\nyou could build basically that integration pretty much the same way as we explained for GitHub\nActions. AWS is just going to trust your own on-premise identity provider to basically\ngenerate tokens that then will give access to particular roles on AWS. But I was thinking also,\ncan you use this for other workflows? I don't know if it's the best way of doing this,\nbut technically you could use event-driven things if you want to basically, I don't know,\nmaybe a physical action in the real world triggers something in AWS. I'm thinking, I don't know,\nmaybe you have an application that every time you enter the office, you swipe your own card\nto track time or something like that. If there is, I don't know, I know OIDC provider connected\nthere, that's OIDC provider. There could be an application that creates a token using that OIDC\nprovider, assumes a role, and then maybe recording a DynamoDB table that, I don't know, somebody\naccesses the building at a certain point. So you could create this kind of, I suppose, actions\nwhere you have a source of authentication and you want to assume a role, but in a kind of\ntime-limited fashion. Now, probably there are better ways to implement this kind of stuff,\nbut I was trying to stretch my imagination on once you understand this integration,\nhow much can you use it? How far can you go? All right.\n\n\nspk_1: It sounds like that this is applicable in any case where you've got a system to system interaction between a non-AWS environment and an\nAWS environment. So it could be used when you've got an on-prem Sys application that needs to talk\nto an AWS application, for example, and you don't want to have access keys configured. We know that\nin EC2 or in ECS, you've got like a profile you can associate with that resource. So you can\nuse that resource. So you don't have to have secret keys, but outside of AWS, it has been\nvery common for people to just use long-lived keys to perform that kind of interaction. So I\nguess this is one way of overcoming that. You just need to think about what is your OIDC provider\nand how are you going to issue those credentials? I know some people would maybe integrate it into\nActive Directory and have some sort of service credentials. So that might be another way of doing\nthat. It might be worthwhile mentioning actually, as a slight segue, there was a very recent\nannouncement for a new feature called IAM roles anywhere, and we can link to this announcement\nin the show loads, but it sounds like another way of doing this kind of system to system interaction\nwhere instead of having an OIDC provider, you use a public key infrastructure. So PKI,\nyou've got a root certificate authority yourself, or you can use AWS certificates manager,\nand you can issue client certificates, and you actually set up a trust chain between your\ncertificate authority and AWS, and then use client certificates as a means to exchange them for\ntemporary credentials. So it's slightly tangent, but related, and it's a very recent announcement,\nso I just thought I'd call it out there. But I guess what this is kind of saying to us is that\nall you need to do is to create something that follows the OIDC protocol,\nand you can pretty much use it to exchange identities for credentials in AWS. So I guess\nthat means it's potentially something that could cause security issues if you don't get it right.\nNow you could create your own identity provider. You could use it to give administrator access\nto all of your accounts. So you need to understand exactly what the trust model is. Is it worthwhile\nmaybe summarizing that? How would you describe the trust model for this OIDC relationship?\n\n\nspk_0: Yeah, so as we said, the first step is to create the trust relationship within\nAWS and GitHub Actions. In this case, we go to AWS IAM, and we create the OIDC provider connection.\nAt that point, GitHub Action can create tokens in the form of JWT, and these tokens are something\nthat AWS should be able to trust and recognize. So with a token like that, GitHub Action can say,\nassume a particular role, and it is basically exchanging that token that automatically generated\nwith temporary credentials that are given by AWS for a particular role. So in summary, I suppose\nthat we are basically creating a configuration where AWS trusts the signature of the IDC provider,\nand with that trust comes the ability to assume a role with temporary credentials.\n\n\nspk_1: Okay, that makes sense. So it seems quite powerful, and it's nice the way it's using\nthe standard, and it potentially opens up support for a lot of other OIDC providers.\nI think we've covered it in quite a lot of detail, so you've given a lot of information there. From\na developer point of view, if you're thinking, okay, that's all very well and good, I'm informed\nnow, but as a developer, I've got a CD pipeline, or maybe I'm creating a new one, or maybe I've got\none that already uses long-lived credentials, and I want to switch over to using short-lived\ncredentials with this new way. What are the steps in summary?\n\n\nspk_0: Yeah, just make sure that you have configured the OIDC provider in AWS so that you have created that trust relationship, and we have explained extensively how to do that manually, but Eoin,\nyou also pointed out that you can do that programmatically using Terraform, or Cloud\nFormation, or CDK, or something like that. So make sure that that happens, first of all.\nThen you need to create your own roles, so you can create a role for every single workflow.\nIf you want to be very strict, make sure to set up the right permissions for every role,\nand at that point in your GitHub Action workflow, you can use the AWS Action, configure AWS\ncredentials to basically have a step before you interact with any AWS resource to get the\ntemporary credentials. So at that point, you can remove all your hard-coded credentials and swap\nthem with this particular step that uses the AWS Action to do this exchange of a JWT token for\nAWS temporary credentials.\n\n\nspk_1: Are there any other resources we should point people to who want to get started with this?\n\n\nspk_0: Yeah, so I was actually reading a very good post by Elias Branche that is kind of a tutorial that guides you step by step on how to do all the things we describe today,\nand has very good examples, and also a lot of screenshots so that you can be sure that you are\nfollowing and doing the right things. So I definitely recommend if you want to do this\nfor the first time to use this tutorial as a reference to guide you through all the process,\nand we're going to have a link in the show notes. But also if you haven't seen our previous episode\nwhere we discuss why you should consider using GitHub Actions rather than CodePipeline,\nmaybe that's a good one that you can check out after this one to make sure you really get all\nthe context on why all this stuff might be interesting for you. That's all we have for\ntoday and we'll see you at the next episode. Thank you very much.\n"
    },
    {
      "title": "46. How do you do machine learning on AWS?",
      "url": "https://awsbites.com/46-how-do-you-do-machine-learning-on-aws/",
      "publish_date": "2022-07-22T00:00:00.000Z",
      "abstract": "The public cloud gives you amazing machine learning powers with a low barrier to entry. Once you know where to begin, you can quickly build solutions to process images, video, text and audio, as well as structured data. In this episode we talk about the managed AI services that are available on AWS and that require zero machine learning expertise (Rekognition, Polly, Transcribe, Forecast, Personalise, Comprehend, Lex, Textract, Translate). We will also talk about services you can use to create and run your own custom models (SageMaker). We will finally cover some different use cases and some of the things you need to consider before you do machine learning in production.\nIn this episode, we mentioned the following resources:\n\nAI as a Service book\nJulien Simon’s YouTube channel\nArticle by MIT researchers “Amazon’s Rekognition shows gender and ethnic bias”\nArticle “One year moratorium on police use of Rekognition”\n\n",
      "transcript": "Eoin: The public cloud gives you amazing machine learning powers\nwith a low barrier to entry.\nOnce you know where to begin,\nyou can quickly build solutions to process images,\nvideo, text, and audio, as well as data.\nToday, you will hear about\nall the available managed AI services on AWS\nthat require zero machine learning expertise,\nservices you can use to run custom models,\nsome different use cases,\nand some of the things you need to consider\nbefore you do machine learning in production.\nMy name is Eoin, I'm joined by Luciano,\nand this is the AWS Bites podcast.\n\n\n\nLuciano: So, okay, when we're talking about machine learning, I'm always very confused\nbecause it's a very broad and loose term.\nSo today, what kind of machine learning\nare we talking about?\nLet's give it a somewhat of definition.\n\n\nEoin: I think we're mainly focusing today\non the latest generation of machine learning,\nso built on deep neural networks\nor deep learning, as it's called.\nSo in the last 10 years,\nthere's been a big leap forward in machine learning,\nmainly down to three things.\nOne is the availability of massive amounts of data\nfrom internet scale web and mobile applications out there.\nAnother one is the availability of the cloud\nand scalable compute resources\nto do training and run machine learning.\nAnd the third one is the improvements\nin the machine learning algorithms themselves,\nand specifically the advancements in deep neural networks,\nwhich have allowed us to do all sorts of things\nlike natural language recognition, image recognition,\nand those are the kinds of things\nthat drive a lot of the machine learning services\nwe see today, like Alexa,\nor maybe GitHub Copilot.\nDali is another one\nwhich people might be familiar with now, right,\nwhich allows you to generate images\nfrom an arbitrary description.\n\n\nLuciano: Yeah, no, that's super cool.\nOne thing that I'm always fascinated about\nwhen talking about machine learning\nis that there seems to be like a very big and long process\nevery time you want to come up with, I don't know,\na new model that solves a specific problem.\nSo what are the different steps\nor the different main areas that are always present\nwhen you want to do ML?\n\n\nEoin: Every machine learning expert I talk to\nor data scientist will say that preparation of data,\nso getting your data, preparing it, cleaning it,\nnormalizing it is at least 80% of the effort.\nSo that's the first one.\nAnd then you have training.\nSo creating the model from that data.\nSo that could be the bulk of the rest of your effort.\nAnd then the inference part itself,\nwhich is like running your model in production,\nthat's like 5% of the effort really.\nSo it's kind of an 80, 15, five split.\nSo there's a huge amount of effort required to train,\nincluding acquiring and preparing all your data.\nAnd that makes kind of pre-trained managed services\nvery appealing.\nI mean, specifically for me,\nbecause I'm not a machine learning expert.\nI'm very interested in the topic,\nbut I'm always looking for managed services\nthat take all that heavy lifting\nand the need to have very specialist skills away from me.\n\n\nLuciano: Yeah, I suppose also the cost.\nLike when you put all this time into that particular phase,\nif you can just use that as a service,\nyou're probably saving a lot of costs\nand a lot of time to production, right?\nSo what kind of services do we find in AWS\nthat give us that kind of experience?\nYou don't need to think about models\nand preparing all the data upfront,\nbut more as a user, I just need this feature.\nJust give me a service that is gonna do that well enough.\nAnd maybe I can fine tune a few things\nfor my particular use case.\n\n\nEoin: There's a lot here.\nSo let's run through them really quickly.\nSo AWS Recognition is one of the most popular\nand that's the computer vision one.\nSo if you've got images or videos,\nit can process those and recognize people\nor text within photographs or objects.\nIt's even got a feature that allows you\nto recognize celebrities in an image.\nSo that works on images or video recordings\nand also streaming video.\n\n\nThen you have text to speech and speech to text.\nSo Polly is the Amazon service that does text to speech\nand Transcribe is the one that does speech to text.\nThere's a couple of the newer ones\nthat are more kind of business oriented.\nLike forecast is about time series forecasting.\nSo if you can imagine a use case,\nif you have all of your historical sales data\nfor your e-commerce platform,\nand you want to project and predict your sales\nfor Q4 2022, you could use forecast to help you to do that.\n\n\nOf course, like events can change that.\nI heard Mark our fourth year machine learning expert\nmentioned that, you know, at the start of the pandemic,\na lot of people were using forecasting models like this\nand they all became completely useless\nin the face of world events.\nPersonalize is another one then, which is also based,\na lot of these services are based on amazon.com\nretail models that they've used and have trained\nbased on their data personalizes one of those.\n\n\nSo if you're browsing amazon.com,\nyou often see product recommendations\nbased on your browsing history and your purchase history.\nSo personalize gives you the ability to do that\nwithin your own service.\nSo if you're building the next version of Netflix\nand you want to do recommendations on video titles,\nthat's the service you'd go for.\nComprehend is one I've used quite a lot,\nwhich is for text analysis.\n\n\nAnd that allows you to do named entity recognition.\nSo if you've got a document and you're looking to identify\nthe people, the places, the dates,\nthe locations in that document,\nComprehend will do that for you.\nAnd it'll also do sentiment analysis.\nSo if you want to monitor social media feeds\nabout your company and figure out if people are complaining\nor very happy and respond accordingly,\nyou could use Comprehend for that.\n\n\nIf you want to do something more high level,\nkind of on that theme, Lex is the chatbot,\none that a lot of people might be familiar with.\nSo this is if you want to have an interactive chatbot\nin your mobile app or a webpage,\nthat's machine learning driven because it'll understand\nthe intent of what the user is trying to say\nand allow you to direct a conversation.\nSo it's like an orchestration for an interaction\nbetween a customer and a robot representing your company.\nThe last couple we've mentioned there are Textract,\nwhich is very useful for document processing.\nSo if you've got images, PDFs,\nand you want to extract all the text out of them,\nthey might have tables and you want to extract that out\nas structured data, Textract will do that for you.\nAnd then there's Translate.\nSo Translate is for translating from one language to another.\n\n\nLuciano: That's super interesting.\nBut I suppose that all the services,\nthey are available to all different kinds of industries.\nSo probably there is like a good enough expectation\nin terms of results.\nBut I assume that given more specific use cases,\nyou probably want to fine tune something.\nSo what are the options there to adapt these services\nand be more accurate for specific use cases?\n\n\nEoin: Yes, some of those services allow you to cross train\nwith your own data.\nSo one example of that would be recognition.\nAgain, so if you're processing images\nand you want to identify something\nthat it doesn't recognize out of the box,\nnow the default set of labels that it does recognize\nis quite large.\nYou can go onto the website and download a CSV\nof all the labels of things they identify.\nIt's a long list.\n\n\nBut you can also add your own.\nSo depending on your own business case.\nOne example of that, and I've seen a few companies\ntry to do this is if let's say you're a global\nconsumer goods company, and you've got 1500 brands\nin your portfolio, and you want to monitor social media\nfor people taking pictures of your product,\nand maybe commenting on your product.\nSo you could process images you see on Twitter,\npass them through to recognition,\nbut train recognition to recognize your logo\nor your products, images of your product,\nand then label them accordingly.\nThen you could maybe use comprehensive sentiment,\nanalyze the text.\nAnd if there's negative sentiment about your brands,\nyou might route that through as a support query\nto the customer support department of individual brands.\nWe actually, the book, myself and Peter wrote,\nthe AI as a Service book.\nWe have examples that do something like that\nwith Comprehend for social media sentiment analysis.\nThis is another few use cases in that book actually.\nSo we'll link to that in the show notes.\nYeah, that's super cool.\n\n\nLuciano: And I suppose being AWS, one of the cool things\nis that probably all the services are very well integrated\nwith all the AWS ecosystem.\nI'm going to assume that you just have,\nas part of the SDK, access to all these different services.\nSo you can, for instance, call different features\nof the service from a Lambda\nor from some other compute service.\nIs that the case?\nOr is there something else to be aware of there?\n\n\nEoin: Yeah, exactly.\nThat's how you use it.\nIt's all through the SDK.\nOf course, many of these services have console access\nif you need to do very ad hoc workflows with them,\nbut the SDK is the way to go really.\nIf you're going to integrate it\ninto any kind of a application workflow,\nand it then is a very good fit for serverless\nbecause you can imagine that if you've got images\nwith data arriving in S3,\nand you want to respond to that and analyze it,\nthings like Lambda and Step Functions really help you\nto stitch that all together very well\nwithout having to put in a load\nof additional compute infrastructure.\n\n\nLuciano: Awesome.\nWhat if instead you want to actually go really\ninto the depths and build your own models\nor do more advanced research and more of your own ML?\nWhat kind of tools are worth considering\nin the AWS ecosystem?\nTools or services, of course.\n\n\nEoin: You have a lot of options there outside of AWS and on AWS,\nbut of course, when you start getting\ninto custom models and training,\nthen you start to think about large amounts of compute\nand also GPUs.\nSo there is SageMaker,\nand SageMaker is difficult enough to comprehend\nwhen you're coming into it for the first time\nbecause it has a large number of features\nwith very confusing names.\nSo maybe we'll try and digest that a little bit\nin a somewhat clear way, and we can start with notebooks.\n\n\nSo anyone used to data science\nand machine learning development is used\nto working with notebooks.\nSo that could be Python, Jupyter notebooks.\nNow you also have RStudio in SageMaker.\nI don't know what that's like,\nbut I've used the Jupyter notebooks version\nand it works pretty well.\nYou also have now SageMaker Studio,\nwhich is like a notebooks, but it's in its own,\nhas its own domain and account system.\n\n\nSo you can actually use it inside of your AWS accounts.\nIt's a bit like Google Colab.\nI'm reading you one called SageMaker Canvas,\nwhich is their attempt to build\nlike a no code machine learning.\nAnd this is mainly concerned with processing\nlike tabular data.\nFrom what I saw when it first came out,\nit's still a little bit limited in what it can do\nand its feature sets, but the idea is good, right?\n\n\nEventually you want people\nwho have business domain knowledge,\nbut no machine learning knowledge\nto be able to train their own models.\nIf you are training, then a big part of the job,\nyou mentioned 80% of the data of the effort is in\npreparing data.\nSo there is a service in SageMaker called Ground Truth\nand Ground Truth will allow you to label that data\nand manage all of the human interaction that's required\nto take individual items in your data set,\nlabel them, mark them as labeled,\nquality control your labeling, also using Mechanical Turk,\nif you want to outsource a lot of that effort\nand also with some machine learning assistance as well.\n\n\nSo it can even do things like creation of synthetic data\nsamples within your data set.\nThen when it comes to actually training itself,\nso SageMaker, the main really, I would say,\nif you're a real serious machine learning developer,\nprobably the main benefit is the training infrastructure,\nbecause it will allow you to build clusters\nof GPU containers that can be used to train your model.\nAnd then it can also help you to tune your model.\n\n\nSo with like deep learning, the typical processes,\nyou have hyper parameter tuning\nwhere you're constantly tweaking different configuration\nparameters to your model, you rerun your training model,\nthen you test it against your test data,\nand you see if your model is improving or disimproving.\nAnd the SageMaker training platform is designed\nto make all of that a lot more automated compared\nto the typical manual flow.\n\n\nThe most important thing at the end of it is getting it\ninto users hands, and that's where inference comes in.\nAnd that's basically when you run the model in production,\nyou pass the data and you get results.\nSo is there a cat in this picture being\na canonical example maybe?\nAnd this is also container based.\nYou deploy your model, you deploy like a Python wrapper file\nand you get back a HTTP endpoint.\n\n\nAnd you can access that publicly,\nmaybe from your web application,\nor you can access it just from another system\nwithin your microservices architecture,\nhowever you're building your application.\nThat was typically like a provisioned mode\nwhere you pick a size for your infrastructure,\nwhether you need GPUs or not, and you run an endpoint\nand 20 minutes later it becomes available\nand you can call the HTTP endpoint.\nBut now they also support a serverless mode.\nSo you deploy your model and then SageMaker will scale\nthe infrastructure for you based on the mental traffic.\n\n\nLuciano: Awesome, so one thing that is interesting is that the work\nthat we do at Fortier, it's not as much on the side\nof let's do ML research, create new models,\nbut it's more on how do we take existing models\nand to put them in production the most optimal way.\nSo generally we think about, I don't know,\nwhat kind of AWS infrastructure do we need to use\nfor data pipelines, model deployment, recording results?\nWhat kind of costs are we talking about?\nSo can we optimize anything to save money,\nlike not overspend on all this infrastructure?\nAnd yeah, we generally start by taking an existing model\nand think how do we put this in AWS\nin the best possible way?\nSo do you have any, I don't know, comment or suggestion\nthat what are the options or the different topics\nthat we generally explore with this kind of use cases?\n\n\nEoin: Yeah, it's very common that people have these really\ninteresting models and the next step is like, okay,\nhow do we integrate, how do we give this to our customers?\nAnd you end up with some prototype that works\non somebody's laptop or in a data center somewhere.\nAnd the question is, okay, how do we make this\nproduction grade?\nAnd it's really typical applying typical software engineering\nbest practices and just applying it to this machine learning\napplication, so just thinking about, okay,\nhow do you manage your data?\n\n\nWhat's the deployment pipeline for your model?\nHow do you manage very different versions of your model\nand the process for going from one version to another,\ntesting and rolling back?\nWhat do you do with your results?\nSo one good thing you might want to put into practice\nis that every time you do inference,\nyou record that result and store it with the data\nso that you can feed that back\ninto future training exercises.\n\n\nYou mentioned like performance optimization\nand cost optimization.\nThese are like typical well-architected things,\nbut you're also applying them to machine learning as well.\nSo looking at the compute infrastructure\nand how scalable is it, how manageable is it,\nwhat observability do you need,\nand how do you optimize that cost\nversus performance trade-off?\nAnd yeah, it's quite typical.\nYeah, the people give us a model\nand it's like a PyTorch model.\nThere's something trained in TensorFlow.\nAnd then we're saying, okay,\nwell, what's the best workflow for this?\nDo we need to build training infrastructure\nor is just inference infrastructure?\nAnd if it's inference infrastructure,\nthen there's quite a few compute options actually.\nAnd surprisingly, maybe we can discuss this,\nbut SageMaker inference is something we've used\nquite a lot, but it's not our number one go-to service\nfor inference.\nYeah, that's super interesting.\n\n\nLuciano: What about other considerations?\nLike, I don't know.\nI know that every time we talk about AI and ML ethics,\nfor instance, it's one of those big things\nthat you'll need to address somehow.\nWhat does AWS do from that perspective?\nOr maybe the customers on there\nneeds to be aware when using AWS.\n\n\nEoin: Yes, if you've been reading about machine learning,\nyou probably have seen many, many instances\nof bias problems in all aspects of machine learning.\nAnd just because you choose a managed service\ndoesn't mean you don't have any responsibility there.\nThere's a couple of interesting studies into this\nthat we can also link in the show notes.\nThere was one from MIT that looked into recognition\nand demonstrated gender and ethnic bias in its accuracy\nin doing person recognition.\n\n\nThere's also the case of recognition is actually used\nin law enforcement and security industry\nin a number of places.\nAnd back in 2020, Amazon announced like a moratorium.\nSo they were suspending recognition for police use\nin the States for one year,\nbecause they were asking for the legislation to catch up\nto make sure that the technology would be applied\nin an ethical way.\nSo these kinds of things kind of show\nthat we have a long way to go\nbefore we can just deploy these things and use them,\nparticularly when it relates to classifying people\nor labeling people.\n\n\nSo this is everybody's responsibility.\nJust because you're using a managed service\ndoesn't mean you can just say it's somebody else's problem.\nWhat are the other considerations?\nSo beyond the ethical one,\nwhich is probably the most important one,\nwe mentioned compute, right?\nAnd I said, SageMaker Infosys isn't the number one.\nThe reason for that is that, as I mentioned,\nit could take like 20 minutes for your endpoint\nto come up and running.\n\n\nIt's also can be quite expensive.\nNumber one, service for inference,\na go-to is Lambda actually,\nbecause if your model can fit in 10 gigabytes of space\nor on a BFS volume\nand can run with less than 10 gigabytes of RAM,\nI would choose Lambda every time, really,\nbecause it scales so quickly and is so responsive\nas your inference workload rises and falls,\nmuch better compared to SageMaker\nand much easier to set up and much easier to deploy.\n\n\nSo I've just had a way better experience\ndeploying machine learning to Lambda.\nAs a developer, for the customer, in production,\nfor performance and scalability,\nit's just always ticking all the boxes.\nSo SageMaker will be further down the list.\nAnother thing then, when you're looking at cost\nand performance is maybe look at edge machine learning\nrather than doing all of the inference in the cloud.\n\n\nSo if you have a mobile app,\nmaybe it's better to do the machine learning\non the mobile device,\nleverage the consumer's computing power\nrather than your computing power,\nthen you can get some cost benefits, performance,\nbecause you don't have any round trip to the cloud.\nAnd there's also a data privacy element there as well,\nbecause the customer's data\ndoesn't have to leave their device.\nSo there is SageMaker Edge, another SageMaker service,\nwhich allows you to run these models on the edge,\nas well as a lot of alternatives.\nLike all the deep learning frameworks\nhave a mobile equivalent.\nI think that's pretty much it.\nAnd then going back to like the typical\nsoftware engineering practices,\nconsiderations before you go into production is\nall of those continuous deployment, best practices,\nchange management, observability,\ngovernance, reliability,\nall of those well-architected pillars\nhave to be applied to machine learning as well.\n\n\n\nLuciano: Okay, last question that I have is, I guess up to which point it is correct,\nI guess up to which point it is convenient\nto go with managed services compared to\nlet's build everything from scratch ourselves.\nSo obviously talking about pricing,\nlike what's the price equation?\nIs it convenient to just go with AWS\nbecause the price is reasonable for most use cases,\nor there are maybe the price is just enough expensive\nthat sometimes you might want to consider,\nI'm going to spend more time,\nbut build my own thing in the long run\nis going to be cheaper.\n\n\nEoin: Well, it depends on your use case.\nMaybe we can give two examples.\nSo let's say example one is you're a technology provider\nthat is selling cutting edge devices\nthat do Fox recognition for small holders with chickens.\nSo you have some device with a camera\nthat you put in the chicken run.\nAnd if it spots a Fox in the field of vision,\nit's going to maybe play some sort of sound\nthat will deter the Fox.\n\n\nSo if you connect that up to AWS recognition,\nyou can imagine that\nif you're doing a number of images\nand the number of Foxes are being detected every night,\nthat's not too bad, right?\nYour volume is low\nand then each small holder is playing a subscription.\nSo it would probably cover your cost with AWS recognition.\nThe other one is like if you're airport security\nand you're scanning all of the incoming passengers\nin real time on a video stream, right?\n\n\nSo then you're running multiple images per second.\nSo a million images in recognition is about a thousand dollars.\nSo for your chicken cam,\nthat's probably very achievable,\nbut for your airport security,\nthen it's probably a case where you would say,\nlook, now it's time to run our own infrastructure\nand do machine learning.\nIt's something more cost-effective\nbecause I think a recognition at scale,\nit would want to be very valuable\nto justify the cost of million images for a thousand dollars.\n\n\nWhen you're talking about your own inference\nlike SageMaker versus Lambda,\nSageMaker, if you look at the price,\nit's like four zeros and a two cents.\nSo what is that?\n20,000th, two 10,000th of a cent,\nbut that's about a thousand times more expensive\nthan Lambda from what I can see in the per second cost.\nNow, a thousand times sounds extreme,\nbut remember that a single SageMaker endpoint\ncan process multiple images concurrently.\nA Lambda processes one event at a time.\nSo it's not exactly comparing apples to apples,\nbut again, another reason to go with Lambda\nor even Fargate if you want to use containers,\nbecause I don't know, SageMaker, unless you need a GPU,\nwhich you don't always for inference,\nthat will come down to your performance requirement.\nBut if you don't need a GPU, then SageMaker,\nyou don't need it.\n\n\n\nLuciano: Right, is there any resource that we can suggest to people as a final thing?\n\n\n\nEoin: Yeah, so myself and Peter Elger wrote the book, AI as a Service, and it's all really about the AI.\nIt's all about that topic\nand how to use these managed services in a serverless way.\nSo we'll link to that book.\nAnd I think our YouTube channel from Julien Simon\nis a really good one for people\nwho are trying to explore the space,\nbecause he worked at AWS for a long time,\nand he has lots of really practical\nkind of use case driven scenarios\nwhere he shows you how to use these services.\nAnd it's very technical.\nHe gives you very unbiased opinion.\nIt's not full of AWS spin.\nIt's really very factual and honest.\nAnd he's since left AWS,\nbut he's still doing this kind of content\nand using SageMaker and reviewing these services.\nSo I think his YouTube channel is really good resource\nfor anyone doing ML on AWS.\n\n\nLuciano: Awesome, and I think with that,\nwe have covered everything we wanted to cover.\nLet us know what you think in the comments.\nAnd yeah, we look forward to know\nif you use any of these services,\nif you found any particular issue that is worth highlighting,\nor if you're actually just having fun\nand learning a lot of things, please share with us\nwhat are you learning\nand what are maybe the next topics you want us to cover.\nUntil then, see you in the next episode.\nAnd we'll see you in the next one.\n"
    },
    {
      "title": "47. Preview: How to build a File Transfer application - live!",
      "url": "https://awsbites.com/47-preview-how-to-build-a-file-transfer-application-live/",
      "publish_date": "2022-07-29T00:00:00.000Z",
      "abstract": "In this special episode, we announce our next initiative: starting some YouTube coding live streams where we build something on AWS. Specifically, we are going to build a file transfer service, just like WeTransfer or Dropbox Transfer! In this episode, we announce why we decided to start this, the logistics, and some of the details of the product we are going to build.\nWe are really looking forward to pairing with you all live on this build! Make sure you are subscribed to our YouTube channel so you are notified when we go live on Wednesday 17 August!\nIn this episode, we mentioned the following resources:\n\nThe YouTube channel that we are going to use to live stream\nPrevious episode: Do you use CodePipeline or GitHub Actions?\n\n",
      "transcript": "Luciano: Hello everyone, today we want to give you a heads up for a new kind of episode that we are working on.\nSo instead of giving you a pre-recorded episode, we are taking a short break for the summer,\nbut then we will be back with actually coding live streams. Our plan is to go live on Wednesday,\n17th of August, and we're gonna be live on YouTube at 5pm Ireland or UK time, which should be 6pm\nstandard time, 12 Eastern time, and 9am Pacific time. Hopefully all my time zone maths are correct.\nOur point of reference is definitely 5pm Ireland, UK, if you want to double check all these times.\nSo basically in this live episode, what we want to do is we are going to start to build\nan application, and this of course is an application running on AWS,\nand over the course of the series we actually want to build a file transfer service,\nsomething that looks a little bit like WeTransfer or DropboxTransfer if you ever use those kind of\nservices. So I suppose the main question is why are we doing this?\n\n\nEoin: Right, well we get lots of feedback from viewers and listeners and people who just really like the real world use cases.\nSo the one you mentioned about migrating an application to AWS was really popular. We've\nalso covered, like we spent 50 episodes covering all the different AWS services and how they work.\nIt's probably time to start putting these into practice and showing people really how it works\nwhen you've got hands on keyboard. And I suppose it's also good just to build the services you use,\nlike file transfer services, because you understand exactly what the idea is, and you can learn a lot\nlike trying to understand how you might build that from scratch. So what would it look like?\n\n\nLuciano: Sure, so just to put things into context, we are not going to try to build like a fully fledged Dropbox with file syncing and stuff like that. We're going to keep it a little bit more\nminimalistic, but still we want to build something that is going to be useful for people and you can\nuse it in different contexts. So the idea is that it's kind of a very simple file sharing product.\nSo you just want to upload one file and then have a URL that you can share with other people for\nthem to download that particular file. And if you are thinking of different kind of examples or use\ncases, for instance, when we record these episodes, we always need to share video file and audio files\nbetween ourselves before we do the final production. So imagine that we will be using a\nservice like this to just upload this video and audio file somewhere, and then we have an easy link\nthat we can easily share between ourselves. Similarly, you might be thinking, okay, I need\nto send a bunch of receipts to my accountant. I have this big zip file, but it's too big to send\nit in an email. What do I do? My accountant probably doesn't have Dropbox, so let's send\nthem a URL. Similarly, and this is actually a very common use case I had many, many times,\nand you probably can relate to that. You have multiple devices and you just want to transfer\na big file between them. Every time it's like, how do I do that? If you have a service like this,\nyou can easily just upload on one side and download from the URL on the other side.\nAnd at this point, why not to use Dropbox? Well, there might be many reasons. Maybe you just don't\nwant to pay for Dropbox, or maybe you want to be in control of your own data and prefer to avoid\nthird-party services and have something where you really understand what's going on, how the data\nis actually being processed and used. Or maybe this is actually a realistic use case I've seen\nin many offices that the network is actually blocking services like WeTransfer or Dropbox.\nIn that case, you literally cannot use those services. One final reason that we can find is\nmaybe you just want to use your own custom URL either for vanity or because you're actually\nbuilding a business and it might make sense to have a little bit more trust by giving URLs that\nrepresent your company or your product. So I suppose, yeah, just to finish off,\nwe are building this product mostly for ourselves to have fun, but in reality, a feature like this,\nit's something that could be integrated in every other product. Imagine you are building,\nI don't know, a chat application and you need people to be able to exchange files in a chat\nsession, or you have just a file management system integrated in a CMS or other kinds of products.\nSo it is actually a very, very common use case to having to deal with uploads and downloads in\nall sorts of SaaS products. So I think by doing this exercise, we're going to, well, first of all,\nlearn ourself how to do this, but hopefully also teach and share with other people this learning\nso that you can go off and build the same functionality in other applications.\nSo do you want to describe, Eoin, what's the plan?\n\n\n\nEoin: The starting point, I think what we can do as the MVP for our file sharing would be, imagine that you call it an API that we deploy onto AWS and it gives you an upload URL and a\ndownload URL. So you upload the file with your upload URL, and then you share the download URL\nwith your friend or with your other device. And that's enough, right? For our MVP, that should\nwork. It might be a little bit rough around the edges in terms of user experience, but\nfundamentally it achieves the goal. So for that, we could use things like S3, of course, maybe\nsigned URLs integrated with API gateway, a bit of Lambda, a sprinkling of IAM and put it all\ntogether with Node.js Lambda functions, serverless framework. And since it's an MVP, you can just use\ncurl to do the uploading and downloading. But that's the first MVP. And I think that would\nbe the core of it from that point. Okay, you mentioned vanity URLs. So maybe the next thing\nwould be to register a domain name, integrate it so that we have much nicer links that are branded\nand we could use Route 53 and certificate manager to help us achieve that. Maybe we have to pick a\nname for this product. So maybe people can let us know over the next couple of weeks, what\nfunky name we could use for our WeTransfer clone. We can get the domain registered in time for the\nfirst episode. Beyond that, I guess we could start building a bit of a front end, right? Maybe not a\nReact front end. How about a command line interface, something like an NPM CLI? What do you think?\n\n\n\nLuciano: Yeah, I suppose that the simplest CLI I can think of is literally you just call the command name, let's call it file upload for now until we have an actual name. And you just say file upload\nthis part. And then you just see the progress bar and eventually you see, okay, this is the URL for\nyour file. This is probably the simplest thing I could think of in terms of CLI. And of course,\nwe can make it nicer. We can have, I don't know, spinning progress bar and all that kind of stuff.\n\n\nBut the very minimum is that you just easily are able to upload a file by just giving a path to\nthe CLI. And we'll probably gonna do all of that with Node.js again, because it's the easiest for\nus. So yeah, it's probably gonna be like a package that you can just NPM install global and then you\ncan use it from any bash terminal. After that, I think that there is a lot of room for different\nkinds of improvements. We can probably also listen to feedback and try to understand what are things\nthat people are mostly interested in. But the ideas that we came up with are, for instance,\nshowing how we could build a deployment pipeline. We also talk a lot about using GitHub Actions and\nOIDC providers. So that can be another excuse to showcase how we could actually deploy this product\nfrom a GitHub Action using the OIDC integration that we have been talking about. We could also\nthink about securing this entire API, because up to this point, we haven't mentioned any level of\nsecurity. As long as you know the host, you can just upload and download. But we might want to\nlimit that. Maybe we can... The easiest thing we could do is just use API keys in API Gateway,\nbut we might end up exploring other options as well. Maybe Cognito or something a little bit more\nsophisticated. And then another topic that we were thinking about is probably you don't want these\nfiles to live in a mystery packet forever. Most likely, if you are using this kind of service,\nyou are transferring big files, you probably don't load them only once and then not using them again.\nSo it probably makes sense to make this file somehow disposable. Maybe make sure they get\ndeleted automatically after a week, a day, or whatever is the deadline that makes sense for\nthe kind of application. We might discuss a little bit about quota, limits, pricing,\nand things like that. So definitely let us know if you already have specific topics in mind that\nyou would like us to actually explore in a kind of live coding fashion, because that is going to\ndrive us in selecting what can we do next after this. Yep. That's it. That sounds good.\n\n\nEoin: Maybe people want to see a bit more front end. It might be nice if you share the link that it gives people\nyour download page and they have an option with what to do with the content. You can even preview\nthe content in some way. There's a lot of different directions we could take this. And I'm\nreally interested to hear what people want to see. So yeah, maybe in terms of feedback, since we're\nlooking for it, one thing is for podcast listeners, we've got a question for you, because we know we\nhave a lot of people who listen to AWS Bites in podcast audio only format. And this presents a\nbit of a challenge when you're doing a live coding demo, live build. And so I sometimes listen to\nYouTube videos of that kind of thing when I'm in the bus or driving. But does it make sense for\nyou? Do you want us to publish these streams as audio only podcasts? Let us know. We're happy to\ndo so if people want to hear it. If people think it's annoying and pointless, we're happy to skip\nthem as well. We're probably going to do a summary episode at the end anyway, where we talk about all\nthe things we learned. So just let us know what you think. We're happy to be guided by you.\nSo we're really looking forward to pairing with you all live on this build. Make sure you're\nsubscribed to the YouTube channel so you're going to be notified when we go live on Wednesday,\nthe 17th of August. Until then, goodbye.\n"
    },
    {
      "title": "48. Building a File Transfer application on AWS - Live coding PART 1",
      "url": "https://awsbites.com/48-building-a-file-transfer-application-on-aws-live-coding-part-1/",
      "publish_date": "2022-08-17T00:00:00.000Z",
      "abstract": "How can you build a WeTransfer or a Dropbox Transfer clone on AWS?\nThis is our first live coding stream. In this episode, we started a new challenge: building a product live on AWS!\nIn this first issue, we managed to implement a very simple MVP using S3, API Gateway, and Lambda.\nAll our code is available in this repository: github.com/awsbites/weshare.click.\n",
      "transcript": "Luciano: Hello, everyone, and welcome to our first live stream of AWS Bites.\nMy name is Luciano, and today I'm joined by Eoin.\nSo hello, everyone.\nHello.\nYeah, if this is the first time for you hearing or seeing AWS Bites, what AWS Bytes is is\nbasically a podcast about AWS where we try to share all the things that we learn and\ndiscover about AWS.\nAnd so far we've been doing it in the form of a regular podcast.\n\n\nWe have a website, awsbytes.com.\nYou can go there and watch all the previous episodes.\nAnd in every episode, we basically try to answer different kinds of questions about\nAWS.\nFor instance, we've been talking about all the things that you need to know when doing\nJavaScript with serverless on AWS, or we compare Terraform and Cloud formations.\nWe have a bunch of different topics.\nAnd by any means, feel free to let us know which one did you like the most, which ones\nwould you like to see in the future, and things like that.\n\n\nBut for today, we want to try something a little bit different.\nWe actually want to try to do a little bit of live coding and actually show what could\nbe the experience of building a product on AWS.\nAnd the product that we have in mind for today is something that looks like a clone of something\nlike Dropbox Transfer or maybe WeTransfer, where basically the idea is that you need\nto share a file with somebody else.\n\n\nLet's find the easiest possible way to do that, or the fastest maybe possible way to\ndo that, which is basically let's upload it somewhere and then have a URL that it's somehow\nsecret and we can take that URL, share it with the person we want to share the file\nwith, and they can use the same URL to download the file that we just uploaded.\nSo the idea is that basically we are going to build a service like that using AWS services.\n\n\nSo probably we're going to be using things like S3 to store the files.\nWe're probably going to be using signed URLs to be able to upload and download things.\nAnd we might be using API Gateway because probably we want to turn that into an API\nthat then we can use from maybe a CLI, maybe a front end.\nSo we have some kind of rough ideas, but it's very important for you to contribute to the\nconversation and suggest maybe different things that we could be doing.\nSo for today, our goal is to, we're going to be streaming for about one hour, I think,\nand our goal is to try to get as far as we possibly can to just to build the first MVP,\nwhere what we expect to do is later to have one Lambda that allows us to upload to S3\nand give us back a URL that can be used to download the file.\nSo yeah, anything else you want to add, Eoin?\nNo, it's really good to be able to get started on this.\n\n\nEoin: And yeah, let's do this.\nLet's see how we get on, how far we get today, and then we can start talking about maybe\nwhere we can take it next.\nAbsolutely.\n\n\nLuciano: Yeah, that makes sense.\nSo I'm going to start by sharing my screen.\nActually first of all, I'm going to mention that we already set up a repository, which\nis the one we are going to be using today, and I'm going to be posting it on YouTube.\nBut if you are following from Twitter or LinkedIn, you can find it at awsbytes slash weshare.click,\nwhich is the name we selected for this project.\nI'm actually going to see if I can show it on screen.\nSo let me share my screen here.\nOkay, this is the repository.\nSo this is the URL awsbytes weshare.click.\nRight now it's pretty much an empty repository.\nWe only have a little bit of readme and an architecture diagram that I'm going to be\ntrying to describe.\nCool.\n\n\nEoin: Actually, let me show it here, which is a little bit bigger.\n\n\nLuciano: So basically the idea is that, as we said, we want to have a quick way for Alice and\nBob to share one particular file through the internet.\nSo the idea is that we can give Alice basically a web server where she can upload the file\nand then she will get back a URL that once shared with Bob allows Bob to download that\nsame file.\nNow, if we zoom in a little bit more on how we are thinking to implement all of that right\nnow, at least for version one, is that we have an API gateway that gives Alice a particular\nAPI endpoint.\n\n\nOn this API endpoint, we can do a post and this post basically, what it's going to do\nis going to trigger a Lambda and this Lambda is going to effectively create a pre-signed\nURL on S3 that is going to allow Alice to upload the file itself.\nSo the first step is that basically with this request, Alice gets an S3 upload and then\nshe can use that S3 upload pre-signed URL to actually put the file into the bucket.\n\n\nBut at the same time with the upload URL, we also generate a download URL that is going\nto be ready to be shared as soon as the upload is completed.\nSo the first step is actually giving back two things, the upload URL and the download\nURL.\nSo at that point, Alice can upload the file and then just share the download URL with\nBob.\nI think there might be different ways to implement that.\nThis is just one way that we came up with and this is the way we think we are going\nto go for today.\nSo if you have other ideas, definitely let us know and we are open to discuss alternatives\nand maybe pros and cons of different alternatives.\nYeah, the URLs will not be pretty.\n\n\nEoin: Maybe we should pre-warn people, right?\nThese URLs will not be very user-friendly, but don't worry, future episodes will take\ncare of that.\nYeah, absolutely.\n\n\nLuciano: I see we have a comment here in the chat by Abu.\nWelcome to the stream.\nI was just discussing this idea with my colleague as a side project and now I'm seeing this.\nSorry, we are going to spoil the side project, but I mean, if you had different ways of...\n\n\nEoin: \n\n\nLuciano: You were thinking about different ways to implement this by any means, chime in and we can chat about that.\n\n\nEoin: Yeah, also different features for sure.\nYeah.\nI mean, I can imagine like we're going to build this and eventually we have the name\nwe share.click.\nSo this is the domain name we're going to use for this deployment, but I can imagine\nother people would maybe take this and use it as a way to store files for themselves,\nto share files between different devices and have it as their own personal Dropbox using\ntheir own AWS infrastructure, right?\nAnd avoid having to go to Google or Dropbox for their file storage.\nSo yeah, it's probably an opportunity here for us to all have our own forks and customized\ndeployments of this application.\n\n\nLuciano: And I see that there is another comment on YouTube by Italo.\nSorry, I cannot put this one on screen, but it's basically asking, should we try to rebuild\nthis thing and then update you on the progress?\nAbsolutely yes.\nFeel free to redo it, copy paste our code, try different things because again, in AWS,\nthere are millions of ways to build anything and you can use different services.\nSo it will be interesting to also see what other people will think about how to solve\nthis particular problem.\n\n\nOkay.\nSo let's maybe start to look at the code and this is a very kind of vanilla repository\nthat is literally nothing.\nSo maybe we can start by doing a little bit of just creating the kind of the structure\nfor the project.\nSo we are going to be using Node.js as a kind of runtime of choice or JavaScript.\nSo let's start by doing animate of the project.\nSo I just run npm init dash y, which is basically just doing like a default package JSON.\n\n\nAnd we can see the result here.\nIt's actually taking some stuff from the repository, which is pretty cool.\nBut other than that, not that special.\nOne thing that we want to do is we want to start to organize this as a mono repo because\nwe eventually want to create an API, the one we just described, but also some ways to interact\nwith this API, probably a CLI application and maybe also front end.\nSo it might be interesting to put all the same things in the same repository just for\nconvenience and actually recent versions of npm make that very easy.\n\n\nBecause we can just create a new folder and call it backend for instance.\nAnd then we can just say that that's a new workspace.\nAnd actually this is an array, if I remember correctly, and we just need to say backend\nis one item.\nNow at this point we can go into backend and do another npm init y and this is how we are\ncreating sub projects inside our mono repo.\nBut also we want to configure a bunch of other things.\nFor instance, we want to add a license.\nWe want to add a gitignore file.\nAnd there are some tools that I really like.\nFor instance, I think there is one called gitignore.\nGitignore, which we can say, I think node.js and it should give us a default node.js gitignore.\nLet's see if my memory is good.\nIt is not.\nIt gave us an empty one.\nOkay.\nSo let's see if we just say gitignore if allows us to select.\nOkay.\nGitignore types.\nOkay, you're going to have to do slash types.\nAnd what do we have?\nDo we have node?\nNode with just one word.\n\n\nEoin: Okay, fair enough.\nCool.\n\n\nLuciano: Now we've got something that looks a little bit better with a bunch of default stuff.\nOkay.\nAnd another thing we can do is npm, I think it's called license and we can select one\nlicense.\nOr do we want MIT or something else?\n\n\nEoin: Sounds good to me.\nOkay, let's keep it super free.\n\n\nLuciano: I don't have that yet.\n\n\nEoin: I think that's good.\n\n\nLuciano: And now we have a license.\nI don't know about other tools.\n\n\nEoin: I like it.\nCool.\n\n\nLuciano: So the next thing that we might want to do is ESLint, I guess.\nOh, yeah.\n\n\nEoin: So we're going to write all this stuff in JavaScript.\nThat's the plan, right Luciano?\n\n\nLuciano: Yeah.\nOkay.\nIt's okay to proceed.\nNow our mileage might vary because they keep changing this command.\nIt's really updated very often.\nBut it's kind of a guided procedure to pick kind of a default or a starting configuration\nfor ESLint.\nAnd it's very convenient because of course we don't need to remember by heart all the\ndifferent options or presets and things like that.\nSo we want to check syntax, find problems and enforce code style.\nWe want to use JavaScript modules or CommonJS.\nI don't remember if we decided on that.\n\n\nEoin: We're using...\nYeah, I think when we're trying to prepare for this, we were using ESM modules.\nOkay.\n\n\nLuciano: So should be...\nLet's give it a go.\n\n\nEoin: Yeah.\n\n\nLuciano: Let's try to be very brave and go with ESM.\n\n\nEoin: There are some interesting corner cases and edge cases you encounter with them, but maybe\nthat'll be part of the fun.\n\n\nLuciano: We have also a question by Mauro on YouTube saying, isn't PMPM usually used for Monorepo?\nHonestly, I don't know.\nI know that PMPM is just a faster version of MPM because it does things more in parallel,\nbut I never use it.\nSo don't know.\nFeel free to try it and let us know if you find any meaningful difference in terms of\nhow to manage the Monorepos as well.\nOkay.\nSo...\nI'm still trying to figure out how MPM workspaces works, especially for service projects.\n\n\nEoin: And maybe this is something we can deal with when we start adding third party dependencies\ninto different services in the repo and figure out how to package them.\nYeah, exactly.\n\n\nLuciano: I also use it for very simple cases.\nSo there might be edge cases that we haven't discovered yet, but the gist of it should\nbe that if you're an MPM installer at the top level of the project, it should go inside\nevery workspace and install all the necessary dependencies.\nSo it's convenient that way that you can just do MPM installer at the top level.\nAnd then it should also give you a way to import from packages in the same Monorepo.\n\n\nSo for instance, we could have a utils package and we could be reusing that utils package\nin frontend, CLI, backend, and things like that.\nSo it gives you a way to do this kind of cross import and your code is automatically linked.\nSo if you do changes, you don't need to publish those changes at independent libraries to\nactually use the changes in the rest of the code.\nAnd then you have other advantages, like you can run a command in all workspaces.\nFor instance, if we do tests, you could be running MPM tests in all workspaces and stuff\nlike that.\nSo it's very, very simple implementation.\nI think for instance, if you are used to using learner, learner has a lot more features,\nbut at the same time, I don't think we need anything fancy right now.\nOkay.\nDo we want to use TypeScript or not?\nI'd say probably no.\nSo I'm going to go with no, but we might revisit that decision at some point later.\nAnd do we want to run our code for now just not VS.\nPopular style guide.\nNow, do we want semicolons or not?\n\n\nEoin: I'll go with, I just defer to the popular opinion here.\nSo I'm easy.\nI always adapt because otherwise it ends up in a big bike sharing discussion.\nOkay.\n\n\nLuciano: Let's see if anyone.\nOkay.\nIs it possible to zoom VS code a little bit?\nIt should be possible.\nLet me know if this is better.\nSo okay.\nI'm going to go with standard just because it's not really a standard, but I like to\npresent it is a standard.\nSounds good.\n\n\nEoin: Configuration in JavaScript and now yes.\n\n\nLuciano: MPM.\nOh, nice.\nNow you can pick.\nOkay.\nInteresting.\nThis is a new option I haven't seen before, but now you can pick your favorite package\nmanager.\nWe are using MPM.\nSo let's stick with it.\nPerfect.\nAnd this should have created a default ESLint configuration for us.\nNow another thing that we should have been doing is type modules.\nGood one.\nYeah.\nBecause this one allows us to use ESM everywhere without having to call the files CJS or MJS.\nSo every JS file will automatically default to ECMAScript modules, which means that we'll\nneed to rename this one CJS.\nCause this is like a special case where it needs to use module.export.\nOkay.\nSo do we want to do anything else in terms of bootstrap in the project?\nLet's see.\n\n\nEoin: I think from a JavaScript point of view, that's okay.\nNext we're onto the more AWS and serverless specific.\n\n\nLuciano: So I'm going to do a commit with what we have so far.\nOkay.\n\n\nEoin: That sounds good.\n\n\nLuciano: So we have created a new ESLint, gitignore, license, backend, package log and packages.\nPerfect.\nOkay.\nNow you should be able to see these changes in the repository.\nOkay.\nSo I suppose the next step is based on what we saw here.\n\n\nEoin: We need to have a way to create all this infrastructure and to start to write the code for our Lambda.\n\n\nLuciano: And this is also another topic where there are lots of different options, but I suppose\none of the most famous ones is the serverless framework.\nSo let's actually show it here.\nServerless framework, serverless.com.\nSo this is basically a way to, it's a framework that you can install and use it in your project,\nbut allows you to define all the infrastructure in a YAML file, and then allows you to deploy\nthat infrastructure together with your code.\nAnd every time you do changes, behind the scenes, it's actually using cloud formation,\nso you can actually deploy changes incrementally and you don't have to worry too much about\nhow do I replicate these changes in different environments.\nYou just do deploy and it's just gonna make sure that your infrastructure converges to\nthe final state that you currently have in your code base.\nI don't know if I'm describing that well enough, but yeah, we're going to see it in action.\n\n\nEoin: YAML will tell a thousand words when we get going with our serverless.yaml.\nAll will become clear.\n\n\nLuciano: Certainly.\nIf I remember correctly.\nOh yeah, we also have DRAM, no semicolons please.\nSo I'm glad we picked that option.\nThat is the bullet there.\n\n\nEoin: Okay, so we can bootstrap our serverless project with the generator.\nThat's our first step, is that it?\nYeah, remind me the command because I'm not sure I remember.\n\n\nLuciano: I think there is an NPX-SLS.\n\n\nEoin: It seems to be, yeah, NPX-SLS or NPX serverless, and then the command is create.\nAnd then we want to do, we want to pick a template.\nSo it's, yeah, dash dash template, or I think you can also use single dash T and AWS dash\nNode.js.\nLike this, right?\n\n\nLuciano: Yeah, and you're within the root.\n\n\nEoin: Yeah.\nOh yeah.\n\n\nLuciano: Actually you want to be in the backend directory, I think for this.\nSo let's copy all of this.\n\n\nEoin: Although you can pass a directory, but I don't know what the path is off the top of my head.\n\n\nLuciano: Let's do the same thing.\nOkay.\nOkay.\nNow you've got a serverless.yaml that is much bigger than you need.\n\n\nEoin: So you can start trimming.\n\n\nLuciano: Let's look what we have here.\nOkay.\nWe get a bunch of comments that we're going to remove just to keep things minimal.\nSo the service is being called backend.\nIt's just taking the name of the folder.\nWe could rename that.\nProbably let's call it WeShare.\nClick, or WeShare or WC, WC sounds a bit funny.\nThen this is basically saying that we use the version of serverless framework, number\nthree, version three.\nThere are different versions and the yaml that you write can change a little bit depending\non which version to use.\nSo we want to use the latest.\nNow here is using by default node 12.\nWe want to use node 16.\nDo we need to specify the region here or is it going to infer it by what we have?\nOh, we lost Eoin.\nI think I'm going to specify the region here.\nWe lost Eoin for a second.\nEoin is back.\nI'm back.\n\n\nEoin: I back buttoned on the browser.\n\n\nLuciano: You ejected yourself.\nDo you remember if this one is mandatory or not?\n\n\nEoin: That's a good question.\nI always specify it because I want to be explicit.\nI'm guessing it is, but I don't know.\nMaybe somebody knows the answer to that and they can tell us.\n\n\nLuciano: I think it's maybe a good default because if different people are contributing to this\nproject and they might have different default regions, probably this is going to be making\nsure that everyone uses the same region at the end of the day, right?\nYeah.\n\n\nEoin: And you might make this configurable, but I think it's good to be clear in that.\nOkay.\n\n\nLuciano: Now here we have an example about how to add permissions.\nI'm going to ignore all of this because we are going to be using probably a plugin that\nallows us to do this in a slightly different way.\nYeah, let's get rid of that.\nSo actually I'm going to delete all of this.\nThen we can also define environment variables, which we don't need right now.\nAnd we can define which files needs to be included or excluded.\nI don't think we need to do any of that.\nRight?\nYeah.\n\n\nEoin: We don't need that right now.\nIf we really want to trim things down later, we can, but generally it's not needed.\nOkay.\n\n\nLuciano: So now this is the interesting part.\nThis is very generic.\nLike what's the name of the service?\nWhat's the framework?\nWhat is the provider?\nThis is actually interesting because serverless framework can work not just with AWS, but\nalso with other providers like Azure or Google cloud, I believe.\nSo here we are basically configuring more kind of generic project level settings.\nWe want to say, we want to use AWS as a provider.\n\n\nWe want to specifically use Node.js for everything that is like compute, like Lambda functions\nand the region is this one.\nAnd this is where we start to specify Lambda functions.\nAnd by default, there is like an L-law function and we can see that this also created an handler.js.\nSo this is basically saying we want to have a function called a law where the code lives\ninside handler, which is handler.js.\nSo this is, we need to change this because by default is using common JS, but you can\nhave kind of a starting point for your Lambda function.\nAnd I think that's all.\nThen here we have a bunch of other examples.\nWe have outputs and other resources.\nDo we want to create, do we want to start from the function or do you think it's best\nto do something else before?\n\n\nEoin: How about starting with the bucket since this is the center of our architecture?\n\n\nLuciano: That's a good point.\nYeah.\nSo let's do that.\nMaybe we can even deploy the bucket and then start writing the function.\n\n\nEoin: Okay.\n\n\nLuciano: We can probably leave the L-law function there and deploy just to show how a function is\ncreated.\nSo resources is basically a more generic thing, which is actually behind the scenes is going\nto use CloudFormation.\nSo here we actually going to be using mostly CloudFormation syntax while functions is kind\nof a higher level idea.\nIt kind of makes it a little bit easier to create CloudFormation resources.\n\n\nIt's way less verbose.\nIt's going to take a bunch of defaults for you.\nBut I think the first time you see this, it might be a little bit confusing because at\nthe end of the day, they are both creating a bunch of CloudFormation resources.\nSo you could be doing everything even without writing this and just writing pure CloudFormation\nhere.\nBut of course, at that point you are kind of losing the benefits of using serverless\nframework.\nSo we are going to be using functions for everything that regards AWS Lambda functions.\nAnd this is going to make our life much easier to do that.\nBut then for things like buckets, we are going to be using resources because I'm not aware\nif there is an easier way to do that in serverless framework.\nYeah.\n\n\nEoin: I mean, when you create a bucket, you have one resource that creates a bucket.\nWhen you create a function with the serverless framework, you have a function which might\nhave a configured version as well.\nAnd then if you have an event trigger, you might have a lot of API gateway resources\nas well.\nSo sometimes like five lines of YAML in serverless framework will actually generate 200 lines\nof CloudFormation with 15 or 20 different resources.\nAnd maybe we can do that later on.\nYou can run the serverless package command and we can see the generated output of CloudFormation.\n\n\nLuciano: We can appreciate that we don't have to write it.\nThat's a good idea.\nLet's do the bucket first.\nSo the idea is that this is the syntax where we can start to specify resources.\nSo you can see that YAML is very nested.\nAnd for every year, it's basically an array of different resources.\nAnd it's actually a key value pair where every item has a name.\nFor instance, here we want to give it a name, I don't know, file bucket.\n\n\nAnd then here we specify all the properties.\nAnd the properties are generally a type.\nAnd in this case, the type is, if I remember correctly, something like this.\nYeah, auto complete is helping me.\nAnd then you generally have properties.\nAnd properties, they will be different depending on the type that you are using.\nIn this case, they are properties that make sense in the context of an S3 bucket.\n\n\nSo definitely we're going to have something like bucket name.\nAnd here, this is interesting because we can call it some random name for now.\nBut in reality, we really want to make it random.\nSo we will see how to do that in a second.\nAnd then there are some kind of best practices that we can use.\nFor instance, generally when you do a bucket, you want to make sure that it is encrypted.\nSo what we can do is do something like bucket encryption.\nSpecify...\nOh, I like that.\nThe auto compute is doing all of this for me.\nBut we can say something like server side encryption by default.\nYeah, that's what I wanted to do.\nAnd here, AES2056.\nNow if you search online, you are going to find the stuff.\nThis is something that we looked up before.\nYou don't need to remember by art all these things.\nBut the idea is basically every file that is going to be in this bucket, it's automatically\nencrypted server side.\nAnd it's going to be using this particular algorithm.\n\n\nEoin: So it's not using KMS keys, which is the other option.\n\n\nLuciano: So AWS is kind of managing all the keys for you.\nYou don't need to worry too much about keys.\nIf you want more control, you can use your own keys through KMS.\nAnd another thing is that we want to limit public access.\nSo we can do this one.\nAnd then there are a bunch of properties that I'm actually checking in another tab.\nSo I'm not going to pretend I remember them by art.\nBut basically we want to block public ACL, which means if somebody is trying to make\nthis bucket public by mistake or whatever reason, this configuration is going to prevent\nthat.\nRight?\n\n\nEoin: Yeah.\n\n\nLuciano: I remember.\nI always copy paste this too.\n\n\nEoin: There are four properties, right?\n\n\nLuciano: And then this is very similar.\nIgnore public ACL.\nTrue.\nLike this is trying to prevent all the possible ways that this bucket could be made public.\nDo we need anything else?\n\n\nEoin: So I think these are the best.\nThese are the good security practices.\nYou know, we've got a config rules that warn us when we create buckets that don't have\nencryption turned on and that are public, that can be made public.\nThe other thing that I have recently started adding into all bucket declarations is to\nturn on event bridge notifications, which is a relatively new feature.\nBut it makes sense because that means you can start reacting to objects being added\nor removed from the bucket using event bridge.\nSo you don't have to use some of the older methods like CloudTrail or S3 notifications,\nwhich were a lot more limited.\nSo it's definitely a good idea, I think, to add in event bridge notification.\nSo if you create a bucket in the AWS console, there's a checkbox for this.\nIn CloudFormation, it's under the notification configuration property.\nYeah.\n\n\nLuciano: And I think we have an example for that if we just want to add it there.\n\n\nEoin: Yep.\n\n\nLuciano: I'm just going to copy paste.\nShould be something like this.\nFor some reason, Visual Studio got better like this property, but I think it's connect.\nBecause it's new, maybe.\n\n\nEoin: Okay.\n\n\nLuciano: I've seen that Andrea has a questionnaire.\nSo he's asking the only...\nIt's more of a comment, I guess.\nThe only issue I found by putting S3 buckets in the serverless configuration is that when\nI needed to remove the stack for any reason, I redeployed all the objects and the bucket\nwere deleted.\nThis is a very good point.\nAnd I think that there are ways to limit that behavior.\nFor instance, there is definitely a way to make sure that the bucket is not deleted.\nI think it's called deletion policy.\nYeah.\n\n\nEoin: This is a CloudFormation property that you can put on lots of different resources like\nDynamoDB tables as well.\nIt's not within the bucket policy itself, but it's actually at the higher level.\nSo it's a sibling of properties itself.\n\n\nLuciano: So we'll need to put it here.\n\n\nEoin: Yeah.\nStill doesn't like it, but...\n\n\nLuciano: Doesn't like it.\n\n\nEoin: Okay.\nI trust you that this is correct.\n\n\nLuciano: Trust but verify.\nYeah.\nWe'll verify what gets generated later.\n\n\nEoin: So one issue on that is CloudFormation won't delete objects from your bucket.\nSo if people have seen that in the past, it might be because you're using some tooling\nthat is deleting objects for you.\nIt shouldn't delete your bucket with objects.\nBut what this does is it just makes sure that even if the bucket is empty, it won't be deleted\nwhen we delete the stack.\nYeah.\n\n\nLuciano: Okay.\nSo at this point, what we can do is we can see what gets generated, right?\nSo we can do SLS package, if I remember correctly, right?\n\n\nEoin: Which is basically...\n\n\nLuciano: It's not deploying, it's just bundling up everything.\nAnd then we can see what would eventually get deployed if we proceed with that package.\nSo it's kind of a preview in what's going to be produced for us.\nAnd we can see that there is a new folder here called.serverless.\nSo if we go in there, we have a bunch of CloudFormations and this serverless state JSON.\nNow I think the one that is most interesting is the first one, CreateStack.\n\n\nEoin: The CreateStack is the one that creates the bucket that serverless framework is going\nto use to deploy its own assets.\nSo all of your stuff is going to be in the update stack, because it does this kind of\ntwo phase update on the first deployment.\n\n\nLuciano: And this is because serverless will need to upload certain assets to be able to proceed\nwith the deploy.\nSo this is kind of a bootstrapping thing.\n\n\nEoin: It's a bootstrap.\nYeah.\n\n\nLuciano: Okay.\nSo this is the one we are actually interested in, which looks almost identical because we\nhave...\nActually this one is the same, right?\nIt's literally serverless deployment bucket.\n\n\nEoin: It should have the same bucket because it has to make sure that it keeps that bucket.\n\n\nLuciano: And now...\n\n\nEoin: Then that's also their bucket, but it's just the policy, the bucket policy.\n\n\nLuciano: This is our file bucket.\nYep.\nOkay.\nIt is actually adding this deletion policy, retained.\nType, is that one properties?\nNow this, some random name is something that we definitely need to change.\nAnd we'll talk about that in a second, but everything else seems to make sense.\nYeah.\nOkay.\nAnd when we do this, when we have a function that will look a little...\n\n\nEoin: When we have a more complex function, it will look a lot more interesting.\n\n\nLuciano: Now why do we need to change this some random name?\nBecause an interesting thing about S3 buckets is that they have to have a unique name across\nevery account and every region.\nSo if we try to use some random name, maybe we get lucky, meaning that nobody else ever\nused this one.\nBut if somebody just tries to deploy the same stack as it is, they will bump into a conflict\nwith whoever deployed first.\n\n\nSo it's better to actually have a way to generate a random string straight away.\nNow this is not something that you can do easily with just a CloudFormation itself,\nbut because we are using serverless framework, serverless framework actually gives us ways\nto interpolate code, let's say, or code generated strings into whatever is going to be the result\nin CloudFormation.\nSo it's kind of a template language at the same time.\nIt's not just giving us an easier way to create some resources, but it also gives us more\nabilities in terms of how do we structure the infrastructure as code.\nHow do we write infrastructure as code?\nWe have more functionality in terms of string interpolation and things like that.\nSo if I remember correctly, it is possible to create a JavaScript file that actually\nexecutes some logic and then the result of that logic, it can be a string and then we\ncan use that string in our template, in our serverless.yaml, right?\n\n\n\nEoin: Yeah, there's a specific function signature and I'm going to link into the documentation in serverless.com, serverless framework documentation that tells you how to do this.\nSo you can check that too.\n\n\nLuciano: So we can call this file, I don't know, unique target name.js.\nActually it needs to be common.js.\nSo we need to do cjs and then basically what we do here, module.exports equal, it can be\nan async function.\nAnd basically whatever we return is going to be a variable that we can use in our own.\nBy the way, I'm using Copilot.\nCopilot is trying to suggest us something which is not really what we want to do.\n\n\nEoin: No, this looks like a Lambda handler.\nYou're writing a Lambda function.\n\n\nLuciano: But the idea is that we might be doing something like this.\nWe might be doing, I don't know, bucket name and something random.\nThen we should be able to reference this bucket name.\nAnd before we implement all of that, let's try to wire things in and see if it's actually\ngiving us something random in the final, in the final CloudFormation template.\nSo here what we want to do is basically we can use, I think it's like this syntax, right?\nWe need quotes?\n\n\nEoin: You don't need quotes for this.\nYou just need to...\nAnd it's like file or something like that.\n\n\nLuciano: Let's see if I remember.\n\n\nEoin: It's file, yeah.\nAnd then there's no colon actually at this point.\nSo it's file and then parentheses.\n\n\nLuciano: Right.\nOkay.\n\n\nEoin: Yeah.\nI can check.\nI'm verifying all of this in the background.\nSo don't worry.\nIt's not all off the top of my head.\n\n\nLuciano: And then the path.\nWe call it unique bucket name.cjs.\nIf I can type.\nAnd then at that point it gives you back an object.\nLet's say like it kind of runs that file and gives you back an object.\nSo we can just reference bucket name, which is one of the properties that we export from\nthat file.\nYeah.\n\n\nEoin: That's it.\n\n\nLuciano: Now you just need to check the spelling of unique there to match the file name.\nNow if we repackage all of that.\nIt was very fast.\nSo if we check here, now it is something random.\nSo just to show that I'm not lying, if we change this to something random too, and we\nrun this again, it should give us something different now.\nYeah.\nSomething random too.\nOkay.\nSo now the trick is that we need to generate something random that is consistently random,\nright?\n\n\nEoin: Yeah.\nYeah.\nSo a lot of times what you would do here is you don't necessarily have to do this JavaScript\nmodule approach.\nYou can just put like an interpolated string using CloudFormation substitutions or serverless\nvariables where you just add in your account ID and the region as a suffix onto your bucket\nname.\nBut for our application, since the URLs we generate will include the bucket name, we\nwant to be a little bit more protective of our account name maybe and the name of our\nbucket or the account name.\nJust make it a little bit more obtuse, I suppose.\nSo we just want to generate something from these variables rather than exposing our account\nID in the bucket itself.\n\n\nLuciano: So I'm going to copy paste a solution that we developed before, but basically the idea\nis that we could hash some information that we get for the current account region and\nuse that as a unique key because at that point we are guaranteed that if you try to deploy\nthe same thing in a different account or a different region, you will get a different\nbucket.\nBut as long as you use the same account in the same region, you always get the same bucket\nname consistently.\n\n\nSo you don't end up with a different bucket at every deployment, which is basically what\nwe are trying to avoid.\nOn one side, we want something pseudo random, on the other side it needs to be consistently\nthe same value for the same account and region.\nSo we can use the create hash function from Node.js.\nAnd then an interesting thing is that you could get some information here by the serverless\nframework.\nSo serverless framework is going to run this code by passing certain things into it.\nAnd this result variable is a function that allows you to actually retrieve information\nfrom the current context.\nSo for instance, we can get the current account like this.\nThen we could get the region.\nYeah, this is exactly what I wanted to do.\nThank you, Copilot.\nAnd then we can get the stage.\nAWS stage.\nActually region is slightly different.\nI think we want provider.region.\nStage should be...\n\n\nEoin: No, I think you could do self provider.region or AWS colon region is a new variable in serverless\nversion 3 that you can use directly.\nInteresting.\n\n\nLuciano: Okay.\nShould we try region then?\nYeah, that's good to go.\n\n\nEoin: Andrea has another useful comment actually about this particular topic.\nSLS print to display all the...\nOh, that's a good one.\n\n\nLuciano: Let's see.\n\n\nEoin: Yeah, we can try that.\n\n\nLuciano: I don't like something.\nStage.\n\n\nEoin: Yeah, we have AWS colon stage.\nThat should be SLS colon stage because that's a serverless specific.\nRight.\n\n\nLuciano: Okay.\nNow it's actually printing all our...\nLike a preview of our transformation without having to package.\nYeah, that's a good point.\n\n\nEoin: Yeah.\nThank you.\nIt resolves all the variables.\nIt's not CloudFormation strictly because it still has the high level of functions and\nstuff.\nOkay.\n\n\nLuciano: So at this point we still are returning something random too, but we have information that we\ncan hash.\nSo basically what we can do is say const input equal and we can do a string like we share\nand then account hash region dash stage.\nThank you, Co-Pilot.\nAnd at this point, the bucket name we want to generate is basically...\nLet's see.\nWhat we can do is we still want to retain a prefix, right?\nSo let's say we share hash plus create hash MD5 update input.\nYeah, that's exactly what I want.\nNice.\nSo this is basically saying take all the string, hash it using MD5, and then prepend this we\nshare dash, whatever is the hash.\nNow at this point, we can just read out this bucket name.\nAnd if we do this again, we should get something slightly different.\nWe share something something.\nAnd because these values are not going to change, we should get the same bucket again.\nMakes sense?\nCool.\nShould we try to deploy all of this?\n\n\nEoin: This makes sense.\nYeah, let's deploy.\nOkay.\n\n\nLuciano: We have some credentials set up.\n\n\nEoin: Do I have credentials?\nI think I did have credentials in another terminal.\n\n\nLuciano: Let's see.\nHere I don't think I have credentials.\nHere I have my own credentials.\nSo how do we do this?\nI think I can deploy from here.\nI can, we can do SLS deploy, right?\nThat's actually let me make sure we have the right credentials.\nI do.\nThat looks good to me.\nAnd now we can do just SLS deploy should be enough, right?\nWe don't need any other option.\nYep, that looks good.\n\n\nEoin: Okay.\nThis is creating the CloudFormation stack.\n\n\nLuciano: \n\n\nEoin: So this should just create our bucket and that default boilerplate Lambda function we have.\nSo if we go into the AWS console, we should see that these resources are being created.\n\n\nLuciano: Meanwhile we have a question from Juan Lopez from YouTube saying the need for uniqueness\nacross bucket name in S3 is something that I had to deal with in the past.\nI will not have expected that S3 bucket names needs to be unique across S3 and not only\nyour account or region as well, I guess.\nAnd I think the main reason for that is that because S3 is like one of the oldest services.\nSo probably some of the thinking that happens now when AWS create a new service didn't happen\nat the time.\n\n\nAnd the other thing is that S3 creates domain names.\nSo in that sense, the name of a bucket is kind of one-to-one to the domain name that\ngets created.\nAnd also some of the rules for creating a bucket name are pretty much the same rules\nfor a domain name, right?\nSo I think that was the idea at the time and probably AWS now is stuck with that decision.\nOkay.\nSo everything was deployed.\nI should be able to go into the account and show you that we have the bucket deployed.\nLet me bring up another window.\nOkay.\nI have it here.\nLet me make this a little bit bigger.\nSo I am filtering because in this account we have so many more buckets, but you can\nsee the two buckets were created.\nThis is the bucket created by serverless framework for dealing with the deployments.\nThis is the one that we just created from our resource.\nOkay.\nSo maybe what I can do now is commit all these changes and that's it to you, Eoin, for writing\nsome Lambda code.\nOkay.\n\n\nEoin: Actually, status, git add.\n\n\nLuciano: Should I add the handler for now?\nYeah, we will change it.\nEither way is good.\n\n\nEoin: Serverless.\nOkay.\nSo whenever you are ready, feel free to share your screen.\n\n\nLuciano: We also have Gil in the chat who is sending us a chicken, a log gill.\nWe also like chickens.\n\n\nEoin: Do you need to stop sharing the channel?\n\n\nLuciano: Yes, I need to switch.\n\n\nEoin: Excellent.\nLet me pull down these latest changes.\n\n\nLuciano: By the way, Andrea is asking if the episode will be available later.\nWe will definitely post it on YouTube.\nWe are also considering to add it as an audio only podcast.\nI don't know if it makes sense, but we will probably try that anyway.\nSo yeah, definitely we will make it available later in different ways.\nHopefully, we will see you next time, Andrea.\nThank you for all the questions and comments.\n\n\nEoin: Great tips.\nOkay.\nSo we're about to set about creating a function.\nSo if we go back to the architecture, maybe it's worth a quick look at that, actually.\n\n\nLuciano: \n\n\nEoin: We mentioned that we're going to create a function here that will create this kind of share that allows you to get a look at an upload URL and a download URL.\nSo we're kind of in a restful sense, we're going to create a share object, but we don't\ncreate anything in a database.\nWe're just going to talk to S3 here.\nSo let's have a look at that.\nIn serverless.yaml, we've got the existing function boilerplate.\n\n\nSo let's try and take this and make something more of it.\nSo let's say we're going to give our function a name.\nNow, this is just a name that the serverless framework uses to identify our function.\nSo let's just call it create share because we're going to create a share resource.\nAnd we want this to be triggered by an API endpoint.\nSo we're going to create an API endpoint.\nSo our handler in our Lambda function code is going to be responding to this event.\n\n\nSo let's call this handle.\nLet's just call it handle event.\nWe might rename our handler to be a little bit more explicit.\nLet's call it share handler.\nAnd beyond that, we need to start wiring in the HTTP interfaces.\nSo we've got a couple of ways of doing this.\nAnd within API gateway, we've got the API gateway rest API, which is the kind of traditional\nway of doing it.\nAnd now you have the kind of simpler and more cost effective way of doing it, which is the\nHTTP API method.\nI know these things aren't particularly well named, but HTTP API is pretty simple.\nSo let's go with that one.\nSo we're saying that this function is going to be triggered by a number of events because\nthis is an array here we're creating.\nSo this function can be triggered by more than one.\nWe're just going to restrict it to HTTP API events.\nSo for that, we just need to give it the method.\nAnd since we're creating this share resource, then this will be a HTTP post and we can give\nit a path.\n\n\nLuciano: Maybe one thing that is worth clarifying for people that never used Lambda before is that\nLambda is not like something that is always running, but it's just a function that is\nautomatically triggered by AWS when a specific event happens.\nSo what we are doing here is basically telling AWS the event that we want to use to trigger\nour Lambda is an HTTP event in particular, like a post request to a particular path.\nYeah.\n\n\nEoin: Yeah.\nThis is definitely worth stating.\nWe can give this a path and what we can do here is just give it the root path and why\nwe do this at the root rather than creating like a specific path, like a share resource\nhere, it will become more apparent when we look at introducing domain names and API later\nAPIs later, because we can create all of the functions and APIs relating to this type of\nresource all within this serverless project.\n\n\nAnd then we can map it to a domain name with a path so we can actually apply the path later.\nAnd this makes it much easier to do that.\nSo that's our handler code.\nThis is a handler module we're going to have to write and this is the event and that's\nit.\nWe will need to add some permissions, but maybe we can come back to that because that's\nmaybe a little bit clearer just to write the handler code that will do the code that responds\nto the event.\n\n\nSo let's start creating that handler now.\nSo since we renamed it, handler.js is not going to work anymore.\nSo let's just rename this to share handler.\nI must remember to use Node.js syntax rather than Python.\nOkay.\nSo let's just rename this one.\nOkay.\nSo we've got all sorts of hours here, like semicolons and common JS modules.\nSo let's just make this and go from scratch.\nSo the syntax of our handler is we're going to create a function.\n\n\nThis will be an async function with the syntax for a Lambda function, which is that it takes\nan event and it also takes context.\nWe will probably not use context in fact, so we can admit it completely, but let's just\nleave it in there for clarity maybe for now.\nAnd within this handler, we're going to do everything we need to create the get URL and\nthe upload URL.\nSo maybe we can just think about the syntax of this.\n\n\nThis is an arrow function.\nThis is an arrow function, so I'll need an arrow.\nSo we've got a few steps, right?\nSo what are the things we need to do in order to interact with S3 and generate an upload\nURL and a retrieval URL?\nSo we're using the concept of S3 presigned URLs, which are really useful feature.\nAnd the beauty of that is that it allows us to offload all of the scalability for retrieving\nand uploading large files to S3 completely.\nSo none of that file data ever has to actually go through any of the systems that we're building\nhere.\nAnd that's really the goal because S3 is way better at handling the throughput and scalability\nthat would be required if the system was to scale.\n\n\n\nLuciano: Yeah, I guess another advantage to that is that signed URLs allow us to define certain like boundaries for which the file can be uploaded or downloaded.\nNow I think for the first version, we're not going to worry too much, but in the future,\nwe might use that for instance, to limit the time that the file is going to be available,\njust as an example, but you can put other like boundaries and they are just built in.\nYou just need to configure specific properties.\nYou don't need to implement additional codes for that.\n\n\nEoin: Yeah.\nSo just referring back to the diagram, then we're creating this function here and we've\nalready declared everything we need to do for the API gateway endpoint.\nWe've declared the post.\nWe're just using the root path at the moment.\nAnd the next thing we need to do is then create presigned URLs.\nSo maybe let's just pseudocode this out or comment about the steps we need to do.\nSo when we think about it, if we want people to be able to upload an object, we need to\nhave some sort of identifier for this file or object.\n\n\nSo we should probably first create a key or a file or a file name.\nThen we'd want to create an upload URL and then we create the download URL.\nAnd finally we'll return something like an object, right?\nSo this is an API gateway Lambda proxy, which means that API gateway is proxying to the\nLambda service internally within AWS.\nSo there's a specific contract that you'll have to obey here.\nThat means when you return your response, it should have HTTP status code.\n\n\nAnd if you want to return a body, it should also have a body and you can also return HTTP\nresponse headers.\nSo in the status code, I guess what we want is a 201, right?\nBecause we're creating something and that would indicate we've successfully created\nsomething and then we're going to create a body.\nSo this will be something with a download URL and upload URL.\nSo let's go through this.\nSo we need to start using the AWS SDK because we're going to interact with S3.\nSo let's set about that.\nIn order to get this to work, we'll need to install a few modules.\nSo Luciano, is it a good time to talk about the AWS SDK v3?\n\n\n\nLuciano: I think we covered it on the podcast actually a couple of months back, but it works quite a different way to the one we're used to, the AWS SDK version two.\n\n\nEoin: Yeah, I think it's interesting to show how it works.\nOkay.\nOkay, good.\nSo these are the two modules that we researched and know we need to use in order to interact\nwith S3.\nSo you have the, everything is a separate module, the AWS SDK v3, that allows you to\nhave very small bundles of modules when you deploy.\nSo we actually have two that we can use here.\nWe have the S3 one and we have the S3 request presigner one, which is a separate module\njust for doing presigning.\nAnd I'm installing those as dev dependencies actually.\nIs that a good idea Luciano?\nOr should we be installing these as top level dependencies?\n\n\n\nLuciano: I would probably consider this top level dependencies because they need to be available when we run our code in production, right?\nIt's not just something we use for building or for testing.\n\n\nEoin: And Lambda does provide the AWS SDK in the runtime, but if you want to be sure that you're\npinning to a specific version that you've tested with, this is a good practice.\nOkay.\nSo now that we've got that, let's set about importing these and getting to use our S3\nclient.\nOkay.\nSo there's, the first thing we need to do is get our S3 client.\nSo the syntax for that is S3 client.\nThere's a couple of ways you can use the AWS SDK version three, one of which is very similar\nto SDK version two.\n\n\nBut the new kind of idiomatic way to do it with the version three client is with the\ncommand pattern.\nSo if you want to be able to create, get an object from S3 or put an object, then you're\nbasically sending a command to get object command or put object command to the S3 service.\nSo let's import the classes we need to do that from the S3 client module.\nAnd even though we're not doing the upload or the download in this handler, we're generating\na presigned URL, but the presigned URL needs to know what is the command that this URL\nwill ultimately fulfill.\n\n\nSo that's the pattern we're following here.\nSo let's just have a quick look.\nMaybe we'll do the download URL first.\nSo maybe what we can do is just have a look at the syntax of the get object command.\nSo let's say we create this command, get object command.\nSo we're creating a new instance of this get object command that needs the properties that\nget object command accepts, which is going to be a bucket.\n\n\nSo we need to figure out what our bucket name is.\nI'm going to suggest that we take that from an environment variable, and then we're going\nto need a key.\nSo the key is the path to this file.\nSo we're missing a couple of things here.\nWe've got red lines all over the place.\nSo what we need to do is make sure we import the environment variable so we can take that\nfrom process.env.\nAnd the key is something we can generate.\nSo let's figure out how we would generate that.\nI think it makes sense to use like a UUID, like you have version 4 UUID for that.\nAnd there's a new, you don't need to install a third party dependency for that anymore\nin Node.js, do you?\nYeah, I think since node 14 or 16, I'm not sure.\n\n\nLuciano: But yeah, you have now built in functionality in the crypto module.\nExcellent.\n\n\nEoin: Can I ask you a question, Luciano, because I don't know the answer to this.\nI saw that I had two autocomplete options here.\nOne was crypto and one was node crypto.\nAnd I've seen people use both, but I'm not sure what the difference is.\n\n\nLuciano: To be honest.\nCan you clarify?\nYeah, node column crypto is the recommended way, I would say right now, because the idea\nis that the module resolution algorithm, the way that it does to resolve a package is by\ngiving precedence to whatever you have in your node modules.\nSo for instance, if you install a third party module called crypto, then you end up importing\nsomething that is not the node core crypto.\nSo by doing node column crypto, you are kind of explicitly saying, I want to use the node\nJS one.\nLike in reality, this is not a very common problem, but because it has been a problem\nand it will be very hard to debug otherwise, I think this is why we have now this new best\npractice where every day you are importing a node core module, it's better to prefix\nit with node column.\nOkay.\n\n\nEoin: Yeah, that makes sense.\nI guess there could be a vulnerability if you have a spelling mistake in here, right?\nYeah.\n\n\nLuciano: \n\n\nEoin: Because then somebody could have published some sort of supply chain attack thing to NPM.\nOkay.\nThat's really good to know.\n\n\nLuciano: Yeah.\nIf you end up packaging a folder called crypto inside your node modules, whatever you have\nthere is going to take precedence.\nYeah.\n\n\nEoin: Yeah.\nOkay.\nSo let's, let's generate a UUID and from that we can create a key.\nNow we could just make the key to be the ID, but if we think about lots of uploads of hundreds,\nthousands, millions of files over time into the same bucket, it might not be a great user\nexperience when you open the S3 console and you see everything in one prefix, because\nthat's three, even though forward slashes in S3 don't really mean anything, there's\nno such thing as paths.\n\n\nThey're more of a, just a user friendly way of browsing through files.\nThe S3 console will use slashes as a, in the same way as you would see in a traditional\nfile browser.\nSo maybe it's a good idea.\nIt's also, you know, there are cases if you have extremely high throughput on your bucket\nthat S3 will try to automatically partition the bucket based on prefixes.\nSo it is good to make sure that the start of your keys have kind of an even distribution\nand that helps S3 to automatically partition the bucket for you so that you can get a throughput\nallocation per partition.\nSo let's put everything into a shares prefix, but we'll use like the first two characters\nof our UUID to just give us some sort of sorting or categorization.\n\n\nLuciano: Remember this used to be very, very common when doing similar things with like network\ndrives.\nI think at the time there was also some, maybe depending on the file system, some performance\nbenefit to that.\n\n\nEoin: \n\n\nLuciano: But yeah, I think as a user, as you described the user experience, it makes a lot of sense to do the same thing here as well, because I mean, if we, if we ever need to debug something\nwhere we know the UUID and we want to go in the S3 console to see the file, we know we\nneed to click twice to see that file rather than scrolling across potentially millions\nof items.\nYeah.\n\n\nEoin: Yeah.\nOkay.\nSo we've got a key that allows us to create the get command.\nAnd from that get command, we can create a URL.\nSo this will allow us to create a retrieval URL for our users to use.\nSo what we can do for that is the, we're going to start using the presigned URL module that\nwe already added into our node modules.\nThere is a separate.\nSo it's a separate module, we need to import that separately.\n\n\nSo this is the AWS SDK S3 request presigner and the function we're going to use is called\nget signed URL.\nThese are asynchronous functions.\nThey're going to return a promise.\nSo we're a way to get signed URL and we need to pass in the command that we created.\nSo the function, you can see the function signature here is we need an S3 client and\nwe need a command.\nSo let's pass in S3 client, our get command and our properties.\n\n\nSo the properties that are kind of important here are the expiry.\nSo how long is this temporary presigned URL going to last for?\nYou want to make sure that it's long enough that people get to upload their content by\nthe time you send it to them, but not so long that maybe somebody could grab it and intervene.\nSo maybe we'll just create a constant at the top of our file that gives us some sort of\ndefault expiration.\n\n\nAnd I think maybe 24 hours seems like a good value to start with.\nThe value is in seconds.\nSo that will allow us to have URLs that last for a day.\nOkay.\nSo we don't have an S3 client.\nWe can create one.\nAnd that's fairly straightforward.\nWe can just do that outside the handler because it can be reused for multiple Lambda function\ninvocations.\nSo all of the code that's outside the handler is going to get evaluated when the function\nis loaded for the first time.\nSo in cold start phase of your Lambda, everything within the handler is going to be evaluated\nevery time an event comes in.\nOkay.\nSo now we've got a retrieval URL and the process for an upload URL is going to be very similar.\nSo much so that I'm just going to copy paste and change everything from get to put.\nSo we'll need a put command, which will use the same bucket and key because we have to\nput it to that key before we get it.\nAnd let's change the name of this to upload URL.\nAnd that will use the put command.\nOkay.\nSo I think now we have everything we need to give our users.\n\n\nLuciano: I think it's also interesting that you can specify two different expiries for the upload\nand download.\nMaybe you would want in real life the upload window to be very small while the download\ncan be even a week, I suppose.\nRight.\n\n\nEoin: That's a good idea.\nYeah.\nBut for now it makes sense to keep it the same because we can optimize it later.\n\n\nLuciano: Okay.\n\n\nEoin: Now, since this is our MVP and we don't have a command line interface, we don't have a\nweb interface, we're giving our poor users two really ugly URLs.\nIt probably makes sense that we don't return any JSON here.\nWe can just return some instructions and the two URLs that they can use.\nSo we know that they could use something like curl as a command line interface to upload\nand download their files.\nSo maybe we can just give those instructions in the output.\nSo the upload would be curl and with curl, we can do minus X put, because this is a put\ncommand to upload.\nSo we need to specify the put method, put HTTP method, and then they can specify the\nfile name and the upload URL.\nSo that's the upload instruction for the user.\nAnd we can say download with curl download URL.\nI didn't call it download URL, did I?\nCalled it retrieval URL.\nYeah.\nGood catch.\n\n\nLuciano: Okay.\nSo I think we have a function so we can export that.\nYeah, we need to export.\n\n\nEoin: Okay.\nActually the...\nYeah.\nI generally prefer to just say export function at the top and not even use the arrow function.\nI think the, yeah, I'm thinking ahead to one of the ideas we have to improve this, which\nis to use MIDI.\nAnd in that case, we'll do it in two separate steps.\nBut okay, this is good and there's a couple of things we added here.\nSo we added some interactions with S3, which means we're going to need permissions.\nWe also added an environment variable usage.\nSo before this is going to work, we need to go back to our serverless.yaml and do some\nchanges.\n\n\nLuciano: Also, before we forget, I think we also need to specify type modules inside the package\nJSON of the backend.\nBecause I think this is what AWS will see.\nAWS is not going to see the top one we have created.\nOkay.\n\n\nEoin: Good call.\nSo it looks like that.\nIs that correct?\n\n\nLuciano: I think so, yeah.\n\n\nEoin: Okay.\nOkay.\nSo let's add some environment variables.\nSo we've got...\nWe could do this per function, but I think since everything is going to be centered around\nthis bucket, we can do this globally.\nSo it will be applied to all functions.\nAnd we want to say that every function will receive an environment variable called bucket\nname.\nSo in order to pick up that bucket name, it has to use the variable that Luciano created\nwith that clever unique bucket name.\nBut we can also use the CloudFormation syntax to retrieve the name of the bucket.\nSo the shorthand for that looks like this ref file bucket.\nAnd if you look in the CloudFormation documentation for AWS S3 bucket, it will tell you that every\nCloudFormation resource outputs a reference.\nAnd for buckets, that reference is the bucket name.\nIt's not consistent across all the different resources.\nSometimes it's an ARN or something else.\nFor buckets, we know this is a way to get the bucket name.\nSo that should work nicely.\nI think for buckets, it kind of makes sense because it's guaranteed to be unique anyway.\n\n\nLuciano: Like an ARN is always unique.\nBucket name is always unique as well.\nAgain, it's not consistent, but if you think about uniqueness, it makes sense.\n\n\nEoin: Yeah.\nOkay.\nNow, when we are adding in IAM permissions, the way I like to do it is using the IAM roles\nper function serverless plugin.\nSo let's add in this plugin into our configuration here.\nSo the way to do that, there is a serverless native way to do that, but I'd like to just\ndo it explicitly with NPM.\nSo we'll do NPM install, and this is a development dependency.\nAnd the name of the plugin is serverless IAM roles per function.\n\n\nAnd this allows you to kind of honor the principle of least privilege by having a separate set\nof IAM policy statements for every single function.\nSo at the top of our serverless.yaml, then we need to declare our plugins array, and\nwe just add in the module we've installed.\nSo this is a plugin that's going to get hooked into the lifecycle when we run serverless\npackage or serverless deploy.\nAnd it will pick up the IAM policy statements for each individual function.\n\n\nSo let's start writing this then.\nSo the syntax is to declare IAM role statements inside the function instead of at the provider\nlevel.\nAnd then we need to start creating some statements for an IAM role for this function.\nSo this is the set of, this is the policy that the function will run with.\nSo every time an event comes in the API, the execution of that Lambda function or the code\nwithin it will be run within this role.\n\n\nSo we want to allow, we need to allow the permissions that we're giving the signed URLs\nbecause we're using the get object command, the put object command, we need permissions\nto do those things.\nSo the actions we want are S3 get object and S3 put object.\nAnd we'll try to be as specific as we possibly can be for this MVP.\nSo what we can do here is we essentially want for the resource identifier should be the\nARN of the bucket with the prefix attached to it.\n\n\nSo it's going to be something like ARN, AWS, S3.\nYou don't need to specify the account or the region because those things are, you know,\nas Luciano said, it's globally unique.\nThen we'll need here the bucket name.\nSo that's a placeholder for now, and then the path, which is shares.\nWe set everything with the go and shares after that it's pretty much random.\nSo we need a wildcard and we can also make use of another CloudFormation intrinsic function\nwith the short term syntax sub.\nAnd this is basically saying to CloudFormation substitute variables inside that with some\nresolved value.\nSo instead of bucket name, we can just put in file bucket and CloudFormation is going\nto take the reference of that file bucket, which is the bucket name and pop it in here\nin its place.\nSo that should be enough.\nYep.\nOne thing that is worth mentioning, I don't know if we made that very clear.\n\n\nLuciano: And again, this is more for people that are trying to do AWS for the first time that in\nAWS, everything is by default, it's like blacklisted, like you cannot do anything.\nSo if we were not adding this policy, what will happen at runtime is that our Lambda\nwill fail as soon as it's going to try to do a get object or a put object operation,\nbecause it doesn't have permission to do that.\nNow that we added this policy, what's going to happen is that the Lambda is going to run\nin a context where we authorized the action of get object and put object only in those\nspecific resources that match this particular expression here.\nSo everything inside our bucket, the start switch shares, and then whatever the file\nname.\nYeah.\n\n\nEoin: Yeah, that's a really good point.\nAnd the other action we'll need is, well, we don't need it actually.\nI was just thinking you could also add a list bucket permission.\nWhether we do it or not, it doesn't really make a massive difference for this application,\nbut you could also allow users to list the bucket.\nWhy would you need a Lambda function that only gets or puts permissions to list bucket?\nWell, the advantage of that, and we just use the ARN directly like this.\nThe advantage of that is it means that if you try to get an object that doesn't exist,\nit will give you a 404 instead of a 403 error.\nSo it'll give you a not found response instead of a permissions error.\nBecause if you don't have permissions to list bucket, then it's not going to tell you whether\nthe object can't be retrieved because it doesn't exist or because you don't have permissions\nto read it.\nAnd I think that's everything we need.\nLuciano, have I missed anything?\nAre we ready to give this a run and try and deploy it?\n\n\nLuciano: I think we can try deploying it and see if it works.\nOkay.\n\n\nEoin: Exciting times.\nSo let's do this.\nSo I'm going to run serverless deploy within the backend folder.\nMaybe I'll give it just some trepidation here.\nI'm just going to try package.\nWe already have a validation error because it says I put in the word event.\nI think what I need there is events, plural.\n\n\nLuciano: It's good that we get this kind of validation from serverless.\nThis would have been tricky to debug otherwise.\nYeah.\n\n\nEoin: And it's pretty new.\nIt's only around the last year, I think.\nSo this validation has been there, but it's really good.\nOkay.\nSo that's all the information that has been packaged.\nLet's have a quick look at our generated CloudFormation.\nOkay.\nSo now we can see that serverless framework is starting to really take over and do a lot\nof work here, like creating a CloudFormation log group, or sorry, a CloudWatch logs group\nlog group for us.\nIt's creating our role.\nSo let's have a look at that.\nThe role can then be assumed by Lambda, which is important.\nIt has permissions to create logs.\n\n\nLuciano: One thing that I would like to mention there is that this is where we see the advantage\nof serverless framework, because if we were doing the same thing with something like,\nI don't know, Terraform or CloudFormation directly, you don't get anything for free.\nLike you really need to know, okay, to create a Lambda, I need to create a role.\nThen I need to create a log group, which are things that you always do all the time because\nthey are required for the Lambda to run.\nSo with serverless framework, we are actually getting all this stuff being created for us\nfor free using best practices, rather than having to copy paste all the stuff every time.\nYeah.\n\n\nEoin: That's a really good point.\nYeah.\nWhile we're on this topic, actually, one of the points that Juan has mentioned on the\nYouTube chat is what is the best way to test locally?\n\n\nLuciano: And it's probably almost something that could be an episode in its own right.\n\n\nEoin: You can definitely test these.\nI think you need maybe to cover this topic very briefly.\nYou can test using, you need to write unit tests anyway to test your handlers and to\ntest the code within those handlers.\nWe've done it in a fairly simple way because we've got a very simple function today.\nBut you can also use now with AWS STK version 3, you can also use their mocks.\nYou can, because we're using serverless framework, you can use local simulation of the API gateway\nand Lambda in order to test things a little bit more end to end before you deploy.\nBut once you've tested your code, unit tested your code, you want to get it into AWS and\ntest on AWS as quickly as possible and then start doing things like integration tests\nand end to end tests for a real world application.\nBecause the local simulations can be very useful from time to time, but there's always\na point at which there are limitations you can't overcome.\nYeah.\n\n\nLuciano: For instance, here you start to bump into this kind of philosophical questions.\nOkay, if I run things locally, should I test against the real S3 or should I simulate S3\nas well locally?\nAnd there are ways to simulate S3 locally, but as Eoin said, the degree of fidelity might\nvary depending on what kind of features we are going to use.\nSo for simple use cases, maybe everything works as expected.\nFor as soon as you start to use more advanced features, you might start to bump into discrepancies\nand you might have this kind of false sense of security because everything works locally.\n\n\nThen you test it remotely and you bump into issues.\nAnother thing that is also very interesting is permissions.\nWhen you test things locally, you end up using the permissions that you have as a user most\nof the time, depending on which tool do you use.\nBut most of the time, all the tools, they will use your own local credentials.\nAnd generally you have very broad credentials as an admin, right?\nBecause when you are deploying, you need a large set of credentials.\n\n\nSo you simulate your code with this large set of credentials and you don't realize that\nyou haven't specified your policies correctly.\nSo yeah, that's another use case that I've seen a lot of people test it locally, everything\nworks, then deploy, run it for the first time and they have a permission error and they\nhave to revisit their own permissions.\nSo I think it's still a very open debate whether you should try everything you can to test\nlocally or whether you should go as fast as possible to a real AWS environment and test\nit there.\nUsually there is some kind of middle ground that gives you benefits, but yeah, for this\nsimple use case, I think it's just easier to try to deploy remotely and see if it works\nthat way.\nOkay.\nYeah.\nFeel free to disagree with our opinion if you know better ways of testing this locally.\nYeah, of course.\n\n\nEoin: Yeah, so we can see in the clip generator information then we've got the API gateway\nresources, including the integration routes.\nAnd then we can see actually the specific policy that's been generated.\nSo this is the one that's been generated by the IAM roles per function plugin and it's\ngot our, as well as permission to create logs, it's got the get object and put object on\nthis bucket.\nSo I think let's go ahead and try and deploy this because we need to, and I have some time\nin case we've made any further typos or mistakes in the code.\nOkay.\nSo this is deploying to CloudFormation now.\nLet's have a flick into the CloudFormation console where we can see the stack.\nThis is from the time Luciano created it previously.\nWe should see that it's doing an update.\n\n\nLuciano: I don't see it update yet.\n\n\nEoin: It says creating CloudFormation stack, which is slightly concerning.\nI would have expected it to.\n\n\nLuciano: Hopefully you are not deploying to another account.\nLet's see.\nIt could be.\n\n\nEoin: I don't think so, but let me check here because I think I have a, yeah, this is the right\naccount here, so maybe what I can do.\nI have, yeah.\nI'm just going to pause this other one here and check if that was the same account.\nYeah, it's the same account.\n\n\nLuciano: Maybe a different region, but that shouldn't be the case because we are to call it in our\nserverless frame.\nIt shouldn't be the case.\n\n\nEoin: So the account is the same.\nWhat did we change?\nDid I change the name of the stack or something inadvertently?\nBut I don't see any other stack being deployed here either.\n\n\nLuciano: Are we using the same account?\nGood question.\n\n\nEoin: Let me check.\nWe are, because this is definitely from your deployment, right?\nYes.\n\n\nLuciano: This is the right time.\n\n\nEoin: Let me give it a full browser refresh for good luck.\n\n\nLuciano: This is where we show the joys of debugging on AWS.\nWow.\nWell, it has deployed somewhere.\n\n\nEoin: Okay, so let me check this identity again.\nI mean, it makes sense.\nThis is the right account.\nYeah, this looks good.\nSo the question is, what's the region?\nI'm actually using credentials that have region variables here.\nWe've got AWS region, we've got AWS default region, just for good measure.\nAnd we have the region defined in the serverless stack.\nSo that's EU West one, and I haven't changed its WeShare backend.\nSo the deployments deployed stack name should indeed be WeShare backend dev.\n\n\nLuciano: This is really interesting.\n\n\nEoin: I'm going to give it one more go, right?\nSo what I'm going to do is I'm going to create a new terminal, a new session in my terminal,\nand I'm going to set up some new credentials.\nSo I'll stop screen sharing for the risk of inadvertently leaking any credentials.\nI can already see what the problem is here.\nWhen I'm looking at my credentials.\nDo you want to try and guess what it is?\nMaybe someone can guess what it is.\n\n\nLuciano: Did we deploy to the wrong account?\n\n\nEoin: I deployed to the wrong account just now.\nSo we're using AWS SSO, right?\nSo we use SSO to get credentials for our accounts and serverless framework doesn't support SSO\ncredentials yet properly.\nThere is an issue on it in their GitHub.\nAnd unfortunately the latest response I saw on that issue is that we're not going to fix\nit anytime soon.\nBut one of the work rounds you have to do then is use some kind of tool that takes your\nSSO credentials and converts them into like credentials on your file system or environment\nvariables.\n\n\nAnd I'm using the approach that uses environment variables.\nAnd I can actually show you what that is.\nSo I think this will work a lot better.\nI'm just going to share my screen again, because I think we're past the point of worrying about\nleaking credentials.\nSo this is the command I use.\nSo Ben Kehoe from iRobot wrote a couple of really useful utilities around SSO.\nOne of them is AWS export credentials.\n\n\nAnd then I can specify my SSO profile and it will convert this into a set of environment\nvariables that I can use.\nSo when I call it get caller identity, when I did that previously, it was using my SSO\ncredentials.\nBut I also had an AWS profile environment variable set.\nSo I actually had environment variables for two different profiles set up.\nI don't know how I managed that in this account.\nThey were both my accounts.\nBut I think what serverless framework did was it picked up AWS profile and used that\ninstead of the other variables.\nLet's go back to our deployment.\n\n\nLuciano: By the way, if you're curious about these topics, we have an entire episode dedicated\nto credentials.\nAnd I'm going to post the link here in the YouTube chat.\n\n\nEoin: Now we can see your stack is being updated, Isilano.\nSo we can actually see the events occur, so it's creating the HTTP API resources.\nIt's creating the log group and it'll be creating the function as well.\nSo at the end of this, we'll be able to take a file, upload it to S3.\nI'll be able to share it with you, Luciano, or anyone else.\nAnd they'll be able to download it.\nThat could be, you know, already this is an MVP.\n\n\nYou've got ugly URLs, but already this is useful.\nIf you want to share it from transferred from one laptop to another, from your mobile to\nlaptop, vice versa, you could use this service as it is and use it as your own personal file\nto share it with your friends.\nNow there's a couple of restrictions, right?\nWe've got some security issues here because anyone who gets that URL, and in fact, anyone\nwho gets the upload URL, in fact, the API Gator URL can start creating upload URLs and\nputting files onto our bucket, right?\nWhich isn't great.\nThey could certainly do a denial of wallet attack if they wanted to by just continually\nuploading large objects and also retrieving them because that's where you really pay because\nyou have your data retrieval costs out of AWS.\nSo the next thing we're going to do is start figuring out how to protect that and lock\nit down a little bit more.\nOkay.\nSo this has been deployed and now you've got an upload URL or sorry, we've got an API URL.\nSo we can start invoking our post URI, creating a share and seeing what response we get back.\nSo let's say clear for this.\nSo let's create a post to our API endpoint.\nAha.\nWhat went wrong?\n\n\nLuciano: Okay, let's give it a go.\n\n\nEoin: Okay.\nSo this is our log group.\nWe've seen you've got a log stream here and it says we can't find package AWS SDK client\nS3, which in fact, if you look at the function code, there is no node modules.\n\n\nLuciano: I'm not really sure why that's the case.\nDo we need to specify something in serverless framework v3 to include dependencies?\n\n\nEoin: It's a good question.\nBut what I think the, what we've got here is I guess if we look, we've got node node\nmodules here.\nI'm just thinking about NPM workspaces.\nThis has bitten me before.\nSo if we look at the parent node modules, I guess we would get AWS SDK.\nOh, client S3 is in there.\nSo I'm wondering if it's failing to pick that up, but we've did this before, right?\nWe have checked that this would work in advance and we didn't come across this problem.\nSo I'm just wondering now.\nCan you try doing NPM install inside the backend folder?\n\n\nLuciano: Yeah.\n\n\nEoin: I don't think it's going to change anything, but I'm happy to be wrong.\nSo we still don't have a node modules here.\n\n\nLuciano: Did you try NPM install already?\n\n\nEoin: I did.\nYeah.\nOkay.\n\n\nLuciano: I didn't see that command.\n\n\nEoin: Yeah.\n\n\nLuciano: That's interesting.\n\n\nEoin: So would it be okay if I just disable the workspaces because we've only got a single\nworkspace and see if this goes away and maybe then this is, this is like a bit of homework\nwe can do for the next time to figure out how.\n\n\nLuciano: Maybe there is a plugin for serverless framework to basically use the workspace definition.\nYeah.\n\n\nEoin: Yeah.\nThat would be good.\nSo, okay.\nLet's try that.\nSo I think what we have to do then is edit the root package.json and remove this basically.\nRemove the workspaces property.\nOkay.\nSo if I do that and I go back to the shell and I do NPM install now within backend, does\nit realize that it has its own world?\nIt does.\nOkay.\nSo now I've got node modules and let's just check that I've got AWS.\nMaybe my problem is here.\nOh, maybe we added them at the top level.\n\n\nLuciano: Interesting.\n\n\nEoin: This is a known problem, not a node problem.\nOkay.\nSo let's put workspaces back.\nI added it at the top level instead of the root level.\nSo let's go back to our NPM install, add them into backend.\nI'm guessing, yeah, that's gone now because we've got workspaces back.\nIf I go back to the root, yeah, I added them in here.\n\n\nLuciano: So we could probably remove them from there.\n\n\nEoin: Yep.\nWhich is NPM uninstall, right?\n\n\nLuciano: Or RM, I think as well.\n\n\nEoin: RM.\nOkay.\nOkay.\nThey're gone from there.\nThey're in here now as primary dependencies.\nSo the AWS SDK version three, right?\nWhat I'm seeing from this, the other learning I'm getting here is that the version three\nSDK is not packaged with the Lambda runtime.\nEven with the version, the node 16 latest runtime.\n\n\n\nLuciano: As far as I remember, there is a discrepancy depending on whether you use common JS or ESM modules.\nI think if you use common JS, you might have some version package there, but your mileage\nmight vary because it's not necessarily guaranteed to be the latest version or any specific version\nyou might get.\nWhatever is the current version, the Lambda runtime.\nOkay.\nSo it's always best to package your own dependencies.\nAt least you are guaranteed that you get whatever you are requesting, right?\n\n\nEoin: Yeah.\nYeah, I agree.\nYeah.\nYep.\nIt's just something you should be, I guess, more conscious of now than we used to be with\nversion two.\nOkay.\nSo what we can also do actually is we can use an npx sls package and inside the serverless\ndirectory you have the full backend uploaded.\nNow the file size looks a little small, so let's have a look at the contents of this.\nZip.\n\n\nLuciano: I'm actually looking at the files uploaded in the Lambda and we have an odd modules folder,\nbut it's empty.\nSo it's interesting.\n\n\nEoin: I think we're still going to have some problems there.\nOkay.\nCan I go back to removing workspaces?\nI think so, yeah.\nOkay.\nAll right.\nSo let's do another install npm install.\n\n\nLuciano: I like that it works even with a typo.\nWith a typo.\n\n\nEoin: Yeah.\nAny sub-stream, any prefix will do.\nOkay.\nSo in node modules now it looks like we have something a bit more expected.\nLet's run one more.\nLet's just do the package command because that's really quick.\nOkay.\nAnd we'll do our unzip again.\nAh yeah.\nNow we've got a lot more going on.\nAll right.\nI'm getting more confident that this is going to work.\n\n\n\nLuciano: I also like that we did everything we could to avoid to bring that UUID dependency and somehow it's there anyway.\nI'm going to guess it's a dependency of the AWS SDK.\n\n\nEoin: Yeah.\n\n\n\nLuciano: So we have Will in the chat saying that AWS SDK version 2 is included in this common JS runtime.\nSo you don't get the version 3.\nThank you, Will, for the clarification.\nOkay.\nThat's right.\n\n\nEoin: So does that mean you get the AWS SDK version 3 if you use common JS or not?\nYou only get the AWS SDK version 2?\nYou get the AWS SDK version 2 as far as I understand.\nOkay.\nYeah.\nOkay.\nAll right.\nLet's give another go to our command.\nSo we're going to curl minus X post to this and hopefully we'll get back a couple of URLs.\nLet's see.\nNot.\nNo such joy.\nOkay.\nAh, okay.\nI've given myself a few more.\nOkay.\nAh, okay.\nI've given a Python syntax in our handler.\nSnake case is no good here.\nSo I think it was handle event, but let's just verify that here.\nNope.\nIt's called handler.\nSo I think I prefer to give it a verb.\n\n\nLuciano: Yeah.\nThan a noun.\n\n\nEoin: So let's stick with handler event.\n\n\nLuciano: It's always weird when you see handler dot handler.\nIt's not very clear what that means.\n\n\nEoin: Yeah.\nOops.\nYeah.\nSo in a couple of seconds we should have all that resolved.\nAnd like with a proper project structure here, we might have a sub we would have a sub directory\nfor our handlers and then separate directories for the logic that occurs within these handlers\nand probably not avoid having all of our modules at the root with all of the code and that\nthe lambda handler itself.\n\n\nLuciano: Will is also clarifying that the SDK version three is not included in any runtime, not\njust runtime, but hopefully will be not JS 18.\n\n\nEoin: Oh, okay.\nOkay.\nCause I was, I was wondering if the reason not to include it was just to keep the runtime\nlight and to like reduce cold start time by, because I suppose what the advantage of the\nversion three SDK is that they're modular and you don't have to bundle them all.\nSo that's interesting.\nI guess having it in the runtime is good for people who are using, you know, cloud nine\nor the inbuilt lambda console editor to try something fairly basic because you don't have\nto worry about packaging modules there.\nOkay.\nWe are.\nNice.\nOkay.\nWe've got our first URL.\nSo now we've got, um, an upload URL and our download URL.\nSo let me, I did promise that the URLs weren't going to be pretty.\nI think I've lived up to that promise.\nSo let's, let's put something up on S3.\nLet's upload something to WeShare.\nSo what'll I use?\nLet's say, let's use our diagram from the repository.\nWhat is it called?\nOh, it's in the, um, in the directory.\nYeah, and it's called MVP-DIAG.png.\n\n\nLuciano: Okay.\nThat's, looks to have succeeded.\n\n\nEoin: So let me just go back to the download URL.\n\n\nLuciano: Ah, I don't know if this is going to work for me, my scrolling and pasting in my terminal.\n\n\nEoin: This might be a slight gotcha for this demo.\nLet's see how we get on.\n\n\nLuciano: Well, I can confirm that I see something in a stream.\nOh yeah.\n\n\nEoin: Okay.\nThis is good.\nYeah.\nI have a problem here with this URL.\nSo let me do it a slightly different way.\nI can show very quickly that here we have this file in S3.\nNice.\nOkay.\nLet me do this again.\nMVP-DIAG.png.\nWhat did I miss?\nMVN.\nMVP.\nMVN.\nIt's like a hangover from my days of using Maven.\nExcellent.\nLet's give our, um, upload, download URL a go.\nSo I'm going to do curl minus V just so we can see like things like the response headers\nand stuff.\n\n\nLuciano: Can you zoom a little bit your font?\n\n\nEoin: Yeah.\nActually, let's just, um, let's just paste this into the browser.\nSo this is the download URL.\nI don't need the curl command.\nOkay.\nSo now I've got a file with a not too obvious name in my downloads folder.\nSo let me do it.\nLet me open up my downloads folder so everyone can see.\nAnd I guess I'm going to have to rename this to.png.\nAnd let's open it up.\nAnd we have our architecture diagram.\nAwesome.\nThat's a success for MVP.\n\n\nLuciano: I think so.\nExcellent.\n\n\nEoin: Let's have a quick check of what we added since last commit.\nAnd I'm going to commit the add handler to generate, upload and download URLs and push\nthat and everybody can have a go.\nAnd deploy it to their own environments.\nAnd what I'm going to do before we wrap up is I'm just going to remove this stack because\nI don't want us to be DDoSed on our bucket.\nYeah, because everyone saw the URL for uploading, right?\nYeah.\nMaybe, maybe we've got a few surprise objects, but we better not share them publicly.\nYeah, absolutely.\n\n\nLuciano: I think just to wrap things up, of course, we did a very simple implementation, like\nthe bare minimum that you could possibly do to get this working.\nThere are a few things that would be nice to do and maybe something we can consider\nfor the next episode.\nFor instance, one thing that we saw is that when you were downloading the file, you totally\nlost the file name, the extension.\nSo you needed to figure out, oh, this is a PNG.\n\n\nLet me rename it and open it.\nThere are actually ways that we could retain the MIME type and the file name.\nSo we could implement all of that in the next session.\nAnd then other things we will be doing are making this a little bit more production ready.\nFor instance, by adding proper logging metrics and stuff like that, we could be using something\nlike MIDI to implement all these features.\nWe will be using power tools as well.\nSo maybe in the next session, we try to do all of this.\nBut by any means, if you have other ideas or specific questions, feel free to ask and\nwe'll try to tackle those questions in the next episode.\nWhich probably is gonna be next week, more or less, same day, same time.\nBut just stay tuned on YouTube and our social media channels, because as soon as we have\nall of that scheduled, we are gonna announce it.\nYeah.\n\n\nEoin: Look forward to it.\nIt was good to get all that setup done, because there's quite a few small pieces there.\nWe were at the start before we got to actually writing the code.\nI think it's gonna be interesting next week.\nWe should be able to plow ahead with making this a little bit more user friendly.\nI don't like the look of those URLs.\n\n\nLuciano: Just as a reminder, in the chat, I'm gonna be posting our repository.\nAnd I think this is all we have for today.\nSo hopefully you enjoyed all of that and we'll see you next time.\nBye everyone.\n\n\nEoin: Thanks everyone.\nWe'll see you next time.\n"
    },
    {
      "title": "49. Building a File Transfer application on AWS - Live coding PART 2",
      "url": "https://awsbites.com/49-building-a-file-transfer-application-on-aws-live-coding-part-2/",
      "publish_date": "2022-08-24T00:00:00.000Z",
      "abstract": "How can you build a WeTransfer or a Dropbox Transfer clone on AWS?\nThis is our second live coding stream. In this episode, we revisited our architecture and added a custom domain to our APIs, and created a new API endpoint that allows us to have download URLs that are much nicer (shorter and branded).\nWe also added support for the Content-Disposition header to make sure that uploads can specify a file name and that downloads will retain the same file name (regardless of the file key in S3).\nAll our code is available in this repository: github.com/awsbites/weshare.click.\n",
      "transcript": ""
    },
    {
      "title": "50. Building a File Transfer application on AWS - Live coding PART 3",
      "url": "https://awsbites.com/50-building-a-file-transfer-application-on-aws-live-coding-part-3/",
      "publish_date": "2022-08-31T00:00:00.000Z",
      "abstract": "How can you build a WeTransfer or a Dropbox Transfer clone on AWS?\nThis is our third live coding stream. In this episode, we made our lambdas better by adding observability best practices (structured logs, metrics and tracing) through Lambda Power Tools for TypeScript and Middy. We also created a simple Node.js CLI to easily upload files from the command line.\nAll our code is available in this repository: github.com/awsbites/weshare.click.\nSome of the resources we mentioned:\n\nLambda power tools for TypeScript\nMiddy: Node.js middleware framework for AWS Lambda\nGetting to Well Architected Faster with AWS Lambda Powertools (article)\n\n",
      "transcript": ""
    },
    {
      "title": "51. Authentication for a CLI app with Cognito - Live coding PART 4",
      "url": "https://awsbites.com/51-authentication-for-a-cli-app-with-cognito-live-coding-part-4/",
      "publish_date": "2022-09-16T00:00:00.000Z",
      "abstract": "How can you build a WeTransfer or a Dropbox Transfer clone on AWS?\nThis is our fourth live coding stream. In this episode, we started looking into adding some security to our application. Specifically, we started implementing a device auth flow on top of AWS Cognito to allow our file upload CLI application to get some credentials.\nAll our code is available in this repository: github.com/awsbites/weshare.click.\nSome of the resources we mentioned:\n\nContent-Disposition Header on MDN\nOAuth 2 Device Auth flow RFC8628\nXKCD Comic about password security\ncrypto-random-string package\nDash offline documentation app\n\n",
      "transcript": ""
    },
    {
      "title": "52. Authentication for a CLI app with Cognito - Live coding PART 5",
      "url": "https://awsbites.com/52-authentication-for-a-cli-app-with-cognito-live-coding-part-5/",
      "publish_date": "2022-09-21T00:00:00.000Z",
      "abstract": "How can you build a WeTransfer or a Dropbox Transfer clone on AWS?\nThis is our fifth live coding stream. In this episode, we continued adding some security to our application. Specifically, we implemented 75% of the OAuth 2 device flow on top of AWS Cognito to allow our file upload CLI application to get some credentials.\nIn order to implement this flow, we need to store some secrets. We decided to use DynamoDB and spent a lot of time discussing our data design and how and why we used the famous and controversial DynamoDB single table design principle.\nAll our code is available in this repository: github.com/awsbites/weshare.click.\nSome of the resources we mentioned:\n\nOAuth 2 Device Auth flow RFC8628\nThe DynamoDB book by Alex DeBrie\nLevelDB\nOAuth 2 Authorization framework RFC6749\n\n",
      "transcript": ""
    },
    {
      "title": "53. Authentication for a CLI app with Cognito - Live coding PART 6",
      "url": "https://awsbites.com/53-authentication-for-a-cli-app-with-cognito-live-coding-part-6/",
      "publish_date": "2022-09-28T00:00:00.000Z",
      "abstract": "This is our sixth (and last!) live coding stream.\nIn this episode, we completed the authentication layer for our file transfer application. Specifically, we completed the OAuth 2 device flow on top of AWS Cognito and updated the weshare CLI application to support this new authentication flow. We also added an authorization layer in front of our file upload API.\nAll our code is available in this repository: github.com/awsbites/weshare.click.\nSome of the resources we mentioned:\n\nGitHub PR with the final OAuth 2.0 device flow step\njwtinfo CLI tool\nenquirer package\nundici HTTP client package\nopen package to open the browser at a specific URL\nora package for animated spinners\nconf package for persisting user settings\n\n",
      "transcript": ""
    },
    {
      "title": "54. Are Step Functions a Low-Code tool?",
      "url": "https://awsbites.com/54-are-step-functions-a-low-code-tool/",
      "publish_date": "2022-10-14T00:00:00.000Z",
      "abstract": "AWS Step Functions are all the rage right now! The visual editor is getting better and better and there are always new capabilities like the recently introduced intrinsic functions. In this episode we will try to answer the question “are Step Functions a Low-Code tool”? In the process, we will give our own definition of what Low-Code means, and we will describe the main characteristics of Step Functions and try to assess whether they match our definition or not. We will also discuss several practical use cases that can be addressed with Low-Code and Step Functions.\nSome of the resources we mentioned:\n\nOur previous episode dedicated to Step Functions and what can you do with them\nFullStack Bulletin Newsletter\nImplementing the Saga pattern with Step Functions by Yan Cui\n\n",
      "transcript": "Luciano: AWS Step Functions are all the rage right now.\nThe visual editor is getting better and better,\nand there are always new capabilities,\nlike the recently introduced intrinsic functions.\nStep Functions are becoming more and more powerful,\nand the amount of code we need to write to define them\nis decreasing more and more.\nSo does this make AWS Step Functions a low-code solution?\nLet's find out.\nMy name is Luciano, and today I'm joined by Eoin,\nand this is a new episode of AWS Bites podcast.\nSo let's maybe start by trying to define what is a Step Function.\n\n\n\nEoin: AWS used to define Step Functions as like a serverless workflow orchestration tool.\nNow their more generic description\nis a visual workflow for distributed applications.\nSo Step Functions are basically a service\nthat allows you to build state machines.\nSo you define states and transitions between those states,\nand you execute them in a managed serverless environment.\nSo there's two types.\nYou've got the standard workflows\nthat can run for up to one year,\nand then you've got the express ones,\nwhich run up to five minutes.\nAnd now you have this visual editor called Workflow Studio,\nand that allows you to do all that stuff\nusing a drag-and-drop visual interface.\nWe did have an intro to Step Functions back in episode 7.\nThe link will be in the show notes,\nand we've got various examples there\nand cover the limitations, pricing model,\nthe differences between standard and express workflows, et cetera.\nNow you mentioned low-code.\nSo what is a low-code tool?\n\n\n\nLuciano: Yeah, so I'm going to try to describe it the best way I can, but I think it's still a terminology\nthat gets used in different ways.\nSo I don't know if my description actually matches\nother descriptions you can find online.\nBut to me, a low-code solution is basically a tool\nthat should allow people to build\nrelatively complicated business workflows\nwith as little code as possible.\nAnd that generally means that you need to have\na visual designer of some sort,\nso something that allows you to drag-and-drop blocks and connectors.\n\n\nYou probably want to have predefined integrations\nwith, I don't know, external services\nand other kind of processing, building blocks, for instance.\nI don't know, blocks that allow you to do control flow,\nlike repeat certain operations or have branches,\nlike if something, do this, otherwise do that,\nor maybe run things in parallel.\nSo basically, you want to build business logic\nwith different ramifications by using a very visual tool\nthat doesn't require you to write any code\nor at least as less code as possible.\nAnd when you actually need to do something very custom\nthat is not built-in in the tool that you are using,\ngenerally, there needs to be some kind of extension,\nopen block, where you can just write code\nand implement that part of business logic yourself.\nSo maybe it's worth discussing some use cases.\nI don't know if you have anything in mind, Eoin.\n\n\n\nEoin: Yeah, one example, let's say you have HR-type applications, so you're trying to use workflow automation\nto manage your hiring process.\nSo the process there would be you collect the results\nof different candidate applications\nand maybe an initial screening process.\nYou store those applicants somewhere\nand then you might have screeners.\nIf they all liked the candidate,\nyou would notify the hiring manager,\nsend the candidate a letter.\n\n\nIf you don't want to proceed,\nyou might send the candidate a thank you message\nand a reason, and you can make this more complex.\nSo you could also say, integrate with the calendars\nof the hiring manager to suggest times to have a conversation.\nYou could integrate with LinkedIn and other platforms\nto fetch information about the candidate\nand prepare a report for interviewers.\nYou could even integrate with some AI service\nto summarize the candidate CV.\n\n\nMaybe you might proceed with caution on that part.\nBut in general, you get the idea, right?\nIt's for this type of workflow involving manual steps,\nsome of which might have a significant delay.\nAnd we mentioned step functions can run for up to a year.\nAnd then obviously at the end, like if the candidate is hired,\nthat could be a manual intervention, of course.\nYou would send the candidate a welcome back.\nSo that's one example.\nIt would have multiple phases, branches, conditions,\nall of that good stuff.\nWhat else can you think of?\nIs there anything that you'd maybe point to from work you've done\nor applications where you have applied step functions?\n\n\n\nLuciano: Yeah, so there is actually a small side project that I've been running for a few years at this point\ncalled Full Stack Bulletin, which is basically a newsletter\nthat I send every week, and it covers topics\nrelated to the full stack world.\nAnd right now it's implemented like as a mega Lambda.\nLike there is one Lambda that does everything.\nBut this is because I wrote this, I don't know, like four years ago.\n\n\nBut right now, looking at this particular example,\nI'm thinking that redoing it as a step function\nwill basically allow me to reduce the amount of code\nand probably make the entire process much nicer to visualize\nand understand when something goes wrong.\nAnd basically just to describe the process is basically\nthis newsletter is a weekly one, and it comes out every Monday.\nBut I want to have a preview that is built every Friday,\nso there is a little bit of time to manually adjust\nwhatever might not be perfectly correct\nor maybe to add a little bit of personal touch.\n\n\nThe idea is that it is as much automated as possible,\nso every week, every Friday, the workflow starts.\nAnd what it does, it looks into Twitter\nfor the links that have been tweeted for the last week\nin the Full Stack Bulletin account.\nAnd it basically uses an external API to try to rank all of them,\nall the external links, so all articles that are related\nto the world of full stack web development,\nand it selects the top seven.\n\n\nThen in parallel, this is like the next phase of the workflow,\nit just goes off and crawls all the seven links,\nand it tries to extract information about every link,\nso the title, a summary, a preview picture.\nAnd also in parallel, other elements that will be part of the email\nwill be a quote, like a tech quote, like an inspirational quote,\nand it uses an API for that as well,\nand the book of the week, which is also a book related to\nfull stack web development.\n\n\nSo these are also other two things that will happen in parallel.\nSo there is all this data fetching that can be parallelized.\nAt some point, all the data is available,\nso you have to join all these different parallel steps,\nand you can render a preview of the email,\nwhich is basically HTML and plain text,\nand all the result of that will be uploaded to MailChimp.\nSo again, another integration with an external service,\nwhich is the system that we use to actually send a newsletter.\n\n\nAnd at that point, it's just a draft campaign,\nso I can go into MailChimp and log in and manually edit the newsletter,\nand at that point, I just save it, and it will go out in the next Monday.\nNow I could have had another step in the step function,\nlike a manual click to say, okay, now the newsletter is ready\nand do other things with it, but right now, it's very simple.\nI don't need to do something extra.\n\n\nThe workflow will actually finish in MailChimp.\nBut I guess you get the idea that it integrates a bunch of different APIs,\ngets data from a bunch of different places,\neventually creates data somewhere else.\nSo I had to write all the code myself in a Lambda to do all these things\nwhile step functions can probably reduce all that custom code\ninto very minimal amount of code, maybe just the integration with MailChimp\nand some other services, what I would need to rewrite\nif I end up using step functions.\nOkay, it makes sense to look at a step function for this for sure.\n\n\nEoin: I know that if you look at the AWS Well-Architected Framework,\nserverless lens, it was specifically called at this kind of thing.\nAnd if you're doing lots of heavy orchestration using Lambda,\nthey'll say this should be a step function.\nSo it makes perfect sense to do that.\nI've in the past also done a similar kind of step function\nwhere I was crawling conference websites\nand actually scraping the page for the conference\nand then sending that off to Amazon Comprehend,\nwhich is the machine learning service for doing named entity recognition.\n\n\nSo it would extract things like locations and dates and people\nout of the conference site and then store that in a DynamoDB table.\nSo it seems a little bit similar.\nBut there is other kind of machine learning jobs\nwhere step functions work really well.\nAnd you'll see lots of examples of using SageMaker\nor other services like Rekognition, Translate, Polly,\nall with step functions.\nSo one example could be you want to do some video processing.\n\n\nSo the first step might be to take the video, split it into frames.\nYou might do that in Lambda or Fargate task.\nThen maybe take each frame and perform some image detection algorithm\non each frame with SageMaker or with Rekognition.\nThen you process the results, collate them, send them to S3,\nand then finally maybe send a message on EventBridge.\nSo another workflow that suits step functions well.\nAnd one of the things you have to do,\nall this kind of stuff can get very difficult if you're doing it in code\nwhen you have to consider the failure modes.\n\n\nThe happy path is usually fine because it's just sequential lines of code.\nBut when you have failure modes, then you have to think about,\nokay, do I need to rewind?\nDo I need to undo?\nAm I leaving something in a partially complete state?\nAnd one thing to call out there is that there's a common pattern\nin distributed orchestration called the saga pattern.\nAnd the saga pattern is when you have multiple steps\nthat essentially form some sort of transaction.\n\n\nSo it could be, you know, you place an order for something\nand you want to reserve that item.\nYou want to send a notification to warehouse maybe.\nBut you also have to process payment.\nAnd maybe if the payment processing fails,\nyou want to unreserve that order.\nSo you kind of have to roll back.\nAnd there's a way to do that with step functions with the saga pattern.\nAnd there's an actually quite old article now by Yan Cui\non theburningmonk.com, which we can link to.\nIt's from 2017, but it talks about using the saga pattern\nwith lambda and step functions.\nSo you mentioned at the start, right, these characteristics of a low code tool.\nAnd we hear a lot about low code.\nAnd I think we're all keen to reduce the number of lambdas we have\nfor just like data processing and kind of mundane stuff.\nDoes step functions tick all of the criteria\nwhen it comes to classifying it as a low code tool?\n\n\n\nLuciano: Yeah, based on our definition, let's try to break it down what step function gives us today and let's try to decide,\nis it a low code tool or not?\nSo the first criteria we put there in the description of a low code tool\nis does it have a visual designer?\nAnd yes, step function have a visual designer,\nwhich is the function workflow studio that we mentioned.\nAnd actually, I want to spend a little bit of time trying to describe how it works\nbecause it's really interesting.\n\n\nAnd I think it's important to understand the features there\nto actually be able to assess, do we consider a low code or not?\nSo the first thing is that initially,\nstep function didn't really have a kind of workflow designer.\nSo the actual implementation underlying is well known\nand you can even just skip the workflow studio\nand use that like more barebone definition.\nAnd it's basically like a JSON description of the state machine,\nwhich is called ASL, Amazon States Language,\nthat effectively what it does, it's very verbose,\nbut it defines every single step, how they are connected\nand the configuration of every single step.\n\n\nSo it's basically you can imagine everything that you could do\nwith the workflow studio behind the scene\nis actually compiling this JSON for you.\nSo the JSON in itself,\nI mean, it's something that people used to use in the past,\nbut it's very verbose and very hard to write and understand.\nSo I think it works well for very small state machines.\nBut if you're really building something a little bit more complex,\nit will be very painful to write.\n\n\nIt's very easy to do mistakes or forget to connect certain steps.\nSo yeah, definitely the workflow tool there\nis going to help a lot for more complex use cases.\nNow, when you start to use the workflow studio,\nyou basically have, by default, you create a new state machine\nand by default, you see that you have only two steps,\nstart and end.\nSo the next thing that you want to do, you open the side panel\nwhere you have all the different building blocks\nand you can start to drag and drop them to build your actual workflow.\n\n\nAnd there is also a nice undo, redo feature\nthat allows you to fix mistakes.\nAnd also you can do that with both the standard and express workflow\nso that that's not really a blocker.\nYou can pick whatever you prefer\nand it's not going to affect your experience.\nNow, the next criteria is, does it have predefined building blocks?\nWe said yes, but what are these building blocks?\nAnd in the workflow studio, you get two different sections.\n\n\nOne is called flow and one is called actions.\nAnd flow is kind of control flow steps.\nSo for instance, you have a choice that allows you to do branching statements,\nlike an if statement or a switch statement\nbecause you also have a default.\nSo it's like if a certain condition happens, go left, otherwise go right.\nOr you can also have a default step as well\nif it doesn't match probably the other two.\n\n\nThen you can also do things like parallel\nor mapping data from an array into other results.\nAnd you can do all of that in parallel as well.\nYou can have like wait steps.\nSo for instance, if you want to add a delay\nbecause you know you need to wait, I don't know, 10 seconds\nbefore you can do the next action,\nyou can codify that with a step.\nAnd you can also have success or fail, for instance,\nwhen you want to, if a certain condition happens,\nmaybe you want to fail early or maybe you want to succeed early,\nlike a shortcut kind of steps.\n\n\nThen speaking about actions, this is where it gets interesting\nand probably a bit complicated\nbecause actions are literally everything you can do with the SDK.\nYou will probably find actions there.\nSo you can integrate with pretty much any other service\nlike DynamoDB, SNS, EventBridge.\nAnd basically you drag and drop every block,\nbut every block represents a specific action in a service.\nFor instance, DynamoDB, you would have a put item action.\nAnd when you drag and drop that action,\nyou basically see that there is a context panel\nthat allows you to define all the,\nit's like you are configuring an API key,\nan API call at that point.\nSo it kind of allows you to say for DynamoDB, for instance,\nwhat is going to be the ID of this record?\nDo you want to add additional fields?\nAnd basically you can combine the incoming state\nin the step function, but also additional state\nthat is part of that particular step.\n\n\nEoin: \n\n\nLuciano: And one last thing that I think is very interesting, and this is one of the new additions\nthat we mentioned in the intro, is intrinsic functions.\nSo these are basically functions that you can use\nwhen you define the mapping between the state\nand the action of that particular step.\nYou can add this kind of lightweight processing function\nthat can allow you to do things\nthat generally you will do with your own lambdas.\n\n\nNow they are kind of built in.\nFor instance, if you have to do certain types\nof array manipulation, if you have to do data encoding,\nlike for instance, base64,\nor if you need to calculate ashes, MD5, or SHA,\nor if you need to do random stuff\nor mathematical operations, if you need to generate UUID,\nor if you need to do, I don't know, string interpolation.\nSo all things that are very common\nwhen you create this kind of integrations,\nbut before you needed to do them\nwith your own custom code in Lambda,\nnow you can just use these functions in the JSON\nand AWS will take care of doing all of the stuff for you.\nAnd of course, if you need to extend\nand do something entirely custom\nthat is not built in in the actions or the flow blocks,\nyou can do it by just creating a Lambda,\nand a Lambda, you write the custom code inside the Lambda,\nand the Lambda will become one step in your workflow.\nNow I'm gonna let you draw the conclusions.\nDo you think this is a low-code solution at this point or not?\n\n\n\nEoin: So when we talk about low-code and no-code, you might think of things like Zapier\nand if this, then that.\nSo those are really kind of no-code tools.\nThey're designed to be free\nof any imperative programming language.\nI think anyone who's used to developing step functions\nover the last few years is very well used\nto writing lots of Lambda functions\nto do basic data manipulation and arithmetic.\nSo those are intrinsic functions,\nthe new ones that you mentioned,\nreally help to make this lower code than before.\n\n\nSo now you can merge JSON objects,\nyou can partition arrays, you can do arithmetic,\nyou can convert JSON to string and vice versa.\nSo it's definitely lower code,\nand I think it's definitely getting there.\nNow, does that mean it's a tool that's not for developers?\nI don't think so.\nIt's still a tool for developers,\nbut we're all trying to,\nwell, maybe we're not all trying to,\nbut a lot of people are trying to reduce\nthe amount of boilerplate code.\n\n\nThey have code that isn't necessarily business logic,\nbut it's just transforming data from one format to another\nor doing simple arithmetic.\nSo you can definitely do a lot more of that\nin step functions now.\nNow, you're still doing code.\nIt's just the code happens to be this Amazon states language,\nwhich is difficult enough to grasp syntax based on JSON.\nThe workflow designer helps to make it a lot easier,\nbut I would say it's very good\nfor designing your step function initially\nonce you've deployed it\nand you've got infrastructure as code for it,\nyou can use it to make edits,\nbut only really to kind of explore what your edits would be.\n\n\nSo I would say deploy,\ndesign the step function using the studio,\nthen take that code that it generates,\nput all the variables into it,\nput it into your code base,\nand then if you need to make changes down the line,\nyou can just see how the structure of your ASL has changed,\nthat JSON, what are the changes in it,\nand then you kind of manually copy and paste that\ninto your code base.\nSo it still requires a bit of developer know-how.\n\n\nSo of course, you still need to understand\nthe AWS service as well and how that works.\nOf course, you need to understand your IAM policies.\nWhen it comes to ASL, one of the more difficult things\nis understanding the flow of data through the state machine,\nand so you have to understand the JSON path,\nelements, the input path, the result path.\nIf you need custom logic,\nyou're still going to use something like Lambda,\nFargate, ECS, AWS batch.\nSo it's not a no-code tool, definitely.\nIt doesn't support external integrations\nwith third-party services.\nSo it's a tool for developers,\nbut it's a tool to enable developers\nto implement complex workflows with much less code\nthan they would have if they were doing it\nin an imperative language.\n\n\nLuciano: Yeah, I think I agree 100% with your assessment,\nand I want to make a comparison with SageMaker Studio,\nwhere actually I think they lower the barrier\nto entry a lot more.\nOf course, I'm not saying that SageMaker Studio\nis a low-code tool,\nbut they kind of remove all the boundaries in terms\nyou really need to know AWS to use this service,\nwhile in AWS Step Functions, you still need to know AWS.\nYou still need to log in into the console.\n\n\nYou still need to understand permissions\nand all that kind of stuff.\nSo I think that's another barrier\nthat makes it harder to use on people\nthat are more on the business side\nrather than the engineering side,\nwhich I think is one of the goals of low-code solution\nto be more friendly to people\nthat don't really have a lot of engineering skills.\nBut they still want to build this kind of workflows.\nSo I definitely agree with your assessment.\n\n\nSo I don't know if you also agree with our assessment.\nDefinitely let us know what you think in the comments.\nAlso, we are curious to know\nif you have used Step Functions recently\nand if you use the new Studio\nand what do you think about it.\nAnd yeah, and also if you have built\nvery interesting use cases,\nvery specific workflows that you think\nare kind of innovative,\nwe are really curious to know\nbecause we are always looking for interesting uses\nand we are always looking for interesting use cases.\nSo with that being said,\nthank you very much for being with us\nand we'll see you in the next episode.\n"
    },
    {
      "title": "55. How do you build an MVP on AWS?",
      "url": "https://awsbites.com/55-how-do-you-build-an-mvp-on-aws/",
      "publish_date": "2022-10-21T00:00:00.000Z",
      "abstract": "Sometimes people talk about an MVP and then say &quot;yeah but we deploy manually&quot; or &quot;we don't have tests yet&quot;. Is that really an MVP or is it something else? And what it takes to build a successful MVP when using AWS? In this episode, we discuss the differences between a prototype, a proof of concept, and an MVP. Then we debate about what's the minimum amount of work you need to put in place to have an MVP on AWS. We debate on whether our weshare.click is an MVP or just a prototype and why. Finally, we provide a list of previous episodes that can help to acquire the foundational AWS knowledge that is needed to be able to build an MVP successfully.\nSome of the resources we mentioned:\n\nThe book  &quot;The lean startup&quot; by Eric Ries\nThe weshare.click repository\nThe YouTube playlist of all our weshare.click live streams\nThe YouTube playlist of AWS foundational knowledge videos\n\n",
      "transcript": "Eoin: If you're building a new workload on AWS or migrating an old one,\nit's always a good idea to start with a Minimal Viable Product or an MVP.\nBut what is an MVP and how does it differ from a proof of concept or prototype?\nWhat is the minimum you should do?\nAnd how do you ensure that everyone has clear expectations around what the outcome should be?\nMy name is Eoin. I'm here with Luciano and this is AWS Bites.\n\n\nLuciano: Okay. I guess we can start this podcast by asking, why are we asking this question in the first\nplace? Why do we feel that there is a disconnect with how do we perceive what is an MVP and what\npeople actually say when they talk about MVPs? Yeah.\n\n\nEoin: I see this quite a lot and maybe it's a bit of a bugbear of mine, but we see teams embarking on a new project and the term MVP\ngets used a lot. So often this term means a lot of different things to different people.\nAnd the lines are kind of blurred between what's an MVP and what's a proof of concept and what's\na prototype. I think it's a good idea to break these terms down and just figure out what the\ndifferent differences are between them before we talk about how to build an MVP and make sure you\nset it up for success. So a prototype is not a product. It's like a rough idea used to demonstrate\nsome capability and it's something really useful, but it's ultimately a communication tool that you\ncan use to show others what a product might be like, or just to get buy-in or even to get funding.\nSo it's like a decision-making tool. Then on the other hand, you have a proof of concept or a POC,\nand that's typically designed to validate some assumptions, to test a hypothesis,\nor to just evaluate whether a technology is going to work for what you want to do.\nSo for a POC, you should have a clear question you're trying to answer,\nor like a risk you're trying to remove, and you should know at the end of it whether you\ncan proceed or not. So neither of those two things, prototype or POC, neither of them are\nreally a product and they don't even have to become the nucleus of a product or grow into\na product. They're just part of the assessment and discovery phase of a project that you might\ndo at the start. So you could even throw away the code when you're finished, right? That's kind of\nwhat you should expect with those things. So they might be minimal, but they're not a product and\nthey're not viable either.\n\n\nLuciano: Yeah, I like when you say you should also be able to throw away the code, because I feel that a lot of companies and people in general gets very attached to their\ncode. It's like the value is the code itself. While that might be true in most cases, the reality\nis that the value is in the discovery that you made, that the value is in the proof that you\nhave created, that your idea might be successful or the technology might work for use case.\nSo you can actually rewrite the code and probably the second time you're going to rewrite it,\nit's going to be even better. So I definitely like what you said there with throw away the\ncode if it needs to be, that's not the value, that's not the product, the value somewhere else.\nBut I guess the next question is given that we define what a prototype is, what a proof of\nconcept is, what is an MVP then if it's not these two things?\n\n\nEoin: Okay, so literally the first thing, it's minimal. So that's like going back to like Eric Ries and the Lean Startup and then the Lean\nEnterprise, all these books are really good at telling you how to focus on what is really minimal\nand you want to deliver the smallest amount of functionality you can get away with.\nJust kickstart that product feedback life cycle. So that's the minimal part. But then probably the\ndifficult one is the viable bit. So when we're talking about viable, we're saying can your team\nsustainably evolve and support the ongoing development and operations starting with this MVP.\n\n\nSo a prototype just kind of held together with sticky tape is probably not viable. So it's\nimportant to think about what viable is. And then when we say product, that means that you're aiming\nthis at real users with real needs and they're trying to get something important done with it.\nIt should be ready to sell. And it should support meaningful scenarios that users are using. So\nsometimes you hear things, people say, oh, we're building an MVP and we've got our MVP ready to go.\n\n\nBut you might see or hear them say sometimes we're still deploying manually or we're already\nplanning a complete rewrite of this. This is just the early version and it's running on a single\nEC2 instance in a public subnet. So maybe what you have in those cases is more of a prototype\nthan an MVP, but it's not just about the technology. And it doesn't mean that when you're\nbuilding an MVP that you have to boil the ocean and all of a sudden you have to achieve perfection.\nIt's just about like you can still have an MVP that doesn't have any code at all.\nIt's a completely manual approach based on Excel sheets and human interaction and a documented\nprocess that can tick all the boxes for an MVP as long as it's sustainable. So it's not about\nwhether you have tech or not. It's important thing is that you've got really good performance\nthat you've got rigor that can actually support the users no matter what happens.\nAnd it's something you can build on and evolve over time.\n\n\nLuciano: Yeah, I really like this definition and I suppose it makes sense to... Yeah, I agree that basically\nthe viable part is the one that most often is the one that gets the least amount of attention. And\nit's something we should be focusing a little bit more at least for when it comes to the technical\nside of things. We always disregard it and say, oh, we're going to do that later. It doesn't\nmatter right now. We can do this manually or we don't need testing right now. And then eventually\nif the business goes well, it's very hard to keep up and it's not sustainable anymore because you\nmiss all these foundational blocks. So I suppose what's your perspective on this? What do you think\nyou should be putting in place as the bare minimum? I suppose mostly from a technical\nperspective to consider that the product is viable and therefore it's sustainable.\n\n\nEoin: Okay. Well, if we take the M and the P first, let's say you... Let's assume that you've got\nthe right minimal set of features from the beginning and then you've got at the product side\na system to gather feedback and iterate on the product. Then in the middle, I'd say if you're\nbuilding a technical solution on AWS, you should try and put in the minimal fundamentals to make\nit viable. And those would be things that you and I have talked about on the show a lot over the\npast 50 or so episodes. So we're talking about infrastructure as code, unit tests,\nend-to-end tests, automated deployment, some level of observability with like centralized logs,\nmetrics, alerting, traces, some security fundamentals, of course, because otherwise\nyou've got a business risk that might really compromise your viability. Having the right\nskills within your team is also something that makes your thing viable. So maybe, especially\nin a startup scenario or even in a migration scenario while you're adopting the cloud,\nthere might be a huge risk to supporting these workloads if you just kind of completely wing it\nand don't have the skills in place or at least a plan to address skills gaps. So even think about\nstuff like that as part of your MVP early on. And then of course, like documentation for onboarding\nand troubleshooting from the start, right? So you can bring people in, new people as the team grows,\net cetera, and you don't have to kind of, I suppose, train everybody up manually or rely\non them to kind of somehow assimilate all of the ideas that are in the code base. It's a good idea\nto have some documented principles and training in place for people too.\n\n\nLuciano: Yeah, I like what you said there. And I think that was one of our previous episodes where we mentioned that we were\nbuilding a particular solution and we opted for just plain EC2 instances where we could have\ngone with serverless and Docker, but we made that conscious choice because we knew that the team\nthat was working on this solution and eventually would need to support this solution didn't have a\nlot of expertise with either serverless or Docker. So we will still want to achieve all the benefits\nof the cloud while allowing the team to be able to support the solution long-term. So there was\nkind of an interesting trade off where from an architectural perspective, maybe we would have\nwanted to use something else, something more innovative, more kind of less management,\nbut at the other side, we didn't want to overload the team with new knowledge that they didn't have\nthe time to create on the team and make it sustainable that way. But at the same time,\nit's interesting because you might wonder, isn't this too much work for an MVP, right?\n\n\nMy opinion is that yes and no, it might look like a lot of work, but at the same time,\nif you do it in advance at the very beginning of your project, it's work that you can actually do\nin an environment that helps you to do it. And it doesn't go against you because you are not focused\non serving the customer, but you're actually focused on building this foundation that later\nwill allow you to serve the customer. So definitely cheaper to do it sooner rather than\nlater in a project. And again, there are different degrees. You don't have to do it perfectly. You\ndon't have to do to have perfect infrastructure as called 100% coverage unit test or observability\nwith hundreds of metrics and alarms. You can build all these things and improve them over time.\nI think what's important is to have a basic, to have an understanding and to have something that\ngives you enough indication of what's happening right now. And then over time, as you build new\nfeatures, you can tweak and improve also these aspects. So yeah, I guess that's also important\nto understand that when we talk about all these items, they don't have to be something like fully\nfledged 100%. You can build the bare minimum there, make sure that it works and it's going\nto support you and over time make it better as you're going to be making better the entire product\nitself. Agreed. Yeah. Keep it, go for the simplest possible implementation of each of those things.\n\n\nEoin: And then you've got a good foundation. You can always iterate on it later, but you've got\nsomething.\n\n\nLuciano: So what do you think in terms of, you mentioned one of the cases before you say that sometimes there are businesses that they don't even have tech at all. They just run, maybe they\nhave Excel. That's the best that they do from a technical perspective, but everything else is\npretty much manual and it's still, it's a pretty successful and viable business. What do you think\nis the risk there when they start to do digital transformation and actually implement a lot more\ntech in their workflow? How do they have to think about an MVP and starting to bootstrap all that\nprocess?\n\n\nEoin: So I guess the main point there is that you can have a really well oiled machine that isn't a machine that's completely human based and people who know exactly what they're doing,\nthey know what the process is, they know where all the data is, they know how to use their Excel\nsheets and they're really good at interfacing with the customer. And then when you take that\nand you're trying to do a digital transformation on it, you've actually got pretty serious\nobligation because you're trying to replicate the success of what that human\nteam has been able to do, but you've maybe you're trying to do it in a more scalable, automated way.\nSo when you think about your minimal viable product, you've got a pretty high bar already\nto meet because you have to make it really as viable as what came before. And so you're trying\nto implement scalability without sacrificing the excellent capabilities of that team of\npeople, which is very difficult thing to do. So I just think the main thing to say there is that\nokay, yeah, you should tick all those boxes that we mentioned already in terms of having\ninfrastructures code and unit tests, but you should be aware that you actually have a risk\nby building a technical MVP if you don't have the same rigor and foundations in place that your\nmanual process has. Yeah, I totally agree with that.\n\n\nLuciano: I actually now want to make a little bit of an experiment because if you have been following this podcast, you probably have\nnoticed that we have done a series of live streams where we built, let's call it a product, I don't\nwant to say an MVP or not. The experiment is actually to try to assess if that was an MVP or\nnot. And the product is something we call WeShare, which is like a, I don't know, a clone of WeTransfer\nor Dropbox Transfer. So upload a file, get a URL, somebody else can download the file with that URL.\n\n\nSo what we did is basically we put in place some of the things that we say that are needed for an\nMVP, but maybe not all of them. So I'm going to try to do a mental exercise to try to list all of them\nand say yes or no, whether we did a good job or not. So we did do infrastructure as code. We used\nthe serverless framework and everything was pretty much infrastructure as code, multiple stacks.\nSo I feel pretty confident saying that we did a good job there. Unit test and end-to-end test,\nprobably more than a no, than a yes. We have a few unit tests, mostly as an example of how you will\ntest specific APIs, but I wouldn't be comfortable saying that this unit test, that the ones we have\nright now will serve customers well, assuming that there will be customers using this application.\n\n\nSo probably more of a no than a yes. We should be doing more work there. Automated deployment,\nabsolutely not. We were deploying manually. You might've seen the pain of SLS deploy,\nwaiting three minutes and then trying again. And yeah, not committed code was deployed and all\nthis kind of stuff. So definitely a big no there. Observability, maybe a 50-50. We did show some\nstuff. We did show how to do custom metrics. We did have structured logs in place, but we didn't\ndo anything useful in terms of actually operating on this data. We didn't have alerts. We didn't\nhave dashboards. We did try to set up tracing, but we kind of failed at the first attempt. And then\nwe said, okay, we're going to revisit this later and never got back to it. So again, observability\nis probably 20%, but definitely not a yes. It's more of a no than a yes. Security fundamentals,\nprobably we did a good job. I guess we covered the basics. We tried to write very restrictive\npolicies. So we have an authentication mechanism. We did try to do at least the minimum kind of\nwork there. So I'm confident on that one. Skills, I believe that if we consider ourselves the\noperators of this product, we did rely on skills that I think we have. So probably yes.\n\n\nDocumentation was probably a little bit more of a no than a yes, but recently we put a little bit\nof effort in actually documenting the entire setup process and how does it work, what it does,\nwhat kind of infrastructure it creates. And people should be able to actually take all we have and\ndeploy in their own accounts. So probably more of a yes, but used to be a no a few days ago.\nThank you for that.\n\n\nSo one interesting thing before we try to assess whether this is an MVP or not is that I think\nour goals are not entirely clear with this particular product. On one side, it was meant\nto be more of an educational product. So the product itself is not the WISCHER thing, like\nthe actual product that allows you upload and download files, but more the whole experience,\nthe showing people how to use serverless, how to use AWS and how do you build something from\nscratch. Like what is the whole experience there? So that's maybe one aspect of the product. Then\nif you want to consider the actual tech product as uploading and downloading files, that's an\nentire other story and it would require to have users, which we don't have right now. So I'm not\nreally sure how we should consider that one. Do we consider a potential future product? Do we want to\nconsider product because we use it ourselves? That's probably to be assessed. What do you think?\nDo you consider this a successful MVP or not? Or maybe it's more of a product.\n\n\nEoin: No, not as a product. As an educational product, I think, yes, it is viable as an education thing\nand as a how to build stuff on AWS, for sure. I think there's plenty of lessons there for people.\nI'm satisfied with that part. But as a product within that, no, because you have to be serious\nultimately about supporting users if you're going to do a product and that's not what this is. But\nmaybe somebody out there will take it as inspiration and build our super successful startup\nahead of it and become a unicorn and we can say we were there at the start.\nRemember about us at that point.\n\n\nLuciano: Yes.\nSend us a gift or something.\nExactly. Buy us a coffee. So I think maybe let's just wrap it up with a couple of thoughts.\n\n\nEoin: So we said that an MVP needs clearly defined minimal scope. You have to have a product part, real users\nwith a real need and you need on the tech part, rigorous foundation with deployments, operations,\nsecurity, et cetera. So you might agree or disagree with the checklist of what it takes to have a real\nMVP. The most important thing really is that when it comes to what it takes to have a real MVP,\nthat should be defined within with you and your team so that everybody agrees what that is and\nthat you're not talking about different things and having completely different expectations\nabout the level of quality and the level of sustainability in that product. Because often\nproblems arise when people assume that, OK, it's an MVP, we can take shortcuts on loads of the\nfundamentals here. And other people are thinking, OK, no, this is a really complete solid product,\nit's just a minimal set of features. So agree those expectations early on and you can avoid a\nlot of confusion and conflict later on. Do you think we can refer people back Luciano to some of\nthe previous episodes? Maybe when it comes to the actual how-to of building an MVP, there's a couple\nof links in there. What do you think?\nAbsolutely.\n\n\nLuciano: I don't know if they are well-structured, but definitely if you listen to them, you will extract some information that is definitely relevant for the topic today. It's\ndefinitely going to give you elements on how to build your own MVP and what kind of technical\naspects you need to focus on and how to do those things in the context of AWS. So we have episode\nnine, which is how do you get started with AWS, which tries to cover all sorts of different things\nthat maybe not obvious if you are approaching AWS as a newcomer. Like what are the main things you\nshould do straight away? What are the main topics you should get yourself comfortable with?\n\n\nActually, I think AWS has a pretty high barrier to entry. It's not just get a server deployed.\nThere is just a lot more context that you need to get before you are in a position that you can\nactually put something into production. So I would recommend that episode to cover all of that.\nThen we also have episode 15, which is is serverless good for startups? This is more of\nan opinion take and it's something that if you have been watching our live streams, you can\nkind of correlate more of an opinion. Would you use the same approach for your startup? Yes or no?\n\n\nWhy? In which circumstances? Probably you're going to find an answer there. We have a few episodes on\nobservability. We have 33, which talks about CloudWatch metrics, 34, CloudWatch alarms, and 35,\nwhich is more about how do you query logs with CloudWatch. We have another one about\ninfrastructure as code, which is episode 31, where we try to compare CloudFormation or Terraform.\nThis is probably one of our most successful episodes, probably because it's a little bit\nof a flame war where I think I vouch a little bit more for Terraform and Eoin vouches a little bit\nmore for CloudFormation. Again, you might have opinions. The important part is to do something\nabout it. Maybe you can use CDK. Maybe you can use Pulumi. Just do infrastructure as code in\nwhatever way. It's more convenient to you, but just do it. Then finally we have episode 37,\nwhich is how do you migrate a monolith to AWS without the drama. If you want to watch our live\nseries of live coding sessions, we have a playlist on YouTube. We are also going to have the link to\nthat playlist in the show notes.\n\n\nEoin: Luciano will try and make all that very accessible to people on YouTube and in the show notes as well. Thanks very much for listening, everybody,\nand catching up on our latest AWS Bites. We'll see you in the next episode.\n"
    },
    {
      "title": "56. What can you do with S3 presigned URLs?",
      "url": "https://awsbites.com/56-what-can-you-do-with-s3-presigned-urls/",
      "publish_date": "2022-10-28T00:00:00.000Z",
      "abstract": "Uploading and downloading files are some of the most common operations for web applications. But let’s face it, as common as they are, they are still challenging features to implement in a reliable and scalable way!\nThis is especially true for serverless environments where you have strict limits in payload size and you cannot have long-running connections.\nSo what’s the solution? If you are using S3, pre-signed URLs can help quite a bit!\nIn this episode of AWS Bites podcast, we are going to learn more about them, and… if you stick until the very end of this episode, we are going to disclose an interesting and quite unknown tip about pre-signed URLs!\nSome of the resources we mentioned:\n\nOur illustrated article on S3 pre-signed URLs\nDocumentation for the pre-signed POST method\nHow to upload files from a frontend app using pre-signed URLs, article by Borislav Hadzhiev\nUsing pre-signed URLs for multi-part uploads, article by Altostra\nDifferent architectures and tips for managing uploads to S3, article by Zach Charles\nUsing S3 Object Lambdas to generate and transform S3 files on the fly, article by Eoin Shanaghy\n\n",
      "transcript": "Luciano: Uploading and downloading files are some of the most common operations for web applications.\nBut let's face it, as common as they are, they are still challenging features to implement in a reliable and scalable way.\nThis is especially true when we talk about serverless environment, where you have strict limits in payload size and you cannot have long-running connections.\nSo what is the solution? If you're using S3, presigned URLs can help you quite a bit.\nAnd in this episode, we're going to be talking about presigned URLs.\nAnd if you stick until the very end of this episode, we're going to disclose an interesting and quite unknown tip about presigned URLs.\nMy name is Luciano and today I'm joined by Eoin and this is AWS Bites podcast.\nSo Eoin, maybe we can start with describing some of the use cases, like what kind of operations do we generally do when we talk about upload and download in the context of web applications?\nYeah, okay. Let's set some context here by talking about a few of the use cases.\n\n\nEoin: So let's say you're signing up in a mobile application for your service and you want people to take a photo so they've got their avatar.\nThat would be an upload. Another one might be you're offering some digital download, like a software.\nPeople are paying for the software and then they want to download a large binary application.\nYou might want to have a download facility that's scalable and fast there.\n\n\nOr a very typical one actually is if you're sending people a newsletter and you want them to be able to download a white paper using a link in the email.\nOr maybe, you know, they give you their signup details and then you give them a link in the email.\nYou also have other things which are maybe less user-facing, but even between systems you might have two applications or two services talking to each other.\nThey might have an API where they've got an event, but associated with that event is like a large file or an attachment, some like larger payloads that you don't want to put in every message.\nSo instead you'll give a link in the message and that link will allow them to retrieve whatever large data that is.\nSo those are the kind of use cases we're talking about. You said it's one of the challenging things that comes up is trying to upload and download files.\nSo what are the challenges?\n\n\nLuciano: Yeah, so first of all, when you talk about uploading and downloading, those are generally what we can call streaming operations.\nSo you have a TCP connection and you will have to transfer bytes for a long enough period of time.\nAnd generally you don't want to put boundaries there because you might have a particular context where you are uploading or downloading very large files.\nImagine, I don't know, videos or I don't know, like you mentioned big binaries because you are downloading an application.\n\n\nSo you can imagine that you need to transfer a lot of data for a sustained period of time.\nAnd if you're using a serverless environment, this is a big challenge because we know serverless environment tends to have very strict limits.\nLike in Lambda you have, I think it's five megabytes, the maximum payload that you can receive in a request.\nAnd also the response that you can send from a Lambda is quite limited as well.\n\n\nSo you can immediately see that if you want to implement this stuff straight in a Lambda, it's not really giving you a lot of amount of data that you can deal with.\nAnd another problem when we talk about downloads is that generally you want to keep all the data in a protected place.\nAnd then you only want to enable specific downloads after certain actions.\nMaybe the user is authenticated and you realize, okay, this particular user is actually authorized to view this resource.\n\n\nSo I would like somehow to give only them the permission to download the file.\nSo this is another challenge because of course if you think about S3 you might think, okay, I'm just going to make a bucket entirely public.\nBut then anyone can, as soon as they discover the key and the bucket name, can download that particular file.\nSo this is not really going to be a sustainable solution.\nAnd if you think, okay, I'm going to put a Lambda in front of that, then again you have the problem of payload size.\n\n\nSo again, what's the solution?\nAnd thankfully if we use S3, there is a feature in S3 that is called Presigned URLs that can help us a lot in solving this particular kind of problems.\nThe idea is that you can easily generate a URL that allows users to upload the content of an object directly into S3.\nThis is the case of upload, but at the same time you can also do the same thing basically to generate URLs for downloading a file from S3.\n\n\nSo again, every time you want to authorize either an upload or a download, you can generate in that particular moment a specific URL for the user.\nAnd the user can use that URL to either upload or download from or to S3.\nSo the interesting thing is that why this is called Presigned, because it's basically the URL contains a lot of information.\nAnd some of this information, like if you ever look into Presigned URLs, is actually quite big URL with a lot of query string parameters.\n\n\nAnd some of these parameters are actually authentication parameters.\nSo literally you have created a URL that has already built in the credentials that are needed for the user to either upload or download that particular resource.\nAnd at the end of the day, this is good because you are relying entirely on a managed service like S3, so you don't have to be worried about the infrastructure.\nIs it going to scale? Is it going to support all the data they need to support?\nSo really you don't need any additional infrastructure or compute, you just need to make sure you generate the URLs at the right time before the user performs that particular action.\nSo I suppose the next question is, how do we actually generate this kind of URLs?\nYeah, like if you want to just generate one ad hoc for whatever reason without building it into the application, you can use the AWS CLI to do that.\n\n\nEoin: You can also use the S3 console in the AWS Management Console.\nAnd you also have like IDE integrations, so the AWS Explorer for Visual Studio, but also allows you to browse your bucket and right click and get a Presigned URL for it.\nSo those examples, the console and Visual Studio, that only works for download, it doesn't allow you to do uploads.\nThe more I suppose powerful, flexible way to do it is with the AWS API or the SDK where you can generate Presigned URLs for uploads and downloads.\n\n\nSo then if we take those two cases, how do you do a download?\nWell, you need to specify, okay, what's the bucket and what's the key?\nYou need to specify, okay, what's the bucket and what's the key? And then you can also specify some additional configuration, like some headers that are associated with the download or an expiry.\nSo how long does this Presigned URL remain valid for?\nAnd once you do that, you will get this really big URL you mentioned with loads of query string parameters, and that will link to the file.\n\n\nSo if user clicks on that or curls it, they will be able to download that file as long as they haven't changed the HTTP request anyway, that would invalidate the signature that's embedded in the URL.\nAnd as long as the expiry time has not elapsed.\nSo that's the GET method and it works in a very similar way for uploads.\nWith uploads, you actually got a couple of options.\nYou can use a Presigned PUT, which works exactly the same way as the Presigned GET.\n\n\nEverything is in the URL and you can also put in the content type and the content length header that's required.\nAnd then basically you just put the body of your file into the HTTP request body.\nSo that's how PUT works.\nPresigned POST is actually like a special feature.\nIt's an additional kind, it's a different kind of a Presigned URL.\nAnd it uses form data, like HTTP form data instead of using like a normal post with an Octet stream or a binary payload.\n\n\nThe Presigned POST, it comes with, you actually get a much shorter URL, but instead of having all of the data embedded in query string parameters, you get a set of fields back that you have to put into your form data.\nAnd a form data is basically like a multi-part body where you specify each field in the form.\nOne of those, all of those fields that AWS tells you you have to provide in your Presigned POST response.\n\n\nAnd then one of them will also be the file content encoded in there too.\nAnd there is a really good post actually talking about how this is sometimes the best option to use.\nIt's by Zach Charles and we'll link to that in the show notes.\nAnd it's a good guide to using the POST method.\nThe real advantage with Presigned POST is that you can specify limits on the file size that's going to be uploaded.\nNice. So I suppose another interesting point of conversation is generally if you're building an application and you receive a file from an upload, you are trying to do something with that file, right?\n\n\nLuciano: There is some workflow that is intrinsically part of your application.\nJust to make an example, you upload a picture and maybe you want to process it with, I don't know, recognition to try to identify objects or text or something like that in that particular picture.\nAnd then maybe you can attach some metadata and store it somewhere and make it available to users.\nSo how do you actually trigger the second part of the workflow?\n\n\nWe know at this point how you can perform the upload, but what actually triggers is the rest of the workflow.\nAnd there are different ways that you can actually do that.\nSome are asynchronous and some can be synchronous.\nThe asynchronous one is basically relying on notifications.\nYou can either set up S3 object notifications or event breach notifications, and then you can listen to those notifications and trigger, for instance, a Lambda and then Lambda can orchestrate the rest of the workflow.\n\n\nOr maybe you can start a step function.\nThere is really no limit in how you actually process it.\nThe only thing you want to know is exactly the point where the file was completed, completely uploaded, and at that point you can receive the notification and decide how to process that notification.\nAnother use case that I've seen is basically, for instance, I don't know, the case that you are uploading an avatar in a web application and then maybe you want to make sure your profile is actually updated to reflect the new avatar.\nSo you can implement that in a slightly different way, for instance, rather than using events.\nWhat you could be doing, you could have two different API calls.\nThe first API call is actually using the pre-signed URL to upload the file.\nAnd then there is a second API call where you say, update my profile with this key, which is going to be my new avatar.\nSo it's a little bit up to the client to coordinate the two different requests, but it's another valid solution.\n\n\nEoin: The other point I was going to mention on the automated processing is that you might ask, is the new S3 object lambda feature something that will help us here?\nBut the S3 object lambda is something that allows you to do lambda post-processing when you do a GET or a HEAD request, but it doesn't support any kind of automation on her post.\nSo no joy here, at least yet.\nAnd if you don't know about this particular feature, we have a blog post by Eoin that describes how to use all of that and what are the limits.\n\n\nLuciano: So we'll have it in the show notes.\n\n\nEoin: Excellent. Excellent.\nNow, I suppose it's also worth mentioning that pre-signed URLs, you don't always have to use them.\nSo if you've got full control over the client application, you have some other options as well.\nSo what you can do instead is just embed the whole AWS SDK S3 client in your front end web application or in your mobile application and just use the higher level APIs that the SDK give you.\n\n\nSometimes there are some optimizations in there around large file uploads with multi-part that will be more beneficial if you just use the SDK directly.\nAnd all you need in order to be able to do that is some temporary IAM credentials that you can use in the client.\nSo it's another way of doing it. So instead of signing the URL with IAM credentials on the server side, you basically just issue IAM credentials using like STS.\n\n\nYou can also do use AWS Cognito with identity pools to do that.\nSo if that's something that you're comfortable with, it's just another approach that you should probably be aware of and maybe think about whether that's best.\nThe Amplify SDK also makes that whole thing a lot easier.\nSo it allows you to, I think through its storage API, allows you to interact with S3 in a reasonably simple way.\nIt's probably worthwhile just talking about some of the limitations.\n\n\nAnd we've already said that for a push upload, you need to know the file size in advance because you have to set that content length header.\nThat's a bit unfortunate because it limits your ability to stream the content from an indeterminate source.\nSo you can't really limit the amount of data you're uploading unless you do some really funky stuff like updating the policy of the bucket itself.\nSo every bucket has a resource policy and you can put lots of restrictions in there.\n\n\nAnd even those restrictions can apply to certain object prefixes, key prefixes.\nBut it's not the kind of thing that you want to be updating all the time for very specific user flows.\nSo there's another blog post that we can put in the show notes from bobbyhads.com and that's worth a look.\nAnd of course, the maximum file size worth stating that it's five terabytes.\nBut that applies on S3, whether you use presigned URLs or not.\n\n\nWorth restating again that if you use that special post presigned URL with the form data, you can overcome some of those limitations.\nYou can, you don't need to specify the file size in advance and you can specify conditions that include what range of content sizes you support.\nAnd that includes a minimum and a maximum content length then.\nPresigned URLs, they do have an expiry time, so they have a limited time, but you cannot limit the number of downloads or uploads.\n\n\nThere's no easy way to do that.\nAnd you can't also easily limit the number of downloads or the downloads based on IP address.\nYou would again have to go and change the bucket policy to put in a source IP condition.\nBut it's probably more work than you really want to do to maintain all of these policies.\nAnd you might end up incurring quotas for like adding so many specific user in specific rules into that policy.\nSo those are the limitations. With that in mind, and given that we've explained what it is, how to use it, do we have any closing recommendations for people?\nYou also gave a hint that you might have a secret tip.\nYeah, I'll try to pay back everyone that is listening so far.\n\n\nLuciano: So basically in terms of recommendations, one of the most common recommendations that you will find is to try to keep URLs, presigned URLs short lived.\nBecause a presigned URL doesn't really expire after you use it.\nIt only expires after the time, the expiry time is elapsed.\nSo if a user has a presigned URL, nothing is stopping them from using it twice or even more.\nSo they can re-upload, they can re-download the file.\n\n\nSo the only real way that you can protect against that is to keep the expiry time as short as possible.\nBut of course, don't keep it too short because some people have observed that if you keep it too short, there might be clock slightly off sync between servers.\nSo if you keep it, for instance, in the order of a few seconds, the link, as soon as the user starts to use it, it's already expired.\nSo probably slightly above one minute or two, it's probably fine for most use cases.\nAnother tip is to enable cores.\nAnd I think this is especially important if you want to use it from the front end.\nIf you want to use both presigned URLs, or I think that applies also if you use the SDK.\nRight, Eoin?\nYou still need to enable cores in order to be able to do API calls from the front end.\nI'm not sure about that. I'd have to think about that.\n\n\nEoin: Okay.\nCut me off guard.\n\n\nLuciano: Worth verifying. If you know it, please leave it in the comments.\nBut now let's get to the secret tip.\nAnd this is something that we actually discovered quite recently and we were quite impressed by it.\nSo it turns out that presigned URLs are not only valid for uploads and downloads,\nbut actually you can use them for any kind of S3 operation.\nAnd if you don't believe us, the simplest thing you can do to actually verify the statement\nis to try to use the SDK to create a presigned URL for a least bagged operation.\n\n\nAnd then use that URL and see what's the response.\nAnd I'll give you another thing that you can try, which is actually a little bit more useful in practice, I believe.\nWhich is, for instance, you can do a multi-part upload using presigned URLs.\nAnd the way you do that is basically you do the first operation of a multi-part upload.\nBy the way, if you don't know what a multi-part upload is, it's basically\nrather than uploading the file in sequence, like byte after byte,\nyou can split that file into multiple parts and then you can upload all the\nbytes of every parts in parallel. So it's basically a way to try to speed up the upload.\n\n\nAnd the way it works is that you generally have to do two API calls.\nOne to start the multi-part upload and one to finish.\nAnd in between, you can create new parts. And when you create new parts, you can basically\nuse the presigned URLs to do that. And at that point, you have URLs that you can use to trigger\nthat upload without needing to have additional credentials.\nAnd actually, this is something we figured out in a blog post by altostra.com.\n\n\nSo we're going to have that blog post as well in the show notes.\nAnd there are examples of code you can see there, which I think makes all of\nthat thing a little bit more clear. So with that being said, I think we are\nat the end of these episodes and we are really curious to know if you knew about S3 present URLs\nand if you have been using them in production, what kind of use cases do you have?\nAnd I don't know if this is your favorite S3 feature.\nIt kind of is my favorite S3 feature right now.\nSo if this is not your favorite S3 feature, please tell us in the comment, what is your\nactual S3 feature that you like the most? So with that being said, thank you very much\nfor being with us. Remember to like and subscribe and we'll see you in the next episode.\n"
    },
    {
      "title": "57. Cognito User Pools vs. Identity Pools",
      "url": "https://awsbites.com/57-cognito-user-pools-vs-identity-pools/",
      "publish_date": "2022-11-04T00:00:00.000Z",
      "abstract": "If you looked into Cognito, chances are that you have been confused by User Pools and Identity Pools (now renamed to Federated Identities). Well, Cognito is not one of the simplest AWS services to get started with but it is indeed very powerful and it can be very convenient to use when you are dealing with authentication and authorization.\nIn this Episode of AWS Bites Podcast we try to clarify what is the difference between User Pools and Identity Pools. When to use one or the other and even when to use them together. Throughout the episode, we will cover several practical examples and use cases.\nIn this episode, we didn’t really mention any resource, but if you want to deep dive on this topic here are some useful links:\n\nOur series of live streams where we build a serverless product and where we also use Cognito User Pools for authentication.\nOur previous episode about S3 Pre-signed URLs (an alternative way to give controlled access to files on S3)\nAmazon Cognito, official documentation\n\n",
      "transcript": "Eoin: Cognito is a frequently used and core AWS service for managing users, authentication and authorization.\nBut getting started with Cognito and knowing what features to apply to different use cases can be really challenging.\nBy the end of this episode, you will know the differences between Cognito user pools and identity pools,\nalso known as Cognito federated identities.\nMy name is Eoin, I'm here with Luciano and this is AWS Bites.\n\n\nBefore we start, we have a favor to ask.\nIf you have been enjoying this podcast, please consider giving us a review on Apple Podcasts or wherever else you get your podcasts.\nIf you follow us on YouTube, consider subscribing and liking our videos and giving us plenty of comments.\nThis will really help us to keep the podcast relevant and discoverable for other AWS enthusiasts.\nThank you so much.\nSo let's get to it. Luciano, what are some of the use cases where you might actually need to use Amazon Cognito in the first place?\nYeah, the most common use case is when you need to do sign up and sign in for any mobile or web application.\n\n\nLuciano: Like that's the service you go to in AWS.\nAnd outside of AWS, a compatible service is generally Auth0.\nSo if you have been using Auth 0, you can always imagine as the AWS counterpart to out zero.\nBut of course, you can do more. You can also, for instance, use Cognito to limit access to APIs.\nFor instance, you build your own Lambda and you use that Lambda as a back end for an API gateway.\nYou could be using a custom authorizer backed by Cognito to make sure that only people who actually have some level of access,\nsome level of login or authentication into that particular application can actually call that API.\n\n\nAnd similarly, that's not limited to HTTP. Of course, you can do that also if you're using GraphQL, for instance, by combining that with AppSync.\nSo basically, this is the use case where you have users that can perform specific actions only if they are logged in.\nSo you can build in all that security layer into your applications.\nAnd very similarly, you can do that for other kinds of resources.\nFor instance, you can allow users to access S3, DynamoDB by using features that are built in in Cognito.\nSo once the user is out and together, recognized, you can somehow give them access to these kind of services and resources in your AWS account.\nYeah, so I think one of the things that I was confused about the most when starting to use Cognito is these two concepts of identity pools\nand user pools, identity pools also now is being called federated identities, which I think just creates more confusion.\nSo what are these two things and what are the fundamental differences?\n\n\nEoin: When I started building serverless applications about six years ago, Cognito was the one service that really confused me the most.\nIt's really great coming from places where you used to have to build this kind of thing yourself,\nthat you got a service that takes care of all of the security needs and the standards you need to comply with.\nBut it's really just not simple to understand at first.\nUser pools and identity pools being kind of similar and having similar names, it kind of adds that confusion.\n\n\nSo let's try and do our best to clarify what they are.\nUser pools allow you to create your own identity provider, so they're used for implementing authentication.\nSo think about sign up, log in, and they allow you to build those sign up and log in flows.\nAnd as well as that, you also get a place to store user data.\nOn the other hand, identity pools or federated identities, they're more for authorization.\n\n\nSo they don't give you a place to store any user data.\nAnd I prefer to think about identity pools as an identity broker because there isn't really a pool of anything.\nIt's just a place that allows you to exchange one set of credentials for another set of credentials.\nAnd what you get back from identity pools are short lived IAM credentials.\nSo you're essentially swapping your already verified identity from an identity provider or IDP.\n\n\nIt's also known as an IDP and you're getting back short lived IAM credentials.\nNow, identity pools don't have a lot of features.\nSo let's just first talk about some of the features of user pool because there's quite a lot there.\nSo you get a place to store your users in groups. You get hosted, sign up and sign in and reset password pages,\nas well as the ability for you to implement all of that stuff yourself and customize the UI.\n\n\nYou have lots of ways to authenticate so you can authenticate with username and password.\nAnd it now supports multi-factor authentication with an app as well.\nYou can do server to server authentication. You can do lots of OAuth 2.0 flows.\nAnd then you can also do social sign in into your user pool with Google, Amazon, Facebook and Apple.\nAs if you don't if you're not using any of those, you can also use SAML and OIDC federated sign in for anything that supports those standards.\n\n\nIf you want to customize any part of the login flow, then you also have Lambda triggers so you can put hooks in place at various points of the sign up and login flow.\nAnd as well, if you're coming from somewhere else, user pools have mechanisms that allow you to migrate your existing users and their usernames into user pools.\nSo that's pretty good.\nSo when a user logs in with Cognito, they will get an access token. That access token is particularly usually like a JWT, a JSON Web token.\n\n\nSo it's a it's a JSON object. It's signed and it's got a lot of properties in it.\nAnd that access token can be used to secure access to some AWS services.\nSo you get your access token. You always get a refresh token as well. And if you're using the OIDC scope, you can get an ID token too.\nOn the other hand, when you're using identity pools, you're swapping a token like that you've already got from your ID identity provider for an IAM session.\n\n\nAnd then once you've got IAM credentials, it's just like having an IAM user.\nYou can use that to make any AWS API call allowed by the policy associated with the role and the session.\nSo at this point, you might realize, since a user pool is an identity provider that issues tokens and an identity pool is a broker that swaps tokens for IAM credentials,\nyou can actually combine the two things to get IAM credentials for the users that are stored in your Cognito user pool.\nOf course, this is always confusing because you can use any other IDP instead of user pools as well.\nSo you've got a lot of options here. So it is worthwhile exploring when to use one over the other and when to use both.\nYeah, absolutely. And I think it's good to to rediscuss that and maybe make an example.\n\n\nLuciano: And yeah, basically, just to summarize what I am understanding from what you just said is basically you want to if you want to have a place where you can sign up users and store attributes about these users and you don't necessarily have an existing identity provider,\nyou should use user pools. Then if you want the state to allow users to access AWS resources, we mentioned S3, could be something else.\n\n\nThen you need basically to use identity pools because the idea is that identity pools will be the broker, as you say,\nthat will convert whatever authentication mechanism you have from user pools or something else into actual credentials that AWS recognizes to give you access to services.\nSo if you need both, you can definitely use both. And again, I want to clarify that with a practical example, because otherwise it's going to be a little bit more abstract, a little bit too much abstract.\n\n\nSo let's make our classic favorite example, an e-commerce website. In particular, what do we want to do here is as many other e-commerce users will be navigating, seeing different products.\nAnd we want to be able to collect all of that information. We want to understand the user journey, the user preferences, so that we can be able to suggest the user products that might be interesting for them in as much real time as possible, of course.\n\n\nSo while they navigate, they should see suggestions that are calculated more or less in real time. So the user will be logging in with username and password.\nSo there is definitely a user pool there that will allow the users to log in, but also store all the necessary information and attributes about every single user.\nThen while the user is navigating, we want to have IAM credentials to be able to send user activity, for instance, clicks or the different pages that the user is visualizing into a Kinesis stream.\n\n\nAnd in order to do that from the client, we need to use something like Identity Pool so that we can get IAM credentials that are authorized to send messages to this Kinesis stream.\nSo in this case, we are basically using both. We are using the user pools for login and storing user attributes.\nBut then we also use Identity Pools to basically get the AWS credentials that are needed to connect to Kinesis.\n\n\nAnother interesting detail that took me a while to realize is that when we want to create a setup like this, what we are basically doing is using our user pool as an IDP for our Identity Pool.\nSo this is how we are connecting the dots between user pools and identity pools and using them together.\nSo we are basically creating that trust relationship saying to our broker for credentials, which is the identity pool, to trust our user pool.\nSo when a user is logged in into that user pool, then the identity pool is giving us credentials or actually is giving the user credentials to be able to connect the services that we want to authorize for that user in AWS.\nThere are ways to access AWS resources, but a limited subset just using user pools.\n\n\nEoin: And there you're just talking about using your token to protect an API and implementing authorization in that API.\nSo how does that work? Let's give a few examples.\nSo if you've got an API gateway or an application load balancer that you can put in a Cognito Authorizer and that can restrict access to APIs to authenticated users from a user pool, you don't need an identity pool in that particular case.\n\n\nSo the access token is just going to be validated by the authorizer that you've configured in the load balancer or in API gateway.\nSo that doesn't really give you any role-based or attribute-based access control.\nIt's kind of all or nothing access for each authorizer you configure.\nIf your API is backed by Lambda, you do get information about the principal or the identity making the request.\nSo you can find out what groups, because Cognito also has the concept of groups of users.\n\n\nSo you can kind of use that to implement your own level of access control by checking what groups are a member of and implementing check your own checks further down the chain behind the API.\nIf you're using AppSync, it's actually a little bit better because AppSync's authorizer also gives you the ability to protect some of your AppSync queries or mutators using annotations that specify,\nokay, this is the user pool, but the user also needs to be a member of a specific group.\n\n\nSo it's a little bit more powerful.\nAn alternative then is that you can say, okay, let's not use user pools to protect this API. Let's instead use IAM authorization.\nSo API gateway has IAM authorization, and then you're just using standard IAM policies and the request to your API has to be signed using an AWS version 4 signature, just like a request to an AWS API.\nAnd in that case, you're back to using identity pools because that's where you get your IAM credentials from if you've got an IDP, as you've clearly explained.\n\n\nSo that means that you don't get any user identity information further down.\nSo once the user has access through their IAM policy as issued by the identity pool, it allows them to invoke the API, but the backing code behind that API, it doesn't have any visibility onto who that user is.\nSo in general, you kind of find that most people use user pools and access token for restricting API access for web and mobile applications and identity pool credentials are more often used for accessing other services directly.\nS3 being a common example, but you also mentioned the case with Kinesis, or it could be Kinesis Firehose or Amazon Pinpoint is another one.\nIf you want to collect data directly from a client with low latency and overhead and let Amazon manage all the scalability and you don't want to put an API and a Lambda function in between the client and that AWS service.\nAre there any other interesting features we should talk about?\n\n\nLuciano: Yeah, let's mention a few other interesting things that might not be too obvious and maybe worth checking out later after you listen to this episode.\nOne interesting thing is that identity pools basically, we said they are just giving you IAM credentials.\nSo you could use that idea to do ABAC or RBAC type of auto-excession.\nSo basically the idea is that because users will have certain attributes, and then you can use those attributes in your IAM policies to create conditions basically.\n\n\nSo you could, for instance, say, I don't know, if a user has this particular group, then that's a condition that allows your policy to organize it.\nFor instance, I don't know, more practical use cases like you have an admin flag or a group and you can use that to restrict that specific actions.\nFor instance, only if you have that admin flag, you can do that particular action. I don't know, maybe delete a user or something like that.\n\n\nSimilarly, you can use the same idea to do, for instance, multi-tenant systems where you could have an attribute that tells you exactly which organization that user belongs to.\nAnd therefore you can allow specific actions only on the resources that are part of that organization.\nSo that could be another thing you could do.\nAnother thing is that you can do social sign-in.\nSo for instance, I don't know, you want to allow login not just by username and password, but also through Google, Facebook or some other social login system.\n\n\nYou can use the OAuth provider as part of the user pool configuration.\nAnd then you can still use identity pools to still exchange tokens for IAM credentials.\nSo again, this is another way that you can combine the two.\nIt's not just limited to access with username and password.\nYou can still do the social login and then if you still need to get IAM credentials, you can also use identity pools.\nAnother thing that is interesting if you ever use Amplify, Amplify is kind of a nice abstraction over all these things.\nIt gives you very easy to use APIs, but just behind the scene is doing all the things that we just described.\nSo it's just an easier way to get those IAM credentials and use them in your own web or mobile application.\n\n\nEoin: Great.\nSo I think that's all we have for today, but we're curious to know if you've been using Cognito.\nDo you have any further questions about Cognito and other things we should cover in the future?\nDo you have any interesting tips or use cases that you've managed to implement with it?\nPlease share with us.\nYou can drop us a comment on YouTube or reach out on Twitter.\nAll our contact details are in the show notes.\nSo thank you very much for listening and for liking and leaving us a review.\nWe really appreciate that.\nAnd we'll see you in the next episode.\n"
    },
    {
      "title": "58. What can kitties teach us about AWS?",
      "url": "https://awsbites.com/58-what-can-kitties-teach-us-about-aws/",
      "publish_date": "2022-11-11T00:00:00.000Z",
      "abstract": "Building actual projects is a great way to consolidate our understanding of AWS. In this episode, we present 4 different project ideas to explore services and concepts in the space of web application development, machine learning, and data science.\nOk, you are probably wondering where kitties come into the equation here. Every one of these 4 project ideas involves kitties! 🐱\nWe can learn stuff and have some fun too!\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nSome of the resources we mentioned:\n\nThe Cat Detector workshop\nAI as a Service (book by Peter Elger and Eoin Shanaghy)\nKaggle cat vs dog dataset\nThe best website in the world: http.cat\nThe true meaning of the 418 HTTP status code\nThe cat breeds dataset on Kaggle\nOur series of live streams where we build a serverless WeTransfer clone\n\n",
      "transcript": "Luciano: The World Wide Web could have been something great, something to move humanity forward.\nInstead, it turned out to be a place where we spend our time slacking off and looking at cat pictures and videos.\nBut honestly, that's actually cool. We can actually use it to our advantage.\nAnd yes, I promise you, we can learn about AWS while having fun with kitties.\nThis is Luciano and Eoin and another episode of AWS Bites podcast for you.\n\n\nOK, after this intro, I feel that I need to be a little bit more serious, but we really want to talk about AWS and ways that you can actually learn more about AWS.\nAnd we believe that you should be building projects on AWS to actually learn really what it takes to be successful with AWS.\nAnd of course, when you build projects, we can also have fun. So we can build projects that involve kitties.\nAnd we have actually prepared a list with four different project ideas that you can build on AWS so that you can practice different skills, different architectures and grow in parts like application development, data science, DevOps and even machine learning.\n\n\nAgain, this is just in the idea that on one side it's good to know the theory, maybe get some certification, but you really need to put those skills into practice if you really want to remember and learn deeply what it takes to build applications and projects on AWS.\nBefore we get into the first project, I want to mention that AWS Bites is sponsored by fourTheorem.\nfourTheorem is an AWS consulting partner that offers training, cloud migrations and modern application architecture.\nYou can find out more at fourtheorem.com and there will be a link in the show notes.\nOkay, with that being said, Eoin, what do you think? Should we start to discuss the first project?\nYeah, the first project I really like and it's one that our colleague Peter Elger came up with for the book that I co-wrote with him, AI as a Service.\n\n\nEoin: And it's a cat detector project and it's a good bit of fun, but it's actually a really good learning one for people who are getting into AWS and maybe exploring some of the newer capabilities that you have there.\nThings like serverless technology, infrastructure as code, but also some of the machine learning services.\nAnd you do not have to be, like this might be useful for people interested in machine learning or computer vision, but it's certainly not a prerequisite that you have to have any specialty there.\n\n\nThat's the whole point of these managed machine learning services that anybody can use them for use cases like this.\nSo obviously the use case here is a really critically important one that is detecting pictures of cats on the internet.\nSo let me talk through briefly how this works.\nSo there's a couple of services here, right? It uses things like AWS Lambda, SQS, Rekognition, that's the machine learning service, and API Gateway as well as S3.\n\n\nAnd it shows you how to build up this architecture slowly piece by piece.\nNow we've got a workshop as well. So there is a GitHub repo with a workshop that goes through this step by step.\nYou can have a look at the book if you want to, but you can just follow the workshop as well and you can see the code.\nSo it brings it through fairly step by step. And it's not a particularly complicated architecture, but it shows you a couple of things like event driven communication over SQS, as well as showing you how Rekognition works.\n\n\nAnd the Rekognition part is actually quite simple. Everything is really managed for you.\nSo what this application does is it allows you to bring up a front end, very simple web page where you can enter a URL.\nWe'll go off to that URL in Lambda, pull down the HTML, find images on that web page, and then we'll take the images and put them on S3.\nAnd then we'll submit the images to Rekognition and it will identify objects in that.\n\n\nAnd then the back end will also kind of generate a word cloud based on the objects detected in the image.\nSo hopefully cats. And it will also like generate, use the confidence of objects that it has found to generate that word cloud.\nSo then in the front end, it'll be able to render the image and what it's found. So hopefully you'll see cat and you might even see things like the breed of cat as well.\nSo this is something you could probably work through in a couple of hours. I know Peter and myself have presented this workshop at various conferences and things.\nAnd it only takes a couple of hours to get through. I think it's really nice one to just get a feel for what modern application development is like on AWS.\nYeah, I really like this one. I remember that I saw a similar one. There is a website called Kaggle.com where you can find interesting datasets.\n\n\nLuciano: And one of the datasets, we'll put the link in the show notes, is basically a dataset where you have pictures of cats and dogs.\nAnd basically you can use that idea to try to build a classifier and distinguish between the two.\nSo there could be an alternative project that you can build to try to develop the same skills of the project we just mentioned.\nBut moving on to project number two, what you could build is HTTP cat clone.\n\n\nAnd if you don't know HTTP cat or HTTP dot cat, that's literally my favorite website ever.\nAnd it's basically a website that is built to make sure that you easily remember what is the meaning of HTTP status code.\nSo if you really want to have a quick feeling, what does that mean? Can you remember what is the meaning of the status code for one eight?\nI can never remember. You always find it referenced everywhere because it's kind of a web joke.\n\n\nSo if you just want to find out, just go to HTTP dot cat slash four one eight.\nAnd what happens is that you will get a funny picture with a cat. But that picture also describes the meaning of the four one eight HTTP status code.\nSo the idea is, OK, what would it take to try to rebuild the same website, literally a clone of this website by using AWS?\nAnd I have at least three different solutions in mind and we can talk through them.\n\n\nAnd I think every different solution tries to exercise different architectures and different services you can use in AWS.\nSo I'm not going to be mandating which one is better. I think they are all equally viable.\nIt's going to be more on you to decide which kind of tools we want to practice with and then maybe pick one approach or the other.\nSo the first approach would be we could build this website literally as a static website.\n\n\nAt the end of the day, there isn't really anything dynamic. It's just a collection of pictures mainly.\nBut we have some HTML and CSS. So once you create all of that and you can use any static site generator, for instance, like Astro 11, whatever you like.\nThere are hundreds of them at this stage. You will be able to eventually will have a collection of assets, HTML, CSS, images.\nSo you need to figure out, OK, how do I put this asset into production so that I can have a public URL that other people can visit?\n\n\nAnd one of the simplest solutions is you could be hosting all these assets in S3.\nSo that becomes kind of your place where you put all the files.\nBut then you need to figure out a way to serve all these files as a website.\nAnd with S3, you can easily enable a feature that is called S3 websites.\nBut that feature, although it works and for a website like this, it might be just enough, it doesn't support HTTPS.\n\n\nSo if you also want to test how to make a static website and serve it over HTTPS, you can also use CloudFront.\nAnd serve the website through CloudFront, which also gives you additional advantages because CloudFront is a CDN.\nSo you are actually replicating all these assets around the world and reducing the latency with the actual users.\nSo that could be approach number one, a static website, S3 and optionally CloudFront.\n\n\nAnother approach could be you could build this as a more traditional website.\nWhile it's true that there isn't really a lot of dynamic stuff,\nnothing is stopping you to still build a web server that is there, accepting requests and deciding which assets should be served back to the user.\nAnd to do that, you have a number of different options in AWS.\nFor instance, you are free to pick whatever web framework, web application framework you like.\n\n\nIf you are in Node.js, for instance, you could be using Express or Fastify.\nIf you are using Python, you can be using Django.\nAny language really has a lot of options for web servers.\nAt that point, it's up to you to build the application and then decide how do we ship the complete application with all the assets to AWS.\nAnd again, different options.\nYou can just go for an EC2, figure out a way to just copy all the necessary software and code into an EC2, spin it up, connect a DNS.\n\n\nAnd at that point, you basically serve traffic on the public web using an EC2 as a backend.\nSimilarly, you could be using Fargate. So if you prefer to containerize all of this code, you can go and deploy it on Fargate.\nOr other alternatives could be you could be using something that is more of an application backend like Elastic Beanstalk or AppRunner\nthat will give you a bunch of tooling already out of the box when it comes to facilitating deployment or scaling things up if you get a lot of traffic.\n\n\nSo those could be other options to explore when you want something a little bit more out of the box and more kind of production ready.\nAnd the third approach could be you could imagine this website a little bit as an API and build it with Lambda and API Gateway.\nThere are interesting concerns that comes in at that point because with Lambda and API Gateway, it's very easy to serve JSON or kind of structured responses.\n\n\nWhen it comes to serving files, you have a bunch of limits. For instance, your payload cannot be more than six megabytes, which should be enough for this kind of website.\nI don't expect the images will be more than six megabytes, but still you need to figure out how do I encode a response that contains a binary payload like an image.\nSo there are different approaches there. Again, you can just use S3 and maybe create presigned URLs, or maybe you can just use S3 and CloudFront and then serve the images of all the other static assets from CloudFront.\n\n\nBut again, it's up to you to experiment and figure out how practical this solution is.\nBut if you really want to use Lambda and API Gateway, I think there are ways to make all of that work.\nNow, one interesting thing is that if you go for option two or three, because you have a backend at that point, you can start to do something a little bit more dynamic.\nFor instance, you could create an endpoint called slash random that just gives you a random image every time.\n\n\nAnd you could use it for fun just to discover new status codes that maybe you're not aware about.\nAnd you could also consider doing that with option one if you really want, maybe trying to use something like Lambda at Edge to kind of intercept specific requests to CloudFront and then serve that response dynamically using Lambda at Edge.\nSo again, option number one, even though it seems very static, you can still do something more dynamic if you really want and you have an opportunity to explore Lambda at Edge.\nSo, yeah, I think that's probably more than enough ideas for how to build applications and websites on AWS with a bunch of different architectures.\nSo I guess let's move to project number three.\nJust before we do, it's probably worth stating in the interest of fairness that there's also a HTTP.dog.\n\n\nEoin: If you want to go down a more data science path and learn how to store structured data on AWS and run analytics queries, there's a huge amount to learn here and I think this is a really big growth area and one where there's a lot of skills sought.\nSo it's really one good one to get into.\nSo we found a cat breed dataset on Kaggle again with more than 65,000 records and pictures.\nThere's a link in the description.\n\n\nThis dataset can be used to train ML models, but since we have a lot of data, we can also use it to just try out some data analytics and exercise those data analytics muscles.\nSo we could take the index CSV file where every record will reference a cat picture and provide other labels like the breed of cat, age, gender, size and coat.\nSo let's say, what could we do with this data?\nWe might want to run some queries and find out what's the most or least common age in this dataset.\n\n\nWhat's the distribution of gender and size?\nOr we could even try to combine different attributes and figure out what is the most and least common combination of breed and gender.\nSo some very simple statistical operations.\nNow, of course, this is not big data.\nYou don't necessarily need to rely on the power of cloud.\nYou could do this in Excel probably pretty easily.\nOr you could write a local script or a notebook using Python and pandas and process a CSV file that way.\n\n\nBut you can still use these small datasets and use very powerful cloud services just to get really quick results and then try and think about experimenting with larger datasets.\nThere's lots of datasets out there, including Amazon has a public open dataset with a public bucket where you can pull down much, much larger datasets.\nSo this is more like step one on your journey.\nSo since you're here to learn about AWS, what could you do with this in the cloud?\n\n\nSo some options would be option one, say, sticking with the idea of a simple notebook.\nYou can use SageMaker Studio or SageMaker notebooks and load in the CSV file using pandas and some of the Python data science kit.\nMaybe if you're into R, you can also do RStudio now in SageMaker as well.\nOption two would be to put the data in an S3 bucket then and to use other services.\nAthena being the probably the most obvious example.\n\n\nSo you would create like an external table in Athena and then you could start running queries there.\nAgain, with a small dataset, you're not really showcasing the power of Athena, but you're showing how you can query data on S3.\nThese projects are always a little bit more fun if you can add some visuals in there as well.\nSo apart from visualizing things in your notebook, you might also want to try some BI dashboards and spin up QuickSight on Amazon as well and try some data visualizations.\n\n\nYou can do some really cool stuff there as well.\nSo for people who are looking to kind of get into basic data analytics and progress a career and look at data science and data engineering on AWS,\nit's a good place to start and you can grow from there and then start looking at all the other services like Glue and Elastic MapReduce,\nEMR and many more, even things like Lake Formation if you're getting into enterprise data engineering.\nSo I think that's number three covered. What have we got for our final exercise?\nSo another idea could be still focusing on the realm of application development, more specifically in the realm of APIs development on AWS.\n\n\nLuciano: We could still use the same cat breed CSV file that we mentioned in the previous idea,\nbut this time, rather than just using for data analytics, we could use it as a data source and build an API on top of it.\nSo we could expose some of this data to a RESTful API.\nAnd one idea could be, OK, what kind of APIs can we expose?\nFor instance, people might want to know what are all the different breeds of cats that are known, at least in this dataset.\n\n\nSo we could create an endpoint called slash breeds. When you call it, you get this list with all the names of the different breeds.\nThen because this dataset has pictures, maybe you also want to list all the pictures for a specific breed.\nThat could be funny. I don't know if you're trying to allow other people to build a mobile application where you can see which breed is the cutest or something like that.\n\n\nMaybe you can display pictures and create a little game that way.\nSo if you're trying to build the API behind it, an endpoint could be slash breeds slash breed ID and then slash pictures.\nAnd that should give you a list with all the pictures that are available in the dataset for that particular breed.\nAnd of course, you might also think how to make those APIs paginated. This is on you to decide exactly what the shape of the API will look like.\n\n\nAnd finally, again, there could be another idea that you can just have a /random endpoint that just gives you a random picture and the details of that picture.\nSo how can we build this API? We already mentioned API Gateway and Lambda, and this is definitely a very valid solution.\nBut there is an opportunity here to experiment a little bit more. And you are not limited to REST.\nSo why not try something like GraphQL? So maybe you can also think about AppSync, for instance, as a way to build an equivalent version of this API,\nbut that exposes the data through GraphQL. In both cases, you still need to think about the data.\n\n\nWhere do we store all this data? Right. And it will be fine to store it in a stream, but every single time there is a request,\nof course, you don't want to load a big multiple megabytes CSV and manipulate it in real time.\nYou probably want something a little bit more structured so that you can respond to the APIs very, very quickly.\nSo an idea here is why not use DynamoDB? So that could be an opportunity to try to figure out, OK, given a bunch of data,\nhow do I store it in DynamoDB so that I can query it efficiently for this particular use cases that I have in mind?\n\n\nOr again, you can still think about, OK, I'm going to be a little bit more traditional, spin up an RDS, put the data there and then query through SQL.\nIt's really up to you to decide how do you want to store the data and how do you want to consume it?\nAnd another thing that you could do is, again, try to think, how do I serve all the pictures?\nBecause, of course, this is going to be the majority of your traffic. If people are using this API and then they want to eventually have access to all the pictures,\nyou still need to be able to provide those pictures. There is a little bit of a shortcut there, because if you look at the CSV,\nin the fields that you have for every record, one of the fields is a public URL that is already on CloudFront.\n\n\nThen you can just use that URL to provide access to the actual picture.\nBut it might be interesting to try to think, OK, what if I had to do that myself? How do I actually expose this information?\nAnd again, you can just go down the route of, OK, I'm going to put all this data in S3, then I'm going to use CloudFront as a way to efficiently serve all the data.\nAnd then you can get links directly from CloudFront.\n\n\nThere are additional concerns that you might try to think about and see what kind of solutions are available on AWS.\nFor instance, one would be authentication. What if you want to allow only authenticated users to consume this particular API?\nAnd of course, if you are hosting all of this, you will have to pay the bill on AWS.\nSo maybe you want to offload some of that cost to your users. That's why they might be needing to have an authentication so that you can actually track the usage.\n\n\nAnd it's actually really cool if you use API Gateway that you can easily create API keys and then you can create usage plans attached to those API keys.\nAnd that way you can make sure that a user is not abusing the system and making too many API calls that will result in an increased bill on your side.\nSo you can experiment with all these ideas. Similarly, you can experiment with documentation.\nHow do you serve a documentation to the users? And again, API Gateway supports some degree of Swagger based or an open API based documentation formats.\n\n\nSo you can try to experiment with those as well. And there are other topics like, for instance, can we do caching?\nThis data is quite static at the end of the day. So maybe it makes sense to think about should we be using a layer of caching?\nAnd again, API Gateway has some options that you can explore.\nSo I think this is just a very interesting project. And it's if you are coming into AWS as an application developer, I think it's very important to understand how do you build an API on AWS?\n\n\nBecause this is one of the most common topics as a developer that you will need to face when building projects on AWS.\nSo really recommend that to try to experiment with this idea if you're going down that path of learning AWS as a web application developer.\nSo this is all we have. These are just four projects that you can try to experiment with if you want to learn more about AWS.\nLet us know if you like them. Let us know if you come up with some variations of these ideas or maybe if you don't like cats and you prefer other pets, definitely send us links for your projects if you end up implementing them, because we would be really, really curious to see them live and working.\n\n\nAnd one last thing that I want to mention is that if all these projects are a little bit scary to you because you don't really know where to start and you want something a little bit more guided, like you want to see somebody actually building something on AWS to give you the confidence that you know which steps you need to follow and you have some ideas on how to go from zero to something actually working.\n\n\nWe actually did a series of live streams where we built an application that is like Dropbox Transfer or basically an application that allows you to upload files in S3, get back a link and then use that link to share the file with somebody else.\nThis is an application we built live so you can see all the live recording. We will put a link on the description and that can be another thing that you can try to rebuild yourself as another exercise to learn more about AWS and some of the more common services that people use on AWS.\nSo with that being said, thank you very much for being with us. Remember to like and subscribe and give us reviews and feedback and we will see you at the next episode.\n"
    },
    {
      "title": "59. What will Serverless 2.0 look like?",
      "url": "https://awsbites.com/59-what-will-serverless-2-0-look-like/",
      "publish_date": "2022-11-18T00:00:00.000Z",
      "abstract": "The definition of serverless has already changed in the few years since it first emerged. There have been many success stories using serverless - in startups and the enterprise. But what comes next?\nIn this episode, we will clarify our definition of Serverless, what are the main challenges with it today, and speculate on what we believe will come next!\nBy the end of this episode, you’ll have heard our thoughts and predictions on what Serverless 2.0 will look like. We will also reveal who we think will be the main challenger to AWS for domination of serverless as it goes mainstream!\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nSome of the resources we mentioned:\n\nThe SLIC Watch project for automated observability best practices\nServerless cloud\nSST (Serverless Stack)\nDarklang\nFaunaDB\nCloud Firestore\nCockroach DB serverless\nDigital Ocean\nOur series of live streams where we build a serverless WeTransfer clone\n\n",
      "transcript": "Eoin: The definition of serverless has already changed in a few years since it first emerged.\nThere have been many success stories using serverless in startups and in the enterprise.\nBut what comes next? By the end of this episode, you'll have heard our thoughts and predictions on what serverless 2.0 will look like.\nWe'll also reveal who we think will be the main challenger to AWS for domination of serverless as it goes mainstream.\nI'm Eoin, I'm here with Luciano and this is AWS Bites.\nThere are a couple of ways to look at serverless today and the term itself is a bit problematic.\nBut you have to say that the trend towards serverless is very real. Luciano, what are some of the ways that you can define serverless?\nYeah, you're absolutely right.\n\n\nLuciano: One of the problems with serverless is that there isn't really a clear definition. There are many definitions.\nSo I'm going to try to summarize all the definitions I know and maybe come up with a somewhat useful definition of serverless.\nBut yeah, the first one that at least historically has been around is the concept of function as a service.\nSo that whole idea that you can just write a function, it's event based, somebody is triggering your function, you run some code, you get stuff done that way.\n\n\nFor a long time at the very beginning of the serverless trend, that was serverless in itself.\nThat's what people associated when they were talking about serverless, they were immediately thinking about function as a service.\nBut in reality, I believe that there is a lot more to it. Like it's not just function as a service.\nEven though function as a service is probably one of the biggest part of the serverless revolution.\n\n\nSo I suppose in a more generical sense, you want to focus on trying to reduce the amount of code and the amount of operations and maintenance that you are doing when building something.\nAnd you can do that by using more managed services.\nSo basically you want to try to stay as close as possible to the business needs and delegate all these concerns that are more on the infrastructure, on the scalability side, on the security side.\n\n\nAnd that's not just from a development perspective, but also from a cost perspective.\nBecause you want to pay for what you use, you don't want to pre-allocate a bunch of money somewhere in a pool and just say,\nokay, this is going to be there for my cloud spending.\nOr you actually want to organically grow your spending as your business gets more and more successful.\nSo that's another very important idea for serverless.\n\n\nGiving you that elasticity in pricing, you start very, very low, possibly zero when you are experimenting with a new business idea.\nAnd then as you grow and as you are more successful in the market, there should be there the opportunity to grow your cost organically with your success.\nOther than that, in general, when we talk about managed services, we can distinguish that in different areas.\nWe can talk about compute, databases, file storage, event API, scheduling and orchestration.\n\n\nAll these things are at the end of the day tools that you need to build your applications.\nAnd cloud providers should kind of lower down the barrier to entry in this concept of serverless.\nBecause again, the definition is that you focus on building things that the business needs, not necessarily on the how and supporting that.\nBut more on let's serve the business, let's make it grow.\nEverything else is something that we just buy when we need it.\nSo that would be my definition of serverless. It doesn't really mean that there are no servers, but it's what we don't think as much about servers.\nWe think more about business capabilities and how them support the business.\nSo what do you think are the challenges that serverless is facing today?\nI completely agree with your definitions there.\n\n\nEoin: I think some of the challenges we see today are not everything is available in proper full serverless mode.\nSo if you look at AWS, we can still see that with serverless search, we don't really have a good solution.\nDatabases are getting there, but sometimes it's more of a miss than a hit.\nAurora serverless isn't quite there yet fully.\nNeptune, the recent announcements, kind of frustrated people because it doesn't seem very serverless.\n\n\nThings don't seem to scale to zero in terms of cost.\nWe could really do with an elastic cache serverless version or some sort of cache that's fast and serverless.\nAnd Kinesis, although we have Kinesis serverless, it's still not quite there yet in terms of scaling to zero.\nSo you kind of have to look to third parties for things like that.\nAnd it's kind of early days, but there's a lot of third parties trying to fill the gap there.\n\n\nSo that's one of the challenges. Now, complexity.\nPeople talk a lot about complexity with serverless, especially when you start out thinking it's simple.\nYou dive straight into it and all of a sudden you realize there are wires all over the place and you don't really understand how things are connected together.\nSo you have to realize that with serverless, you're building a decoupled, scalable, on demand system made up of these small cloud components.\n\n\nSo you ended up with a highly distributed system that's inherently more complex to manage than a less scalable, but more cohesive monolith.\nSo you have to then understand all the nuances, the restrictions and the controls from the cloud vendor, like identity and access management, infrastructure as code.\nDeployment.\nSo if we look at the cloud serverless serverless, right, like Lambda, DynamoDB, AppSync, Step Functions, API Gateway, they all require a pretty good amount of knowledge and technical expertise if you really want to run them in production.\n\n\nDynamoDB is a prime example here. It doesn't take a lot of skill to put an entry in a DynamoDB table and take it out again.\nBut if you want to actually scale that in production over time, then you really have to understand the cost, performance, scalability, how to design indexes, partitions, and if you're really going to make it work well.\nSo it's almost deceptively simple, but you have to understand what the long term impact is going to be like.\n\n\nAnother one of the challenges, local development and testing.\nSo I think that's when I look at the surveys around challenges and servers, I think this is one that comes out top.\nYou rely on full cloud environment to really replicate your production environment. There is no like for like local replica mechanism.\nSo running locally really requires you to under structure your code and set up tests in a way that you can run some parts of your code locally, but that if you really want to test it, integrated or end to end, you have to deploy to the cloud.\n\n\nSo it all places this obligation on you to put in place more effective testing and testing asynchronous event driven distributed systems is hard. There's no escaping that.\nSo the tooling has to improve there. Another one is that I suppose there's a lack of standards. So every cloud does it differently. Every vendor, Azure, AWS, Google Cloud, all the others.\nThey do things like serverless services differently. They do their functions differently. So there's a learning curve.\n\n\nYou might also be concerned about the stability of interfaces. So the way we develop serverless applications today is at the same as with the way you're going to do it in three years. Will your code still last or will it gather dust?\nSo all of this is possible to overcome, but you know, you need good training, best practices, expertise, standards and high levels of automation.\nAnd this is probably one of the reasons why serverless adoption, somewhat surprisingly, is less than expected in startups and more than expected in enterprise in large companies.\n\n\nNow usually the startups were the early adopters. That was the case for to a certain extent with serverless, but generally startups are trying to get time to market so they stick with the tools they know.\nThat's the way it should happen generally, but enterprises can afford the time and investment to invest in the tooling for this next generation of technology.\nSo you do see some of the more innovative big companies moving forward with serverless and investing in the training and the tooling.\nSo I guess with all that said, what do we think we need from the next generation of serverless or serverless 2.0?\nYeah, I think I'm going to repeat some of your points because you definitely highlighted some of the major weaknesses of serverless today.\n\n\nLuciano: So on the other hand, serverless v2 is going to try to resolve all these issues or at least some of them.\nSo the main one that we mentioned is that there are lots of different services for different types of tasks, but at the end of the day, not all of them are truly serverless in that essence that you don't have to focus on managing the service itself, scaling it, and also in terms of pricing that it doesn't scale to zero.\n\n\nI don't know, it forces you to think about like instance sizes rather than how much am I using this particular service. So definitely we need more of these serverless, first I'm going to call them, services for everything like caching, orchestration, high performance computing,\nso that coming to a new project, you don't really have to think this part is serverless, all these other parts is not serverless because there isn't a solution yet.\n\n\nIt's like you should be able to embrace serverless 100% in your architecture.\nSimilarly, you mentioned for instance DynamoDB and how misleading can that be. So, and I want to definitely echo that feeling. I am the first one that says that DynamoDB is really, really good when you know exactly how it works and when you figure out all the access patterns to your data.\nBut the truth is that if you're working in a new project, most of the time you don't know in advance of the access patterns for the future years, so it becomes very easy at the beginning to adopt but then later on, it might trick you in different ways and it's very hard to change the data as you go and implement new access patterns.\n\n\nSo definitely, and this is just not to mention all the amount of configuration that you have to put into it anyway because even if you don't have to think about databases, you just think about tables, you still need to put a decent amount of configuration in every table you create.\nAnd that might not be obvious at all to understand how that configuration will convert into scalability capabilities, or the ability to change the data as you go or the ability to perform different kinds of queries.\n\n\nThat requires a lot of expertise and again, it kind of conflicts a little bit with that idea of servers that you want to think more about business logic rather than all these details that are more operational details.\nSo I suppose that one idea there for Serverless V2 would be, can I just have a default table where I can store data and it's going to automatically scale and deal with in a way that allow me to support different kinds of query capabilities without having to think, now I need to create a new index or now I need to replicate this data in a different shape somewhere else.\nHopefully all this stuff can be somewhat automated and give the users just the ability to think more about the data rather than how to make it work at scale.\nNice.\n\n\nEoin: Other than that, another topic is probably observability. It's something we covered in the past in other episodes and we also invested in some ways that allow people to automate all the observability concerns.\n\n\nLuciano: By the way, if you're curious, check out SLIC Watch on GitHub. We'll give you a link on the show notes.\nBut the idea is that if you want to do good observability today, it's definitely possible. There are lots of tools in AWS and all these tools are actually quite good.\nIt just takes a lot of time to configure all of them correctly and connect all of them together and instrument all your code to really take advantage of these tools.\n\n\nThere is very little automation and with the experiments that we have done, with the research we have done, we realized that there is a lot of room for automation.\nIt's just AWS is not investing enough in this area yet. So this is the reason that we hope to see more in serverless v2.\nAnd similarly, this is also another topic we discussed about in the past. As a developer, there is this kind of developer experience where on one side you are feeling, okay, I'm just writing functions.\n\n\nAnd I feel a lot more able to focus on the business logic when I'm writing this one function that really encodes a specific workflow and I don't have to think about everything else.\nBut then in reality, when you go and deploy that function, it never really works the first time because one time you forgot a permission. Another time, maybe you didn't configure the networking correctly.\nThere are so many things that can go wrong from how that particular function is wired into the rest of the cloud environment.\n\n\nSo one solution to this problem could be that you have a better tooling that can somehow look at your code and also look at the rest of your architecture and your environments and make sense of maybe you are forgetting to create a policy and can tell you in advance,\nmaybe before you deploy, you are using this SDK call that is going to require you S3 access. I don't see any policy attached to this function that gives you S3 access. Are you sure this is really what you want to do?\n\n\nSomething like that would be already very useful. And I saw that some time ago on GitHub, somebody was trying to build something like that, but it feels like the need is clear, but still not.\nWe are not seeing a lot of investment in that direction. And hopefully, again, wishes are that serverless v2 will address also this problem as well.\nThen I have other one. I'm going to go quickly through them. Faster deployments. We know that deploying a significantly big serverless application might take, I don't know, even 15 minutes or more.\n\n\nAnd you don't want to be spending 15 minutes every time you are doing a deployment. Imagine you are fixing a very bad issue in production. You want to deploy it as quick as possible. You don't want to be waiting for a pipeline to trigger and then spending the next 15 minutes looking at it, hoping it will go faster.\nAnd similarly, infrastructure as code, it's an area that we have seen a lot of innovation, actually, if we think about serverless framework, some, but all this innovation seems to have focus around the function as a service aspect of serverless.\n\n\nIt's making it easier to configure everything related to functions. But for everything else, you are still writing like barebone infrastructure as code where you need to specify every single detail.\nSo hopefully the same trend that we saw around functions to making them simple, how to generate some policies, how to generate some of the wiring around networking and security, maybe we can see the same also for other parts of the infrastructure of different kinds of serverless architectures.\nSo given what we just said, what do we think is going to happen there? Is this something that, I don't know, today we are seeing somebody in this space already addressing some of these concerns?\nYeah, I think there are examples, right? If we look to some of the third parties, we can see innovation that is already maybe a generation ahead of the cloud vendors in terms of the serverless offerings.\n\n\nEoin: And maybe this has already been the case. Like we can see from the history, we had Heroku and Parse and all these other players building platforms as a service that try to make it easier for developers to adopt new technologies.\nAnd we're seeing examples of that in the serverless space now. So they might provide us with some kind of clues as to where the next version of serverless is coming from.\nOkay, so let's look at a few examples. Vercel is one that immediately jumps to mind. So it's mainly for front end and agile functions, sorry, agile functions, edge functions.\n\n\nBut they've really had success in terms of adoption because they made it easy for developers to connect a repo, get automatic deployments, monitoring environments, branch previews, and just make that experience really simple.\nSo whoever is looking at product management in these cloud vendors, I'm sure they're looking at these as ways they can simplify the whole process.\nAnd you mentioned serverless framework. And I think this year, maybe late last year, they launched their serverless cloud.\n\n\nAnd that's also trying to address all of these same challenges, right? With faster deployment, seamless local front end with a cloud backend.\nSo you can do front end development and integrate it into your cloud, but it's all updated locally.\nGetting rid of the YAML configuration altogether, simplifying data storage and access like we talked about, simplifying events as well.\nAnd their approach is basically they'll do that for you and run it in their cloud infrastructure, which is going to run on top of a public cloud anyway, but it's all abstracted away from you.\n\n\nSo you don't see what the raw resources are under the hood.\nOn the other hand, you have examples like SST from the serverless stack.\nAnd when I started learning serverless and experimenting with serverless years and years ago, I was using the serverless stack tutorials, which are really, really excellent documentation on getting up to speed with serverless.\nI'm really happy to see them continually innovating. And the SST tool is really taking off.\n\n\nIt's really one to watch right now. Right. So this is open source tooling.\nIt provides a simpler level of abstraction for building applications than we're used to.\nBut it's still your account and your resources. So it's still creating Lambda functions and everything under the hood.\nIt builds on top of the CDK, adds a nice UI.\nThey've got loads of starter examples and all in all tries to make the experience a little simpler, like with Vercel or with serverless cloud.\n\n\nBut you still have your own infrastructure and you can still see what the raw pieces are under the hood.\nAnother one that you might take inspiration from, just because I suppose it's so futuristic in some ways, is Darklang.\nAnd Darklang is, I almost see it like a concept car, whereas it's not a car you're ever going to buy.\nBut it might give you some kind of indication as to where the future is headed, because the interface is very different.\n\n\nThey're trying to provide a UI and a back end and everything all in one application.\nSo you don't use your normal editor or your source provider. It's all in Darklang.\nAnd you code up to data sources in this kind of visual editor.\nThe whole point that they're trying to push towards is zero deployment time. So sub 20 millisecond deployment time.\nSo as soon as you edit your code, it's already deployed and you just turn it on with a feature flag.\n\n\nThe appeal of that is massive for me, because I think because local development in serverless isn't really possible,\nyou spend your time trying to optimize your deployment of the cloud.\nGetting it down from 30 seconds or 45 seconds down to zero seconds would be a big win.\nAnd the last thing I suppose we could mention, just in terms of looking at where the innovators are coming from,\nthere's a load of serverless databases coming out all the time now.\nAnd I think they're really pushing the simplification of data storage and data access and data scalability.\nSo some dimension are FaunaDB, which is really big in the serverless space.\nFirestore from Firebase is, has been around quite a long time, actually,\nbut it's a good example of making that developer experience simpler.\nCockroachDB is another one. And then you have lots of kind of domain specific databases, if you like,\nlike content backends, headless CMSs like Sanity. I think we covered some of those on previous podcasts.\n\n\nLuciano: Yeah, I really like that you kind of somehow underlined that there are these two different approaches where on one side,\nyou are trying to simplify the space and make it more managed.\nBut at the same time, you are abstracting a lot like it's not clear what's actually running under the hood.\nWhile there are other approaches that are just giving you extra layers that are simpler.\nBut then at the end of the day, it's very clear how these layers translate to actual infrastructure.\n\n\nYou are still in control of that infrastructure. So it feels like more tooling than actual new solutions.\nAnd I really like that. I'm not sure if I have a preference for one or another.\nBut I think at the end of the day, they both try to improve the developer experience in different ways.\nGiven also what you said, looking at all of this innovation,\nwe mentioned that there will be players competing outside the main cloud vendors,\nbut also the cloud vendors could be doing something in this space to improve themselves.\nSo what do you think will happen in the future?\nYou're going to see a mix because different people require different modes, depending on the context.\n\n\nEoin: If you're a startup and you're just focused on speed, time to market, then simple tooling that abstracts everything for you.\nAnd you don't have to look at all of the complexity under the hood might make a lot of sense.\nOr even for enterprises building line of business apps where it's just low code.\nThey're just looking for low code, simple applications.\nI would still say like cloud vendors are going to grow the serverless offerings and hopefully simplify both the services and the tooling.\n\n\nWe mentioned that AWS SAM is kind of going in the right direction.\nThat should make serverless more compelling for startups and enterprises.\nBut we can also see services are getting simpler, especially the newer services like EventBridge, Step Functions.\nThese are really changing how we think about serverless, right?\nBecause they're completely different offerings.\nThey have loads and loads of features, but they're not as complex to manage as some of the services that have come before.\n\n\nSo I think we can expect a lot more of both types of innovation from the cloud vendors.\nAnd as well then with the startups and the innovators, third parties.\nThe issue there, I guess, is that not a lot of large companies are going to build critical infrastructure on top of platforms from startups.\nBecause of the perceived risk there.\nUnless those startups then go and get acquired by the major cloud vendors and get strengthened and grown from there.\n\n\nSo that's going to be a challenge for some of the players like serverless cloud and also some of the serverless database vendors.\nThey have to mature and validate their offering and reach a certain critical mass before lots of people will flock to them.\nWe've been here before with Parse, where it was acquired by Facebook and later shut down.\nAnd people have built on top of that mobile backend and lost out as a result.\n\n\nIt does lead us to that initial tease, right?\nWho is the best placed challenger for AWS when it comes to the next wave of serverless?\nAnd I throw it out there that the most likely challenger is Microsoft.\nAnd it's not because of Azure, although that is certainly a strong challenger.\nBut since Microsoft released VS Code and acquired GitHub and then NPM became part of that ecosystem as well,\nthey've invested a lot in developer tooling and in owning the places where developers live.\nSo now you can develop code with cloud-based editor or with VS Code on your desktop.\nYou can build and deploy with GitHub actions to a GitHub repository.\nYou can have copilot write half the code for you, albeit with some dubious results.\n\n\nLuciano: But it's not a big leap for Microsoft or GitHub to add lower code compute and database and keep building out that developer-centric ecosystem.\n\n\nEoin: And it makes perfect sense for that to be very serverless in nature.\nSo it might be through Azure or it might be done with different branding or a different take.\nBut I think Microsoft have positioned themselves very well here.\nAnd while AWS is obviously a strong market leader, I think there's a good challenge there.\nMicrosoft have been smart in how they've aligned themselves well with the developer community.\nWhat do you think? Do you have any other challengers to AWS?\nI like your idea that Microsoft maybe, if you look at Azure, I don't know if I would agree, like the state of Azure right now.\n\n\nLuciano: But given how Microsoft is positioned more globally, I absolutely agree with your assessment.\nThey are in a unique position to build something new.\nAnd if they connect all the dots that they already have there and make them even better, I think they are up to something really, really interesting.\nSo I think it's a good bet. But I have maybe a little bit more controversial one.\nAnd I've been really looking at DigitalOcean in the last few years.\n\n\nAnd I think it's an interesting one, even though you wouldn't really associate them with serverless today.\nBut I really like their approach. They started by just giving you visual machines. It was very simple to get one.\nThe documentation and the all-over user experience was really, really good.\nThey put a lot of effort into designing all of that experience.\nThey put a lot of effort into creating guides, tutorials, case studies.\n\n\nAlso, the pricing model is extremely simplified.\nThey never really wanted to give you anything that could scale to massive levels.\nBut for starting, building a small startup, you always got a good offering.\nAnd over the years, they have kept the identity where everything is simple, everything is accessible, the pricing is clear.\nLots of tutorials and good development experience.\nBut interestingly enough, they also started to expand their own offering.\n\n\nThey started to give databases, Kubernetes, functions as a service.\nSo it's really interesting to see if they start to invest more into this serverless space.\nWhat's going to happen in the future? Because I really like the way that they're focusing on the developer experience.\nSo probably they are in a position to address some of the challenges that we just described.\nWhich at the end of the day, they are mostly all around the developer experience.\nAnd now developers are empowered to actually build cool stuff.\nSo that's my bet.\nI might have other ones. For instance, I really like the direction of things like Code Sandbox or similar tools.\nBut you might argue that what GitHub is already doing with Codespaces is pretty much in line with that.\nSo that might be just a component that maybe somebody else is going to acquire.\nIt's going to require one of the successful comps to be more competitive in that particular direction.\nI really like that. It's a really interesting alternative option.\n\n\nEoin: I think all this competition in space hopefully makes it better for everybody.\nAnd that's something that we really need.\nSo I guess the question as we finish up here is where would you place your bet?\nOn AWS, DigitalOcean or something else completely?\nAnd what do you think is the future for serverless?\nThanks very much for joining us today. Let us know all your comments on Twitter.\nLet us know what you think of all those questions. And thanks for joining us again.\nThat's all for today. We'll see you next week.\n"
    },
    {
      "title": "60. What is AWS Lambda?",
      "url": "https://awsbites.com/60-what-is-aws-lambda/",
      "publish_date": "2022-11-25T00:00:00.000Z",
      "abstract": "AWS Lambda is one of the most famous AWS services these days. If you are just starting with your cloud journey you might be confused about what Lambda actually is, what are the limitations, and when you should be using it or not.\nIn this episode, we provide a beginner-friendly introduction to Lambda and summarise everything there’s to know about it: when to use it and when not, differences with containers, the pricing model, limitations, and integrations.\nBy the end of this episode, we will also chime in with some of our opinions and share whether we believe that Lambda is the future of cloud computing or not!\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nSome of the resources we mentioned:\n\nThe original announcement of AWS Lambda from 2014\nUsing Lambda for High-Performance Computing (AWS Blog Post)\nLambda vs Fargate vs EC2 pricing (article by Eoin Shanaghy)\nOur previous episode of what serverless v2 will look like\nLambda tiered pricing announcement by AWS\nUnderstanding AWS Lambda Scaling and Throughput (Video by Marcia Villalba &amp; Julian Wood)\nKeet by Holepunch (P2P messaging app)\nSocketSupply (a company building a P2P platform)\n\n",
      "transcript": "Luciano: AWS Lambda is one of the most famous AWS services this day.\nIf you're just starting your cloud journey,\nyou might be a little bit confused about what Lambda actually is,\nwhat are the limitations, when you should be using it or not.\nToday, we want to provide a beginner-friendly introduction to Lambda\nand summarize everything that there is to know about it.\nBy the end of this episode, we will also chime in\nwith some of our opinions and share whether we believe\nthat Lambda is the future of cloud computing or not.\n\n\nMy name is Luciano and I'm joined by Eoin,\nand this is AWS Bites podcast.\nAWS Bites podcast is sponsored by fourTheorem.\nfourTheorem is an AWS consulting partner offering training,\ncloud migration and modern application architecture.\nFind out more at fourtheorem.com\nand you'll find the link in the show notes as well.\nSo again, as we said in the introduction,\nthis episode is mainly focused to provide an introduction\nfor people who haven't really explored serverless and Lambda yet.\nSo if you already know Lambda,\nI don't know if this episode is really that interesting for you.\nMaybe you can just skip to the end,\nwhere we talk more about opinions and the future of the Lambda.\nAnd in any case, let us know what do you think about all of that.\nSo without any further ado, what do you say, Eoin?\nShould we start by defining what Lambda is?\n\n\nEoin: Yeah, and I went back to the 2014 announcement from AWS\nwhere they announced Lambda for the very first time,\nand there's a really nice description in there\nwhere they say that it's a compute service\nthat runs your code in response to events\nand automatically manages the compute resources for you.\nAnd that's it in a nutshell. I really like that.\nIt's a beautiful description.\nAnd there's a lot of features and additions they've made since then,\nbut we probably forget that that's the simple beauty of Lambda at its core.\n\n\nSo they also say that it makes it easy to build applications\nthat respond quickly to new information or new data.\nSo that really covers it, right? It's code that responds to events.\nSo it's really event driven,\nand it manages the compute resources for you.\nAnd that's really the most important part.\nSo it's like an alternative way to do compute, right?\nWe've gone from having to make servers, then virtual machines,\nthen we went to containers,\nwe got cloud computing to manage all of this for you.\n\n\nSo all the time, we have these new generations of compute services\nthat are taking away the complexity\nand managing a lot more for you on the cloud side\nso that you have to do less and less.\nNow, sometimes that results in more complexity.\nIt kind of shifts the complexity to other places.\nBut with Lambda, what we're talking about is a stateless, event-driven.\nYou hear this quite often. So what does stateless mean?\n\n\nWell, it means, I guess that in some applications,\nyou might have a server that keeps running\nand it's able to store state in memory,\nor you might have like user sessions.\nLambda is very ephemeral, right? It's very short-lived.\nThey only run for up to 15 minutes.\nSo you don't have the luxury of storing state.\nAnd that also brings benefits because you don't end up\nwith accumulating what we call a cruft,\nyou know, lots of state that accidentally accumulates.\n\n\nAccumulates in long-lived compute.\nSo these things only run for a short amount of time.\nIf you have to store any state,\nthen you have to put it explicitly into a state store like DynamoDB\nor ElastiCache Redis or S3.\nBecause it's event-based as well,\nit's a very different model to like a web server.\nIf you've got a web server with a web framework,\nyou typically are able to handle multiple concurrent requests\nwithin your unit of compute.\n\n\nSo within a container or virtual machine with Lambda.\nIt's weird in the sense that you can only run one event at a time\nin a container, an invocation.\nAnd this is counterintuitive to people,\nbut it also makes it very easy to reason about some cases\nbecause you know that at any given time,\none sandbox, they call it, a container,\nis only processing one request at a time.\nThey're very quick to scale up and down.\n\n\nNow, sometimes you have the issue of cold starts,\nwhich are often overstated, I would say,\nbut something to be aware of, definitely for sure.\nIt supports multiple language runtimes\nand you could create your own runtimes.\nAnd because it's managed, the instance is managed,\nyou don't have to worry about what machine it's running on.\nThe container orchestration service is managed.\nThe wiring of the events and the results\ninto the Lambda is managed for you and the runtime itself,\nwhether it's Node.js or Python or whatever else, is managed for you.\nAnd this is why it's called serverless,\nit's because you don't have to manage servers.\nSo that's as much as I could say about the definition of Lambda.\nWhat are some of the use cases, right?\nIf people haven't used it before,\nif they've only played with it a little bit,\nwhere can I start applying Lambda day to day in development jobs?\n\n\nLuciano: Yeah, so the most common use case that I've seen with Lambda is probably into the realms of APIs.\nSo creating REST APIs, GraphQL APIs,\nLambda is a very, very good backend\nfor implementing that compute layer.\nWhat is the business logic that specific API quest\nneeds to actually execute?\nAnd it works really well in combination\nwith other services like API Gateway.\nSo generally, we'll define your API through API Gateway\nand then have Lambda as a backend for the actual business logic.\n\n\nAnother good use case for Lambda is Webbooks.\nWebbooks is basically when you have a system\nwhich generates some kind of event\nand it can notify another system about that particular event.\nAnd that generally works just through a simple URL.\nSo the system generating the event needs to know a specific URL\nwhere to actually forward that particular event.\nAnd one example could be you are using a newsletter system,\nlike, for instance, MailChimp.\n\n\nMailChimp gives you the opportunity\nthat every time you have a new subscriber,\nyou can notify a Webbook endpoint\nand receive the information about the new subscriber.\nAnd that way you can implement your own custom integration.\nI actually use something like that and I use Lambda as a backend.\nYou can simply create an API using API Gateway.\nThat way you get a URL or you can also use Function URLs\nand that can be used to trigger a specific Lambda\nwhere you implement your own custom business logic.\n\n\nAnother use case could be system integration.\nSo again, a webhook is kind of a system integration already,\nbut you can have more advanced types of system integration\nusing different protocols.\nAnd Lambda can be the place where you write your glue logic,\nfor instance, even if you need to convert data from one format\nto another to make it possible for different systems\nto communicate with each other.\n\n\nOther use cases are background processing,\nvery, very common for Lambda.\nOne of the most common tutorials you'll find out there\nis how to create image thumbnails using Lambda.\nSo that's definitely a use case\nthat in the space of background processing,\nyou get pictures somewhere, you can load these pictures,\ncreate multiple variations of this picture all in the background\nwhile your application is still running somewhere else.\n\n\nAnother interesting use case,\nthis is a little bit more of a new use case, I feel.\nIt's something we've been talking about before.\nAnd we also did a joint blog post with Amazon itself\nand it's in the space of high-performance computing.\nIt is an up-and-coming use case for Lambda,\nbut you can definitely use Lambda also in this particular space.\nAnd we will post a link in the show notes\nwith the blog post I just mentioned,\nwhere you can find all the details about this particular use case\nand how we actually use Lambda to be able to fulfill the requirements.\n\n\nVery similarly is the space of ETL.\nWe already mentioned that Lambda can be a very good layer\nto load data, transform it, and start it somewhere else.\nSo you can also use it that way just to perform ETL kind of workloads.\nAnd there are some more esoteric use cases, I feel,\nbecause I mean, I've seen that in the documentation,\nbut I haven't seen them being actually used in practice.\nBut for instance, if you use RDS, which is the database service,\nSQL database service that AWS gives you,\nyou can actually create custom functions in the database\nand then use Lambda as the compute layer for this custom function.\nSo that basically means that you can\ninvent your own Postgres function,\nand the business logic lives in Lambda.\nAnd AWS will take care when you're using that custom function\nin your SQL statements to actually invoke your Lambda\nto perform specific custom operations.\nSo with that being said, we have a bunch of use cases in mind.\nBut I suppose that there are also a lot of cases\nwhere you wouldn't really want to use Lambda, right?\nIt's not a silver bullet for every use case.\nWhat do you think, Eoin?\nFor sure, yeah.\n\n\nEoin: For like if you have a long running job that you just can't split,\nor it doesn't make sense to split it into something\nthat runs in a few minutes, why bother with Lambda?\nYou would just try and use a simple container service\nthat could just stay up for the duration of the job indefinitely.\nIf you've got something that's stateful,\nand that would be often legacy applications,\nlegacy web servers that are using sessions\nthat require in-memory storage and a server that stays up,\nthey're just not a good fit.\n\n\nSo if you find yourself with a legacy stack,\nand you're trying to shoehorn it into Lambda,\nmaybe you should think it's probably not worth the effort.\nAnother thing is really important to notice.\nWe talked about this being event-driven, right?\nEvery event comes into Lambda over HTTP,\nand it comes into the service, then gets processed by your function.\nThere's no kind of open TCP connection support.\n\n\nThere's no streaming support.\nSo you can do things like Web Sockets,\nbut you're not really using Lambda.\nYou're using API Gateway to achieve that.\nSo you don't have anything where you need a socket or an open connection,\nor you're kind of real-time streaming\nwhere part of the data gets processed.\nAs the socket stays open, that's not a fit for Lambda.\nSo if you can imagine a real-time game server, it's not a good fit.\n\n\nBut speaking of payload, when your payload,\neither the request or the response, is bigger than six megabytes,\nthat's also not a fit for Lambda because that's the limit.\nAnd if you've got high constant predictable load,\nso from a pricing perspective, right, let's think about your traffic.\nIf your traffic is constant and doesn't ever drop,\ndoesn't ever peak beyond a certain level,\nthen you're probably not really taking much of an advantage by going to Lambda.\n\n\nYou might as well keep stick with something,\nespecially for pricing perspective,\nbut it's worth doing the cost calculation\nbecause maybe it's cheap in both cases,\nand it's a trade-off between complexity\nand all the other features of Lambda.\nNow, when we started and we defined Lambda,\nwe talked about Lambda as the original simple definition of Lambda.\nAnd I think it's worthwhile calling out that in recent years,\nas Lambda has added more and more features,\nyou could say that it's become more complex,\njust because there's more knobs to twiddle,\nmore configuration options, and more things you can do with it.\n\n\nSo if you look at some of the recent features,\nyou have provision concurrency now,\nand you have support for destinations and\ndifferent architectures including, you know,\na lot of other features,\nand you have a lot of other features that you can do with it.\nSo you can take advantage of that.\nAnd you can also do that with Lambda.\nAnd I think it's worth calling out that\nin recent years, as Lambda has added more and more features,\nyou could say that it's become more complex,\njust because there's more knobs to twiddle,\nmore configuration options and more things you can do with it.\n\n\nSo if you look at some of the recent features,\nyou have different architectures, including ARM,\nand different options in event source mappings,\nand EFS support and your VPC options.\nAnd this is only going to grow and grow and grow\nas Lambda grows and evolves.\nSo it's worth kind of reassessing Lambda and thinking,\nwell, is this now too complex for my needs?\nI thought this was something simple that I could just fire up,\nand I didn't have to worry about, oh, which container or instance\nshould I run my code on?\n\n\nI could just have a nice ephemeral piece of compute\nthat runs in response to an event.\nSo now has it become complex?\nI can reconsider my options and go for containers.\nI think this is worth thinking about.\nIt's a topic of conversation these days.\nAnd if we talk about containers,\nyou could compare it to deploying with ECS\nor to Kubernetes with EKS or another option.\nAnd I wouldn't say that it's inherently more simple\nto do that with, to deploy services with container-based solutions.\n\n\nI guess the difference is with Lambda is that you're more constrained\nin how you do events.\nThere's very specific ways in how you do your events come in\nand how you do logging and how you do tracing.\nAnd container environments are less constrained\nbecause they let you do whatever you want\nand integrate with whatever you want and run whatever framework you want.\nSo in some ways, they seem like an easy path to get started with\nif you have a comfortable set of frameworks you want to work with.\nBut sometimes that complexity won't reveal itself\nuntil you realize you have to manage your container environment at scale\nand you realize you have to actually understand\nhow that framework works under the hood\nand how it works when you've got edge cases\nand performance of security problems.\nSo I think it's always a set of trade-offs\nand you have to really kind of look deeply\nand kind of see what your case requires.\nBut I'm interested in your opinion, Luciano, what do you think?\nHas Lambda gone more complex?\nIs it more complicated than containers?\nYeah, I have been exposed a little bit to Kubernetes and Lambda.\n\n\nLuciano: Maybe a little bit more to Lambda than to Kubernetes.\nSo what I'm about to say might be a little bit unfair to Kubernetes,\nbut my feeling is that Kubernetes is a great tool and it's very generic.\nIt's kind of agnostic for the most part\nto the cloud provider you're going to be using.\nSo that's amazing.\nBut at the same time, I feel that it requires a little bit more knowledge\nand understanding before you can be proficient with it.\n\n\nSo definitely the barrier to entry is higher with Kubernetes\nthan it is with Lambda.\nAnd it's also an unfair comparison because, of course, as we said,\nKubernetes is a more general purpose kind of runtime,\nwhile Lambda, it's very specific.\nI try to solve one problem in a very opinionated way.\nSo, of course, it's kind of easier that Lambda has a smaller surface\nand it's easier to get started with.\nBut at the same time, that surface can get very, very big\nas you get more into the weeds\nand you start to build more and more complex serverless applications.\n\n\nAt that point, you will need to start understanding\nabout networking and security and IAM\nand a bunch of other AWS-related topics\nthat might not be something that you have done before.\nSo that surface might just bleed into a bunch of other AWS concepts\nthat you just need to master to be able to actually use serverless well.\nSo I suppose that at the end of the day,\nwhat I'm trying to say is that, yes,\neasier to start with serverless and AWS using Lambda,\nbut then as you start to build more and more complicated applications,\nthere is always a certain degree of complexity\nthat you will need to deal with\nand you need to start to build a more realistic understanding\nof the stack that you are working with.\n\n\nSo this is probably equally true in both Kubernetes and AWS Lambda.\nSo just keep that in mind.\nDon't just say one is easier than the other in absolute terms.\nAnd one of the interesting points that I heard many times people complain about\nwhen it comes to the complexity of Lambda specifically is pricing,\nbecause it's very easy to just say it's cheap and convenient,\nbut that's not always the truth.\nYou need to make an exercise and understand, first of all,\nwhat's the model and then given your specific use case,\nhow do you actually apply that model and figure out, okay,\nthis is more or less how much it's going to cost me.\n\n\nSo let's have a quick look at what is the pricing model with AWS Lambda.\nThe first thing to understand that we didn't mention so far\nis that when you provision a Lambda,\nit's really important that you specify\nwhat is the amount of memory that you want that Lambda to have.\nAnd one non-obvious thing is that\nwhen you provision a certain amount of memory,\nthat will come with very specific CPU configuration.\n\n\nAnd the more memory you configure, the better the CPU.\nSo basically, you don't control the CPU, you just control the memory,\nbut the more memory you allocate, the better the CPU.\nSo sometimes if you just want a better CPU,\nyou'll need to allocate more memory,\neven if you don't really need that much amount of memory.\nThis is just the model that Lambda gives you,\nand I suppose it's tightly related to the pricing model\nand to the allocation model that AWS needs to figure out\nwhen they really need to provision your Lambda,\nsomewhere in a cluster.\n\n\nSo once you provision your own Lambda\nand you select a certain amount of memory,\nthen the cost, there are actually two different pieces\nthat contribute to the final cost.\nOne is the execution time and one is the invocation cost.\nExecution time is literally given one Lambda,\nhow long does it run? You are going to pay for that.\nAnd invocation cost is how many millions of invocations are you doing?\nAnd there is a price on that.\n\n\nFor the execution cost is literally a function of the time\nin millisecond and memory.\nAnd just to give you an example,\neven though maybe it's not really meaningful,\nif you go for the lowest, which is 128 megabytes of memory,\nfor every millisecond, you pay 0.00000000,\nthat's nine zeros, $21.\nSo it looks like it's infinitesimally small.\nBut if you are running your Lambda function for long\nand the limit is 15 minutes,\nyou will start to actually see that cost.\n\n\nSo again, this is one of those false things\nthat might be very misleading.\nIt looks like an infinitesimally small number,\nbut it multiplies up if you actually use this feature a lot.\nSo make sure to do the maths to really understand\nwhat's going to be the cost for you.\nAnd when it comes to the invocation cost, you have 20 cents of dollars per million of invocations\nthis is in Ireland.\nI think it might be slightly different\nif you go in different regions.\n\n\nBut again, seems like a very low cost,\nbut if you actually use Lambda a lot,\nit's not unlikely that you will be doing multiple millions\nof invocations during your billing period.\nSo that cost might add up as well.\nI suppose one kind of observation that we can make\nis that Lambda can be very, very convenient\nfor when you have very spiky use cases\nor when it's very, very hard to predict\nwhat's going to be the actual consumption of the service.\n\n\nClassic example, you are building a startup,\nyou're trying to validate an idea.\nProbably two people are going to be using it\nthe very first few months\nwhile you try to validate the idea.\nBut if you end up being very, very successful,\nyou might end up very easily with like thousands of users\nin a very short amount of time.\nIf you are very lucky and successful,\nmaybe even millions of users,\nso that might just skyrocket the usage of your platform.\n\n\nAnd in that case, you didn't really have to make\nan upfront investment to support the traffic.\nSo this is probably where the convenience of Lambda\nand its pricing model is kind of at its best.\nAnd the opposite case is actually interesting\nwhere you really can predict in advance the cost\nand the use and the load.\nSo in that case, actually Lambda comes\na little bit more expensive by how much we are actually\ngoing to link to an article that Eoin, you wrote some time ago,\nwhich has some good numbers in it and compares Lambda,\nI guess, Fargate, I guess, EC2.\n\n\nBut the bottom line of that is that\nwhen you can predict the actual usage\nand that usage is pretty much constant,\nyou probably can make an effort into using\nsomething more traditional like EC2.\nOf course, assuming that you don't have a lot of TCO\ninto provision in those EC2s,\nif you just look at the compute cost,\nthat compute cost will be much lower\nthan the equivalent compute cost of Lambda.\nAnd if you think about that, that makes sense\nbecause AWS is making you pay the actual compute with a premium\nbecause they take care of all these kind of infrastructures\nspinning things up and down for you,\nwhile in EC2, all that cost is on you.\nSo it kind of makes sense that if you just compare compute per compute,\nLambda is more expensive than something like EC2.\n\n\nEoin: It's true. It's interesting also to just note\nthat since that article was written,\nthey have introduced tiered pricing for Lambda as well.\nSo if you are using a huge number of invocations\nfor batch processing,\nlike you mentioned one of the use cases earlier,\nand you're starting to get into really big volumes,\nthere is no tiered pricing.\nSo the pricing actually goes down in tiers.\nAnd so I think it's kind of moving in hopefully a better direction.\nIt would be nice to see it kind of be more comparable to EC2 eventually,\nso that we don't have to think, okay, it's too expensive.\nLet's completely change our architecture\nbecause the pricing is too expensive because you hate to have to do that.\nAbsolutely, especially because it's not going to be a small change.\n\n\nLuciano: to just move from Lambda to something else.\nSo yeah, definitely worth keeping in mind\nthat if you grow really a lot,\nthere will be this kind of pricing discounts\nthat you basically will have in your billing.\nAnother interesting topic that you briefly mentioned,\nbut I want to give a little bit more details\nis what are the limitations that you have with Lambda?\nBecause some use cases, you just cannot solve that with Lambda\njust because there are limitations in the architecture model of Lambda.\n\n\nAnd you already mentioned the payload,\nwhich is six megabytes request-response,\nbut this is true only for synchronous invocation.\nWe'll talk a little bit more about that in a second.\nFor asynchronous invocation, that limit is actually much lower.\nIt's 250 kilobytes.\nSo you need to be careful, for instance,\nif you are triggering a Lambda from EventBridge,\nthat payload cannot be in the order of megabytes.\n\n\nIt can be up to 250 kilobytes.\nAnother interesting thing is the amount of time\nand amount of source code that you can ship into a Lambda.\nAnd this is a little bit tricky because you can ship\nactually Lambda code in two different ways.\nOne is through a zip file and one is through a container image.\nIf you go for the zip file, the uncompressed sites inside\nthat zip file cannot be bigger than 250 megabytes.\n\n\nIf you go for a container, it's actually much higher than that\nbecause you can ship as much as 10 gigabytes of source code.\nNow, why is this important?\nBecause sometimes, especially when you're using languages\nthat will have big native libraries, for instance,\nto connect to databases or to perform other kinds of operations,\nyou might have very big binaries there.\nSo sometimes you just try to stuff a bunch of different libraries into it.\n\n\nMaybe some of them will end up with big binaries.\nIt's very easy to just go slightly over this 250 megabytes limit.\nAnd in that case, you need to start to think,\nokay, how do I split my Lambda maybe into multiple Lambdas\nso that you kind of reduce the size of the source code?\nOtherwise, you need to think about using containers,\nwhich is slightly more complicated in my opinion,\nbut gives you a lot more freedom in terms of source code size.\n\n\nAnd finally, there is another interesting thing,\nspeaking about running Lambda at scale.\nWe say that one event at a time gets processed in one Lambda,\nlet's call it container or instance.\nSo what happens if you get, for instance, in an API,\ntwo requests simultaneously?\nMost likely, the Lambda runtime is actually going to spin up two Lambdas,\nand each Lambda is going to take care of one of the concurrent requests.\n\n\nSo what happens if you get thousands of requests simultaneously?\nProbably thousands of Lambdas will be spawned up in a short amount of time.\nBut of course, there is a limit at some point.\nAnd that limit is by default 1000 of concurrent Lambda executions.\nAnd this is not just for one specific type of Lambda.\nThis is, if I remember correctly, across an account and a specific region,\nthat's kind of a cumulative limit.\nSo if you have lots of different APIs\nand different users are hitting different APIs,\nit's actually very likely that eventually you will bump into this limit.\nNow, this limit can be increased.\nYou just open a ticket with AWS,\nyou provide reasonable modips for having more concurrency,\nand most likely you're going to get that.\nBut there's something to keep in mind,\nthat it doesn't really scale indefinitely to massive concurrency.\nThat's a good one. Yeah.\n\n\nEoin: And while we're on that topic, I recently caught up with an episode of the Fubar Serverless podcast,\nwhich is really excellent.\nAnd I'm going to link it to the show notes here, actually,\nbecause we're not going to go deep into Lambda concurrency scaling and throughput.\nBut there was an episode of Fubar Serverless with Julian Wood,\nwhich talks about all the fine details and Lambda scaling and throughput.\nAnd if that's something that interests you,\nor if you're thinking about really getting into Lambda,\nit's a really good primer.\n\n\nLuciano: Yeah, that's a good one to call out.\nAnd one last point that I have is talking about integrations.\nOne actually of the very positive things about Lambda\nis that if you buy into the AWS ecosystem,\nLambda integrates pretty much with like almost any other service.\nSo mastering Lambda gives you the ability\nto actually connect all the different services together.\nSo it's truly the more abstract compute layer\nthat you can have to create workflows in AWS\nwhere you need to connect different components, different services.\n\n\nSo definitely one more reason to learn Lambda,\neven though you're maybe not trying to buy into the,\nlet's build everything serverless,\nyou're still going to come across Lambda for very specific use cases.\nAnd again, talking about that sync versus async execution model,\nthis is something that becomes important in this context,\nbecause you really need to understand what does it mean for a Lambda to run,\nto being invoked synchronously,\nand what does it mean to be invoked asynchronously.\n\n\nMy mental model is it is synchronous when you invoke the Lambda\nand you wait for the Lambda to give you a response,\nwhile when you don't really care about a response,\nyou just want to fire off something in the background,\nprobably you want to go for an asynchronous invocation.\nSo it's a little bit more like fire and forget.\nAnd the reason why these details are important\nbecause you also get different behaviors.\n\n\nFor instance, when there are failures,\nyou might get an automatic retry when actually multiple retries\nwhen there is the asynchronous model,\nwhile you don't really get retries,\nit's up to you to re-invoke the Lambda function\nwhen you are invoking it synchronously and there is a failure.\nSo just something to call out if you are thinking about different workflows\nand different kinds of integrations,\nlook into these two models and try to understand\nwhich one is more suitable for your use case.\nNow, I think we are getting close to the end of this episode.\nSo let's try to get into the more kind of visionary part.\nAnd let's try to discuss what do we believe\nis the future of cloud computing?\nIs it going to be more AWS Lambda-like\nor is it going to be something else?\n\n\nEoin: I think that Lambda has already been a game changer and it's kind of changed how people think about cloud computing\nand the evolution of cloud computing.\nAnd there's no going back from that,\nbut it's not an all or nothing thing.\nAnd it's not Lambda or containers or Lambda or anything else argument.\nIt's just the fact that Lambda has shown people\nhow you can build really powerful architectures,\nreally advanced systems without having to provision servers.\n\n\nTherefore, people are getting used to the idea\nthat they don't have to maintain and patch all this infrastructure themselves\nand there are easier ways of doing things.\nSo even if it's going to be using containers in the future,\nthe systems that are running them are going to get a lot simpler.\nAnd we have this convergence of functions as a service\nand the container model and blurred lines between\nwhat the capabilities are between these two compute models.\n\n\nWe see that with Fargate becoming a little bit more serverless, perhaps,\nand Lambda adopting container image support.\nThey still have very different execution models,\nbut the feature sets are expanding\nso that they're kind of impinging on each other's territory.\nSo I don't really worry too much about\nwhether it's all going to be Lambda in the future or not.\nIt's more about the direction of travel\nand how everything's hopefully just going to get much simpler.\nBut I'm definitely interested. We're just a pre-invent time.\nI'm definitely interested to see where Lambda goes next\nor reinvent in the next few weeks.\nSo what's in your crystal ball?\nIs Lambda the future of cloud computing and that's it?\n\n\nLuciano: I actually think that Lambda will be the future of cloud computing, but maybe only for the next three, five years,\nbecause I expect that something entirely new might come along\nand there might be innovation.\nAnd this is mostly motivated because I see two big trends\nthat are somewhat against, not necessarily against Lambda itself,\nbut more against the idea that you need to use\none massive cloud provider and rely entirely on it.\n\n\nAnd of course, you might argue that some of these ideas\nare kind of politically driven or socially driven,\nbut there is also a cost element to it.\nAnd I'm hearing about some interesting solutions\nwhere the idea is more that rather than relying on a cloud provider,\nyou should rely more on the computer that is available on the edge.\nBut by edge, we don't mean edge services by cloud provider,\nbut actually the devices that people use to access the services themselves.\n\n\nMobile devices, laptops, and so on.\nThere is massive compute available out there.\nAnd it is possible with technology that we already have available today\nto offload some of the computation, networking, data sharing,\ninto the devices of people actually using the service itself.\nAnd there are actually two very interesting companies\nthat are operating in this space and providing a lot of innovation.\nOne is Whole Punch, who just launched a service called Kit.io.\n\n\nWhich is more of an example of something that you can build with this model.\nAnd they have a bunch of open source libraries that you can use today\nto actually build something like Kit.io yourself.\nAnd that's going to be using totally peer-to-peer based compute and resources\nrather than relying as much on cloud providers.\nAnother company that is traveling on a very similar direction\nand is providing more of a runtime to build these kind of prototypes or projects\nis socket-supply.co.\n\n\nSo definitely look into the websites of this company just to understand\nwhat kind of new ideas they're trying to propose\nand what would be possible in the future if we actually,\nmore and more people will start to buy into this model.\nAnother complaint that I hear a lot about serverless in general\nis that if you look at different cloud providers,\nthey are kind of offering something similar\nbut at the end of the day is not standard.\n\n\nSo it's not really something you can easily abstract,\nlike build ones and ship everywhere.\nAnd I feel that there needs to be a little bit more standardization,\nlike something similar to what happened with containers,\nto try to standardize more and more the kind of serverless offering\nin terms of events, in terms of what the compute interface is going to look like.\nAnd maybe that's something that will create more innovation,\nthat might create new products,\nthat might create even new contenders in this space.\n\n\nAnd I expect that maybe a technology like Wasm\ncan have a big impact in this space.\nBut again, I'm only hearing kind of very early conversation,\nso it's very hard to predict what can happen there.\nSo I suppose that's everything we have.\nAnd I'm really curious to know what do you think about,\nwell, first of all, if you're starting to look into Lambda,\nwhat is your feeling?\nIs it something you are going to be using?\n\n\nWhat kind of projects do you have in mind?\nAnd looking more and more into Lambda,\ndo you see that kind of technology fulfilling your needs?\nOr not, and why?\nAnd if you have been using Lambda for a while,\nwhat do you think about our kind of visionary predictions?\nDo you think that they're going to be correct?\nDo you have a totally different perspective?\nIs Lambda going to be more and more prevalent in our future?\nOr we are going to be seeing something entirely different?\nLet us know in the comments, reach out to us on Twitter,\nand we will be loving to have a chat with you\nand explore more these topics.\nUntil then, see you in the next episode.\n"
    },
    {
      "title": "61. How do I control AWS cost?",
      "url": "https://awsbites.com/61-how-do-i-control-aws-cost/",
      "publish_date": "2022-12-02T00:00:00.000Z",
      "abstract": "Let's face it: when it comes to AWS, cost is one of the scariest topics!\nWhy? Mostly because the underlying model can get very complex.\nThere are too many variables and ultimately it's just hard to predict how much is a given workload going to cost you on AWS. Are going to be bankrupted by this unpredictable cost? Probably not!\nIn this episode, we share some suggestions and tools on how to approach cost when going to AWS. It's not a simple topic, but it's something you need to embrace, learn and get confident with. With a bit of effort, cost will not be so scary anymore and you'll be able to take advantage of all the awesome services and features of AWS without being so worried about cost!\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nSome of the resources we mentioned:\n\nAWS FREE Tier FAQ\nAWS Activate Program\nFourTheorem on MAP (Migration Acceleration Program)\nWhat are the benefits of tags (past episode):\nAWS Horror stories (past episode)\nAWS Well Architected Framework cost optimisation pillar\nHands-on labs on AWS cost calculation\nAWS Pricing calculator\n\n",
      "transcript": "Eoin: Things were simpler back when we could buy hardware\nand a few software licenses,\nhoping that it was enough,\nbut not too much to run whatever we needed,\nand that was it.\nIn the cloud, we don't have to pay for much upfront,\nand we can scale way beyond what we originally anticipated.\nThis flexibility, though, comes with a trade-off.\nWe are talking about cost complexity.\nIf the cost of cloud is holding you back,\nthis episode is for you.\n\n\nWe're going to talk about understanding AWS pricing\nand billing,\nand share tools and tips to get better visibility\nand control on your AWS costs.\nWe will also let you know some ways\nyou can get AWS to pay the bill for you.\nI'm Eoin, I'm here with Luciano,\nand this is the AWS Bites podcast.\nAWS Bites is sponsored by fourTheorem.\nfourTheorem is an AWS consulting partner\noffering training, cloud migration,\nand modern application architecture.\nFind out more at fourtheorem.com.\nYou'll find the link in the show notes.\nLuciano, cloud computing is often compared\nto your domestic utilities like electricity.\nYou pay for what you use,\nand this is probably very appealing\nif you've got very low or very sporadic usage,\nsince you don't pay for the idle time.\nSo if this is the case,\nwhy does cloud computing cost scare people off so much?\n\n\nLuciano: Yeah, I think it's because everything looks great\nwhen you don't have surprises,\nand you can have some surprises\nbecause the billing model is a bit complicated.\nSo people are naturally scared about,\nokay, what if I didn't get it right?\nWhat if it doesn't turn out to be as cheap\nas people say it should be?\nBecause so many variables,\nI don't really have a lot of confidence\nthat my understanding of this model is correct.\n\n\nAnd the way I'm gonna be using\nthis cloud computing magic thing,\nmaybe it's not necessarily the same as other people do.\nSo it might be cheaper for them,\nbut it might not necessarily be cheaper for me.\nSo, and another interesting thing\nis that there is this concept of free tier,\nwhich is kind of a way for AWS\nto allow people to just step into AWS\nand play around and try different things.\nTheoretically without incurring any costs.\n\n\nBut again, you need to be very careful\nabout what is the promise there?\nWhat are the limits that you need to be careful?\nLike when are you within this free tier?\nIt's not something obvious.\nIt depends on how you use AWS.\nSo it might be very easy for you to,\ndon't even realize that you are actually going\nover the free tier and end up with a bill\nthat might actually be even significant for you.\nWe actually have a link in the show notes\nwith the definition of what the free tier actually is\nand some frequently asked questions around it.\n\n\nSo that's another thing that we suggest people to look into\nif you are still a little bit skeptical about AWS\ngetting started, what is the free tier?\nWhat can you get effectively for free?\nSo that can be a very good resource\nto kind of figure out exactly where to start\nand what can you get for free.\nI want to give you another example\njust to make the point that sometimes things\ncan be surprisingly expensive.\n\n\nAnd that's something that you hear stories\nand then scare people off.\nThen they say, no, I don't want to go to AWS\nbecause I don't want to be that next person\nto have a horror story about billing.\nAnd this is very typical and it's about NAT gateways.\nWe have been talking about them a lot in the past,\nalso in our \"Horror Stories\" episode.\nSo it's a recurring one.\nAnd the point is that whenever you are building\nan application in the cloud,\nyou need to do some networking\nand it's very common that you create\nyour own private networks.\n\n\nAnd then when you need to have compute\nthat needs to access the internet,\nyou end up provisioning a NAT gateway\nin your private networks.\nAnd NAT gateway is a service from AWS.\nSo it comes with its own cost structure\nthat you need to understand.\nAnd it looks very cheap because it's like 5 cents per hour.\nAnd then you pay a certain amount for data processing,\nwhich might feel like a little bit of a need than cost.\n\n\nAnd at first glance can look very cheap as well\nbecause it's 5 cents per gigabyte,\nmore or less depending on the region.\nOh, there is also an additional 10 cents\nfor the amount of data that goes\nto the public internet, I believe.\nSo depending on how much traffic you are actually having\nfrom your private VPC to the public internet,\nthat might end up escalating very, very quickly.\nAnd you might end up with a massive bill\nwithout even realizing,\nlike just with a NAT gateway, you end up paying so much.\nSo this is just one of the other stories\nthat people use as evidence that the cost structure\nis not easy to understand.\nAnd you might end up with surprises\nthat are not nice to have.\n\n\nEoin: I think there's just some things you have to be aware of\nthat you can't really escape.\nAnd the NAT gateway is a classic example,\nbut I think data transfer in general\nis one of those things that's frequently complained about\nwhen it comes to AWS pricing.\nIt's always free to get your data in,\nbut unsurprisingly, maybe it's costly to send it out.\nSo you pay for data transfer between AWS regions\nfor data transfer out to the internet.\n\n\nYou also pay for data transfer\nbetween availability zones in a region.\nSo you can have an EC2 instance\nthat's talking to an RDS database in a different AZ,\nand you'll pay for that.\nS3 pricing has become a bit better.\nWe talked about that in a few previous episodes\nbecause of the competition from Cloudflare\nwith their S3 alternative.\nNow you can get 100 gigabytes out for free.\nSo maybe we can paint a picture of cost complexity\nwith an example.\n\n\nSo if you imagine we have a typical web server architecture\nwith a database cluster, EC2 instances,\nsome EBS volumes attached to those,\na load balancer, internet gateway, NAT gateway, DNS,\nyou've got a number of services there already.\nBut for each of those services,\nyou'll also have multiple different pricing dimensions.\nSo if we just look at the EBS component,\nthe price for your EBS volumes,\nso having this disk essentially attached to your instance\nwill depend on the volume type,\nand you will choose a different volume type\ndepending on whether you're optimizing for throughput\nor IO operations.\n\n\nAnd then it's a function of that volume type,\nthe size, the throughput, the number of IO operations.\nAnd then you also have things like EBS snapshots,\nand you might have backups as well.\nAnd these are difficult things to understand.\nLike EBS snapshots are kind of incremental in deltas.\nSo it's not very easy to always predict\nwhat the cost is going to be.\nAnd in that example as well,\nload balancer cost is another tricky one.\n\n\nWith load balancers, you pay a standing charge,\nbut also you pay for LCUs,\nwhich are load balancer capacity units.\nLots of different AWS services have <something> CU,\nlike DynamoDB has write capacity units,\nand even the new Neptune serverless has, I think,\nNeptune capacity units or something.\nWith the load balancer case,\neach LCU gives you like a number of connections,\na number of new connections, data throughput,\nrule evaluations.\n\n\nSo you have to work out all these things for your workload\nand figure out what the cost is going to be.\nSo unfortunately, a lot of the advice here is,\ndo your homework and become good at it.\nIt's just becoming part of software architecture\nthat you have to understand pricing models\njust as much as you do,\nunderstand performance and scalability.\nBut look, it's not all horror stories.\nThere's actually quite a lot you could do.\n\n\nIt can also be really advantageous in terms of cost\nif you get it right.\nI mean, some people get away\nwith ridiculously low AWS bills.\nI think one good example was A Cloud Guru,\nwho ended up building their entire learning management system\non serverless and AWS,\nand frequently publicly talked about how,\ndespite the fact that they were pretty much\nthe poster child for serverless applications\nfor a number of years,\nwith hundreds of thousands of users and massive revenues,\ntheir AWS bill was essentially zero.\nSo there's a good side to this as well.\nBut what would you recommend, Luciano,\nfor first early adopters to AWS\nto try and ease the burden and complexity here?\nBecause it does end up a bit scary.\n\n\nLuciano: Yeah, I want to echo what you just said,\nthat you need to do your own homework and get good at it.\nAnd there are some tools that can help.\nFor instance, there are billing simulators,\neven official AWS ones that you can use.\nAnd we might have a link in the show notes.\nBut what I find most of the time more useful\nis just use a spreadsheet\nor whatever tool makes sense to you,\nbecause that's, I think it forces you a little bit more\nto understand these dimensions\nwhen you have to build the model yourself.\n\n\nLike it makes you go a little bit deeper\nthan just using a simulator\nwhere you might forget to look at a particular field.\nAnd then you won't consider a particular dimension,\nwhich might end up being very relevant\nin the final cost calculation.\nAnd building your own spreadsheet,\nit can be an interesting exercise.\nI don't know if you have startup experience,\nbut when I end up doing this exercise myself,\nit always feels like I am doing a business plan\nfor a startup.\n\n\nAnd when you do a business plan for a startup,\nit's always a lot of like guesstimation,\nbecause there are so many dimensions\nthat you might have a feeling on\nwhere this might go over time.\nBut again, it's just a wild guess,\nmore or less informed,\ndepending on how much control you actually have\non that particular dimension.\nAnd most of the time,\nyou don't really have a lot of control.\nSo it could be interesting to do all of that exercise,\nbut when you do that exercise for a startup,\nthere is an element of how much it's gonna cost me,\nbut you always try to add another element\nof where can I save money?\n\n\nLike, can I get some credits or something from somewhere?\nAnd you can definitely do the same exercise\nwith AWS as well,\nbecause as we said in the intro,\nthere are also ways to get discounts\nor get AWS to pay something for you in a way or another.\nAnd one thing that is actually really, really interesting\nis a program called AWS Activate\nthat can give you credits\nif you are a startup or a solo founder.\n\n\nWe are gonna have, again, the link in the show notes,\nbut the idea is that if you are a startup,\nyou can get up to $100,000 in credits.\nThere are, of course, constraints and limits\nthat you need to check\nto make sure you actually classify for that.\nBut if you can, of course, it's a huge advantage\nto have that kind of level of discount,\nespecially for a company that is not stable yet.\nYou might be scared that the cloud can just bankrupt you\nif you don't do it in the right way.\n\n\nAnd at the very beginning, maybe you're not confident,\nyou're not gonna have all the skills\nto get the right architecture set away.\nYou might need to experiment,\nyou might need to pivot, try different things.\nSo just having that extra cash available for you\nthat you don't have to spend yourself,\nI think it can just boost your confidence,\nit can just boost your success rate as well,\nbecause you will have a lot more freedom to do mistakes\nand then recover from them.\n\n\nSo that's also something you can do\neven if you are a solo founder\nand you are just starting,\nand it's actually quite easy to get Activate credits\nup to $1,000.\nSo even if you just have a very simple idea\nthat you want to test, and again, you don't have any revenue\nbecause you don't even know\nif that idea is gonna be successful,\nyou can pretty much get the credit\nand probably build everything for free\nbecause probably with $1,000,\nyou can build a lot of stuff and validate your idea.\nAnd then maybe decide if it's the case to invest more,\ngrow the company, hire somebody,\nmaybe try again for the Activate program\nat a bigger level and get more credits.\nAnd we need to keep in mind that of course,\nif you are a company that has been in the market\nfor very long, you might not be considered a startup.\nSo what are the option there\nif you are an established company?\nDoes it mean that you don't get any credits\nor is there something else you would do there?\n\n\nEoin: Yeah, there's even more options, I would say,\nfor larger companies.\nBefore we get to that, actually,\nI've previously used AWS Activate credits.\nI was a solo founder of a company.\nI think my advice there is ask for the credits.\nDon't just look at the official channels.\nI mean, generally you have to go through an accelerator\nor incubator to get them,\nbut AWS are keen to give you those credits.\nAnd it's an investment from them as well\nbecause I was in a case where I used those credits,\ndid a lot with AWS, went on in subsequent companies\nto do even more with AWS\nand it's all good for their business, right?\n\n\nSo they want people to get stuck into AWS\nand have success stories building on AWS.\nAnd you can apply multiple times as well\nto hit that $100,000 limit.\nSo definitely don't be shy\nwhen it comes to applying for AWS Activate credits.\nFor larger companies, there's one great program,\nwhich is called the AWS MAP program\nor the Migration Acceleration Program.\nAnd this is a significant fund for migrating to AWS.\n\n\nAnd when we're talking about migrating to AWS,\nthat could be an existing on-prem application\nor it could be hosted somewhere else,\nor it could even be something that's a set of Excel sheets\nand a business process you have internally in your company.\nAnd there's a load of funds available for that.\nSo if you, they break it down into three phases.\nYou've got a fund for the assessment phase.\nThis is kind of where you're doing discovery\nand planning for your migration.\n\n\nAnd they'll pay for a partner like fourTheorem\nto do a lot of that work for you.\nAnd they'll fund up to $60,000 for that.\nAnd then the next phase is Mobilize.\nSo this is basically when you're getting ready\nto actually do the migration.\nSo that could be running proof of concept projects.\nIt could be preparing your AWS organization\nwith landing zones, et cetera.\nAnd they'll pay for half of that as well.\n\n\nThen when we get into the actual migration,\nAWS will give you credits or cash back\nfor up to 25% of the annual recurring revenue.\nSo for anybody migrating to AWS, it's a bit of a no-brainer.\nYou do have to reach certain targets.\nSo there's two different versions of the program,\nbut it depends on your, ultimately your annual revenue.\nBut I would always say,\nno matter what you're doing with AWS,\nalways pester your solutions architect and account manager\nas much as possible to do as much for you.\n\n\nThe more you talk to them,\nthe more you'll find out about these programs.\nYou can also talk to us about this.\nLike I mentioned, fourTheorem,\nour employer and the sponsor of the show\nis an accredited MAP partner.\nSo we can do these migration workloads.\nThere's a lot of due diligence that goes into making sure\nthat the people who help you with the workloads\nknow how to do it.\nSo we'll give a link in the show notes\nto the fourTheorem page just about MAP,\nbecause it covers a very nice,\nit's a very nice summary of what it provides there.\n\n\nBut you can also do this for multiple workloads.\nSo if you're an enterprise\nand you've got lots to migrate to AWS,\nyou can apply for multiple MAP programs.\nSo when it comes down to that back to, you know,\ntips and tools for optimizing costs then,\none thing I'd recommend setting up is AWS Budgets.\nAnd of all the tooling that's out there\nfor setting up cost optimization\nand getting control of budgets,\nAWS Budgets is a pretty simple one.\n\n\nIt doesn't take very long to set up\nand it gives you a bit of a safety factor right away.\nSo with these budgets, the simplest approach\nis just to set a cost amount for your organization\nor for each account.\nAnd when your budget is either in reality\nexceeding that threshold or is forecast,\nyou can configure it to use real data or forecasts,\nit's forecast to reach your threshold, you'll get an alert.\n\n\nSo that alert can come in via email\nor it can come in via Slack over SMS, et cetera.\nYou can make them more advanced.\nLike you can select specific services for your budget,\nlike have a different budget for EC2\nand a different one for S3.\nBut the simplest thing is to just start off with,\nokay, let's put X dollars on each account\nand measure over time and adjust the budgets.\nAnd it works pretty well.\n\n\nYou can also set up budgets for things like\nReserved Instances and Compute Saving Plans,\nwhich we'll mention later.\nThe last thing on those budgets\nis that you can even automate actions.\nSo when budgets are exceeded, you can take serious action,\nlike denying people in the accounts\nfrom doing EC2 RunInstance.\nSo if you really, really, really have to ensure\nthat you're not going to incur those costs,\nis there's always this balance between giving developers\nand AWS practitioners freedom,\nbut also controlling costs.\n\n\nSo you need to decide how much leeway you're going to give.\nThe different companies have different philosophies\non that, you know.\nI remember hearing, I don't know if it was Brian Scanlon\nor somebody else from Intercom speaking at an event\na few years ago, talking about it,\ntheir AWS approach to costs\nand their approach to cost was always retroactive.\nSo give developers as much freedom\nto create the resources they need,\nand they'll look at the cost in retrospect\nand see if they need to take action.\nTheir belief was that it's always more,\nit's better for business to allow people freedom\nand innovation and accept the cost surprises\nthat might come along, rather than slowing people down\nand always having that ongoing expense\nin terms of people power.\n\n\nLuciano: Yeah, there are other tools that you can find\nin the AWS console that can be very useful\nto try to understand cost\nand also manage it in a way or another.\nOne of the most famous is definitely Cost Explorer,\nwhich is an interesting one because it's not the simplest,\nbut when you get the grips on it,\nit can be pretty helpful because you can effectively\ndrill down and make kind of interactive visualization\non what is your actual cost.\n\n\nThere are some rough edges, for instance,\nsometimes the usage type, it's a bit confusing.\nLike it's a bit hard to actually attribute some costs\nto something specific.\nAnd this again has to do with the way\ncertain type of costs are grouped together,\nbut all over it's a pretty useful tool.\nAnd as soon as you learn what are the things\nthat are a little bit misleading,\neverything else is still very, very valuable.\n\n\nOne interesting thing is that it has a forecast feature.\nSo it gives you like a figure of how much are you\nprobably gonna be spending at the end of the month\nfor a particular visualization that you are building.\nAnd that of course might be more or less accurate\ndepending on how predictable is your cost.\nLike if you have a very spiky service where the fact\nthat it's spiky doesn't really depend on anything\nthat you do recurringly, maybe, I don't know.\n\n\nYou just have randomly mentioned in the news\nand therefore your traffic multiplies by 100.\nOf course, this is not something that the cost calculator\nis gonna be able to predict very well.\nSo it gives you more or less a figure of if everything\nremains more or less as the same shape\nthat we have observed before,\nit could be actually pretty accurate.\nSo something to keep checking to make sure that your cost,\nyou have a figure that is kind of also a little bit\nprojected in the future.\n\n\nOne case where that can be useful is for instance,\nif you are testing new services,\nlike you're spinning up clusters, so I don't know,\nCassandra, just to make an example,\nthen you can see that prediction actually skyrocketing\nbecause it gets projected in the future.\nSo that can be one case where this particular feature\ncan be very, very beneficial because it can give you\nan early indication that you have done some important change\nthat is actually projecting at much higher cost\nin the future.\n\n\nSo keep that in mind, try it, play around with it,\nand maybe you can find some value in it.\nThere is also Cost and Usage Reports, which allow you to export effectively CSV to S3.\nSo you can take all the data about cost\nand analyze it with whatever tool\nis the more convenient for you to do kind of fine-grained\nqueries on the data and you can aggregate it as you need.\nIt is updated once per day.\nSo at the end of the day, you can use all the data\nand you can build your own tooling around that\nif you really want to have fine-grained control\naround the cost data.\n\n\nOf course, you can also query it with AWS tools.\nFor instance, Athena, it's a very good tool to query data\nin a structured form in S3.\nSo definitely you can just use Athena\nto do more advanced queries.\nYou don't need to build your own infrastructure\njust to query that data.\nSo I suppose the next question will be,\nassuming that you are a large enough company\nwhere you have maybe different departments,\ndifferent groups with different responsibilities,\nand you might have different microservices,\nso different areas of your company are responsible\nfor different microservices,\nprobably you want to see cost at the department level.\nWhat can we do there?\nLike what kind of suggestion would you have there?\nMaybe my feeling is that you can use a cloud service\nand maybe my feeling is that you can use accounts\nand organize things by accounts.\nMaybe you can use tags, but I don't know Eoin,\nif you have any tip or best practice\nthat you would recommend there.\n\n\nEoin: I agree 100%.\nI think the two things you can do there,\nI think generally people recommend tags\nand that was always because it was normal\nto have shared accounts in the past.\nSo tagging was more important.\nTagging is still important and that depends on your business\nand how it's structured and how many cost centers you have\nand where the budget is allocated.\nTo have like a cost center tag if you need to,\nand then to have project specific tags as well,\nbecause you'll end up with resources\nthat can be shared in some accounts\nand that's just the useful thing to do.\nBut if you've got kind of a modern AWS landing zone\nwith different accounts for different workloads,\nthen it's actually easier then to allocate costs\nbecause you can just look at it per account.\nBut I would definitely recommend having both if you can.\nBy the way, we have an episode dedicated to tagging.\n\n\nLuciano: We are gonna be posting the link on the show notes\nif you want to deep dive on that topic.\n\n\nEoin: You mentioned like with cost and usage reports,\nyou get data that's updated once per day.\nSo what do you do if you wanna react quicker?\nAnd sometimes people feel like a day isn't enough.\nI could have run up $50,000 by then.\nSo one thing I would recommend is for having good,\nif we go back to our CloudWatch episodes,\nwe talked about having active monitoring\non CloudWatch metrics.\nAnd those can be a useful proxy for billing\nbecause if you've got excessive service usage,\nthat can mean a billing spike as well.\n\n\nSo if you've got Lambda\nand you're worried about all of a sudden having like\nsome sort of cascading Lambda or recursive Lambda bug\nthat causes a herd of functions to be allocated\nwith your maximum account concurrency for 24 hours,\nthen I would say, put an alarm on the concurrent executions\nof your Lambda functions and monitor that.\nYou can put an anomaly detection alarm\nor you can say, look, I don't expect any of my functions\nto be invoked very frequently,\nespecially if you're like early stage startup,\nbecause I'd like just put an alarm\nif all my functions are being invoked\nwith more than 100 concurrent executions\nfor an hour long period.\n\n\nAnd all of a sudden you've got an alarm\nand that can give you early indicator\nthat there might be something\nthat might eventually cause a billing issue.\nYou can also create your own metrics.\nSo there's useful metrics around API usage.\nThere's a set of CloudWatch metrics called API usage metrics.\nSo you can find out how often certain APIs are called.\nYou can use that to build, get cost insights in advance,\nbut you can also create your own metrics.\n\n\nAnd we've done this in the past with containers.\nSo if you want to monitor the number of containers,\nif you've got large fleets of instances or containers,\nAWS doesn't give you very good metrics out of the box\nthat tells you how many instances of each type\nyou've got running all the time.\nSo you can create your own, right?\nJust have a function or on the schedule\nor something that monitors the number of instances\nof a certain type you want to monitor,\ncreate your own metric and put an alarm on it.\n\n\nAnd then you can automatically turn off resources\nwith Lambda functions or Systems Manager\nbased on those alarms.\nSo if you don't want to be focused\non just detection and mediation, you can get strict.\nIf you wanna get real draconian,\nyou can be taking more of a preventative approach.\nI mean, I guess, but like there is a reasonable case\nfor having some more preventative measures in place.\nSo you can use Service Control Policies.\n\n\nA very simple thing is to, say,\nexclude regions that you don't expect to be using, right?\nBecause then you don't have somebody who accidentally\nor just does an experiment in a region you're not monitoring\nand all of a sudden you end up with a cost in that region.\nSo just limit the amount of surface area\nyou have to observe for your billing data\nby turning off regions.\nBut you can also turn off certain services.\nIf you don't expect people to be using Sumerian,\nthen turn it off, right?\nYou can just turn it off at an organizational level\nand you can also limit resources of specific types.\nLike you can limit access to those really awesome,\nexpensive EC2 instances that have like terabytes of RAM.\nWhen we're talking about costs,\nare we normally talking about compute?\nLike EC2, containers, Lambda,\nis that where the bulk of cost is gonna come from?\nYeah, I think it might, but it also might not.\n\n\nLuciano: There is definitely a lot more to explore.\nAnd every deployment is somewhat different.\nYou might end up using entirely different set\nof resources in AWS.\nSo I think it's important to try to have a full picture\nof where the cost might be.\nAnd then you try to apply that to your use case\nand see exactly, okay, my use case,\nI might be using more storage rather than compute\nor more networking rather than anything else.\n\n\nAnd so yeah, definitely do the research\nand try to spend the time to understand\nwhat your workflow looks like\nand what are the things\nthat you're gonna be using the most\nbecause those will be probably the ones\nthat will contribute the most to the final cost.\nAnd there are certain things that you might want to look\ninto to understand how can you actually fine tune\nyour cost depending on what you're actually going\nto be using the most.\n\n\nFor instance, if you feel that instances,\nlike virtual machines, is the things\nthat you are going to be spending most of your money\nin the AWS building, you can look into,\nokay, how do I optimize instances?\nSo you might start to look at different distance types\nthat maybe are more optimized for your specific workloads.\nYou don't need to buy a big machine\nwhere you have tons of CPU and tons of memory\nand graphic cards, maybe if you just need a good CPU.\n\n\nYou can find machines that give you only the good CPU\nand they don't really give you everything else\nif you don't need it.\nAnd this is just to make an example.\nThere are so many combinations of EC2 instances\nand even with re:Invent,\nwe are seeing new ones being created all the time.\nSo definitely there is a CPU configuration,\nsorry, a EC2 configuration instance type\nthat can match exactly the kind of use case\nthat you have.\n\n\nSo do your research and look into that.\nBut that's not the only option\nbecause sometimes you think you need a lot of EC2 instances\nbecause you have a very variable workflow.\nBut in reality, you don't always need\nall these EC2 instances all the time.\nSo you can start to think about,\nokay, how can I make it cheaper?\nSo maybe you can figure out,\ncan you use auto scaling groups to spin up\ninstances up and down depending on\nwhat is your actual usage, what is your actual traffic?\n\n\nOr another thing that you can do is\nyou can think about spot instances,\nwhich is another entirely different model,\nwhich is really interesting.\nMaybe we'll have a dedicated episode\nto talk more about that.\nBut the gist of it is that you are not going to be paying\nthe full price of an EC2 instance.\nYou're going to be paying something,\nsome kind of discounted price.\nBut because the caveat is that\nthat instance is somewhat volatile,\nlike it's not going to stick there forever.\n\n\nYou can use it as long as that instance\nis not being used by somebody\nthat's willing to pay a bigger price for it.\nSo it's kind of an auction market for\navailable EC2 instances.\nBut if you can afford for your compute model\nto have an instance that can go down any minute,\nthat can be much cheaper for you to use.\nSo you might have a pool of instances\nthat are actually reserved,\nbut you can also extend that pool with spot instances\nthat will give you extra compute\nwhen you need it at a much cheaper price.\n\n\nAnd another thing is Reserved Instances,\nwhich is when you pay some amount upfront.\nAgain, this is something maybe worth going into\nmore detail in a future episode,\nbut the idea is that if you know in advance\nyou're going to be needing these EC2 instances\nfor the next five years,\nwhy not trying to get a discount on them?\nTry to reserve them for five years,\nAWS is going to give you a better deal on those instances.\n\n\nSo this is another way that you can decrease\nthat compute cost.\nAnd there are other things like Compute Savings Plans.\nSo again, the idea is once you understand\nwhat is the bulk of your cost,\ntry to think what are the things that I can play with\nto reduce that cost.\nAnd sometimes it's architecture,\nsometimes it means using other services,\nsometimes it's trying to figure out a better usage model\nwhere you try to fit your actual usage\nto something where you have less waste, basically.\n\n\nYou are paying exactly for what you use.\nWe already mentioned cost calculator,\nwe already mentioned spreadsheet,\nso try to exercise those muscles and use it\nand you'll get better at it.\nYou'll understand more and more the more you use AWS\nand I think it's going to become less and less scary.\nAnd it's going to become just another capability\nthat you have in your business\nto every time you want to do something also to put in place,\nsome time and some expertise\nto understand the cost in advance\nand include that as part of building new workloads\nand new capabilities into your business.\nI think that's all I have in my side.\nI don't know Eoin if you have any closing advice\nthat we can give people?\n\n\nEoin: I think, yeah, I definitely recommend people checking out\nthat AWS Pricing Calculator.\nYou just go to calculator.aws\nso that should be pretty easy to remember.\nYou don't even need to look at the link in the show notes,\nbut it's a web-based application, right?\nAnd you can just pick your services,\nenter the different units for the different dimensions\nand it'll give you a cost and you can share,\nyou get a shareable link\nthat you can share with other people\nand you can also download a spreadsheet from it as well.\n\n\nThere are some bugs and niggles with it.\nI've heard people say that sometimes\nall of the calculation data doesn't stick.\nSo just be careful with that.\nBut I always recommend doing a spreadsheet approach anyway,\nespecially if you're doing a serious cost calculation\nand not just a back of an envelope kind of a calculation.\nWhat you can do with spreadsheets\nis that you can get a little bit more granular\nand you can say, okay, based on this number of users,\nthen I expect this number of API requests\nand this number of function invocations.\n\n\nSo then you can actually play around\nwith the business inputs as well.\nLike what if I get, you know, peak usage on Black Friday?\nWhat would that look like in terms of cost?\nSpreadsheets are good.\nAnd then you can use the spreadsheet\nto validate the cost calculator outputs and vice versa.\nI think it's also worth mentioning\nthe AWS Well-Architected Framework Cost Optimization Pillar.\nIt's one of the six pillars.\n\n\nAnd that documentation is, you know,\njust good advice on cost optimization in general\nand putting the practices in place.\nThere's also a hands-on lab.\nAnd then just a reminder as well,\ndon't forget to check out the AWS Activate and MAP programs.\nWhat better way to deal with your building\ngetting AWS to pay it for you?\nWe have experience of both.\nSo if you have any questions,\nfeel free to reach out to Luciano or myself.\nAnd finally then if you like AWS Bites,\nplease share the link with a friend or a colleague\nbecause, you know, our audience is growing more\nall the time as the audience grows.\nWe got lots of good ideas, suggestions and feedback.\nSo keep those comments coming on YouTube and Twitter as well.\nThanks for joining us.\nWe'll see you in the next episode.\nSee you in the next one.\n"
    },
    {
      "title": "62. Top 3 re:Invent 2022 announcements",
      "url": "https://awsbites.com/62-top-3-re-invent-2022-announcements/",
      "publish_date": "2022-12-09T00:00:00.000Z",
      "abstract": "re:Invent 2022, the biggest AWS conference of the year is just over and there were tons of interesting announcements: many new features and some interesting new AWS products! But we are not going to bother you with yet another walkthrough of all of them. In this episode of AWS Bites podcast we just discuss our top 3 announcements and explained what we liked and what could have made them even better!\nWe will talk about EventBridge Pipes, Step Functions Distributed Map, and Application Composer.\nWhat are your favorite announcements? Let us know on Twitter or in the comments!\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nPrevious Episode on EventBridge\nPrevious episode on cost\nEventBridge Pipes Official Announcement\nStep Functions Distributed Map Official Announcement\nApplication Composer Official Announcement\nSNS Payload Message Filtering announcement\nVerified Access announcement\nCloudWatch Cross-Account Observability announcement\nOfficial AWS Blog post with Top AWS re:Invent 2022 announcements\n\n",
      "transcript": "Luciano: Have you been following reInvent 2022?\nIf you're following this podcast, you are probably into AWS\nand aware of everything that was announced at AWS this year.\nSo don't worry, today we're not going to give you another recap of reInvent.\nWe're going to spare you all of that\nand just focus on three announcements that we are most excited about\nand we want to tell you exactly why we really care so much\nabout this particular thing.\nSo of course, this is going to be a very personal take.\nSo we do look forward for disagreements\nand hearing what did you like the most instead.\nMy name is Luciano and today I'm joined by Eoin\nand this is yet another episode of AWS Bites podcast.\nAWS Bites is sponsored by fourTheorem.\nfourTheorem is an AWS consulting partner offering training,\ncloud migration and modern application architecture.\nFind out more at fourtheorem.com.\nYou'll find the link in the show notes.\nSo, okay Eoin, let's maybe start with the first announcement.\nWhat did you like the most?\n\n\nEoin: I think the standout one for me was one that really heads in the right direction and I'm talking about a new addition that allows you to reduce\nthe amount of glue code you have to write in your applications.\nAnd when you're building serverless applications\nand event-driven applications, this is really important\nbecause you can end up with a proliferation of Lambdas\nthat don't do very much otherwise.\n\n\nSo we're talking about EventBridge Pipes.\nNow we've talked a lot about EventBridge in previous episodes\nas part of our event series.\nEventBridge allows you to create event-driven applications\nby publishing messages to a broker or a boss\nand setting up pattern matching rules for consumers.\nEventBridge is also one of the fastest developing AWS services.\nSo this announcement followed quickly from the release of EventBridge schedules,\nwhich is another really exciting pre-event announcement.\n\n\nWhat we like about EventBridge is that it's extremely easy to get started with it.\nThe amount of setup is reasonably minimal,\nespecially compared to other event services\nand you can send and receive events straight away.\nIt also supports native events,\nso you can listen to other things happening in your AWS account very easily.\nSo a new object was created in a bucket, you can listen to that.\nAn ECS container has stopped, you can listen to that too and react to it.\n\n\nBy default, by the way, you listen to an event,\nlike one of these created by an EventBridge\nby creating an EventBridge rule.\nA rule is generally a pattern that allows you to capture\nall of the events that conform to that pattern\nand there's a content filtering syntax that allows you to do that.\nThen you can specify one or more targets\nthat will receive that into an invocation,\nlike a Lambda function, an SNS topic, a queue,\na Kinesis data stream or a HTTP endpoint.\n\n\nSo what are EventBridge Pipes and why are they so cool?\nWell, the idea with EventBridge rules\nis that you're dealing with PubSub type interactions.\nSo you have one producer and multiple consumers.\nWith EventBridge Pipes, they're point to point.\nSo it allows you to take an event from one source\nand pass it to another with optional filtering,\ntransformation and enrichment.\nSo the goal here is to avoid you having to write more custom code.\n\n\nSo for example, you would typically previously have to do\na lot of Lambdas that take data from one source,\nput it into a queue.\nYou don't have to do that anymore.\nSo let's dive into it a little bit\nand talk about the various constructs in a pipe.\nWerner Vogels in the keynote compared this to Unix pipes\nand the Unix philosophy with standard edit and standard input\nand text as the interchange format between them.\n\n\nSo it's very much along the lines of that same principle.\nSo you have event sources,\nand most of the services supported by EventBridge rule\ntargets are supported here.\nI think it was actually, they mentioned in the blog post\nor some of the Twitter commentary about this,\nthat the event sources were very much inspired\nby Lambdas event source mappings.\nSo you can take events from DynamoDB streams,\nfrom Kinesis data streams,\nSQS, Kafka, your own Kafka or AWS's managed Kafka,\nalso Amazon MQ,\nalso SQS,\nand you can then send them on to Step Functions,\nKinesis data streams, Lambda, third party APIs, API gateway.\n\n\nAnd you can put a, just like with an EventBridge rule,\nyou can put an input transformer.\nSo you can transform the event before you send it on to the target.\nYou can also filter events, which is really, really, really important.\nSo you use the same syntax as you do with EventBridge rule patterns,\nand then you use that to essentially filter out\nwhat the subset of the events coming from that source\nthat you want to forward to the target.\n\n\nSo that's pretty much it.\nIt's a bit like if you imagine if you're a Unix fan,\nyou might cut a file, pipe it to grep to filter out some of the lines,\nand then send that on to the WC command to get a word to it.\nSo it's a similar idea, right?\nYou've got sources, filters, and targets.\nOne of the things that you can do with EventBridge Pipes then\nis also enrichment.\nSo this allows you to call out to other services or an API\nto get additional data and add it into the event.\n\n\nSo you can call out your Lambda, Step Functions,\nor HTTP API or an API gateway.\nAnd then you can also transform the result of that too.\nThe other thing I'd probably mention is that pipes also support DLQs.\nSo again, this is like a fairly reliable way\nof taking data from one system, passing it on to another.\nSo just to kind of summarize EventBridge rules,\nI think it's going to be very powerful.\nHopefully, it'll allow a lot of people to delete Lambda functions\nthey don't need anymore,\nand focus on using Lambda for kind of meaningful computation\nrather than just transporting data from A to B.\nThe main difference is just to summarize that,\nit's point-to-point,\nwith pipes, but it's pub-sub with EventBridge rules.\nWith pipes, you don't have to write code to take an event from sources\nand put it into EventBridge.\nLike with EventBridge rules, you're writing a rule for an event\nthat's already coming along to the bus.\nSomebody still has to put it onto the bus.\nWith pipes, it's taking care of taking the data from the source for you.\nAnd the other difference between pipes and rules\nis that pipes have enrichment enrichment.\nSo you can do that with a lot of different types of enrichment.\nEnrichment support as well.\nSo what do you think? Is that your number one as well?\nI was a little bit tempted to go with that one.\n\n\nLuciano: I was really excited about that one.\nBut since you covered it already,\nI'm going to talk about the other one that I really liked,\nwhich is Step Function distributed map.\nAnd also Step Function is a topic that we have been talking about in the past.\nSo what's so interesting about distributed map?\nSo in a Step Function, you can already do a map step.\nAnd that map step, it's something useful\nwhen basically you want to take a bunch of different input.\n\n\nFor instance, coming from the previous state, you have an array\nand you want to do something repeated n times\nfor every item in that particular array.\nAnd that works really well.\nThere are a lot of practical applications for that,\nbut it's very limited.\nYou cannot process more than 40 items concurrently.\nSo where distributed map is trying to improve things\nis to try to raise that limit much, much more\nand give you a much higher throughput\nif you really have to process a large number of things concurrently.\n\n\nAnd it also takes a slightly different approach.\nI'm going to try to describe how.\nBut the first thing worth mentioning is that where the limit is 40 for regular map,\nwith distributed map, the concurrency limit is 10,000 items.\nAnd it's even more interesting than that\nbecause you can process up to 100 million items in total.\nSo a full distributed map can have a maximum number of 100 million items\nand they will be processed 10,000 at a time.\n\n\nSo you can imagine what's the difference in scale already.\nNow, how does it work in practice?\nBecause the model is slightly different from what you would use with a regular map.\nSo each map step is basically running a child Step Function\nand that Step Function has its own execution history.\nSo it's kind of, in a way, an orchestrator of children Step Functions\nevery time you're running that distributed map step.\n\n\nThe input is taken from S3.\nThis is another big difference.\nLike with regular map step, you generally can take a...\nor either the entire state of the function\nor a portion of that state if you are mapping with the JSON syntax.\nAnd basically you are just saying,\ntake this array and repeat some other steps for every item in that array.\nInstead, with distributed map, you need to take data from S3\nand that needs to be some kind of structured file format.\n\n\nIt can be a JSON, a CSV, or you can even use an API call like list objects.\nSo that basically is the way that you can load a lot more data into Step Functions,\nwhich I think is another limitation that we have with the traditional map step,\nwhere you are only limited to the state, which is not a lot of data.\nWith distributed map, you can actually process big data files\nand repeat that operation with very high throughput and concurrency.\n\n\nSo what are some of the use cases?\nDefinitely batch processing.\nSo for instance, if you have a lot of files in S3,\nmaybe representing some valuable piece of business information.\nI don't know, maybe something around analytics for your e-commerce.\nAnd maybe you can have for every product in your e-commerce,\nyou might have a JSON file that tells you exactly\nall the IP addresses that look at that product.\n\n\nAnd you might want to do some analytics to try to figure out in which regions\nevery single one of your products can be relevant for people.\nSo you can do some marketing.\nThat could be a use case where it could be the Step Function\nthat takes all your files in parallel.\nAnd then every sub Step Function will be crunching all the data\nand give you some analytics about that.\nThis is just to give you a random example,\nbut you can come up with other examples like, I don't know, financial modeling.\n\n\nSo you might be running some models over your data set\nand try to come up with some results about maybe, I don't know,\ncalculating a risk score for specific deals\nthat your organization is working on.\nOr another use case, which is apparently our favorite one,\nbecause we end up mentioning it in almost every episode,\ntransforming images, maybe creating thumbnails of images\nthat you have somewhere in S3,\nor maybe even just extrapolating information from those images,\nconnecting with other services,\nand try maybe to do some computer vision analysis\nand then figure out, okay, what is a description for every single image?\n\n\nSo you can imagine basically all these kind of orchestration workflows\nwhere you're starting with a lot of data from S3\nand you just want to map basically S3 files to something else.\nYou can create a Step Function and use this distributed map functionality now.\nNow, there are of course also some limitations.\nSo although I am very excited, I am also a little bit disappointed\nthat they didn't go just a step further,\nwhich would have been even more amazing.\n\n\nSo maybe this is my wish list for reInvent 2023 already.\nBut basically what I was a bit disappointed about\nis that you can only deal with a flat list of inputs.\nSo basically that means that you run,\nyou can imagine the mental models,\nlike there is no correlation between inputs.\nYou just run everything concurrently.\nOf course, there is a concurrency limit,\nbut you cannot create rules such as like a dependency graph\nwhere you could say, I need to run this file first,\nthen I can use the output of this other file to run something else\nand create a more kind of complex orchestrated way of running the workflow,\nwhich can be very convenient in some cases.\n\n\nAgain, I'm thinking risk modeling,\nwhere maybe you have data that needs to flow from one deal to another\nfor that compute issue to make sense.\nAnd another thing is that you cannot dynamically add items to the execution run,\nwhich basically means that even if you wanted to have\nyour own custom orchestration logic from the outside\nand push things into this pool of things to process,\nthat's not really something that is supported today.\n\n\nYou just need to define everything in advance\nand the Step Function is just going to take that input,\ncreate this kind of execution and run it,\nand then eventually you'll get your results.\nAnd also cost might be an issue\nbecause you are basically doing a huge amount of stage transitions,\nstep transition in the Step Function.\nSo you need to be careful and try to come up with some numbers\nto make sure that the cost is going to be reasonable for you,\ndepending on the type of computation you're going to do\nand the number of files that you're going to process.\nSo that's everything I have about distributed map.\nI don't know if you want to add anything that I might have missed.\n\n\nEoin: It's a bit like the first one in that it's also something that could potentially allow you to remove a huge amount of glue code\nand orchestration logic.\nSo I think it's really, really a great step in the right direction.\nAnd I think those wish list items you mentioned\nwould just really make it fantastic altogether.\nThe price issue is definitely a concern\nbecause it's a bit like the pricing model for Step Functions\nwasn't designed for this level of scale.\n\n\nBut you're still paying like two and a half cents\nfor thousands state transitions in Step Functions.\nSo you can imagine if you've got a million state transitions,\nit's now quite possible to reach that. That's $25.\nSo if you're running that multiple times a day,\nit adds up over the month.\nSo you have to work that out.\nSee our previous episode on pricing.\nAbsolutely.\nOkay, well, maybe we could talk about number three\nin our top three lists then.\n\n\nAnd it's pretty hard to choose\nbecause there were some pretty good announcements elsewhere.\nCode Catalyst is another one that's worth a mention.\nBut we're going to talk about Application Composer.\nSo Application Composer is a completely new tool in the AWS console\nfor visually designing new applications.\nOr visualizing existing applications.\nSo this is in preview right now.\nSo it's not generally available,\nbut you can try it out and give it a go.\n\n\nAnd I've done that and I've found it to be much,\nmuch better than previous attempts at this kind of thing,\nlike the CloudFormation designer.\nNow, it's really focused on serverless applications right now.\nBut the way it works is that you can build an application\nfrom scratch using a drag and drop interface.\nVisually, it looks good. It makes sense.\nIt's reasonably simple to use.\nAnd it will generate the CloudFormation template for you\nand also generate things like IAM policies you will need.\n\n\nNow, it doesn't support all of the CloudFormation resources.\nSo there's a set of about 12 or 15 services.\nSo classic things you'll find in a basic serverless application,\nlike an API gateway, Cognito user pools, tables in DynamoDB,\nEventBridge rules, Kinesis, Lambda, S3, SNS, SQS and Step Functions.\nAnd I would love if it supported the many hundreds of resources,\nthere are thousands even, that you can get in CloudFormation\nand maybe we'll get there. But it's a pretty good start.\n\n\nSo one of the things it can do then as well,\nif you're using Chrome or Edge browsers,\nis it can actually synchronize with your code on the file system.\nSo if you're taking the approach of visualizing an existing application,\nyou can point it to the directory, pick your template,\nand it will visualize that for you.\nAnd if you make changes, it will sync them back to the file system.\nSo that's using File System API in the browser.\n\n\nIt doesn't work in Firefox because Firefox doesn't support that.\nAnd in that case, you just have to load your template manually.\nI did try out an example.\nSo I was building a recent service application.\nIt had two features.\nI was using AWS SAM and this supports AWS SAM, so that was a good fit.\nMy application was using nested stacks.\nAnd it was also using Step Functions\nwhere the state machine definition was loaded separately from a JSON file.\n\n\nIn that case, it doesn't support nested stacks.\nI mean, it could load the file,\nbut it just showed me that there were a number of stacks.\nIt couldn't show me the resources within the nested stacks.\nSo I thought, I said, okay, that's fine.\nSo I'll just load the individual stack template.\nAnd I did that and I was able to load up all my Lambda functions.\nAnd it was able to recognize that there was a state machine,\nbut it didn't parse the state machine definition.\n\n\nSo it wasn't able to draw the lines between my state machine definition\nand the tasks that were invoked in the Step Function stages.\nI did try building from scratch and creating a Step Function.\nAnd in that case, you can put the definition in,\nin line, basically in state machine resource,\nand that seems to work fine.\nBut I was thinking as I was doing that,\nwouldn't be nice if it's seamlessly integrated\nwith the Step Functions Workflow Studio,\nso that you can go directly from designing your state machine\nand your CloudFormation resources and your serverless SAM resources\ndirectly into the actual state machine design.\nAnd if those two tools blended well together,\nthat could be really, really powerful.\nI think this is really good thing,\nbecause one of the things about serverless applications\nis that they can be hard to kind of understand\nhow everything fits together, because you've got lots of resources\nthat are sometimes loosely coupled.\nThis is a good step in the right direction.\nAnd I think it's going to be really useful,\nvery good for people starting off, I would say as well,\nwith serverless development,\nbecause when you're just looking at lines and lines of YAML,\nit can be a bit of a headache,\nbut when you can visualize it nicely and talk through it,\nit's like having a live kind of physical architecture diagram\nfor your solution.\n\n\nLuciano: Yeah, totally agree.\nAnd I think this is one of the pain points\nthat we hear the most about when talking with people in the industry\nor our customers, that it's always very hard to keep\nin sync your architecture diagrams\nwith your actual architecture running on AWS.\nSo this might be a step forward in that direction.\nIt could be a tool that kind of gives you\nthat automatic visualization of your actual stacks,\nrather than trying to keep two different things in sync.\n\n\nAnd you know that that's really, really hard to do it well.\nSo really happy to see this being announced,\neven if it's not perfect, I think it's a great one to mention,\nand it's a step forward for sure.\nSo just to try to wrap things up here,\nI want to mention that we will have the links\nfor the individual announcements, EventBridge Pipes.\nStep Functions Distributed Map,\nand application composer in the show notes.\n\n\nBut also there are three things, three additional things\nthat we were kind of discussing,\nand they were in our shortlist as well.\nSo maybe just worth a quick mention.\nSo we have SNS payload message filtering,\nverified access, CloudWatch cross-account observability.\nWe are not going to spend more time on those,\nbut you can find the links in the show notes as well,\nif you want to deep dive on these other announcements.\n\n\nWe will also have another link, which is our unofficial blog post\nwhich highlights all the top announcements\nof AWS reInvent 2022 directly from AWS.\nSo that's another great source if you have missed something\nand you just want to see exactly what was announced\nand deep dive on what's more interesting for you.\nAnd with that, I think we are at the end of this episode.\nWe are really curious to hear\nwhat are your top three favorite announcements.\nI realize that as probably our background,\nwe have been mostly focused around the area of application development,\nbut probably in the audience we have people that are more focused\non networking, ML, data analytics,\nand there were a lot of announcements in those areas.\nSo I'm really curious to see what you liked the most\nand if you were excited about the new things that were announced.\nSo definitely leave us a comment, chat to us on Twitter,\nand let's be in touch.\nUntil then, see you in the next episode. Bye.\n"
    },
    {
      "title": "63. How to automate transcripts with Amazon Transcribe and OpenAI Whisper",
      "url": "https://awsbites.com/63-how-to-automate-transcripts-with-amazon-transcribe-and-openai-whisper/",
      "publish_date": "2023-01-13T00:00:00.000Z",
      "abstract": "We built a Step Function that allows us to generate high-quality transcripts for AWS Bites podcast!\nAfter evaluating different approaches and technologies we ended up using Amazon transcribe and OpenAI whisper. They both have their pros and cons but combined together they gave us everything we were looking for with quite a good degree of accuracy!\nIn this episode, we describe our use case, our research, and how eventually we did go about productionizing our final solution.\nIf you run a podcast and you would like to do something similar, we have open source our solution. It's called PodWhisperer and you can find it on GitHub: github.com/fourTheorem/podwhisperer.\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nPodWhisperer repository on GitHub\nAmazon Transcribe\nOpenAI Whisper announcement blog post\n\n",
      "transcript": "Eoin: We have updated awsbites.com and added transcripts to every single episode.\nWe don't have a massive number of episodes yet, but we're proud to have published 62 episodes so far.\nSo it's no small amount of audio to transcribe.\nSo how did we go about doing all this work and how are we going to be able to keep doing this consistently in the future?\nWell, there's a simple answer. Automation and AI.\nFor the longer and more elaborate answer, you'll have to stick around until the end of this episode.\n\n\nWe'll tell you how to build your own transcription automation pipeline and we'll also share all of the code for our solution.\nMy name is Eoin, I'm with Luciano and this is the AWS Bites podcast.\nAWS Bites is sponsored by fourTheorem.\nfourTheorem is an AWS consulting partner offering training, cloud migration and modern application architecture.\nFind out more at fourtheorem.com You'll find the link in the show notes.\nSince this is the first episode of 2023, happy new year to everyone.\nWe're glad to be back publishing new episodes after the break.\nSo back to transcripts for this podcast.\nLuciano, can you start maybe by giving a quick recap of what the actual need is?\nWhat problem are we solving?\n\n\nLuciano: Of course. So yeah, this is a podcast and basically in every episode we talk a lot.\nWe say a lot of nonsense. Sometimes we say also something interesting, at least I hope.\nAnd of course it would be great if we could provide together with the videos\nand the audio files also proper transcripts.\nAnd that would be nice if we can do that consistently for every single episode.\nWhen the episode comes out it is also available in a text format basically.\n\n\nSo people who prefer to read this conversation rather than just consuming the video or the audio format,\nthey can just simply use that transcript as a way to consume the information we are trying to share.\nAlso transcripts are very useful because they can be used for search engine optimization\nwe embedded them in our own website with the hope that that contributes\nto make our content more discoverable because on the web we provide a better description for\nthe kind of content we are producing. And in general transcripts can also help people\neven just watching the video or listening to the audio to easily find exactly what is the place\nwhere we were talking about a specific concept. Maybe they are listening to an episode again\nbecause there is something they want to refresh so maybe they remember that we talk somewhere\nabout Step Functions they can easily search in the transcript just to exactly figure out at\nwhich point we start to talk about that particular topic. So definitely there is value in creating\nall these transcripts. But yeah the main question is how did we do that? How can we generate\ntranscripts in general? What did we do?\n\n\nEoin: Yeah this is something we've looked at a few times in the past and never found any ideal option until very recently. So some of the options are like doing\nit manually hiring somebody who's professional at this. The other one is grabbing the closed\ncaptions that are automatically generated by YouTube because all our episodes are already\non YouTube or we can generate them in another way. So having someone do it manually like having a\nprofessional individual like a freelancer or a company who specializes in this is appealing\nbecause it leads to really high quality results by people who do this all the time.\n\n\nThe disadvantage really the main hurdle is that it takes time to find somebody who's reliable enough\nand build a relationship with them and set up that whole process. It can also be expensive\ndepending on your budget and then you have communication back and forth that can introduce\na delay every time you need to publish an episode. So overall because it adds to the lead time it's\nsomething we were pretty reluctant to do. Regarding YouTube closed captions, we could have done this\npretty I suppose integrated this into our workflow after we publish a new video on YouTube we could\nwait for some time for YouTube to generate those closed captions and then try and integrate some\ncode to download them and integrate them into the build process. That seems like a decent enough\nsolution but there's two major problems with it. Number one, the quality of the transcripts isn't\nthat great it lacks kind of punctuation and grammar and sentences and that sort of stuff.\n\n\nAdditionally, the YouTube transcripts don't identify different speakers so if you just\nconverted it into a blog a wall of text it would be literally that just a wall of kind of stream\nof consciousness text without any punctuation or identification of speakers. So the last\nsolution left is to kind of generate the transcripts ourselves somehow and since this is\nan AWS podcast it's to be expected that we would use something like Amazon Transcribe which is AWS's\nmanaged service to perform speech to text. So you give it an audio file and it gives you back text\nand we like the simplicity of that we're always advocating for using managed services\nyou can use the Transcribe SDK or the API to generate the transcription in a programmatic way.\nWith a transcribed client, you call the start transcription API and provide reference to\nan audio file and an output prefix and it will generate that as a JSON file it can also generate\nsubtitles formats like SRT and WebVTT. So it runs in batch mode it can also do real-time\ntranscriptions but we would be using batch mode for this since it's kind of for on-demand content\nand you can get notified with EventBridge when it's finished. Luciano, do you want to talk about\nwhat the pros and cons of Transcribe are and like why we ultimately ended up kind of using it but\nnot entirely?\n\n\nLuciano: Yeah of course so Transcribe is quite good because it gives us that feature that we really liked and we we felt it was missing on YouTube which is basically you get different\nspeakers you get a label that tells you this person is starting to talk another person is\nstarting to talk from this point so we can retain in a text format that feeling that this is just\nnot a wall of text but it's an actual conversation between multiple people. Unfortunately actually\nthere is another one good thing that you can customize it so you can add custom models and\nvocabularies to fine-tune the results so if you have a very specific domain you can basically put\nmore work into it and get more accurate results but in general we were not satisfied with the\nlevel of quality. It is not that bad I think it's still quite a good tool so you can use it for most\nthings but for the kind of scope that we had in mind we feel that good and perfect are very\nnoticeable points like we were aiming for the 99% good while transcribed maybe around the 97%\ngood and we feel that that 2% of a difference is actually quite noticeable when you are reading\nsome text and you expect it to be higher quality. So we were looking for something that could be a\nlittle bit better so that basically led us to explore other avenues. And pretty much during the\nsame time where we were looking for an alternative, there was a blog announcement by OpenAI that\nintroduced this new tool called Whisper which is effectively another tool to do text-to-speech so\nto try to recognize speech and convert it to text. So this came around\nlast September I think and we are going to be linking the announcement blog post in the show notes.\nIt's the same group of people that created ChatGPT and DALL-E so you probably heard of them\nbecause right now their products are all the rage and Whisper is probably the least known of these\nthree but nonetheless it's a very interesting product. And we were really excited to try it\nso we quickly spin it up and tested it and we were definitely blown away by the level of accuracy.\nSo we immediately thought okay we want to use this because this is giving us the level of quality\nthat we want to provide in the end and if we can automate all of that process this is going to be\nsomething that we can keep doing very easily without too much overhead in our existing process.\nStill, it wasn't perfect unfortunately there were a few small problems one is that it did not\ndistinguish between speakers. So on one side we're getting more accuracy but again we are losing that\nability to distinguish the speakers and the other thing is that it's not built in in AWS as a managed\noffering so if we were to productionize so to speak this solution in AWS, we'll need to figure\nout exactly how do we take the model and run it in AWS. So Eoin, do you want to detail our solution in the end?\n\n\nEoin: Yeah exactly, so we wanted to get the best of both worlds right so we have OpenAI Whisper which is this fantastic model that you can run it's basically they deliver it as a container\nthat you can run and as a very nice user-friendly developer friendly interface where you just give\nit an audio file and it gives you the transcript. It might be worth mentioning that can also do\ntranslations as well so if you want to transcribe but also generate Italian text or even transcribe\nfrom different languages into English this is something that's really good at too. We did run\nit standalone. It comes in different sizes so you have it depending on your compute resources\navailable to you you can run the tiny model, small model, the medium model or the large model but if\nyou want to use anything and get a result within a reasonable period of time like even less than 30\nminutes you probably need a GPU so that's something that's worth bearing in mind with OpenAI. So our\nsolution - what we wanted to do was use the accuracy of the OpenAI Whisper transcript but take the\nspeaker labels from the Amazon Transcribe output output so that we'd have an accurate labeled time\nlinked transcript and merge the results and end up with a JSON file that we could use to generate\na transcript for the website with sections that say this is what Eoin said this is what Luciano\nsaid and make it readable for people almost like a blog post, right? So we built this using Step Functions.\n\n\nIt's using SageMaker so that we can run that Whisper model with the GPU and it's also using\nLambda for lots of little transformation efforts like if the input audio file isn't an MP3 we\nconvert it to MP3 because Transcribe - it's one of the formats it supports and sometimes we're using\nM4A audio and Transcribe doesn't natively support that. So, how do we do this? How do we even kick off\nthis process? Well, after we finish recording an episode of AWS Bites we do a bit of editing we\ncreate a video and we create an audio file. That audio file gets pushed up to Anchor which\ndistributes the podcast to all the podcast channels but we also take that audio now and we\ncopy it into an S3 bucket and that kicks off a whole automated process with this step function.\n\n\nWe of course we had the previous 61 episodes or so to consider so we also had to do a backfilling\nprocess so we pulled down the RSS feed and kicked off this process for each of the 61 previous\nepisodes by copying that audio up to S3. Interestingly, I suppose it's worth mentioning\nthat you know there is a cost associated and probably a lot of people will be wondering what\nis the cost to run this because we have to run SageMaker with a GPU we also have to use Transcribe\nboth of those things can be expensive if you use them at scale we talked about that in our previous\nepisode on AI services.  We did work this out I can't recall exactly the channel was around\nit was definitely less than a dollar per episode for the whole process to run so it's not too\nominous compared to other alternatives at all I would say. So is it worthwhile talking about some\nof the orchestration here? How does it all fit together? We mentioned we have Step Functions\nright. So we pre-process the input if needed with FFmpeg, we trigger the two transcription jobs at\nthe same time so the Transcribe job and the SageMaker job transcribe means we have to use\nthe AWS SDK within step functions and then kind of poll until it's complete. With SageMaker we've\ngot like a more native integration with Step Functions where we could just say run this batch\ntransform job that'll kick off a docker container in the background with the right um compute\nresources it will pass our input audio into it it can run a batch of jobs the way we set it up\nwe generally just do one transcription at a time because we're only doing one a week and when we\nget both results then Step Functions will allow us to take both of those inputs and kick off a\nLambda to process the results. So that's like essentially taking these two sets.\nBoth systems will give you a set of segments with start times and end times one of them has speaker labels\nso we have to do this run this algorithm essentially to merge the two what else is there\nto mention in this process. What other bells and whistles do we have?\n\n\nLuciano: There are some additional things that we do to try to to get a little bit higher quality with our final result, so for\ninstance we noticed very common mistakes. For some reason, Whisper doesn't like our names\nlike it was getting my name wrong a few times it was mostly getting your name correctly, Eoin, but\nspelled in a different way I think that is that I just yeah just too many ways so it was interesting\nthat it was getting correctly but just the spelling of course yeah you need to guess which\nwhich spelling is the right one so we basically figured it out there are also some other cases\nwhere for instance name of services in AWS sometimes would be consistently wrong or some\nsmall things like that so basically, by reviewing the first results we created a dictionary\nand we pass the output and apply what substitution wherever we see these common errors and we apply\nthe correction so all of that is somewhat automated and we can keep improving our dictionary\nas we find more issues like that. Then, the other thing is that at the end of the day our website\nis the place where we want to output this result in a way that is visible to people and\nthey can consume it so we need somehow to hook this entire project into the process that builds\nour website and our website is also open source. We'll put the link in the show notes. It is a static\nwebsite built with Eleventy so what we do is basically every week every time there is a new episode we\ntrigger a new build and that will generate a new version of the entire website, all the html pages\n, assets and so on and publish that online on a CDN. So what we wanted to do is somehow integrate\nthis process to be able to hook into our website build process and we thought that it would be very\nnice if the Step Function could just do a PR just trying to send the generated file directly into\nthe repository for our website so we did all of that with an additional Lambda at the end of the\nprocess so you might be wondering at this point did we manage to fully automate everything and\nI will say unfortunately not entirely yes but I think we are close enough. We definitely reduced\nall the manual work to the bare minimum and but what's left to do. What we still need to do\nmanually or at least we want to do manually to retain a decent level of quality there so this is\nalso the reason why we we do a PR because first of all it gives us an opportunity to review the\nresult of our transcript before it gets merged and the other interesting thing is that the PR\neffectively is just publishing trying to publish a JSON file in our website repository. This JSON file\nis not ready to go straight away because the speaker identification is just telling us something\nlike speaker 0 and speaker 1 is not able to tell we which one is which depending on the voice is\njust distinguishing between between two different people so we need to quickly look up who is the\nfirst person to talk and just assign the name to to the right label. So this is something we can\neasily do manually directly from the GitHub UI by editing the PR and in the process, we also quickly\nreview, we just eyeball the entire text and if we spot any other obvious mistake we can easily fix\nit manually before merging the PR. So I think that describes more or less the process and what do we\ndo in an automated fashion and what we still do manually. What else do we want to share?\n\n\nEoin: Yeah I think just to summarize I think this has really been a step forward in the transcription\ntechnology and I'm really happy with the level of automation we now have I think it's just the\nright balance between manual effort and automation it's great that you can now use AI and be really\nconfident that you've got a result that people can read without finding it kind of jarring or\ndistracting to read. Some of the things that it really surprised me with OpenAI Whisper is\nhow you mentioned, like, product names and AWS service names it seems to just know what they\nare and get them right most of the time some of the things where it's less accurate is just things\nthat are hard to predict. Like, AWS Bites isn't exactly a top international brand yet so sometimes\nit would spell it with b y t e s instead of b i t e s so there are some things where you'll always\nhave to do those vocabulary substitutions you mentioned but overall I think this is just mostly\nmostly hands off and you end up with a really good result for very little cost. So if this is\nsomething you want to do for your own podcast, the good news is that everything we just told you\nabout is open source so you can find a repo on GitHub. It's called PodWhisperer as a tribute\nto OpenAI Whisper because this is primarily aimed at podcasts but of course you can use this for\ntranscribing meetings any other kind of audio you could think of. You can follow the instructions in\nthe readme and deploy this into your own AWS account so feel free to contribute back to the\nproject if you think there's something missing, improvements you'd like to make, something you'd\nlike to change and we'd really love to hear from you. And we'd gratefully appreciate the chance to\ngrow this and spread it around even further. This is all we have for this episode. We hope you liked\nit and we look forward to hearing your feedback on our transcripts. By the way, if you happen to\nfind a mistake in one of our transcripts you can easily submit a PR like Luciano said. The link\nwill be in the show notes to the AWS Bites static website repo. It will help us fix the issue and\nimprove the quality of what we're doing. It's really nice to have everybody contributing to\nthe podcast. We're really enjoying that so far so thank you and we'll see you in the next episode.\n"
    },
    {
      "title": "64. How do you write Lambda Functions in Rust?",
      "url": "https://awsbites.com/64-how-do-you-write-lambda-functions-in-rust/",
      "publish_date": "2023-01-20T00:00:00.000Z",
      "abstract": "Are you curious about using Rust to write AWS Lambda functions?\nIn this episode of AWS BItes, we will be discussing the pros and cons of using Rust for serverless applications. With Rust, you'll be able to take advantage of its fast performance and memory efficiency. Plus, its programming model makes it easy to write safe and correct code. However, Rust is not a native runtime for Lambda, but rather a library that implements a custom runtime built and maintained by AWS. This custom runtime is built on top of the Tokio async runtime and even has a built-in middleware engine, which allows for easy hook-in of reusable logic and building your own middlewares.\nBut what if you're new to Rust? Don't worry, we'll also be walking you through the steps on how to write your first Lambda in Rust. From cargo-lambda to the serverless framework plugin for Rust, we'll be sharing different alternatives for building and deploying your Rust-based Lambda functions.\nSo join us on this journey as we explore the exciting world of Rust and Lambda.\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nRust 1.0 original announcement\nThe Rust programming language home page\nFirecracker runtime\nLuciano's Twitch profile\nAWS Rust runtime library\nTokio, Rust async runtime\nExample of how to enable the tracing middleware in Lambda\nRustup tool to install the Rust toolchain\nReference article on how to write a Lambda in Rust using cargo-rust\nCargo-lambda, a Cargo extension that helps with writing, running, testing, and deploying lambdas written in Rust\nServerless framework plugin for Rust\nEoin's article on Container Image Support in AWS Lambda\nAWS SDK for Rust\nCoding challenges to learn rust\n\n",
      "transcript": "Luciano: Unless you have been living under a rock, you probably notice that Rust is gaining more and\nmore traction by the day. So today we want to talk about writing Lambda functions in Rust.\nWe will be discussing why you might want to do something like this, analyzing pros and cons,\nthen we will look at the steps needed to author and publish your very first Lambda in Rust.\nFinally, we will give our opinion on what's going to be the future of Rust in Lambda,\nand why we are so excited about it.\nMy name is Luciano and today I'm joined by Eoin and this is AWS Bites podcast.\nAWS Bites is sponsored by fourTheorem. fourTheorem is an AWS consulting partner offering training,\ncloud migration, and modern application architecture.\nFind out more at fourtheorem.com. You will find the link in the show notes.\n\n\nEoin: Luciano, I know you've been learning Rust and sharing as you learn on your channel, which we can put the link for in the show notes. Could you start with a quick description?\nWhat is Rust and why is it gaining so much traction?\n\n\nLuciano: Yeah, I'm gonna try to give my own view.\nI think Rust is a relatively new programming language, and by that I mean that the first release, the 1.0, was in 2015. So it's not that new.\nThere is a good bit of background there and also existed for a while even before version 1.\nSo it is, I think, still relatively new, meaning that it's getting traction more and more these\ndays, but has been around already for a while. So it's not an immature language.\n\n\nThere is some good background there.\nIt was initially adopted in the context of Mozilla to rewrite some parts of Firefox,\nor at least that was the probably the first serious project where Rust was being adopted.\nBefore that it was more of a research language. It is a strongly typed compiled language that\ndoesn't have a garbage collector, so that's a very interesting, for instance, in comparison with Go,\nthat is also strongly typed, but as a garbage collector.\n\n\nIt was initially born as a system programming language because it was particularly focused\non memory safety and performance, so trying to give people a safer experience when compared\nto things like C and C++ with also more modern toolchain.\nBut the interesting thing is that these days it doesn't get promoted anymore as a system\nprogramming language, but it gets promoted more as a general programming language.\n\n\nAnd the website says a language empowering everyone to build reliable and efficient software.\nAnd this is specifically true because in Rust these days you can build pretty much anything\nfrom operative systems up to front-end application using WebAssembly.\nSo that I think that the language is becoming very very general purpose these days,\nwithout losing those major characteristics like being very strongly typed and being very efficient\nand having an interesting memory model. It is used also by AWS heavily, and one of the more famous\nexamples is Firecracker, which is the engine that powers Lambda and Fargate, and it's open source,\nso you can check it out if you want to see what a moderately complicated Rust project looks like.\n\n\nEoin: Okay that's pretty interesting. So even if you're already using Lambda,\nFirecracker is already in the picture there because it's running\nthe Lambda infrastructure, that container micro VM infrastructure is already written in Rust.\nOkay so that's pretty interesting, but I guess the idea of writing Lambda functions themselves in Rust\nis something that's still pretty new. So what has been your experience with that so far? Have you given it a try?\n\n\nLuciano: Yeah so I want to emphasize that I'm not an expert in Rust by any means.\nIt's a language that I've been playing with so to speak for the last three years or three years and\na half, mostly doing coding challenges, some simple games, building small libraries.\nI also did a lot of Advent of Code and I've also been streaming that on Twitch with some friends.\nWe're going to have the link in the show notes if you're curious to see these kind of exercises.\n\n\nSo I didn't really do anything production ready. So my perception right now is just from the point\nof view of somebody that has just been having fun with Rust. We might do this episode again after we\nbuild our first production ready Lambda in Rust and maybe reconsider some of the observations today.\nBut I have actually built one Lambda in Rust for my own personal use case. I have basically a simple\nautomation workflow on Twitter where if somebody follows me there is a hook that starts and it\nbasically is going to send a welcome message to this person saying hey thank you for following me,\ncan I help you with anything and it's just a nice way to get in touch with my followers, start a\nconversation and get to know them a little bit better.\nAnd I built this in Rust just because that was the perfect excuse to try to see what building a Lambda in Rust would look like in a very\nsimple use case. Even though it's not really that simple because it still needs to do a lot of\nsophisticated integration because it needs to connect with the Twitter APIs which are HTTP\nrequests there needs to be some state stored in DynamoDB so there is also the SDK that gets used\nplus I try to do some testing automated deployment and so on so even if it's a very simple Lambda\nthere is still enough complexity to try to understand what writing Lambdas in Rust would look\nlike.\n\n\nEoin: All right, that's pretty interesting, so I guess people can check out your Twitch channel to have a look at what you've been doing.\nWhen I'm hearing about writing Lambda functions in Rust, I kind of think back to the limited experience I have at writing Lambda functions in languages\nlike C#/.NET and Java. I've used Java quite a lot in the past but I wouldn't jump to it immediately for Lambda.\nOne of the reasons is that I guess historically it has been slow, right?\n\n\nThere's a lot of steps taken recently to overcome that with things like GraalVM and then\nSnapStart very recently at re:Invent but generally when it comes to compiled languages, I wouldn't\nrush to them because I feel that dynamic languages are just a lot simpler when it comes to writing\nLambda functions, especially when dealing with JSON payloads, etc.\nSo I feel like there's more steps to perform when you're using compiled languages. What is Rust like in that context and what is the\nappeal of using it for Lambda? And then I suppose the other question is what's the build and\ndeployment experience like? Are there a lot more steps or do they manage to simplify it somehow?\n\n\nLuciano: Yeah, so there is definitely an extra step for Rust as well because it's a compiled language and you need to make sure that you compile your code before you are able to execute it and in this\nparticular case, you need to compile your code for the specific target architecture which is the one where the Lambda effectively runs.\nIt's not the one that you have in your own development\nenvironment so there might be a little bit of complexity there if you need to cross compile\nfrom, for instance, an architecture to another. There are tools that can help and we will probably\nmention some of them later but yeah there is definitely an extra step there just because of the characteristics of the language.\n\n\nOn the other hand, it's a very fast and memory efficient language\nwhich affects very positively cold starts. I've seen incredibly small cold starts sometimes even\na single digit millisecond cold start but generally doesn't go above 20 milliseconds for relatively simple Lambdas written in Rust.\nAlso one thing that I really like is that the programming model\nhelps to write safe and correct code and this is actually one of the reasons why I'm enjoying\nlearning and writing Rust just because the language in itself is very mature.\n\n\nI think it brings in a lot of the nice things that came out of many different other languages.\nFor instance there are no null types. You need to use optional and you need to be very explicit when some data\ntype is going to be there 100% or there may be a case where it's not going to be there.\nAnd every time you have this kind of situation, you need to explicitly decide what's going to happen when the value is not there.\n\n\nAnd this is something that I found helps a lot to prevent a certain classes of\nbugs so by just learning the idiomatic way of writing Rust and the constructs that the language\ngives you, I think it's much easier to end up writing code that is actually more correct and\nthere are less bugs which is definitely another advantage to keep in mind when choosing a language.\nAnd it's also very modern in terms of toolchain and ecosystem on libraries. For instance, there is\na tool called Cargo which is somewhat similar to NPM if you come from Node.js, but it's very rich and\nallows you not just to install dependencies but also to build your code.\n\n\nIt can easily be extended also for testing and for a bunch of other things, so there is this one tool that does almost everything\nyou can possibly need and if you compare that for instance with Python where you have like 12 or\nmore tools if you have to do installing dependencies, packaging, building native dependencies and so on it's a very different experience there.\nAnother thing is that it supports async, so you can write\nvery efficient servers or code that relies heavily on I/O without having to necessarily spin up multiple threads.\n\n\nSo that's another interesting thing for a compiled language and I think actually\nRust spend a lot of time, the community of Rust spend a lot of time trying to come up with a\ndefinition of async that doesn't affect performance and it's actually considered one of the most\nefficient implementation of async so also very interesting that aspect.\nThe thing that is not as nice or at least initially might look a little bit weird is that there isn't that official\nruntime for Rust in AWS, so if you look at the list of the supported runtimes Rust is not there.\n\n\nSo how it is even possible that you can write lambdas in Rust? Well effectively you can do that\nby creating your own custom runtime. AWS makes that a little bit easier than it seems if you have\nto do it from scratch because they literally give you a library that is an official library written\nby AWS (and we will have the link in the show notes) and that library is basically something that you\nuse in your own code in your own handler and basically at the end of the day you are packaging\none single binary which is a custom runtime that contains the runtime itself from the library and\nthen your handler which is basically your own custom code.\n\n\nSo you can imagine you wrap your handler with a custom runtime but that runtime is given to you by AWS in the form of a Rust library.\nAnother interesting thing why I think that this is a very interesting and modern approach is that\nthis custom runtime is fully async and in Rust there are different runtimes for async.\nOne of the most famous is called Tokio (T-O-K-I-O) and Tokio is very interesting because it's kind of an ecosystem of libraries.\n\n\nIt's not just the runtime but there is a lot more that it gives you\nand one of the things that gives you is something called Tower which is a way to define services.\nSo in the most generic sense and you can imagine that a Lambda is a service on its own and a service\nit's something that automatically is like a middleware engine when you use this library\ncalled Tower so basically you don't need something like Middy if you're writing a Lambda in Rust because it's already built in.\nYou can just write your middleware straight away and connect them to\nyour handlers and if you want some examples of what you can do with that there is in the\nTower repository itself - actually no, in the Lambda runtime repository itself - in the AWS Rust runtime,\nthere is an example on how to use the Tower library in combination with the runtime to enable\ntracing and you just enable it as a middleware around your handler code and of course you can\nbuild your own custom middlewares for validation logging, deserialization, error handling and so on.\n\n\nEoin: Is this a little bit like Middy for Rust, then? It's just built into that runtime library they provide?\n\n\nLuciano: In a way, I would say yes because  you already have that engine so when we say Middy, it is probably just Middy Core, just the engine. Then there isn't yet like a very mature ecosystem of middlewares that are specialized for Lambda. That can be an interesting other open source project if you wantto you or somebody else wants to explore building something like the middleware ecosystem for Rust.\n\n\nEoin: Yeah okay, that's an interesting approach because I guess every language has a slightly different take on that.\nSo I think, based on what you said, I'm kind of convinced that there's enough\nof a benefit there for me to try it but I've got literally zero experience with Rust, I've just seen the hype emerge.\nSo where do I start? How would you start? Would I start with Rust first or\nwould I just jump in and try and write my first Lambda function in Rust? What's the best place to begin?\n\n\nLuciano: I'm gonna assume for now that you know a little bit of Rust, even if maybe that's not the case. We can discuss later some tips on how to get started and let's just focus for now on if\nyou know a little bit of Rust, how do you write your first lambda in Rust.\nOf course, you need to have Rust installed. There is actually a very good tool called Rustup which is basically a tool\nthat allows you to install Rust in your system but also to keep it up to date every time there\nare new releases so definitely, the most recommended way to install Rust in any machine.\n\n\nAnd that way you can easily keep it up to date. This week actually I came across an interesting article\nthat showcases exactly like what is the experience of building a Lambda in Rust from scratch.\nIt's actually not an Hello World Lambda, it's actually an interesting Lambda because it's doing some query\nover a JSON file that is compressed. There is actually quite a bit of logic.\nIt can be an interesting use case and this is coming from a company called Scanner which does logs and tracing\nso they are actually showing some of their code at the end of the day, probably it's a simplification\nbut it's an actual business case and they actually use Rust as a Lambda in their own environment.\n\n\nOne thing that I learned through this article is that there is a relatively new tool\nfrom somebody in AWS that is called Cargo Lambda which is an extension of Cargo, the tool we\nmentioned before that does package management, testing, building and so on.\nYou can install this extension and this extension basically gives you a bunch of helpers directly that gets directly\nbuilt in into Cargo to start a new Lambda in Rust, so, create the scaffolding for just like the\nstructure of the code and how do you organize the libraries that you need and also gives you the\nability of testing your code locally and also compile it and deploy it to AWS.\n\n\nSo it's kind of one shop kind of tool that you just install, it extends your own main tool, which is Cargo, and then\nyou just run 'cargo something' to to do all the different things that you might want to do for\nrunning writing and running and deploying lambdas written in Rust.\nSo the experience is that generally if you use something like Cargo Lambda, you will do, i think it's 'cargo-lambda new' or something\nlike that to create the structure. That gives you already a Rust project with runtime already installed.\n\n\nIt's a little bit like Python or Node where you have a file where you define all\nyour external dependencies, so all this structure is created for you you will have this cargo.toml\nwhich is the basically the the package.json of Rust if you want and, in that package.json you\nalready will have a reference to the latest version of the Rust runtime and then it also\ncreates a main file and the main file is basically your entry point when you compile your Rust code.\n\n\nIt's going to start from that file, it's going to look for a main function and that's what gets\nexecuted first and that main function is already hooked into the Rust runtime as a library so\nliterally what you have left to write is your handler.\nSo using this tool, the experience of writing Lambda code is not very different from the experience you would have when writing something\nin Python or in Node.js and another interesting thing that it's a common gotcha at the beginning\nis that if you're used to writing Lambdas in Python or JavaScript, it's very easy to to receive\ngeneric pieces of JSON in your event, process them read some data and do something with it and then\nreturn another very generic JSON string as an answer.\n\n\nOf course, when you use compiled languages/strongly-typed languages, it gets a little bit more complicated because you cannot just say whatever data type.\nYou need to be very specific with the data you are receiving and the data that you are returning.\nThere is a library that can help a lot when you want to still keep that very generic\nability to handle JSON input you don't want to create like very strict serializer.\n\n\nMaybe you want to accept very generic JSON. This library is called SERDE\nand it's the most common library for serialization the serialization Rust and it also supports JSON.\nWith this library, you can write an handler that can receive any arbitrary JSON and then you\nhave different ways to extrapolate data from the arbitrary JSON. It effectively gives you\nan object that's like a tree and you have to traverse this tree and verify, okay am I getting\nan object here, is this probably something that exists, is it a string, is it an integer.\n\n\nIt takes a little bit more code to actually deserialize - well - to extrapolate your information but you\ndon't have to deserialize into a very specific type if you want to do that.\nActually, I think the runtime comes with for instance HTTP types and the most common types already built in so if you\nare building handlers that are already targeting very specific types of default events in AWS\nthere are different types already built in that you can just use and you will have access to all\nthe different properties that are expected for those events.\n\n\nThen the next step is, how do you do local testing. So with cargo lambda you can just run 'cargo-lambda watch' I think is the command\nwhich allows you to spin up like a local version of your lambda and then local version restarts\nif you do changes in your code. So it's kind of a live autoloading server at that point and then\nyou can use 'cargo-lambda invoke' I believe is the other command to send an event to this\nlocally running server and see exactly what happens if your code receives that event.\nSo you can keep doing that until you are happy with with the result of your code and then at that\npoint, you can run 'cargo-lambda build' and that will effectively compile your code and you can even\npass a flag to build for graviton which is something I haven't tested yet but I'm very excited to try\nthat because I'm hearing that that makes the performance in certain cases even more interesting.\nAnd there is also a template that you can easily when you bootstrap your project you also get\nGitHub CI template that you can use if you want to automate the testing and building process in yourCI if you use GitHub Actions.\n\n\nEoin: I like the idea that this cargo lambda tool gives you this kind of one-stop shop for everything you need to do to create and build rust lambda projects but I'm\nkind of wondering how does it fit in with the rest of the ecosystem that we know already, like SAM and\nServerless Framework, which allow you to create multiple functions and also deploy other resources\nalong with that as well. Is the rust movement with Lambda moving towards its own\nkind of closed ecosystem that doesn't link through to the other tools or can you use some of those other tools with rust as well?\n\n\nLuciano: Yeah I don't think we have a critical mass of Lambda Rust projects today to say that the ecosystem is going one direction or another.\nI think it's still very very new. We saw actually when we were creating the the show\nnotes for these episodes that there is a serverless plugin so if you're using the Serverless Framework,\nthere is a Rust plugin that should help you to to build things correctly and structure things\ncorrectly if you're using Rust. We haven't tested it yet, so your mileage might vary but there seems\nto be different kind of tools, it's not just Cargo Lambda.\n\n\nYou can also use container images, so you could package rather than building this zip file with a binary inside, you can package everythingas a container.\nAnd we can reference an article that Eoin you wrote some time ago if you want some generic guidance.\nYou will need to adjust that for Rust and maybe you can find some other\nmaterial that gives you more guidance on how to do that specifically for Rust.\n\n\nI did try myself to do things manually because when I wrote my first Lambda I didn't know about this cargo lambda\nmaybe wasn't even there it takes a little bit of work because you need to figure out exactly what\nare the right flags to compile your code for the right architecture and then how to zip your file\ncorrectly so that the runtime is actually bootstrapped correctly so it's not that obvious\nand when I did it the first time there wasn't a lot of guides out there so I had to spend a little\nbit of time doing trial and error until I figured out exactly how configure I should have worked\nso I'm looking forward to to spend more time playing with cargo lambda because it seems like\nit's the kind of tool that I would have wanted when I was doing this particular exercise and\nI ended up doing a lot of stuff manually trying to figure out things by trial and error.\n\n\nThen another interesting thing is that in that particular example that I mentioned before that\nTwitter integration I needed to connect to something like DynamoDB so you might wonder\nokay, what's the SDK experience like? It is good actually because now and by now I mean probably\nthis was reinvent of two years ago if I'm not wrong there is an official rust SDK officially\nsupported by AWS before then there used to be somebody writing it as a library so it maybe\nwasn't the most up-to-date experience. It wasn't that official experience anyway now there is an\nofficial SDK so you might expect that it is as good as the other ones if it's maintained by AWS.\n\n\nIt is still a little bit too verbose because I think it's just the nature of\nstrictly-typed languages that you need to when you use this kind of SDKs you are constructing a lot\nof objects and combining them and then calling specific methods.  And it tends to be a lot more\nverbose when done with a strictly-typed language compared to something like JavaScript or Python.\nAlthough one thing that I like is that there are a lot of builder pattern utility so they can give\nyou shortcuts to build objects where you can basically use all the defaults and just specifies\nthe thing that you want to customize. It still takes a little bit of work to get used to it coming\nfrom the JavaScript SDK or the Python SDK but I think eventually you can easily get used to it and\nthe typing system it can actually be a good guidance to see exactly which options are supported,\nwhich ones are not required and so on. So yeah, could be a bit verbose but also could be a little\nbit more guidance than you get when using JavaScript or Python.\n\n\nMaybe I can say why I am particularly excited about Rust and Lambda together. The first thing and we already mentioned that is\nbecause for all these cases where memory and speed are directly related to cost and this is\nparticularly the case for serverless. Using Rust can really give you the most optimized experience\nyou can possibly create so you can eventually save a lot of money, so basically what I'm trying\nto say is that if you're writing something in Rust for Lambda, you should be able to get fairly\noptimized memory consumption and performance.\n\n\nSo you are reducing to the minimum the two dimensions that eventually affect the cost of running your own Lambda.\nI think in all these cases where you might be concerned about performance and cost probably investing in Rust might be a good investment.\nAlso again we already said that that the language itself pushes you to write code that\nis generally more correct so it's potentially harder to create specific kind of bugs so that\ncan be another advantage if you have very business sensitive areas where you want to make sure that\nyou are not writing bugs or at least reduce the risk of writing bugs. So I expect Rust to become\nmore and more prominent in general so we will see a larger ecosystem, more libraries, more tutorials,\nmore examples, more people willing to help or even to be hired in your company with that expertise\nalready. So that, I think, contributes in the future of Rust and Lambda so we will probably see more\nuse cases of Rust and Lambda just because the ecosystem is growing.\n\n\nEoin: Yeah it sounds like there's a lot of benefits like from what you said, I can think of, like, you got a modern ecosystem,\ngreat tooling right for Rust itself and also for Rust with Lambda now you've got the performance\nbenefit which was definitely going to be of interest to people who are like really trying\nto optimize. and then also that relates to cost you know so if you've got a really hot Lambda and\nyou can benefit with an order of magnitude improvement in performance potentially with Rust\nthat has a huge appeal. So there's lots of positives there.\nIn the interest of balance, are there any drawbacks we should be aware of? And maybe what's your advice in summary? Should people try\nand adopt it now or hold off trying Rust with Lambda?\n\n\nLuciano: Yeah so I think the main decision point on whether you should be using Rust or not is probably the learning curve so there is still a\nlittle bit of learning curve. It's not a language that most people know and it's a language that\ncomes with a very specific background that is closer to system programming than it is to\nweb programming. So if you are coming from web programming, there might be a little bit more to\nlearn there before you can fully appreciate the language and then you need to learn all the\ndistinctive characteristics of the language; the syntax, the memory model and so on.\n\n\nSo there is definitely a little bit of learning curve there so it's not something to underestimate that learning\ncurve because if you are building Lambdas you're probably trying to optimize for speed of delivery.\nWhen you think that you need to learn a new language and you combine with that probably there\nis some sacrifices to be made there in the short term to give your team time to learn the language,\nmaster it enough that they can be proficient with it and then you are going to get all the benefits\nthat Rust gives you. So that's something that only you can evaluate in the context of your company\nif it makes sense. But if you are doing maybe some side project or some very small experimental Lambda\nit might be something worth trying just to get a feeling for what it looks like.\n\n\nI would say though that before you try it, if you don't know Rust itself, maybe it is worth\nlearning a little bit of Rust first so just to get used to the characteristics of the language.\nYou can do some coding challenges. I really like exercism.org - we will have a link in the show\nnotes - because it has it's a totally free platform. First of all it looks like leetcode, like one of\nthose platforms where you can do coding challenges. But they have a specially good Rust track that is\nbasically a sequence of I think 50 exercises that guides you through all the basics that you need to\nlearn to master the main concepts that you need to know in Rust.\n\n\nSo that could be a very good way to just get started. It's actually quite fun and the website is really well polished so maybe that's a\nlow effort way to to get started with Rust and then when you complete that track maybe you\ncan start to try now how does it look like to actually try to write a Lambda with it.\nSo I think that's that's probably everything for today we probably cover way more than we wanted to\ncover originally I really encourage everyone to to try this and give us your feedback if you found\nthe experience interesting useful or maybe was just too difficult and you couldn't really progress\nand you had to go back to JavaScript or Python or something else. And if you build something with it\nalways let us know what do you build because you know we are always curious to get use cases and\nunderstand how people use Lambda, Serverless, AWS and definitely we can have some nice conversations\nafter that and we can also revisit our own assumptions and our own understanding of Rust\nand Lambda together and maybe come up later on with a new episode with a refreshed perspective\non this particular topic. So thank you very much for being with us today and we look forward to\nseeing you in the next episode.\n"
    },
    {
      "title": "65. Solving SQS and Lambda concurrency problems",
      "url": "https://awsbites.com/65-solving-sqs-and-lambda-concurrency-problems/",
      "publish_date": "2023-01-27T00:00:00.000Z",
      "abstract": "In this episode of the AWS Bites Podcast, we dive into the serverless pattern of using AWS Lambda together with SQS. We explain the basics of both Lambda and SQS for those who may not be familiar with them. We talk about how we use Lambda, a Function as a Service offering in AWS, to write our own functions and have AWS run them in response to certain events. And we also discuss SQS, a scalable and managed queuing system available on AWS, which we use to offload work to background workers.\nWe delve into how the two services work together through the use of &quot;Event Source Mapping&quot; in Lambda, which polls our SQS queue and makes synchronous Lambda invocation requests when messages are available. We also mention how this feature provides us with the ability to control batch size and window, as well as specify filters to save execution time and cost. But we also share one of the limitations we faced when using SQS and Lambda together which was the lack of control over concurrency and the potential for excessive throttling.\nBut recently, AWS has released a new feature called &quot;SQS maximum concurrency support&quot; which allows us to specify a maximum number of invocations for an Event Source Mapping. This solves the problem of excessive throttling and eliminates the need to use reserved concurrency. It also allows for more control over concurrency when using multiple Event Source Mappings with the same function. We explain how this new feature has improved our workflow and made it much more efficient.\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nAWS Lambda\nAmazon SQS\nSeries of blog posts by Zac Charles covering the original problem and the solution in detail\nOfficial AWS blog post with the announcement of the maximum concurrency feature\nOur previous episode on SQS\nOur video-series on AWS event services\n\n",
      "transcript": "Eoin: Using AWS Lambda together with SQS is a very common serverless pattern\nthat has always suffered from some special limitations.\nWe covered SQS in a dedicated episode last year,\nbut recently we've had a significant new feature solving a common pain.\nAnd today we want to dive deeper into using SQS and Lambda together\nand tell you all you need to know about using SQS triggers,\nabout scaling and concurrency.\n\n\nI'm Eoin, I'm here with Luciana,\nand this is another episode of the AWS Bites podcast.\nAWS Bites is sponsored by fourTheorem.\nfourTheorem is an AWS consulting partner offering training,\ncloud migration and modern application architecture.\nYou can find out more at fourtheorem.com\nand you can find that link in the show notes.\nLuciano, let's start with addressing the basics\nfor anyone not intimately familiar with AWS Lambda and SQS.\nWe have a lot of seasoned experts out there listening,\nbut we also know that plenty of people are listening and watching\nor taking their first steps with AWS or these serverless offerings.\nDo you want to give a quick overview and a quick elevator pitch for SQS and Lambda?\nOkay, I'll try my best and I'm going to start with Lambda.\n\n\nLuciano: So Lambda is basically a function as a service offering\nthat is available on AWS.\nSo what that means is that it is effectively a managed compute service\nwith a very simple abstraction.\nAs developers, we are familiar with the concept of a function,\nwhich is basically a piece of code that takes some inputs,\nexecutes some logics and returns some output.\nAnd Lambda basically takes that particular model\nand provides it as a managed on-demand compute layer.\n\n\nSo as a user, you write your own Lambda function,\nyou can use many languages, so pick the language of your choice.\nYou write your own business logic in that particular shape,\nso in that particular function format,\nand then you just tell AWS when to run that particular function.\nAnd generally, this is in response to a particular event.\nJust to give you an example, that event can be an HTTP call\nif you're using something like API Gateway\nand you are trying to implement an API in a serverless fashion.\n\n\nIt can be a schedule, I don't know, every Monday at 9am,\nmaybe you want to do something,\nso you can trigger the Lambda that way on that schedule.\nOr maybe you want to react to certain files being created in S3,\nthat can be another trigger.\nOr maybe, because we are going to be talking about the integration with a queue,\nyou can trigger your Lambda as a response to a new message\nbeing available in SQS, which is a queueing system available in AWS.\n\n\nSo let's just talk more about SQS as well.\nSo, very similar to Lambda, SQS is another managed,\nscalable service provided by AWS.\nSo if you need a queueing system and you don't want to manage\nall of the deployment updates, security patches, scalability by yourself,\nyou just go on AWS and you provision a new SQS queue.\nAnd just to explain why you would want to do that,\nlet's present an example.\nOr actually, in a more generic sense, let's just say that you have some\npiece of functionality going on, but also you might want to do\nmore work on demand in the background.\n\n\nFor instance, I don't know, you are sending transactional emails,\nor you need to resize some pictures, or you need to run some workloads.\nFor instance, you have some text available in picture format,\nand you want to extract the text, maybe running an OCR algorithm.\nSo all things that you don't want to do in line,\nyou probably want to offload in the background,\nand maybe you want to parallelize that kind of compute.\n\n\nSo what you could do there is you could create a queueing system,\nmaybe use just SQS, and then every time a new job becomes available,\nyou get the definition of that job.\nRather than doing it straight away in the process that receives\nthe definition of the jobs, you just send it to the queue,\nand then the queue is going to keep it in storage somehow,\nand other workers in the background can just ask to the queue,\nis there something I can do?\n\n\nThey can pick up the work and just do it in the background.\nNow, this brings a few interesting advantages.\nThe first one is that you are not blocking the core application.\nIf it's a web server, the web server can reply to the user as fast as possible.\nThis is what the user expects on the web,\nwhile all the heavy work is offloaded to the background.\nIf you have a peak of traffic, maybe that creates a lot of work\nthat you'll need to do in the background.\n\n\nSo by having a queue and having workers, you have a decoupled system,\nand basically you can decide to scale up the workers' part.\nSo add more and more workers to be able to respond\nto that increased demand for background work.\nAnd that can be very elastic, so when the demand is over,\nmaybe you go back to a normal,\nyou can remove all the workers that you don't need anymore.\nAnd also that adds resiliency, because if something fails,\nthe queuing mechanism can automatically recognize that a job failed\nand put it back in the queue, so that means that another worker\nwould pick it up again later and you can retry it automatically.\nAnd even more interesting, if that particular job keeps failing,\nyou can add rules to basically move that job on the site.\nThey are generally called dead letter queues.\nSo it's basically another queue where you store all the jobs\nthat you were not able to process, and a human can just go there\nand try to figure out why this consistently failed.\nMaybe there is a bug.\nYou can fix that bug in your code and then push the message back\nto the original queue, and at that point,\nyou are able to reprocess that message correctly.\nSo that's basically giving you ways to never lose jobs\nand consistently be able to deliver on what the user expects.\nWe want to talk about Lambda and SQS together.\nSo how do they work together?\n\n\nEoin: So with SQS, you're always using a poll-based model.\nYou would need something to poll the queue,\nretrieve events, process them, and then delete them.\nIt's a fairly simple API, really,\nwhen it comes to consuming messages from SQS,\nand we covered that in detail in the previous episodes.\nSo traditionally, you'd use EC2 or a container\nor some other piece of long-lived compute running on AWS\nor even on-premises or anywhere else.\n\n\nWith AWS Lambda, it's a lot simpler\nbecause you don't have to run a poller yourself.\nThe polling service is actually provided\nas part of Lambda's Event Source Mapping feature.\nAnd you may have used SQS and Lambda together\nwithout knowing that there was such a feature\nbecause a lot of things like the serverless framework or SAM\nkind of create this for you transparently under the hood\nwhen you create that trigger.\n\n\nSo within the AWS Lambda service,\nyou've got this Event Source Mapping feature,\nand this is the bit that's doing the polling.\nIt's also the same feature that handles Lambda triggers\nfrom Kinesis and Kafka, MQ, and DynamoDB streams.\nSo if you imagine a simple architecture diagram,\nyou've got your queue on the left\nand a Lambda function on the right.\nThe Event Source Mapping is essentially a box in the middle\nthat's running that polling and passing messages from the queue\nand invoking Lambda functions with the messages.\n\n\nAnd those invocations are actually synchronous.\nSo they're not, with Lambda,\nyou've got synchronous and asynchronous invocations.\nEvent Source Mapping is using a synchronous invocation\nand waiting for the function invocation to complete.\nSo Event Source Mappings are very good\nbecause they give you a few neat features for free\nthat you would have to implement yourself.\nLike you can control the batch size and the batching window\nin terms of the number of events that arrive in a batch,\nwhat kind of time interval they need to arrive within.\n\n\nAnd since about a year or so ago as well,\nyou can also specify filters.\nSome messages are filtered out\nbefore they reach your function.\nSo that can save you a lot of execution time\nand cost as well.\nSo if you imagine, if you've got an instance\nor a number of EC2 instances polling from a queue,\nyou're in control of the processing rate and the concurrency\nbecause it's directly linked\nto the number of workers you have, right?\nYou can retrieve a batch of messages\nand process them with whatever cluster size\nor worker pool size you have running.\nNow with Lambda, the Event Source Mapping\nis doing this for you.\nSo it's in control of the concurrency.\nAnd it's this fact that's been a source of pain\nfor a lot of users.\nAnd this was the case until very recently\nwhen the AWS announced a feature\ncalled SQS maximum concurrency support.\n\n\nLuciano: That makes a lot of sense to me,\nbut maybe we can provide an example\nof what is the pain we are talking about\nso we can make it more obvious to everyone.\nAnd also how is this new feature helping\nto kind of ease this particular type of pain?\n\n\nEoin: Yeah, no, this is good.\nLet's try and give some sort of an example.\nSo let's say you've got a queue that has messages relating\nto signups for your SaaS application.\nSo user fills in a form, signs up,\nthey're now customer reviewers.\nAs part of this whole signup flow,\nmaybe you've got this event driven mailing list\nsubscription features.\nSo when a user signs up, you go off\nand you want to make an API call to MailChimp, for example,\nso that they're going to receive\nyour weekly user mailing list.\n\n\nNow let's say in this contrived example\nthat MailChimp has a rate limit\nof 10 invocations per second to this API.\nSo you've got a queue and a Lambda function\nthat takes the signup event and initiates a subscription\nwith the MailChimp API.\nSo you want this to scale as users sign up,\nyou want to have this resiliency you talked about,\nbut you don't want to flood this API\nbecause it's got a rate limit.\n\n\nSo let's talk about the behavior\nbefore we've got this recent change\nin how Lambda and SQS work together.\nSo the Event Source Mapping in Lambda\nstarts five pollers by default,\nreading messages in batches from the queue.\nSo let's say you've just got a batch size of one,\nbut you've got more than 10 messages coming in per second.\nSo when messages are available,\nthe Event Source Mapping will pass these\nto running Lambda containers.\n\n\nBut if you've got more messages coming in\nbecause your service is really popular\nand people are signing up at a really, really fast rate,\nthe Event Source Mapping is still going to scale,\ntry and scale up the number of Lambda workers\nby making synchronous invocation requests.\nAnd it's going to increase that concurrency level\nby a factor of 60 every minute.\nAnd it will keep going up to the account concurrency limit\nor the reserve concurrency or 1000, whichever is the lowest.\n\n\nSo in order to prevent your function\nfrom exceeding that relative API limit,\nyou might set the reserve concurrency to five\nbecause you're thinking, okay,\nmaybe this function takes about 500 milliseconds to invoke.\nSo in order to keep it at 10 per second,\nI'm going to just have five concurrency.\nBut Event Source Mappings don't seem to know anything\nabout your functions reserve concurrency\nor the account level concurrency.\n\n\nIt just keeps scaling up.\nSo the Lambda for service will stop there\nfrom being more than that number of concurrent workers,\nbut Event Source Mapping\njust keeps trying to invoke functions anyway.\nAnd this results in invocations being throttled.\nSo this can happen in a lot of different cases,\nlike when other functions\nare consuming the account concurrency\nand there just isn't the available capacity as well.\nIt can even happen if you've got cases\nwhere you've got multiple Event Source Mappings\ninvoking the same function, which is also possible\nbecause each one is scaling independently.\nSo this has been a source of a lot of pain\nfor use cases like this.\n\n\nLuciano: So what happens when the throttling actually occurs\nand how can we actually leverage\nthis new maximum concurrency feature\nto make our life easier?\n\n\nEoin: Yes, when the throttling occurs,\nmessages are going to go back onto the queue\nonce the visibility time has been reached.\nAnd if this keeps happening after a number of retries,\nwhich is configurable, the message will be discarded,\nor if you've got a dead letter queue,\nit will end up in your dead letter queue.\nSo the new maximum concurrency feature\nis really doing exactly what it implies.\nIt's specifying a maximum number of invocations\nat the Event Source Mapping level.\n\n\nSo you don't need to use a reserved concurrency\nfor the function,\nalthough you can use both together in combination.\nSo it solves the problem by essentially\ncapping the number of concurrent invocations\nand reducing the excess of throttling\nthat can happen with the default behavior.\nSo it's helpful for our example,\nwhen you don't want to flood the third party API\nwith requests that might cause a rate limiting error,\nit means you don't have to use a reserve concurrency,\nwhich can be annoying for people,\nbecause when you use reserve concurrency,\nyou're also taking away capacity from other functions.\nIt's also nice for anyone using multiple events\nor mapping with the same function\nthat you can now control the concurrency\nof each trigger independently,\ninstead of just the function as a whole.\nYeah, that makes a lot of sense.\n\n\nLuciano: Basically before we were effectively hacking the system,\ntrying to limit the number of execution\nwith something that was not necessarily meant\nto be used in that way.\nAnd that was creating the side effect\nthat the event source mapper\nwas still trying to trigger your Lambda\nand probably you would end up with a lot messages\nin the dead letter queue,\njust because there wasn't capacity to execute them,\nnot because the messages were actually failing.\nSo probably this will lead to a lot of false positives\nand then somebody needs to look at them, retry them,\nand a lot of like overhead\nfor just because for lacking the capacity of saying,\nI don't want you to run more\nthan a certain number of Lambdas at any given time\nfor this particular source of events.\nSo that makes a lot of sense.\nIs there any other improvement\nthat we would like to see in this integration\nbetween Lambda and SQS?\n\n\nEoin: One of the things I mentioned\nwas that the scaling rate of Lambda and SQS,\nit's adding 60 concurrent function executions per minute.\nNow this is pretty slow scaling rate.\nAnd if you've got like batch processing workloads\nwhen suddenly you've got tens of thousands\nof requests coming in and you want to scale out\nto maybe hundreds of thousands\nof Lambda functions concurrently,\nadding 60 every minute is really slow.\n\n\nAnd I've encountered this myself\nand then had to use other mechanisms.\nSo if you have just the Lambda API\nand you call invoke directly with the async mode,\nthen you can scale to thousands of concurrent functions\nin seconds.\nAnd I know for a fact that that's using SQS internally\nto manage that queue of invocations as well.\nSo it's still a bit strange that SQS\nseems to be really slowing down your scaling rate\nand other events sources don't,\nlike with Kinesis, it's tied to the number of shards.\nSo this is a bit limiting.\nSo I would like to see if there was a new future coming out\nin this integration, I'd like to see that changed\nand make it more configurable so that you could at least,\nif you choose to,\nyou can scale up much faster than that.\nThat would be my number one next feature.\nThat makes a lot of sense.\n\n\nLuciano: I also have a slightly related comment\nthat another feature that is available in Lambda\nis that you can consume messages in batches,\nnot necessarily just one by one.\nNow this is not necessarily going to solve this problem\nbecause yeah, this problem still exists,\nbut it's another dimension that you might use,\nfor instance, to handle throttling\nor to handle cases where you want to,\nwhere maybe the task that you need to perform\nis very small and therefore it makes sense\nto try to get this task together.\nSo you pull once from the queue and then you,\nthat Lambda that gets executed can do\na certain number of dams together\nrather than just doing that one by one.\nSo this is just something to keep in mind\nand something we mentioned in the other SQS episodes.\nSo maybe check out that particular feature\nif you're trying to figure out what kind of patterns\nyou can use when using SQS.\nWith that being said,\nare there resources that we want to recommend people\nif they want to deep dive?\n\n\nEoin: A lot of people have been writing and talking about this maximum concurrency feature recently,\nbut I think the best place to go is the series of articles\nwritten by Zach Charles,\nwho described this problem very well\nwhen he originally encountered it,\ndescribed how to reproduce that problem\nand has now written a followup in that series\nabout how this solves maximum concurrent,\nhow this maximum concurrency feature solves the problem,\nbut also some other things you might want to watch out for.\nSo that is definitely the go-to guide here.\nWe will also include a link to the AWS blog post\nand sample code provided with the announcement.\nThere's a SAM template that you can use\nto explore the new feature.\nAnd of course, do check out our previous episode on SQS\nand all our other series on all the AWS event services.\nSo that's it for today's episode.\nThank you very much for joining us\nand we'll see you in the next episode.\n"
    },
    {
      "title": "66. AWS SAM v Serverless Framework",
      "url": "https://awsbites.com/66-aws-sam-v-serverless-framework/",
      "publish_date": "2023-02-03T00:00:00.000Z",
      "abstract": "Discover the Ultimate Battle: Serverless Framework vs AWS SAM!\nAre you building and deploying serverless applications and don't know which tool to choose? Look no further, as we dive into a comparison of the two heavyweights in the serverless world - AWS SAM and Serverless Framework. Find out their unique features, ease of use, and what the future holds for these Infrastructure as Code (IaC) tools. By the end of this episode, you will know which one is right for you and your projects!\nJoin us as we explore the pros and cons of each tool, from the flexibility and ease of use of Serverless Framework to the cloud-side deployment management of SAM. Learn about the different syntax options, supported languages, and credentials management (especially SSO).\nGet the inside scoop on the installation process and build and deployment capabilities, including the new &quot;sam accelerate&quot; feature for faster development. Discover the difference between handling multiple components and stacks and how each tool keeps up with new AWS features.\nDon't miss out on this exciting episode as we determine the winner in the ultimate battle of Serverless Framework vs AWS SAM!\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nOur previous episode discussing Cloudformation vs Terraform\nOur previous episode discussing the Serverless development experience\nServerless framework SSO support issue on GitHub\nSAM connectors for permissions\nOur previous episode on AWS OIDC providers\nSLIC Watch plugin for easy serverless observability\n\n",
      "transcript": "Luciano: There is a growing number of tools for building and deploying serverless applications in the cloud.\nWe are going to focus on two of the main alternatives, AWS SAM and the Serverless Framework.\nWe are going to compare the features, the usability, and also make some speculation about\nthe future of these two different products. If we do a good job by the end of today's episode,\nyou should know which one you are going to choose for your next project.\nMy name is Luciano and I'm here with Eoin and this is AWS Bites podcast.\nAWS Bites is sponsored by fourTheorem. fourTheorem is an AWS consulting partner offering training,\ncloud migration, and modern application architecture. Find out more at fourtheorem.com.\nYou will find this link in the show notes.\nSo, we are ready to get started and let's talk about Serverless Framework and SAM,\nwhat are these tools for?\n\n\nEoin: Serverless Framework and SAM, they are both infrastructure as code tools.\nI suppose that's the first thing we can say then. They are geared towards serverless applications, but the\napplications you build and deploy with them, they don't have to be serverless. They are useful for\ndeploying AWS infrastructure in general and just providing kind of a layer of convenience over CloudFormation.\nSo, CloudFormation, we have covered this in previous episodes, especially\nin our very popular CloudFormation versus Terraform episode.\n\n\nCloudFormation is very explicit and it can be very verbose. So, with Serverless Framework and SAM, you're providing\na more concise user-friendly interface that's just a little bit more implicit and provides a lot of\nshortcuts and defaults for you. They are designed, I would say, to make the process of writing\nFaaS functions, so Lambda functions easier, but they also provide some convenient wrappers for\nAPIs and step functions and things like that. And they also not just allow you to write the\ninfrastructure as code template in an easier way, but they provide certain tools for building,\npackaging, deployment and local invocation as well. Doing all that stuff manually is a lot of\nwork. So, maybe at this point, we can just give a quick run through what are the highlights of\nServerless Framework, what are the highlights of SAM, and then we can dive into some of the,\nI suppose, the more detailed differences between the two. So, let's talk about Serverless Framework\nand for many, this is still the original and best. It was very popular when it first came out and it\nwas probably, I would say, very instrumental in helping many people get excited and start adopting\nserverless in the first place. I would say the community, all of us serverless developers owe\nquite a lot to it and to the maintainers. It supports multiple cloud providers, which sometimes\nwe forget about, especially if you're working in AWS all the time, but it has backends for Azure\nand Alibaba cloud and Cloudflare as well. So, you can deploy to multiple cloud providers with it.\n\n\nWhat that means really is that the tooling remains the same, but of course, your code and your\nconfiguration syntax will be significantly different depending on the provider you choose.\nAnd a big thing about Serverless Framework is its plugin ecosystem. And we'll talk a little bit more\nabout that. So, that's the Serverless Framework. And then a couple of years ago, SAM came along.\nSo, SAM has only been generally available since 2020. And it's an open source project as well,\nbut it's developed and managed by AWS and it targets AWS as a cloud provider alone.\n\n\nSo, the way they implement it is as an extension to CloudFormation using something called cloud\nformation macros. What that means is that more of the deployment with SAM is happening cloud side.\nSo, it's managed by the CloudFormation service for you. Whereas with Serverless Framework,\nthe template is compiled wherever you're doing your build and deployment on your laptop or on\nyour CI CD server. So, you could say that the SAM approach is purer in that it's one that pushes\nmore of the deployment responsibility to AWS to manage. An interesting point to know, we've talked\na lot about CloudFormation here. Recently, they announced SAM Terraform support. Now,\nthat might excite a lot of people. It's in preview at the moment, but it isn't trying to allow you to\ndo everything you can do with SAM with Terraform. It just allows you to run a limited subset of\nfeatures. So, if you've got Lambda functions in a Terraform project, you can use the SAM\nCLI tooling to build and invoke them locally. That's all it's doing for you.\nThat's the headline features for SAM and Serverless Framework. Luciano, do you want\nto dive into some of the ways we can compare and contrast SAM and Serverless Framework?\n\n\nLuciano: Of course. I will say that the first main difference is a little bit conceptual.\nI think it's kind of that difference between opinionated versus less opinionated and more flexible.\nI would probably put SAM into the more opinionated bucket while serverless tries to be a lot more\ngeneric and flexible. And just to give you an example, SAM doesn't allow you to modify a\nresource if it's not effectively declared in your stack. So, you need to be a lot more explicit.\n\n\nAnd this is not something you can work around. It's just the way that the framework is designed.\nWhile SLS will give you ways to do that if you need to. And there are other differences that go\nin this direction. For instance, a lot of things in serverless are done through plugins. So, there\nis a natural approach to just build a plugin and you can extend with additional functionality if\nyou want to. It's not as easy to write plugins in SAM. And we will talk a little bit more about that\nas well. Another big difference is the template syntax. Even though they both use mainly YAML,\nor at least by default YAML is the main thing you would use to write the SAM templates or the\nServerless Framework templates, I would say the Serverless Framework is, again, way more flexible\nthan SAM. In Serverless Framework templates, you have a very expressive sublanguage. Like,\nyou can nest expressions into the YAML. And these expressions can basically allow you to\ninterpret data from different sources. And one of the most impressive ones, and you can just say,\nget data from this JavaScript file. And what happens behind the scenes is that it's going\nto literally invoke the JavaScript file so you can build your own custom logic. And then whatever\nthe JavaScript file is exporting is the data that gets interpolated in your own template. So, that's\none of the easiest ways that I've seen in any system to basically, in any configuration-based\nsystem to allow you to write your own dynamic code whenever you need something a little bit more\nbespoke. And what's even more interesting is that in recent versions of Serverless Framework,\nyou can even avoid YAML entirely. And you can write the configuration template in JavaScript\nitself or even in TypeScript if you want to. And that opens up for even more dynamic scenarios,\nbecause of course that's JavaScript code that gets interpreted when the template is executed.\n\n\nSo, you can effectively run JavaScript code at that point in time if you need to do something\neven more custom. And I've seen some interesting examples where people will use that, for instance,\nto use to generate OpenAPI documentation using maybe schema definition that they have defined\nalready in JavaScript or TypeScript. So, this is just to give you a feeling of how powerful\nthat approach is. You might argue that it's not like it loses a lot of the declarative that's\ntypical of this kind of systems, but it's an interesting trade-off and in some cases it might\nbe worth exploring this particular approach. Now, going back to SAM, SAM, as we say, is a lot more\nstrict. It gives you a syntax that is much closer to the CloudFormation syntax. And it's basically\nkind of a trimmed-down version of that CloudFormation syntax because it's just giving\nyou some macros that will be later on expanded in the cloud into proper CloudFormation syntax.\n\n\nSo, it's just a way to basically allow you to write more concise syntax for functions,\nevent triggers, step functions, APIs, policies. But at the end of the day, when the code is\ngetting prepared to be deployed in the cloud, it will be expanded again to a fully-fledged\nCloudFormation. The way that you can interpolate variables is much more limited than Serverless\nFramework because you can only use parameters and variables that are the standard ones available\nalready in CloudFormation. You could be hacking that a little bit if you create your own pre-processing\nstep. For instance, you could use any templating language that will eventually emit that SAM yaml,\nbut of course it's a bit of a hacky way and this is something that you will need to build yourself.\n\n\nIt's not built-in in the tool. Another interesting thing regarding syntax is the recently SAM\nintroduced cfn lint, which is a tool that has been available for a while but now is kind of built-in\ninto the SAM experience and it's a very nice tool that allows you to validate templates just to make\nsure that the syntax that you are writing in that yaml is conformed to whatever syntax CloudFormation\nis going to be able to accept. So, it's an extra validation layer that is closer to effectively the\ntarget that you have there. Now, let's talk a little bit about languages that are supported\nand the languages that are actually used to write these tools because I think that gives you some\nideas also why one is more flexible than the other, at least in my opinion. Serverless Framework,\nyou will probably notice using it that it's very well built for the Node.js experience.\n\n\nLike, kind of everything works out of the box if you're writing Lambdas in Node.js\nand I don't think this is a coincidence. I think it's probably a natural consequence of the fact\nthat Serverless Framework itself is written in JavaScript so the authors of the framework\nprobably had a lot of expertise with that particular language and they probably spend\na lot more time fine-tuning the experience of using Serverless Framework for writing JavaScript code.\n\n\nBut, of course, other languages are supported but generally they require plugins.\nAgain, that idea of extensibility through plugins. So, your mileage might vary depending on whoever is\nwriting these plugins, the community that is maintaining them and how much effort was already\nput into these particular plugins. SAM is well known to work well with JavaScript and Python\nbut supports other languages natively and just if you're curious to know, if you can check out\nthe SAM repository, you will notice that SAM is written in Python. So, not surprising that it\ndoesn't allow you, for instance, to embed JavaScript files into the configuration file\nbecause it would be a lot trickier to kind of execute them with something written in Python.\n\n\nHow do you get started? Let's compare the experience between SAM and Serverless Framework.\nServerless Framework is a JavaScript project and as such is provided as a Node.js module\nwhich basically means that if you are well used to the Node.js ecosystem and all its tooling,\nyou can just do npm install serverless and basically you have Serverless Framework available.\nAnd that means that you can even easily embed Serverless as a development dependency for one\nof your projects. So that way you can have, for instance, two different projects, two different\nserverless projects and one project could use Serverless v3 for instance, another project could\nuse Serverless v2 and all of that is managed seamlessly by npm. So you can have two different\ndeployment scripts and the two executables, Serverless v2 and Serverless v3, are totally\nmanaged independently and they are logged in into their own projects.\n\n\nWith SAM, it's a little bit different because SAM provides you native binaries for the major operating systems, Windows, Mac and\nLinux, so you'll need to download them and install them in your own system. SLS is well known to have\nkind of templates for serverless projects so you have this command, this sub-command in the SLS\nCLI called sls create which can be used to say, for instance, I want to start an API in TypeScript\nand there will be a specific command that you can run that is just going to scaffold all of that\nboilerplate code for you. There is something similar in SAM called SAM init so you might get\nslightly different templates of course but the idea is that you don't have to start from scratch,\njust pick a set of technologies and probably there is a template ready for you that gives you most of\nthat boilerplate. Now, SAM might be a little bit more user-flendy because when you run SAM init\nit's interactive so it will let you pick different options in a more interactive way,\nwhile SLS is just probably cloning a git repository where all the code gets provided to you.\n\n\nBuild and deployment might be another interesting topic and in this case I think SAM is probably a\nlittle bit ahead than Serverless Framework because the first thing is that SAM uses\nCloudFormation change sets and this is something that serverless didn't do for a long time,\nonly recently they added support for that but there are important differences there.\nIf you don't know, CloudFormation change set is basically a feature of CloudFormation that allows\nyou to, rather than just deploying the differences in a template, it will basically do a preview for\nyou, it's basically going to assess what is going to change and then it's something that you can\neffectively decide I want to go ahead and deploy this or maybe you realize it's not really doing\nwhat you expected so you can just stop that particular deployment. So with SAM, basically in\nyour deployment process through SAM it will very obviously show exactly the effect of this change\nset so it's very easy for you to visualize that something has changed which is actually going to\nbe deployed and decide whether you want to proceed or not. With SLS that kind of happens as well but\na little bit more behind the scenes so you'll need to go into the AWS console to actually see\nwhat's going to be the effect of the change set so it's a little bit more involved while I think\nthe experience with SAM is much smoother and user-friendly. Another interesting thing in terms\nof building is that SAM uses Docker out of the box which can be very convenient when in your project\nyou have native dependencies for instance in Python or Node.js. If you are working on a Mac\nmachine or a Windows machine probably the local files that you have for these native dependencies\nare not going to work out of the box in a Lambda environment which is a Linux-based environment\nand maybe with a very specific architecture that might be different from the one in your\nparticular development machine. So when you use Docker the build process is actually done in an\nenvironment that is going to be much closer to the environment of Lambda so the risk of packaging\nsomething that is not going to work in the target architecture is much more reduced. So this is very\nconvenient. You could do the same thing with serverless but it's not really a well-known feature\nbecause you'll need to explicitly pass a flag to enable that. Another final point about deployment\nwhich is really interesting I think is one of the most innovative features of SAM and one of the\nstrong points of SAM is a feature called SAM Accelerate which is relatively new. We mentioned it\nin one of our previous episodes, episode 19 where we talk about the serverless development experience.\nSo basically SAM Accelerate is something that is meant to reduce the feedback loop that you have\nwhen you are developing a new feature. You do some changes and in order to see if these changes in\nyour code actually do what you are expecting them to do you probably need to do a deployment and\ntest it in the cloud and this is generally something that takes minutes so it's kind of\ninterrupting your development loop. It's distracting and it might be frustrating if you have to do it over and over.\nWhat SAM Accelerate does is basically automatically syncing your code as you do changes.\nYou lose some of the guarantees like you are not for instance able to do rollbacks\nas easily as you would do with a full deployment but it gives you a very quick feedback loop to\nsee these changes almost in real time so it might be really a big accelerator in that development life cycle.\nSo definitely a really interesting feature that if you haven't tried yet I really\nrecommend you to try because you can have that feeling that you are developing locally while\nalmost immediately being able to see the changes reflected in a real cloud environment.\nWhat do you think? Is there anything else worth mentioning in the difference between SLS and SAM?\n\n\nEoin: Yeah that was really a comprehensive set of differences.\nMaybe a few small things I could add on. One that really bugs me is that with service framework it still doesn't work with\nSSO credentials which is like I think a lot more people are using SSO for sign-on now\nor IAM identity center as it's now called. There's been a GitHub issue open on the serverless\nframework for a couple of years and recently they've commented on that issue that they just\ndon't have time to add the support. It's been unfortunate SAM supports SSO credentials already.\n\n\nMaybe as well for people who are thinking about how they structure their project and dividing\ntheir project in a repo into multiple services or stacks. SAM supports nested stacks. That's a\nreasonably recent addition I believe. It's a nice way of organizing your project and you can deploy\nthe top level stack and it will deploy whatever changes are necessary within all the stacks\nunderneath. Serverless Framework did have a plugin to support that but it's not really not maintained\nlike a lot of the less used plugins. But it does have a really good new feature called serverless\ncompose that allows you to manage and deploy multiple stacks together. So that was a really\nnice one. Might also be worthwhile talking about how good these two tools are at supporting new\nAWS features as AWS changes and I think for both it's pretty good actually. SAM support is\ngenerally pretty good now. CloudFormation support is usually a lot faster than it used to be when it\ncomes to supporting new features in AWS services. But the serverless maintainers when it comes to\nnew features like when container image support came out or ARM graviton processor support came\nout for Lambda, those are significant chunks of work for those teams but they really seem to\npull those out really quickly and get them released out to the community which is pretty fantastic.\n\n\nWhen it comes to permissions, getting permissions right is always a tricky thing about building\nAWS applications and serverless applications. Now SAM has a few things that are attempts at making\nthis easier. One is they have some pre-baked policy templates. So these are just ways of\nkind of reducing the amount of boilerplate you have to write for IAM. And the other one is a new\nfeature called SAM connectors and this is when you've got this kind of pseudo resource called\nour connector that you create and it allows you to say for this DynamoDB table give access to this\nfunction and make it give it write access or read access. So the idea is to make it a little bit\nmore understandable I suppose for people who don't understand IAM. I'm kind of in two minds about it\nreally I'm not sure I like any of these things. I think when it comes to IAM it's much better to be\nexplicit even if it means more verbose syntax and just understand what you're doing and be very\nfine-grained. With the Serverless Framework you've got a nice way to specify IAM statements for\nfunctions especially if you add in the very common IAM roles per function plugin. So I would much\nfavor that approach to it and not to try and dumb it down and provide this kind of half solution to\nsimplifying IAM. Let's talk you mentioned JavaScript so I might just talk about\ntranspilation. If we've got esbuild that's supported in SAM already for TypeScript and JavaScript\nnew versions of the language so the Serverless Framework you can add the serverless esbuild\nplugin and it works too. So you're generally set for both of them and then both of them provide\nlocal development experience so you can do local invoke on both of them and you can use a local API\ngateway simulation. The only difference with Serverless Framework is that the API simulation\nis provided with a plugin serverless offline where SAM runs it locally for you automatically\nand then I suppose there's a big standout one with CI-CD for SAM and I really like this one\nnow because it allows you to basically bootstrap a complete continuous integration pipeline and\ndeployment pipeline using any of a large set of providers so CodeBuild and CodePipeline within AWS\nbut it can also support Jenkins or GitLab CI-CD, Bitbucket and GitHub Actions. So this will create\na pipeline which will deploy to multiple environments and also supports feature branch\ndeployments. We had an episode a while back about OIDC providers and why you should use that method\nfor getting your GitHub Actions to authenticate with AWS. SAM will create that for you so all of\nthe complexity of creating that and creating the associated role is managed when you bootstrap your\nSAM pipeline so that's really nice one. And maybe just lastly before we start wrapping up and giving\nour more opinionated view on which you should choose when it comes to just extensibility you\nmentioned the plugin ecosystem in Serverless piano and we can give an example because you and I and a\nfew other the fourTheorem team are working on a really significant plugin for Serverless Framework which\nnow supports SAM as well it's called SLIC Watch and it's for adding alarms and dashboards into\nthese projects automatically. It has to basically look at your whole template find out what functions\nAPI step functions DynamoDB tables you have and creates alarms and dashboards for you.\nWhen it comes to Serverless Framework it's pretty easy to write a plugin. The documentation is a\nbit hairy but there's enough examples out there where you can figure out how to use it.\nWhen it comes to SAM your only option really is to create another CloudFormation macro\nand then I suppose that's in it's nice in some ways because it's similar syntax to\nSAM itself you just add another macro and CloudFormation service is going to do that\ntransformation for you but the thing is deploying macros into people's accounts it takes a bit of\nwork a little bit more work than just adding a serverless plugin because you have to deploy them\nwe can't just publish a macro to the public registry in CloudFormation you can't do that\nonly AWS can do that so it's a pity we can't do that and it just makes it a little bit more of a\nstep but once you do that it works the same you know we can use 90 of the same code to deliver\nto deliver a slick watch whether you're using Serverless Framework or SAM because the\nmacros are just implemented as Lambda functions and we can run them as JavaScript, same JavaScript\ncode, so I think we've covered fairly exhaustively all the differences between these two tools\nso maybe this is the part that people really want to know which should they use which direct which\nwhich one is the one that people should choose for the project today and what's the future look like, where are they going?\n\n\nLuciano: Yeah that's that's definitely I know opinionated piece but we're going to try to bring some evidence into the answer like we're not going to say pick one or the other but we are\njust going to give you some indications like if you favor a specific thing maybe one tool is\nbetter than the other so definitely I like the sum generation of pipelines that you mentioned this\nis a really amazing feature it is always quite painful to generate pipeline codes and then\ntesting it you probably end up doing millions of deployments just trying to figure out is my syntax\ncorrect does it really do what it wants it to do so the opportunity to just bootstrap something\nthat it's reasonably doing most of that work and for different kind of cicds it's an amazing feature\nso if that's something that you really care about and something that you have been spending a lot of\ntime in the past probably going with some and leveraging this feature can be a big enabler\nfor you and your team if you are targeting other cloud providers not just AWS that's an argument\nfor for Serverless Framework because of course some doesn't even try to support other cloud\nproviders so your only option really is to use Serverless Framework there between the two\nso again worth remarking that your code and your configuration are going to be significantly\ndifferent there is no magic cloud abstraction cloud provider abstraction but the tooling\nremains most of the same so you could be using the same tool for both let's say AWS and Azure\nif you go with Serverless Framework then if you really want flexibility of configuration again\nthis is another point in favor of Serverless Framework like if you really have requirements\nthat force you to have very dynamic data that gets interpolated into your templates you're going to\nhave an easier time just going with Serverless Framework because with some you'll need to figure\nout your own pre-processing logic if you want to do something like that and this is also similar\nif you need to write custom extensions maybe you want to do something special at build time\nyou have I don't know very specific build requirements or maybe you need to generate\npieces of CloudFormation dynamically depending on different parameters it would be much easier\nto write your own custom plugin for Serverless Framework rather than having to create your own\nCloudFormation macro get it that deployed and then integrate it with some so another point in\nfavor of Serverless Framework if you care about that kind of degree of extensibility so I would\nsay that all in all Serverless Framework used to be a lot more mature than some so there might be\na little bit of an opinion if you search around that some still new and not mature enough but i\nthink this is kind of slowly changing because these days some really has caught up with all\nthe features of well at least most of the features or the main features of Serverless Framework and\nit has even some new features like the pipelines that we mentioned or the ability to synchronize\nyour code with the cloud in real time that makes it really a well-worthy choice I personally used\nto choose a lot Serverless Framework as a default in the past these days i'm kind of always thinking\na little bit more about it like if I don't really need that level of extensibility i'm probably\ngoing to default to some because it feels like a safer choice going forward and this is probably\na good pivot point where we try to speculate a little bit more about what is going to be the\nfuture of these two projects because this might inform your decision on whether you want to pick\none or the other for a given project so again this is probably the most opinionated piece of\nthis episode so take it with a pinch of salt but our opinion is that it feels that AWS is investing\na lot on some so we expected that it's only going to get better and better and they are both open\nsource projects but it's important to see that ownership piece like while some is clearly owned\nby AWS and it's in the best interest of AWS to keep it up to date and add more feature and make\nthat development experience as nicer as possible because it brings more business to AWS itself\nthe Serverless Framework is a lot more open and as a community but at the same time the project\nitself is backed by a company so it's a little bit difficult to understand how is this company\ngoing to get a return there are investors of course they need to pay back on that investment\nand even if they care about the community and the user experience there might be decisions there\nthat we cannot predict that will affect negatively or positively the experience of the user in the\nend so I think there is a bigger question mark there on what's going to be the future of\nServerless Framework when we compare it with some and I have a slightly related note to that point\nwhich again is just my own bugbear with Serverless Framework I used to think that the serverless\ndocumentation was quite good like the documentation itself on the website but also a lot of related\nblog posts and it was very easy when I was doing serverless in the very early days and using\nServerless Framework to just search for anything and you will easily find either a documentation\nor a blog post and find the answers you were looking for these days for some reason and quite\nunexpectedly it is not as easy anymore I find myself myself ending up more and more on the\ncommercial side of the offering that Serverless Framework has to give to users rather than\nactually finding the documentation for the open source tool that I'm looking for I don't know if\nthat's intentional just to drive more business to the the commercial offering or it's just a\nside effect of trying to combine a commercial offer together with an open source project so\nthat the website itself becomes a little bit more convoluted and even organizing the content gets\nof course more difficult so I don't want to blame necessarily the serverless company for trying to\nmake revenue out of this project but the net effect on the users is that it gets more confusing\nto figure out how to use the open source part of the project so just to conclude I will say that\nit is also fair to say that SAM doesn't really have a great documentation either it's probably okay\nbut sometimes when you want to do something that is slightly off the standard it's not very easy\nto understand how to do that so maybe this is just a suggestion for AWS to try to invest a little bit\nmore time in trying to document different kinds of setups different more advanced features that\nyou might have with SAM and maybe provide a lot more examples because those will will\ndefinitely help people to get started with all the different features that you might have\nissues and all the different kinds of permutations of projects.\nOkay I think that that's probably more than enough that we want to cover for this episode and we gave\nyou a lot of opinions but of course we will be curious to know what is your opinion do you prefer\nsome do you prefer serverless what are you using for your current projects and are you going to be\nusing something different in your next project and of course this is only a portion of this\nconversation because of course there are other tools there is CDK there is Pulumi there is\nTerraform so if you don't use either SAM or Serverless and you end up using other tools,\nwe would love to know why what are your requirements why did you make this choice.\nAnd maybe we can do another episode in the future where we try to explore some other of these tools\nand compare the differences. So thank you very much for being with us today\nand we look forward to seeing you in the next episode.\n"
    },
    {
      "title": "67. Top AWS Developer productivity tools",
      "url": "https://awsbites.com/67-top-aws-developer-productivity-tools/",
      "publish_date": "2023-02-10T00:00:00.000Z",
      "abstract": "Are you tired of feeling overwhelmed by the vast AWS landscape? Do you find yourself constantly struggling to keep up with all the tasks at hand? Look no further! In this episode of AWS Bites podcast, Eoin and Luciano share their top six time-saving tools to help you reclaim your productivity and make the most of your AWS experience. These tools are designed to make your life easier and allow you to achieve more in less time.\nBut don't worry, this won't be a boring lecture. Get ready to have some fun as they reveal their top tricks and tips, from profiles and SSO to terminal gems and CLI magic. These tools will have you feeling like a kid in a candy store, soaring through your AWS work with ease. And if that wasn't enough, they've got a few extra special surprises in store to take your AWS skills to new heights.\nSo buckle up and get ready for a wild ride, it's time to have some fun with AWS!\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nOur previous episode discussing SSO and OIDC identity providers: AWS Bites Episode 45\nCLI application to switch profiles and roles easily: Granted.dev\nAWS SSO CLI on GitHub\nStarship universal terminal prompt\njq CLI helper\nijq (interactive jq)\nAWS CLI --filter option: AWS CLI filter option documentation\nDash documentation app for Mac\nGitHub Copilot\nIAM Policy Simulator\nAWS SDK for Pandas on GitHub\nAWS CloudShell\nSLIC Watch\nOur previous episode on SLIC Watch: AWS Bites Episode 34\nMiddy Serverless Framework for Node.js\nOur previous episode on Middy: AWS Bites Episode 41\n\n",
      "transcript": "Eoin: The breadth of AWS services is huge.\nAnd when you're working on a cloud-based application,\nsometimes it seems like there's so much to do\nand it's easy to get lost and distracted.\nToday we have six time-saving tools to share with you\nto help to make you a more productive AWS guru.\nMy name is Eoin and I'm joined by Luciano\nand this is the AWS Bites podcast.\nAWS Bites is sponsored by fourTheorem.\nfourTheorem is an AWS consulting partner\noffering training, cloud migration\nand modern application architecture,\nfind out more at fourTheorem.com.\n\n\nYou'll find that link in the show notes.\nGoing into today's episode then, Luciano,\nwe're going to try and share our top six tools\nfor AWS productivity,\nthings you may or may not have heard about.\nAnd if you hang around to the end,\nwe've actually got a couple more bonus tips and tools\nto help you take your AWS expertise to the next level.\nAnd I'm going to start off with one that I think is really\na time-saver for me and really helps you to get organized\nand helps with a bit of security as well.\n\n\nSo there's a couple of things that I'd say first\nbefore I reveal what it is.\nIf you're using AWS with the SDK or the command line,\nI would really recommend using AWS profiles\nand try to avoid just sharing environment variables\nand setting up environment variables\nfor different accounts and roles as you need them.\nProfiles just make it a little bit easier\nbecause you've got named profiles\nfor different AWS accounts and roles.\n\n\nAnd you can set those up using your credentials file,\nbut it's even better if you set it up with AWS SSO.\nSo we talked a bit about this before.\nWe've got a previous episode, which we can link in\nall about identity management on AWS\nand why you might use SSO instead of just IAM users.\nWhen you're using SSO, then you can end up\nwith an organization with lots of accounts.\nSSO allows you to have different sets of credentials\nmanaged automatically, so you never have to copy\nand paste environment variables\nor lines into your credentials file.\n\n\nIt's all managed by the SSO credential process for you.\nBut that can get a bit cumbersome\nif you've got tens or hundreds of accounts to work with.\nAnd if you're working with different companies\nor different customers, you really need to be careful\nthat you're using the right set of credentials at the right time.\nAnd there's a really good tool that I started using\nin the last year, and you can find it on granted.dev.\n\n\nAnd the tool is called Assume,\nand it's a CLI tool that you can install,\nand it will allow you to assume a role on the command line.\nAnd it will basically scan your config file,\nyour AWS config file, allow you to select which permission set,\nwhich role you want to assume,\nand then those credentials will be loaded into your shell.\nSo that's one of the things it does.\nThe other thing it does, which I really like,\nis it works with your browser.\n\n\nSo I'm using Firefox containers,\nbut this granted.dev tool also has its own containers\nplugged in for Firefox that allows you to log into the console\nusing a given profile.\nSo you don't have to set up the Firefox container manually\nand log in every time.\nIt will automate a lot of that for you.\nSo you can have multiple different tabs open\nfor different AWS accounts.\nThey get different colors. They're very easy to identify,\nand it streamlines all of that process for you.\n\n\nNow, one thing I'd say is that you do need to somehow load\nall of your accounts and SSO permission sets\ninto your AWS config file for this to work,\nand I use Ben Kehoe's aws-sso-util do that.\nIt will essentially populate your config file\nwith all of the permission sets and all of the accounts that you have,\nand you can give it multiple URLs and just get it to scan all the URLs,\nthe start SSO URLs you need.\n\n\nAnd then you can suddenly have like hundreds of profiles\nin your config file automated for you,\nand then granted.dev will allow you to log in on the CLI\nor in the browser really easily.\nAnd there's an alternative for this.\nSo some of our colleagues at Fortherium I know use another one\nfrom Synfonatic on GitHub called AWS SSO CLI.\nIt kind of does the two things together.\nIt works in a slightly different way, so you can try both, I would say,\nbut I've been really happy with the assume tool from Granted.\nSo that's it.\nI suppose when you have this set up, then another thing to be aware of\nis you have to still keep ensuring that you're aware\nof what profile you're using at any given time.\nI think you've got a solution for this, Luciano.\n\n\nLuciano: Yeah, definitely. So my solution for a long time has been always run\nAWS STS get-collared-entity every once in a while.\nBut of course, that's not perfect science.\nYou end up forgetting about that,\nand you might end up doing stuff on the wrong account\nwithout realizing it a little bit too late.\nSo recently I started to use something called Starship Terminal,\nwhich is just kind of a universal terminal prompt.\n\n\nBy that I mean that it works with most of the shells that you might be using.\nSo I worked with Bash, Z Shell, Fish Shell and so on\nin a kind of a seamless way.\nAnd the reason why I really like this particular one\nis because it's almost zero config.\nOf course, you can configure it and customize it to your need,\nbut out of the box, it comes with a lot of very good defaults.\nAnd one of the things that it does out of the box\nis that it's capable of recognizing your AWS profile,\nlooking at different things it's able to display in your terminal\nwhich profile are you currently using.\nSo that can be something very good to just keep there\nbecause every single command you're going to have an indication\nof which profile is going to be used for that particular command\njust in your terminal prompt.\nOf course, you can do the same if you prefer to do it with Z Shell\nand using plugins, or maybe you want to manually customize your Bash prompt.\nBut all of that stuff will require a little bit more work\non the configuration side.\nSo you need to be a little bit more expert on the specific tools\nto be able to achieve the same kind of thing.\nAnything else we can do to be productive in the terminal?\n\n\nEoin: There's a tool that I really like, but it's actually been discontinued.\nBut I'm going to talk about it anyway, because there is an alternative\nand it's called AWS Shell.\nSo people probably know the AWS CLI.\nIt's fairly widely used across all the operating systems.\nAWS Shell was kind of an extension to that, that AWS published,\nand it gave you a much more interactive layer on top of the AWS CLI.\nNow, the AWS Shell project has kind of been discontinued\nsince the version 2 CLI came out.\n\n\nBut its functionality has kind of been integrated now\ninto the AWS CLI version 2, at least most of its functionality.\nSo now you've got this option, and the option is\n--cli-auto-prompt.\nSo CLI auto prompt.\nAnd when you run AWS with dash dash CLI auto prompt,\nit will then give you like an interactive Curses type interface.\nSo when you type AWS, it'll suddenly give you a pre-populated list\nof all the services you could use.\nYou type in EC2, and then it'll give you all of the actions you can use,\nlike describe instances.\nAnd once you've done that, it'll give you a list of the options,\nand you can browse documentation for the options in line in your terminal.\nSo if you're tired of switching between one terminal for AWS CLI\nand one for AWS CLI help, or your browser,\nthen this is a way to get it all done in one shell,\nand it works really well.\nSo I really like that one. Any more CLI tips?\n\n\nLuciano: Yeah, I actually have another one.\nSo one of the things that I often find myself doing\nwhen working with the CLI is consuming large blobs of JSON,\nbecause when you use AWS CLI, maybe you, I don't know,\ntry to describe a cloud, a cloud formation stack,\nor maybe list a bunch of resources,\nand you get back sometimes very big objects,\nand it's hard to just find whatever you're looking for in that big object.\n\n\nSo there are many solutions to this problem,\nbut one of my favorite tools is JQ, which is a CLI helper.\nSo you can just pipe output of other commands that produce JSON into JQ,\nand then JQ will do a bunch of things for you.\nLike, by default, if you don't pass any parameter,\nit's just going to nicely format and color the JSON.\nSo if your JSON is not already made as much human readable as possible\nwith proper indentation and spacing, JQ will do all of that for you,\nand that's already a big step to make it a little bit more readable.\n\n\nBut you can also go another step forward,\nand you can use specific kind of expression that JQ supports\nto try to filter down that output.\nAnd this is something that sometimes you can use to build, like, automation scripts,\nbecause sometimes you want to do a specific AWS command.\nFrom the output of that command,\nyou want to extrapolate only a very specific piece of information,\nso you can pipe that command into JQ\nwith a specific query to extrapolate that information,\nand then maybe you can pipe that other command to something else,\nwith Xargs or some other kind of pipeline.\n\n\nSo that's something that I find myself doing quite often,\nand it can be very convenient.\nNow, there is a little bit of learning curves.\nLearning the JQ expression,\nit's something you'll need to try to practice a little bit and spend some time.\nYou will get it wrong, so it can be a little bit frustrating at first\nto try to run the same command over and over with different filters\nuntil you actually get what you want.\n\n\nSo I recently discovered another tool called iJQ,\nwhich stands for Interactive JQ, which is basically the same thing.\nSo you can still pipe the output of another command into this tool,\nbut it starts kind of an interactive and courses type of view,\nwhere you can try different filters,\nand in real time it's going to show you the effect of that filter\non the actual JSON object.\nSo that's something you can use to kind of speed up that process\nof trying to figure out what's the right filter\nfor what you want to try to achieve.\n\n\nSome people might actually know that something similar\nis already built into the AWS CLI,\nand there are actually two flags that you can use\nif you don't want to bother installing JQ or learning,\nspecifically, JQ.\nSo these two flags are dash dash filter,\nwhich is on the work server side,\nso you are basically running a command\nand AWS will execute a filter on the server\nand just return less data to your client.\n\n\nOr if you just want to filter client side,\nso you're still getting all the data from AWS,\nbut then client side you are displaying less information\nbased on your filter.\nIn that case, you can use dash dash query.\nSo filter to do... dash dash filter to do the server side filtering\nof the output, dash dash query to do the same thing,\nbut on the client side.\nI'm not too sure if the filter one is available\nfor every single command,\nbut I'm quite sure that the query is pretty much available in every one.\n\n\nSo maybe worth double checking which one you have available,\ndepending on the commands you're using.\nAnd finally, I have one last tip that I find it useful,\nespecially when working with something like S3 or DynamoDB\nor in general data storage,\njust when you basically are trying to figure out\nhow to do a specific change.\nBut that change might actually be disruptive.\nSo if you are not 100% sure that your command is right,\nyou can use this flag called dash dash dry run,\nwhich is going to kind of simulate that command\nand give you some useful output,\nbut without actually generating mutations on your account.\n\n\nSo it's kind of a nice way, for instance,\nwhen you want to see if a specific folder needs a sync\nwhen you're trying to sync with S3,\nbut you are not sure that you are checking the right folders\nor maybe that you want to actually sync in that moment.\nSo in that case, you can use the dry run flag\nand it's going to show you exactly what is going to change\nif you remove the dry run flag,\nbut without applying the changes.\nSo you can double check and if you are okay with it,\nremove the dry run flag and run it again.\nSo I think that covers this particular point,\nbut kind of leads me to ask you a very specific question.\nAre there specific tips more on the documentation side?\nLike how do we learn more about SDK or CloudFormation?\nBecause these are other things where I find myself\nmost of the time wasting time trying to figure out\nhow do I do a specific thing that I want to do?\nI know it's possible, but I don't remember exactly\nwhat's the right way of doing it.\n\n\nEoin: Some people rely on extensions in their IDE to integrate that.\nI just use Dash, which is a Mac OS application\nfor local offline documentation.\nAnd I really like the way it works because it's simple.\nAnd what I really like about it\nis that it keeps me out of the browser.\nSo if you end up going to the browser for documentation,\nthere's a risk that you end up getting distracted\nand falling down a hole on the internet\nwith another tab.\n\n\nBut if you use Dash, then it's just for documentation\nand then you can load documentation sets\nor doc sets into it.\nSo there's one there for the AWS CloudFormation docs\nand AWS SDKs.\nActually, funnily enough, we talked about the new SDK\nfor TypeScript recently in JavaScript.\nIt doesn't seem to have doc sets for the version 3 SDK yet.\nIt still has the older version 2 AWS SDK for JavaScript.\nSo that's something that might limit you.\n\n\nI find that actually the version 3 documentation\nis quite limited anyway,\nso I don't think it makes a huge difference.\nBut yeah, if you're doing...\nDash also has great doc sets\nfor all of the programming languages as well.\nSo if you just want JavaScript docs or TypeScript docs\nand lots of other stuff, check it out.\nAnd I think there's an alternative for Windows and Linux as well.\nI don't know what they are,\nbut if you search for Dash alternative, you'll find them.\nSo yeah, that's a hot tip, I think,\nand a good one to make sure you don't get distracted\nas you're switching between code and documentation.\nWhat do you think Luciano?\nWhat else would you use to lean on?\nWhat other questions have you got to help you\nto be more productive at developing code on AWS?\n\n\nLuciano: One thing that I've been using a lot in the last few months\nis GitHub CoPilot.\nAnd I find it in general very useful if used with moderation.\nOf course, you shouldn't trust GitHub CoPilot suggestions blindly,\nalways terrified that they really are doing exactly\nwhat you were trying to do.\nAnd sometimes there are subtle mistakes that can be there.\nSo definitely double check, triple check, GitHub CoPilot suggestion,\nand just be sure that it's actually what you want to do.\n\n\nBut when you do that, it's still saving you a lot of time\nbecause sometimes it auto completes with significant amount of code\nthat it will take you a while to write yourself.\nOr maybe you don't remember exactly the signature of something\nand it chances are that what CoPilot is giving you\nis more correct than what can you remember.\nSo that's definitely something I would suggest.\nAnd it seems to work quite well with SDK,\nat least the one for JavaScript and Python, so far for my use cases.\n\n\nSo sometimes you don't remember exactly how to do something\nlike copy a file into S3.\nIf you start to write code, chances are that CoPilot\nis going to give you a good enough suggestion\nthat will kind of speed up your development time.\nAnd again, always double check,\nbut it's sometimes faster to go through CoPilot\nand then double checking later if, for instance, with your IDE,\nyou can highlight the specific function and see the documentation.\n\n\nBut again, it's writing a significant chunk of code for you,\nso that can speed up your development time.\nNow, if you don't like CoPilot for whatever reason,\nthere are alternatives.\nI didn't use them, but I heard people using Code Whisperer or tab 9\nas alternative to CoPilot.\nSo they might be good enough and they could be interesting to try out.\nSo moving on, when we were preparing this particular list of suggestions,\nwe wanted to select only six of them just to try to keep the episode short.\nBut I think there are still some mentions that we want to give.\nSo Eoin, what do you have as honourable mentions?\n\n\nEoin: One that I'd really recommend.\nBookmark this one, the IAM policy simulator.\nIf you need to troubleshoot access denied errors with IAM,\nthe IAM policy simulator opens up a fairly crude but very useful web UI\nthat you can use to simulate any action against any service.\nAnd you just have to already be logged in with credentials into a console,\nthen you open up in the same browser and it will allow you to run a simulation\nand it should be able to identify whether you get access denied or not\nand with reasonable level of accuracy why you're denied.\n\n\nSo if it's a explicit deny or missing allow\nor if it's something in a service control policy or permissions boundary,\nyou should be able to see that.\nAnother one is for Python developers,\nor actually anybody doing data work in general is the AWS SDK for Pandas.\nThat's the new name for it. It used to be called AWS Wrangler\nand that's a Python module that provides a whole set of very convenient APIs\nfor dealing with data.\n\n\nSo reading Parquet files from S3 and writing them,\nit's built on top of PyArrow and it allows you to read and write from S3,\nRedshift, other AWS data services\nand it even allows you to do things like reading logs\nwithout having to do all the pagination and stuff yourself.\nSo definitely check that out if you're a Python developer.\nLast one is Cloud Shell and it's sometimes forgotten,\nbut it's right there at the top of every browser session\nwhen you log into the AWS console, you just click on that terminal icon\nand you can get a shell into your AWS account\nthat you can use for exploring with resources in your AWS account.\nAnd you get a limited amount of storage\nand it's retained for 30 days or something like that.\nSo I think it's completely free for just using one for every account,\nfor every user.\nAnd the only thing right now, the only thing that's missing\nis that it doesn't have VPC access.\nI hope that's coming soon.\nSo what have you got? What are your three from the best of the best?\n\n\nLuciano: I like your list and I definitely vouch for IAM policy simulator.\nSomething saved my life a few times.\nSo it's not perfect. Sometimes the labels don't quite make sense,\nbut you still get a lot of value from it\nto still give you a lot of useful information\nwhen you are in trouble with permissions\nand you need to figure out why. So plus one on that.\nOther than that, I have a few ones.\nI'm going to be a little bit selfish because we have worked on some of them.\n\n\nSo the first one is SLIC Watch, which is, I'm just going to say,\na plugin for serverless framework,\nbut in reality, we have extended it to work also with CDK and other tooling.\nSo probably it's going to work with most of the infrastructure\nas code tools that you'd like to use.\nAnd basically what SLIC Watch does is tries to make serverless observability easier\nby just giving you a set of defaults built in.\n\n\nLike it's going to create dashboards, alarms, based on best practices,\njust by looking at what's your current infrastructure\nthrough your infrastructure as code definition.\nSo check it out. We'll have the link in the show notes.\nIf you like it, let us know. If it doesn't work for you,\nalso let us know why. It's an open source project,\nso you are also welcome to contribute, open issues, send SPRs.\nIf you find it useful, let's work on it together and let's make it better.\n\n\nThe second one is Middy, which is a serverless framework for Node.js.\nSo the idea is that when you write Lambda code,\nespecially if you are building an application that contains a lot of Lambdas,\nmaybe you are building an API and you have a lot of endpoints\nspread out in different Lambdas,\nyou might end up with a lot of code duplication.\nLike there is a lot of concerns that are kind of repeated between Lambdas.\n\n\nFor instance, the way you do validation, the way you do access control,\nthe way you serialize and deserialize, input and output,\nand all these kind of concerns,\nyou end up repeating that code over and over\nin a way that is generally not quite testable or usable.\nSo Middy helps with that.\nIt basically uses the middleware pattern heavily\nto try to extrapolate all this concern outside\nfrom your business core logic of your handler.\n\n\nSo it's trying to keep your handler as pure as possible\nand moving all this concern outside\nin a way that makes them more usable and testable.\nAnd there are a bunch of built-in middlewares\nthat you can just use for all sorts of most common type of concerns\nthat we have seen in the last few years.\nRecently became the most used framework for Lambda\nin the Node.js space just by the number of weekly downloads.\n\n\nSo that's just an estimate that it's only useful to people\nand you might want to check it out if you're not using it yet.\nLast one, which is closely related to Middy, is PowerTools,\nwhich is a library...\nIt's actually multiple libraries for different languages\nthat is built by different teams in AWS.\nFor instance, you have the TypeScript one,\nwhich is actually good even for plain JavaScript, of course.\n\n\nYou have the Python one, but there are also the Java one, I believe.\nSo it's kind of a set of libraries that do more or less the same things.\nSo they try to make your experience while writing Lambda a little bit better\nby giving you especially tools for observability,\nlike making it easy to do structured logging,\nmaking it easier to register custom metrics,\nand also tracing as well.\nSo check it out.\nI especially like the TypeScript one and the Python one.\nI've used them a lot and they are quite useful.\nThey will save you a lot of time\nbecause all these things will take time to set up correctly.\nAnd if you use the library, it mostly works out of the box.\n\n\nEoin: We've actually got an article on the PowerTools\nthat we can link in the show notes as well,\nas well as the link to our previous episodes on SLIC Watch and Middy.\nSo hopefully people will find that useful.\nAt this point, I guess I'm curious what we missed.\nWhat are the glaring obvious tools and productivity tips\nthat everyone out there uses?\nSo please put them in the YouTube comments\nor send them to us on Twitter.\nWe want to know about them.\nMaybe we'll have enough for another episode.\nThanks very much for listening to this one.\nIt's been great to have you with us.\nKeep listening and we'll see you in the next episode.\n"
    },
    {
      "title": "68. Are you well architected?",
      "url": "https://awsbites.com/68-are-you-well-architected/",
      "publish_date": "2023-02-17T00:00:00.000Z",
      "abstract": "If you've been using AWS for a while, you might have heard the term &quot;well-architected&quot;. But what does it really mean? Don't worry if you're not quite sure, because we are here to help!\nIn this episode of AWS Bites, we will be diving into the world of well-architected and explaining what it means, both in general and in the specific context of AWS. We will be covering the well-architected framework, the different tools, and facets that come with it, and answering some practical questions like &quot;should you care about building well-architected workloads?&quot; and &quot;how do you know if your workloads are well-architected?&quot;.\nWhether you're a startup or a mature organization, learn why building well-architected systems is crucial for the long-term success of your business.\nBy the end of this episode, you'll have a solid understanding of the world of well-architected and why it's so important. Let's dive in!\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nThe well-architected framework\nThe well-architected paper by AWS\n[The well-architected tool](//The well-architected tool)\nWell-architected labs\n\n",
      "transcript": "Luciano: If you have been using AWS for a while, chances are that you have already heard the term\nwell-architected. It might seem like something obvious at first, but do you really know what\nthis term actually means? Would you be able to provide your own definition if somebody asks you\nwhat well-architected actually means? Now, if you're hesitant to try to give an answer to this\nquestion, this episode is for you. Today, we are going to cover what does it mean to be well-architected\nin general and in the context of AWS. We will discuss the Well-Architected Framework\nand all the different tools and facets that revolve around this particular framework. Finally, and\nthat's probably the most important part of this episode, we will try to answer some very practical\nquestions such as should you care about building well-architected workloads? And if you care, how\ndo you even know if your workloads are well-architected? What if you find out that your\nworkloads are not well-architected? What do you even do? Where do you even start? How can you make them better?\n\n\nSo let's dive in. My name is Luciiano. Today I'm joined by Eoin and we are happy to present you\nanother episode of AWS Bites podcast. AWS Bytes is sponsored by fourTheorem. fourTheorem is an AWS\nconsulting partner offering training, cloud migration, and modern application architecture.\nFind out more at fourtheorem.com and you will find this link in the show notes.\nSo we said well-architected is a very loaded term. Let's start by trying to give a generic\ndefinition of what we mean with that particular term. I think in general when we say well-architected\nwe refer to the quality of a system of being well designed in regards to certain things like security, reliability, performance, costs, etc.\n\n\nA well-architected system should follow modern best\npractices and we should make sure that it can deliver the value that is supposed to be delivered\nto the users in the more consistent way. And at the same time it should be easy to operate for\nthe company, should be cost effective, and so on. So saying that something is well-architected can\nalso be a little bit subjective because if you ask different people they can give you very different\ndefinitions of what it means to be well-architected. Probably something based on their own experiences,\nsomething based on what they've done in the past, the solutions that they know,\nthe cloud providers, all the tools that they've been using the most. And if you get people with\nvery different backgrounds chances are that their answer will be very very different.\nSo there might be a little bit of source of confusion when we talk about this terminology\nin general sense but even more when we talk about this particular term in the context of AWS.\nSo where does that confusion come from, Eoin, Do you have an opinion?\n\n\nEoin: Yeah, well it's definitely subjective but at least I would say in the context of AWS they have used the term well-architected for a whole umbrella of tooling and documentation\nto try and enforce a little bit more of an opinionated view on what well-architected means\nto people building on AWS. And I think we welcome that because we always want AWS to be a little bit\nmore opinionated. So it identifies a set of concepts that fall under this umbrella and the\nframework presents a number of these concepts in different categories so that you can kind of\nevaluate your approach to building and architecting AWS applications. So the parts to this are, well\nthe original one is the paper. So about seven or eight years ago they published a well-architected\nwhite paper and this was the first document that tried to define what it means for a cloud workload\nto be well-architected. And they've kept that up to date. So I think the last update was last October\nand this is the general set of principles and the principles are divided into what are known as the\nWell-Architected Pillars. So if you're going to be looking at well-architected on AWS you'll probably\nbecome familiar with the six pillars. There used to be five and there are operational pillars.\n\n\nOperational excellence which is like using infrastructure as code, deployment practices,\npredicting and handling failures, etc. Then you have security. You also have reliability which\nincludes high availability and scalability. And then you have performance and efficiency. So\nensuring that you're using the optimal resources and adopting newer technologies and not wasting\nresources. Related to that then you have cost optimization which is a pillar of its own.\n\n\nAnd related to that again is sustainability which is the latest newly introduced pillar of the\nWell-Architected Framework. So those are the six pillars. So there are basically six categories\nwithin well-architected when you're evaluating a workload. Then you also hear about this term called the Well-Architected Lenses.\nSo with Well-Architected Lenses you still have your\nsix pillars but it's basically like looking at it from the perspective of different types of\napplications or applications in different industries. So you have a lens for machine\nlearning, a lens for data analytics, there's a serverless lens which we've mentioned a couple\nof times before in previous episodes, high performance computing, IoT, games and you can\nactually create your own lenses. So there's an ability for you to define your own I suppose\nrules and apply those pillars to a specific type of application. So you've got your pillars and\nthe lenses and then there's the actual process of evaluating workloads on AWS. So that's the\nwell-architected review and it's essentially a formal process that allows you to go through\nthese lenses, ask specific questions that really you kind of have to honestly answer and figure out\nhow well you are meeting this idea of a well-architected workload according to AWS's principles.\nAnd to help with that they give you a tool. So that's the well-architected tool which\nyou can find in the AWS console and that's a service that you can use to conduct these\nwell-architected reviews and score yourself and monitor any risks that have been identified and\ntrack your remediation of those risks. You might also come across the well-architected labs which\nare essentially training resources, an online workshop split into several labs that allows you\nto learn the methodology required to conduct these well-architected reviews. So all of these\nthings would provide links for in the show notes. So that's the rundown of all the different\nproducts if you like in the well-architected ecosystem on AWS. In general, Luciano, would you\nsay who should care, should you care at all about building well-architected workloads?\n\n\nLuciano: Yeah, my first aesthetic answer would be of course you should care because nobody wants to build systems that are suboptimal and maintainable, insecure, expensive and so on. At least not\nintentionally. Maybe you don't know any better but of course you always want to do the best work you\ncan possibly do in any field, right? I think that this is just a general statement.\nBut at the same time, we always need to be pragmatic because if you are a startup, probably\nyour first concern, your main priority is to just ship something in the hands of your customer so\nthat you can actually validate if you are providing value to them and if your business\nmakes sense or if you need to pivot into something else. So in that particular case, of course I\nwouldn't suggest startups to be extremely methodical and focused on trying to deliver the\nbest possible architecture because your focus should be somewhere else and maybe more on the\nproduct side, maybe more on the validation side. Of course if your idea makes sense and your business\nstarts to get positive feedback, you want to start growing that business, you want to start to\ndeliver to more customers and at a certain point this architectural concern becomes more and more\nrelevant because of course if you have a good architecture you are increasing your chances of\nproviding value consistently to your customers. So I think as a general statement you should care.\nThere might be a point in the history of your organization where this becomes more and more\nimportant so it makes also a lot of sense to try to identify at which phase of your company\nare you in and whether it starts to make sense for you to explore this particular topic or not.\nSo I suppose if you are at that phase where you say okay I actually have a business, I'm making\nmoney, I have paying customers and I want this customer to be served as best as possible\ncontinuously and of course I want to operate all of that service in the best possible way for my\nown sake, like cost effective, easy to change and so on. Where do I even start? How do I even know\nif my workloads are well architected? And another question that I generally hear is like should they\ndo that before or after I go to production? Because maybe you just did a private beta\nor you have some prototypes that you have been using, you are not really considering your product\nproduction yet so should you do a review anyway or maybe should you wait before you are already\nin production and maybe reconsider your architecture later?\n\n\nEoin: Yeah well I guess the framework is there to help you answer all of those questions so maybe it's important to mention first the\nframework isn't meant to be used to evaluate a whole organization, it's actually meant\nto evaluate a specific workload. So you can have multiple workloads with your business and they\ncan all be at different levels of well architected and you can conduct well architected reviews for\none or multiple workloads and they don't have to be synchronized in any way.\n\n\nSo I guess the first thing you would need to do then is just identify which workloads are the most business critical\nand focus on those first and is it probably a good idea to do one and get into the groove of\nusing the well architected review process and then apply it more broadly.\nSo you can use the framework and the tool to convert to conduct that review and that review is a guided process that\nhelps you to explore all of the topics that are important to the framework and make sure that you\ndon't miss anything and there's a set of questions there that are kind of designed to make sure that\nyou have to kind of really think about it and answer it and in some cases you can say okay well\nthis question or this particular topic does not apply to my workload but in general you have to\nreally bear all when you're conducting that review. And when it comes to the question about whether\nyou can use this for development workloads or production you can actually use it for both.\nYou can flag your workload as pre-production and conduct a review. It's a pretty good way to get a\nsense of where you're at before you launch into production and maybe you can evaluate which of\nthe risks you need to identify and remediate before you go live in production.\nAnd it's also important to note that you can repeat it multiple times so you can take multiple passes through it\nand you can schedule that you do it every three months or six months or whatever makes sense to\nyou as you keep improving the architecture of your workloads and stay on par with the new best\npractices and cloud capabilities. So once you've done that review it will highlight any weaknesses\nin your architectures and give you pointers that you need and can start to take action on.\nSo if you've done this, Luciano, and you've been through the process where do you even start when\nyou've got something that you identify and unfortunately you've got a set of risks you\nrealize that you're not as well architected as maybe you hoped you were, what do you even do\nor where do you start to make them better?\n\n\nLuciano: Yeah, so as you said you get some kind of feedback out of this process and that basically means that when you do the process using the tool at the end\nof this process you will end up with a report and that report contains a number of items and\nthese items are classified by high risk and medium risk which of course is just an indication that\nAWS, the tool specifically, is trying to give you but it doesn't really know anything about your\nbusiness it just uses whatever information you provided through the process so it's still up to\nyou to then take this kind of feedback from the report analyze it and decide what really is a\npriority for you and what maybe something that can wait. So I suppose the next step will be\nunderstand the report, understand every single item in that report, prioritize what's more\nimportant to you and maybe start to create some actions in your backlog to try to address some\nof these points. You definitely don't need to address them all as Eoin said it's something\nthat you can repeat over time and get it better at it and of course every time you repeat it maybe\nthere are new recommendations maybe your business has changed maybe you have implemented certain\nimprovements in your architecture so your posture will always be a little bit different.\nBy repeating this over time you will always get new feedback and new opportunities to improve\nyour architecture and just to make a practical example just by looking at the latest pillar that\nwas introduced only recently that particular pillar is something that if you did a review\na couple of years ago you didn't even have so if you redo the review today probably there would be\na number of very good pieces of advice that you can apply on that specific sustainability pillar\nthat maybe you haven't explored before and it's something you might want to start to address.\nSo since we I think it's useful to do maybe some more practical examples just to\nto give a real feeling of what kind of questions do you get when you when you do the review\nshould we maybe try to list some of the most interesting questions do you think?\n\n\nEoin: Yeah sure so if you look into the well-architected tool in the console I suppose maybe it's important to clarify the tool isn't doing any kind of analysis of your workload it's not reaching\nout into your resources on AWS and identifying weak spots it's essentially a questionnaire type\ntool a guided process that you go through with you know experts on well-architected we can go\nthrough it yourself and it asks it asks questions in a way that you kind of have to honestly answer.\n\n\nSo let's take an example so when you do that use this tool you can select like the you will always\nanswer questions about the foundational white paper and then you can also select lenses that\nyou want to apply on top of it as well the tool only supports two at the moment the serverless\nlens and the SaaS lens but you can also add your own custom lenses as I've already mentioned so\nyou can add your own questions that are specific to your industry or workload so one question that\npops up when it comes to operational excellence is just to give an example that it's not all about\nthe technology one question is how does your organizational culture support your business outcomes.\n\n\nAnd there's a selection of things that you kind of have to take here really so you have\nto make sure that you've got like executive sponsorship for your cloud workload that team\nmembers are empowered to take action when outcomes are at risk and all of the questions have this format, right.\nA set of check boxes, you can just check them you can add notes and details to it if you\nwant but you can also say if one of these doesn't apply to you for whatever reason so if you need\nif and that will happen it will happen from time to time the questions just don't apply to you.\n\n\nThen on a more technical sense in the performance efficiency pillar it will ask how do you monitor\nyour resources to ensure that they're performing so you will have to confirm that you're collecting\nperformance related metrics and that you're analyzing those when incidents occur and that\nyou're generating alarms and monitoring those alarms proactively.\nSo that all that all makes sense and then if take an example from the serverless lens, it doesn't\nhave a lot of questions in it but it'll ask you how do you evaluate your serverless applications health.\nThe serverless lens is actually a lot more opinionated in having its\nrecommendations than the base, foundational white paper so it will want to make sure that\nyou've got distributed tracing involved in your application and that you've got structured\ncentralized logging as well. So there's specific things you'll need in there so there's a whole set\nof questions there and when you go through a review you can skip over questions you can go\nback and fill them in later. So it's quite flexible in that sense. What do you think,\nLuciano, would you recommend that people do this alone that they just go in and try it out\nthemselves? Should they get AWS or a partner to come along and help them with the process?\n\n\nLuciano: Yeah, I think you can definitely do it by yourself because as you said you can just open the well architecture tool from the AWS console and it's pretty much a guided process you just need to\nmake sure you keep an open mindset and try to give the most honest answers to every single question.\nEven when there are things that of course you wanted them to be different there is no point in\ntrying to hide and maybe give an answer that is not really describing the reality of things\nbecause if you do that and you pretend things are better than they are then you are going to be\nmissing out on opportunities for making your architecture better and improving the company\nthe company on its own. And I think it's important to remember that this is not an audit this is\nsomething that you do to try to get better it's not something that you do because you're going\nto get some kind of certification or you're going to get, I don't know, the ability to trade in a specific market.\n\n\nIt's just something that you do because you generally want to be better as an\norganization at delivering the produce that you are delivering and of course there are also other\nnice side effects to the business. Because, if you do this process you're probably gonna be able to\nsave money in terms of your cloud expenditure, you might be able to make your organization a little\nbit more flexible in introducing new features, recovering from incidents and all these kind of\nthings that definitely will give you more leverage in the market to be more effective with your product.\n\n\nAnd that's another segue that I think it's important to do this process with different\nstakeholders in your business so you shouldn't just get your own architect in the company\nand tell them, \"okay, go off and do this exercise\". It's something that you should actually do by\ninvolving a bunch of different people in your organization because definitely there are questions\nthat require the perspective of people that are not technical, like people that are maybe more\nin finance or product or people that, if they are technical, they are in specific areas like security.\n\n\nMaybe your architect doesn't have all these answers so it's really important that you bring\nthe right people for the right questions and they also need to try to give the most honest answer\nthat they could give for the particular question now with that being said you can do all of this alone.\nBut if you if you want another opportunity to try to keep the entire process as honest\nas possible and maybe have even more guidance, I think it's useful to bring in a partner or to ask\nAWS itself if they would be able to provide a guided review. A partner or AWS itself\nof course they have less of an interest in the inner workings of your organization and they can\ntry to keep the process more honest, they can also bring their own expertise, they probably have\nseen different companies in different industries going through the process and that probably helps\nto avoid some of the common mistakes that you might be end up doing by yourself.\nIf this is something that interests you, keep in mind that we at fourTheorem do this kind of activity.\nIf it's something that you are considering to do, give us a shout we, can probably help you to start and do\nyour first well-architected review. So with that being said I think this is all we have for today.\nThank you very much for being with us. We are really curious as always to get your feedback on\nany of these episodes. If the well-architected review is something that you have done already,\nwe'd love to hear your feedback on it. Was it useful? Did you discover something important?\nDid it actually make your company and your operations better or if it's something you did and you were\nnot satisfied with it, we are even more curious to know what did go wrong and if we can learn\nsomething from it. I think it's amazing that we will be sharing this learning with everyone\ninterested so definitely give us a comment here or reach out on Twitter and we'd love to talk with\nyou more. Until then, thanks again and we'll see you in the next episode.\n"
    },
    {
      "title": "69. Do you know what’s in your cloud account?",
      "url": "https://awsbites.com/69-do-you-know-what-s-in-your-cloud-account/",
      "publish_date": "2023-02-24T00:00:00.000Z",
      "abstract": "Do you know what horrors lurk in your AWS account? Aren't you afraid of the murky waters of an old and cluttered AWS account, which might be rife with security risks and other unexpected dark forces?\nFear no more!\nIn this episode, we share our best tips to discover every resource in your neglected AWS account and, whether you decide to clean things up, delete what's needed, or just put some order into the mess, we give you some practical suggestions on what kind of tools or services you could you to achieve your task.\nThroughout the episode, we reveal some of the secrets and hidden potential of AWS Config, Resource Explorer, Resource Groups, and CloudTrail.\nFinally, We talk about third-party services and open-source projects such as Resmo, Steampipe, and CloudQuery, which can even span the realms of AWS and help you with other clouds and services.\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nAWS Config\nAWS Resource Explorer\nAWS Resource Groups\nAWS Tag Editor\nCloudTrail\nResmo\nSteampipe\nCloudQuery\nOur previous episode: &quot;How do you move away from the management console?&quot;\ncloud-nuke\naws-nuke\nterraformer\nFormer2\n\n",
      "transcript": "Eoin: There are loads of reasons you might want to find out\nwhat resources are in your AWS account.\nMaybe you've got a lot of cruft you need to clean up,\nor you want to sort out security issues and get some compliance.\nHow would you explore and query what actually exists in your AWS account\nso that you feel in control of a tidy cloud environment?\nToday, we're talking about tools and services\nthat help you to answer the question, what's in my cloud?\nWe are going to give you some practical suggestions for tools\nto give you visibility and queryability in AWS.\nI'm Eoin, I'm here with Luciano, and this is the AWS Bites podcast.\nAWS Bites is sponsored by fourTheorem.\nfourTheorem is your AWS partner for migration, architecture, and training.\nFind out more at fourtheorem.com, and you'll find the link in the show notes.\nLuciano, what are we trying to achieve\nwhen we're talking about finding out what goes on in the cloud?\nWhat are our use cases?\n\n\nLuciano: Yeah, I think it's very common that you look at your account and you realize it is a bit of a mess,\nand there are a lot of resources there that maybe you created years ago\nand you don't remember anymore.\nIt is actually very common that because AWS is such a learning journey\nthat when you start using it,\nyou either didn't have a lot of time to learn how to do things properly,\nor maybe you were just experimenting,\nyou weren't even sure you would end up using AWS for a long time,\nyou didn't even use infrastructure as code,\nso you might end up creating a lot of things manually.\n\n\nAnd basically, what could happen is that eventually,\nyou might have something that you actually run\nin some kind of production environment,\nbut at the same time, it is leaving with all that crap\nthat you accumulated over the years.\nSo that might be one reason why you might end up\nwith an account that is not really tidy.\nSo you might want to figure out, okay, what's going on right now\nand what should I clean?\n\n\nOther reasons could be maybe you just want to be sure\nthat you are not using resources that you don't really need.\nSo it's more of a cost optimization reason.\nYou want to remove everything that you are not actually using,\nbecause of course, you might be paying for that.\nOther reasons could be more related to compliance or security,\nwhere maybe you want to make sure that you don't have resources\nthat are not really up to date or up to the latest standards.\n\n\nOne example could be maybe you want to make sure you don't have lambdas\nthat are still running in Node.js 14 runtime,\nand you might want to update those in case you find them out\nto the more recent ones.\nOther ones could be, again, in security terms,\nyou might want to check your VPCs\nand make sure that you don't have, I don't know,\nVPCs that are too open,\nor maybe you are not even running any resource on them,\nor maybe you want to make sure your street buckets are not public\nand all these kind of things.\n\n\nSo it could be also more specific about compliance,\nbecause maybe at some point you define a set of good standards\nand you want to make sure that every time something changes in the account,\nit is still respecting the standards that you set up for your organization.\nSo in order to do all these things,\nbasically you need some kind of capabilities.\nYou need to be able to list all the resources in your account\nand somehow understand what are the different types of every resource,\nso you can kind of categorize them logically.\nIt might be useful to be able to run queries or filtering,\nbecause you might end up with a very big list with thousands of items.\nThat's not going to be very helpful,\nso you should have some kind of control\non figuring out exactly what's in this list.\nAnd of course, what if you have things in multiple regions,\nor even worse, in multiple accounts,\nthose are like extra layers of complexity.\nSo ideally you need tools that can do all of these things\nacross regions and across accounts.\nAnd I believe that we have some tools in AWS to do that, right?\n\n\nEoin: Yeah, and your choice of tooling in AWS is going to depend on whether you're doing just like an ad hoc exercise,\nor whether you're trying to put in place some kind of formal,\ncontinuous inspection of resources over time,\nand then also really meet like third-party compliance\nor internal corporate compliance.\nSo if we're talking about doing it really properly\nwith that continuous practice,\nthen AWS Config is probably the good place to start.\n\n\nSo this is something that keeps track of resources\nand registers resource configurations.\nSo it's called configuration items.\nIt's kind of a standard practice in IT service management\nto record your configuration items,\nyour assets in your cloud, and keep track of them.\nSo Config supports quite a large number of services.\nIt's growing all the time, so it's not every service.\nAnd this is something to be aware of.\n\n\nI think it's about 84 services, from what I can count.\nAnd that includes support for about 240 resources.\nSo what it does is it gives you a basic repository\nof all of the items in your cloud\nby continuously monitoring for you.\nAnd it gives you then query capability.\nSo there's an advanced query support\nwhere you can run SQL-like queries.\nIt's not exactly SQL, but it's a subset of SQL\nthat you can run on most of those resources.\n\n\nSo again, not supporting all of the resource types\nthat are captured, but probably most of the ones\nthat you're interested in.\nAnd one of the nice things about AWS Config\nis that it will allow you to aggregate\nall of the configurations from all of the accounts\nin all of the regions you want.\nSo we would typically set up AWS Config\nfor all of the regions that you support.\nYou disable the regions you don't want to support,\nand then you gather all of that data\ninto one centralized aggregator within AWS Config.\n\n\nAnd then you can run queries to find out\nhow many EC2 instances do I have across all my organization,\nall accounts, all regions, and what's their configuration.\nSo that's really nice.\nYou'll get a picture of the configuration,\nand then you can see how the configuration item\nhas changed over time.\nAnd the other thing that it's really aimed at within Config\nis evaluating compliance rules.\nSo you can use a lot of the pre-canned rules\nthat AWS will give you, and those rules can also then form\npart of what's called a compliance pack.\n\n\nSo that's a collection of rules all together\nthat might tell you whether you are meeting the requirements\nof SOC 2 or NIST or CIS benchmarks\nor whatever industry standard you might need.\nYou can also add your own rules using either Lambda functions\nto evaluate them or the CloudFormation guard syntax\nto check if resources meet your standards, essentially.\nSo all in all, if you want to up your compliance gain,\nbut also just do querying of resources\nand monitoring over time, Config is a good source.\n\n\nNow, the downside with Config is always,\nwhen it comes to certain cloud deployments,\npricing might hit you,\nbecause it's priced per configuration item recorded,\nand that's priced every time it changes.\nAnd also, if you're evaluating compliance rules,\nevery time they're evaluated, you're charged as well.\nSo it's like 3 tenths of a cent for item recorded\nand one tenth of a cent every time you evaluate a rule.\n\n\nAnd a lot of the time, the pricing there is fine,\nbut if you're somebody who deploys frequently\nand you're creating lots of resources\nand you're trying to capture rules for lots of different resource types,\nthat can quickly escalate and you get an exponential effect there.\nYou also might get a surprise when they add support\nfor new resource types that they didn't support before,\nbecause your pricing, your bill will then go up,\neven though you didn't change anything.\n\n\nSo it's definitely worth looking at.\nI wouldn't dismiss it just based on pricing,\nbut just understand the pricing model well.\nNow, since we're talking about pricing,\nwe'll go to something then next in AWS, which is completely free.\nAnd this is released last year, end of last year roughly,\ncalled Resource Explorer.\nAnd it's not really for continuous monitoring,\nbut it's more for ad hoc querying.\nHow do I find out what's in my account right now?\n\n\nHow do I find that EC2 instance\nthat I'm pretty sure I created last week?\nAnd Resource Explorer is pretty easy to set up.\nIt's much simpler than Config, for example,\nand it builds an index.\nSo it basically builds a cache that you can query\nand then you can query even just from the search bar\nat the top of the AWS console.\nSo it's quite nice, quite easy to use,\nand you can aggregate all of the data\nfrom all of the regions you want into one account,\nor into one region.\n\n\nSo it makes it easy to query cross region.\nUnfortunately, the downside is that it does not support\nmultiple accounts yet.\nSo you're just running a query,\nyou have to run your queries per account.\nAnd that's a bit of a downside,\nsince people are using more and more organizations\nwith lots and lots of member accounts.\nThe number of services supported is still quite small.\nIt's just 18 services, but within each service,\nit's very comprehensive in the actual number of resources\nit supports.\nLike within RDS, you can query parameter groups, for example,\nwhich is very low level of granularity resource.\nBut it is free.\nSo I would say, I suggest to people,\ndefinitely try that one out.\nIt's very easy to onboard and get up to speed with.\nBefore we talk about maybe the non AWS services,\nwhat else have we missed in AWS, Luciano?\n\n\nLuciano: Yeah, there are another couple I will mention.\nFor instance, resource groups and tag editor,\nwhich are a little bit older than the ones you mentioned.\nThere are some, probably they're going to be,\neventually be superseded by config and resource explorer.\nBut they are also free.\nSo maybe also worth having a look.\nAnd I suppose they're not necessarily built\nfor this particular intent,\nbut they can be useful for instance, to tag resources.\n\n\nAnd you have different pieces of utility\nto actually find resources in bulk and apply tags.\nSo maybe something that once you know\nwhat you have in your account or you start to discover,\nyou can go to tag editor to apply tags.\nAnd we'll talk a little bit more about that later on.\nAnother one which is actually really interesting\nand you should definitely rely on that one is CloudTrail.\nAnd the reason why I think it deserves some mention here\nis that even though it's not necessarily built for search,\nit basically tracks the entire history\nof how your resources change in a given account.\n\n\nSo it's kind of an audit log.\nAnd therefore you can use it in many different ways.\nFor instance, you could go all in\nand use CloudTrail to build your own inventory,\nmaybe dump all the data in something like Elasticsearch.\nAnd at that point you have a lot of like queering power\nand you can be able to really understand\nwhat's going on in an account.\nAnd even more interesting, CloudTrail can help you\nto answer the question, how did something get into my account?\n\n\nNot just it is there, but also you can try to figure out\nthe history of that particular resource.\nSo who created it, when it was created,\nhow many times did it change?\nSo definitely worth considering because of that.\nAnd more often than not, you might find something\nthrough one of the other tools we mentioned.\nAnd then you might want to go to CloudTrail\nto actually try to understand the history\nof that particular resource.\nSo CloudTrail is definitely something\nthat is recommended to learn and to enable in every account\nand then start to use it heavily.\nYeah, so you mentioned that there are other resources\nthat are not AWS services.\nSo which one of those?\n\n\nEoin: Well, one of the areas where third-party services\ncan really help is where you just talked about CloudTrail\nand how Config and Resource Explorer\nwill give you the ability to find out what's in my account now.\nBut you have to go to CloudTrail to find out how did it get there\nand what's the story behind this resource.\nBut those are two disconnected things\nand you might go through, use Athena to query your CloudTrail logs\nto try and do some detective work to go back.\n\n\nAnd I've kind of been there before.\nIt's a little bit cumbersome and it takes time.\nBut there's one commercial offering\nthat I have been playing with recently\nand I managed to have a conversation\nwith the founder of this company.\nSo I decided I'd give it a try.\nAnd the product is called Resmo.\nAnd this is essentially a paid SaaS application\nand it's similar to AWS Config\nin its intent, but it supports a lot more than just AWS.\n\n\nSo it's designed to support all the cloud providers\nand also a lot of different SaaS applications.\nSo the advantage, I would suppose,\nas always when you go to a third-party provider\nis that you're getting a much more consistent\nand usable experience, much easier to get started.\nYou don't have to do any configuration.\nIt's all done for you.\nYou can start once you set up access to your AWS account,\nwhich I thought was a fairly seamless onboarding experience\nand not too intrusive from a security point of view either.\n\n\nIt will scan your resources and immediately\nyou can start querying them with like full SQL syntax.\nIt'll also give you lots of nice queries out of the box\nand compliance out of the box.\nThis is kind of refreshing having set up Config a few times.\nWith Config, you have to pick your compliance rules\nand compliance packs and turn them on and set up notifications\nwith commercial offerings, in particular with Resmo.\n\n\nThat is all out of the box.\nAnd the pricing model is different then again.\nSo it's free for up to 3,000 resources\nand then you start paying per month as the number of resources.\nYou hit basically tiers after that.\nSo, yeah, if you're looking for something a little bit more user-friendly,\nthat definitely ticks all the boxes.\nAnd as I say, also allows you to search outside of AWS\nand start correlating resources across different providers,\nwhich is really nice.\nSo that's one of the commercial ones\nand I'm sure there's lots more.\nWhat about open source offerings?\nThere's actually a couple of options there, right?\nSo if you're trying to save on cost\nor you don't need to invest in any of these tools,\nwhat suggestions would you give Luciano?\n\n\nLuciano: Yeah, so the two that I know that are probably the ones I would recommend people to just give it a spin\nand see if they work well for you.\nOne is called Steampipe, steampipe.io,\nwhich comes actually from a company called Turbot,\nturbot.com, which is focused on extensive like cloud security,\ncompliance, posture management.\nSo they probably created Steampipe\nbecause it's something that they've been doing a lot\nin their line of duty.\n\n\nSo they basically created this as an open source tool\nand everyone can actually use the basics of this approach,\nwhich again, it's SQL-based.\nSo it will allow you to run kind of SQL queries\non top of your account\nand it will give you the ability to figure out exactly\nwhat exists in your account by running these queries.\nThere is a CLI, there isn't really a UI,\nbut yeah, it's an open source tool.\nSo probably you can get a lot of value,\nyou can get started quite quickly from your own desktop\nand that way you can just keep going.\n\n\nAnd then if you need something more sophisticated,\nyou can move into some commercial offering.\nVery similar is CloudQuery, cloudquery.io,\nwhich is basically an application that syncs\ncloud resources to a Postgres database.\nYou can even use other destination if you want to.\nAnd then at that point you can use Postgres SQL\nto basically query what do you have in that account.\nSupports other vendors, so it's not just an AWS tool.\n\n\nYou can use, for instance, Vercel.\nI've seen uses, supports other cloud providers.\nI've seen it also supports GitHub,\nso you can probably query your repositories\nand the people in your organization\nthat have access to those repositories.\nI think it also supports authentication providers\nlike Okta and OutZero.\nSo definitely there is a huge variety there\nand this can be actually useful to even understand.\n\n\nFor instance, if you are using these services\nin correlation with your AWS account,\nmaybe you can do queries to try to understand\nwhat is the relationship there.\nAre you syncing data from one to the other?\nAnd is this data actually in sync?\nIs there something that you are missing?\nAnd it also supports compliance rules.\nSo you can define your rules and use this tool\nto evaluate against those rules\nand see what's your current posture.\nAgain, this is a CLI tool, so there isn't really much of a UI,\nbut nonetheless it's something that can be useful\nand you can be really productive with it.\nNow, the next question I have is\nwe have been talking a lot about tools\nthat will allow you to discover exactly\nwhat do you have in your account.\nSo you might end up having some surprises and figure out\noh, there is a lot of stuff that I don't want to be there.\nSo what do you do then?\n\n\nEoin: Yeah, we actually had this episode way back, episode 11, which was how do you move away\nfrom the management console and we'll link that in the show notes.\nSo that gives you a bit more detail in there\non how you can do that.\nI would say though there's kind of no silver bullet here.\nIf you have a huge number of resources\nthat aren't part of infrastructure as code\nand you need to tidy them up and clean them up,\nthere's a bit of labor involved.\n\n\nIf you've got an account and you're happy to clean up everything,\nthat makes it a lot easier for you.\nSo there's a couple of tools there.\nOne is called Cloud Nuke\nand the other one is called AWS Nuke\nand they kind of do what you would imagine\nand allow you to destroy everything in an account.\nBut they also have support for filtering the resources\nyou want to delete based on resource type or tag.\nSo what you could do,\nyou could always manually delete things one by one.\n\n\nThat's sometimes the best way of doing it.\nBut it's a good idea to kind of label things first.\nIf you're cleaning out your house,\nyou might have one box for things to throw away,\none box for things to keep and another for things to give away.\nSo you take a similar approach here\nand you'd use tags to label things.\nSo you might tag things first by project or by happy to delete\nand then you can kind of query that tag later,\ngo through them and either import them into infrastructure as code\nor just delete them.\n\n\nAnd there's nothing there that will make that process really automated for you\nunless you're happy to just blank it, delete everything.\nIf you've got old, I'm always happy if I find an old resource,\nI don't know what it is, but then I look at the tags\nand I see it like a CloudFormation stack name\nbecause then I can just go to CloudFormation,\nlook at the whole application\nand decide if I want to delete the whole stack or not.\nAnd that's one of the beautiful things about CloudFormation\nis that it'll show you that it's part of an entire application\nand you can then destroy the whole thing together.\nBut what if there are things you want to keep, Luciano,\nwhat would you recommend if you want to tidy everything up?\n\n\nLuciano: Yeah, you already mentioned it is a good idea to start to define all these things as infrastructure as code\nbecause at that point you can store that in a repository\nand you can actually start to create a process\nto keep these things in order and evolve them over time\nwithout having to think too much about kind of a manual process\nthat might change every single time.\nSo it will help you to keep things more in order going forward\nand to actually distinguish what's really important and what's not\nand what's part of a specific application\nrather than maybe something else\nthat you are just playing with for some time.\n\n\nAnd of course you can do all of that manually,\nbut it might be a lot of work\ndepending on how many resources do you have\nto convert into infrastructure as code.\nSo there are some tools that can help.\nAnd to be fair, I would like to say that your knowledge might vary a lot.\nLike these tools are not necessarily perfect.\nThey have their own quirks.\nSo they might be good to try them,\nbut I still find that the manual approach\nis always a little bit more reliable.\n\n\nIt takes more time, but at least you are in charge\nof deciding what needs to happen,\nwhat really needs to be kept,\nwhat kind of tags are you going to apply.\nSo definitely it might be a lot more frustrating\nhaving to do all that work manually,\nbut I think the end result might be a little bit higher quality\nif you go for the manual approach.\nNonetheless, feel free to try those kind of tools\nfor automation. One is called Terraformer\nand the other one is called Former2.\n\n\nThey are slightly different in what they offer\nand we'll provide the links in the show notes\nso that you can check the set of features.\nAnd again, I can just remark that it is important\nthat even if you decide to keep things,\nit's actually even more important in that case\nto apply correct tags because at that point\nyou can even track the cost.\nSo maybe you decided to keep something\nwithout really knowing how much it's going to cost you.\n\n\nYou can tag it and then you can start to observe cost\nand then maybe later on re-evaluate that decision\nand maybe realize, no, this is too expensive\nfor the value that I'm getting.\nI'm just going to get rid of it.\nOr maybe you can actually realize, no,\nthis is actually cheap enough.\nI can keep going and maybe I will be working\non this a little bit later.\nAnd another idea could be consider moving things\ninto different accounts if you find yourself\nhaving a lot of mixed things,\nwhich I probably do a lot with my own personal account.\n\n\nI have some small kind of production projects\nmaybe from some of my own personal automation stuff\nor small applications that I built.\nBut then I mixed up that a lot with, I don't know,\nmaybe I'm just playing with some concept,\nI want to try some service,\nand then I ended up mixing all of these resources.\nThis is definitely not a good practice going forward\nand if you find yourself doing that\nin corporate accounts, I think it's probably a good idea\nat that point to start to separate and create sub-accounts,\none maybe for production application\nor even multiple ones for different applications\nand then keep other accounts for more kind of experimental processes\nwhere you can start to apply different policies,\nyou can start to apply different rules to clean up resources over time\nand you are not going to have the risk to incidentally\nmaybe deleting things that you're actually using in production\njust because you are trying to experiment\nwith some new service or some new tool\nthat maybe is going to give you some kind of value.\n\n\nEoin: Yeah, that's really good advice and I'm really interested to hear, did we miss anything, are there tools out there\nthat could make this a little bit easier,\nespecially the cleanup part, are there other tools\nthat people use out there for querying their cloud\nand finding out what's going on under the hood?\nThanks very much for listening,\nlet us know your suggestions in the comments\nand we'll see you in the next episode.\n"
    },
    {
      "title": "70. How do you create good AWS diagrams?",
      "url": "https://awsbites.com/70-how-do-you-create-good-aws-diagrams/",
      "publish_date": "2023-03-03T00:00:00.000Z",
      "abstract": "Are you ready to level up your software architecture skills? In this episode, we deep-dive into the world of diagrams and show you why they are essential for creating robust and scalable cloud architectures!\nStarting with the basics, we explain why diagrams are so important and why you should be using them in your work. We'll discuss different approaches to creating diagrams mentioning the popular C4 method and some alternative approaches.\nIn the context of AWS we will share some insider tips about using AWS icon sets to enhance your architecture diagrams and make them look as professional as possible.\nNext, we'll take you for a tour of the various tools you can use to create diagrams, from manual drag-n-drop tools like Visio, DrawIo, Excalidraw, and LucidCharts, to programmatic tools like Mermaid, Python diagrams library, and Kroki. We will also share some exciting insights into generating diagrams from infrastructure using tools like CfnDiagrams and the Terraform graph command.\nFinally, we'll close this episode by showing you how to share your diagrams and collaborate effectively with others.\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nOfficial AWS Icon set\nCloudCraft\nExcaliDraw\nDiagrams.net\nLucidChart\nWhiteboard substitute TLDraw\nMermaid\nPython diagrams library\nKroki URL-based diagram renderer\ncfn-diagram by Lars Jacobssen\ncdk-dia by Tom Roshko\nAWS Application Composer\n\n",
      "transcript": "Luciano: In today's world of cloud development, collaboration is key and communication is critical.\nUnfortunately, sometimes words can be boring and ineffective when trying to describe complex\nsystems and their components. And this is especially true in AWS where you have hundreds\nor even thousands of different kinds of resources to deal with. This is where diagrams comes in.\nBeing able to sketch and understand diagrams is an essential part of the work of any cloud developer.\n\n\nI'm Luciano here with Eoin and this is AWS Bites Podcast. And today we'll explore why diagrams are\nso essential and how to create effective ones. We will also look at different tools available\nto make the process easier from manual tools to programmatic ones. And finally,\nwe will discuss how to share your diagrams with your team and stakeholders effectively.\nSo let's dive in.\nAWS Bites is sponsored by fourTheorem. fourTheorem is an AWS partner for migration, architecture,\nand training. Find out more at fourtheorem.com. You will find this link in the show notes.\n\n\nSo just to get started, I want to clarify why are we even doing diagrams. And of course,\ncommunication is key. Unfortunately, we cannot always write software alone. We need to work in\na team. So we need to figure out how to collaborate and communicate effectively with other humans.\nSo words sometimes are really boring and pictures can tell us a lot of things. As we say,\na picture is sometimes worth a thousand words. So in general, when you are trying to describe\na complex architecture with lots of moving parts, lots of components,\nif you can provide a visual representation of that, it's going to be much easier,\nfirst of all, for yourself to understand it, but also for other people to understand what's going\non and being able to contribute. So the next question is where do we start? What do we even\nput in our diagrams?\n\n\nEoin: When we're talking about AWS diagrams, maybe we should talk about where those sit and the other type of diagrams you might create. So if we think about the resolution and\nwhat kind of a message we're trying to get across, we could go back to the, there's a standard out\nthere, which is reasonably well known called the C4 standard for diagrams.\nIt describes, you know, component level, context level, container level, and code level. And so it basically\ndescribes four different hierarchical levels of detail you can put into architecture diagrams.\n\n\nNow, I wouldn't follow that model necessarily all the time, but I think it's useful just as a,\nto give you a bit of context so you can start thinking about the different levels.\nSo you might say at one level, you've got like an enterprise level, which just shows all of the\nsystems in a whole organization. You don't want to have AWS icons in there. That's generally block\ndiagrams at a very high level showing very high level interactions between systems. But then you've\ngot maybe a level down from that that shows some detail, still a block diagram on, you know, a\ndomain within that system. But if you get into bounded contexts or very specific workflows,\nthat's the level where you will want to probably start showing some of the physical resources that\nyou're going to be deploying. So it's almost like a deployment diagram. And in some ways,\nI would almost say that this is a bit like a component diagram or in the olden days,\nI would say when you had monolithic applications in object oriented languages, and you might have a class diagram.\n\n\nSometimes that was way too much level of detail in general, it was I would say,\nand difficult to keep in sync with your code. But in some ways, AWS resources can be as fine-grained\nas classes were back then, because you've got so many of them and there's so many small subtle\ninteractions between them all. So it's very important to get the right level of resolution\nright. If you have hundreds of resources on one diagram, it's very unlikely, it might be useful,\nlike a large map of navigating your system. But if you want to portray a message to other developers,\narchitectures, architects, business stakeholders, you probably want something smaller.\n\n\nSo I would say that maybe think about individual stacks or deployments, individual microservices, diagram\nthose using your AWS icons, and then look at higher level block diagrams to show the picture\none level up from that. So when we're talking about AWS icons, we see lots of different flavors of AWS icons.\nIt's a bit of a bugbear of mine for some reason, but you do have older diagram icon\nstyles and newer ones. I think they came up with a release in 2017, 2019, and 2021. The 2017 ones\nlook pretty outdated by now, but you still see them a lot in the wild and sometimes mixed with\nthe new styles. I think it's a good idea to use the latest style where available and also to\ninclude the labels because it's not obvious to everybody what all those icons mean, especially\nwhen you've got 50 different orange icons that look subtly different, just put the label on there.\nBut those are available on the AWS website. So this PowerPoint version, there's PNG and SVG\ndownload available. And most of the tools include them embedded in, or you can import them.\nMaybe before we talk about tools, I would say that the last thing, and one thing I've heard the C4\nadvocates talk about quite often, which is a really good idea, is always put a title and\ndescription with your diagrams. I've been guilty of creating diagrams in the past and I go back\nto read them myself. And because I didn't put a title and description, I'm not sure exactly what\nthe scope is and what story it's trying to tell. That really helps. And if people read a title and\na two line description, it'll sometimes really do wonders for making sure that people are\ncommunicating at the right level. So that's really what we're trying to achieve.\nWe're trying to communicate at that level. And the question is, how do you create them? What are the tools to\nproduce them? Where do we start with that?\n\n\nLuciano: So before we move into that, I just want to mention that I love your comment about icons. And I found very funny that often people confuse the\nLambda icon with the Half-Life video game icon, because they're so similar that it's very easy to\nconfuse them. But that happens when you just search for Lambda rather than having an icon set\nthat is already created for AWS. So that's just another way of saying that if you have an icon set\nalready incorporated in your diagramming tool, it's just easier to end up with the right icons\nwithout having to search a specific icon all the time on the web.\nNow, there is a reference set of architecture diagrams AWS have as well.\n\n\nEoin: But one of the things, the icons also come with, you have a resource icon and then the service icon. But not all\nresources have both different flavors. So you have one icon, which is for the Lambda function,\nand one which is for the Lambda service. And it can become difficult to use and to know which\none to pick and to use consistently. But I guess the important thing there is just to pick one\napproach and stick with it rather than mixing them.\n\n\nLuciano: So going back to tools, I think we just need to discuss, first of all, what are the things that will look into specific tools for diagrams?\nAnd then we can mention what are some different kinds of tools, because there are different\napproaches to creating diagrams and different tools to try to satisfy these different approaches.\nSo the first thing for me, at least, is how easy the tool is, especially if you are talking about\na visual tool, how easy it is to actually put things on screen, resize things, move them around,\ngroup them, connect them with arrows or, I don't know, boundaries. And I found that different tools\nhave very different characteristics in that respect. Some make this process very, very easy.\n\n\nOther ones are a little bit more cumbersome and you might end up spending a lot of time just\ntrying to put things in order in the screen. So that's, for me, a very important dimension.\nThen we already mentioned icons. So is that tool something that gives us easy access to AWS icons?\nAnd if it does, are the icons up to date with the latest versions? Or at least are they categorized\nin such a way that it's going to be easy for me to pick up the right icons? And then finally,\ntrying to distinguish different kinds of tools, we have manual tools, which is generally the visual\nkind of approach that is probably the most common, where you have to draw everything,\nbut you have these panels with lots of icons and you can drag and drop and move things around.\n\n\nBut we also have other types of tools. We have, for instance, programmatic tools,\nwhere you use some kind of programming language or maybe declarative language is more correct to say.\nSomething like mermaid or something similar to markdown, where basically with that particular\nsyntax, you can express what are the building blocks and how do they connect with each other?\nGenerally, these tools are a little bit more generic, but maybe you can also use them to\nbuild your own AWS architecture diagrams. And finally, there is another category of tools,\nwhich is probably one of the most interesting, but we'll talk more about the implications of\nthat approach, which is when you have already your infrastructure defined as infrastructure as code,\nand you can use a tool that is able to read and understand that and produce an architecture\ndiagram from you out of the box. So you don't need to do anything manually. Just give it\nyour infrastructure as code and the tool is going to produce you. Hopefully a nice diagram.\nSo let's start maybe with the manual tools. Which ones are your favorite, Eoin?\n\n\nEoin: Yeah, when I started creating diagrams, I was using tools like Rational Rows and then Visio.\nIt's funny that Visio is still around. I try not to use it very often, I guess, because these days\nwe're a little bit more focused on collaborative SaaS-based tools. And I've been using Lucidchart\nnow for over a decade. And it does AWS diagrams really well. It also does higher-level block\ndiagrams really well. It's completely browser-based. And you do have to pay for it,\nbut it does have good collaboration tools for teams and different users and permissions\nwithin your organization. Where it can get difficult then is when you're sharing with\nother people outside your company. If you're working on a private project but you can't\nshare the diagram publicly, because that means people need to have an account. And with the\nfree version, they can't necessarily edit the diagram that you shared with them or add\nresources to it because they may not have the license. So that's a bit of a challenge with\nLucidchart, but I'm just so familiar with it after using it for so long. There is a free\nalternative, which is I think really popular out there now.\n\n\nIt used to be called draw.io, but it changed to diagrams.net. And it's very similar to\nLucidchart, but it's free and open source. The reason I'm kind of drawn to that these\ndays, if you'll excuse the pun, it's got a desktop app and also a Visual Studio Code\nplugin. And it also allows you to store the diagrams in a variety of different backends.\nSo you can store it in your Google Drive, but you can also store it locally on your file system.\n\n\nSo if you're imagining that you've got a diagram in your code base sitting alongside your code,\nif you're using the VS Code plugin, it allows you to version that diagram with your code. And that's\na really nice benefit. Another one I really like is ExcaliDraw. And I think this is another browser\nbased one, but it allows you to create a much more informal style. It isn't big on AWS icons. I think\nif you want to get the full set, you kind of have to import them yourself. And there's another one\nwhich is also reasonably popular. For a long time, I didn't like it. It's called Cloudcraft.\n\n\nAnd you might recognize those kind of 3D diagrams that it creates because they were popular with a\nlot of blog post authors for a while. I always found it a little bit mind boggling because I\nhad to kind of crane my neck to view the diagram. I'd much prefer a 2D diagram, but I know that\nthey've since added support to render them as 2D as well. So that's quite a nice one.\nAnd I think Lucidchart and Cloudcraft both have some level of support for importing your diagrams\nfrom AWS. But I have to say, I've tried these things and I've never found it to be effective.\nOn the subject of manual tools, when we were all working in offices back in the day, we just used\nto use whiteboard markers, draw these diagrams on a whiteboard, take a picture and maybe digitize\nthem later with something like Lucidchart. These days, if I have to do something like that, I use\nTLDraw, which has a really nice rendering and drawing algorithm. So it feels super smooth and\nreally as close as I've seen to just a whiteboard marker experience. I use it with a Wacom tablet,\njust a simple basic Wacom tablet. And it works really well. You've actually used it as well,\nLuciano, but I think with a iPad, is that right?\n\n\nLuciano: Yeah, that's actually an interesting one because I use it in the iPad using the iOS and just opening a browser. And then you can use the Apple Pencil\nand it works really well. But recently I discovered another way of using it, which is even more\nconvenient because if you use a MacBook, you can connect the iPad as a second screen. Then at that\npoint you can drag a browser window into the iPad and open TLDraw there. At that point, that's\nactually a touch screen. So you can still use the Apple Pencil to draw. And at that point, because\nyou are in the same operating system, you can easily copy paste the result or render files\ndirectly into your operative system. And it gets even easier to share the result of your sketches\nin this TLDraw whiteboard. So it's a really convenient way of using it. I'm actually liking it a lot.\n\n\nAnd the other thing is that there is a beta version. So you can go to beta.tldraw.com,\nwhich is still not perfect. There are still things that don't work 100%, but that seems like an even\nnicer and easier to use version of this tool. So one of my favorites so far in this category.\nSo should we move to programmatic tools maybe? We explored already manual tools and what we can do\nwith them. So what are some programmatic tools that maybe you can use to sketch diagrams?\n\n\nOne of the most famous that I've seen is called Mermaid, which works really well for C4 and ER\ntype of diagrams. The problem is that to do AWS, really you will need to import icons. And I don't\nknow if it's even possible to import icons right now. So it is really good for doing the kind of\ndefining blocks and how these blocks are connected with each other. But then if you want to kind of\ncustomize the output and maybe style it in different ways, you don't really have a lot\nof flexibility. Then another one is a Python library that is called Diagrams. So the way\nyou use it is basically with code. You import this library into a Python script and then this\nlibrary will give you factory classes that allow you to instantiate different things. And you can\nalso instantiate resources, classes that represent resources in AWS. So you can use this diagrams\nlibrary to basically describe how your architecture is going to look like.\n\n\nAnd then the outcome of that is that it's going to render kind of a screenshot of an\narchitectural diagram for you. There are other ones. There is one called Pikchr, which is\nspelled P-I-K-C-H-R. And another one called Kroki, which are online tools. And I don't\nremember exactly what Pikchr does, but Kroki is kind of something that does it all. It basically\nsupports all the most common programmatic tools for diagrams and basically allows you to easily\nsee examples of each and every one of them. And also a very nice feature that basically allows\nyou to render all the different kinds of diagrams that are supported by just constructing a URL that\ncontains basically the entire content of the diagram as code. And then you can easily share\nthat URL with anyone and they will see the diagram rendered. So definitely an interesting tool to\nexplore if you just want to easily give a preview to somebody for an architectural diagram, or maybe\nif you want to embed this in a repository just to get a preview of that particular diagram.\nNow, in general, we'll say that the advantage of using these tools is that you can easily keep the\ndiagrams together with your code because effectively it is code that is text that you\ncan keep in your code base and version it. And the disadvantage, of course, is that you will\nneed to learn the specific language of that tool. Like everyone has a slightly different flavor and\nit might take a while to get used to the semantics of that particular language and be proficient withit.\nSo maybe there is a bit of a learning curve, but I think eventually there is a lot of value\nthat you will unblock by using some of these tools. The next category is generating diagrams\nfrom your infrastructure as code. Do you know any tool for that, Eoin?\nThere's a couple I've used quite recently, actually, that I think are pretty impressive.\n\n\nEoin: The first one is called cfn-diagram. So this is for AWS and it's for CloudFormation, hence the\nCFN. So it's by Lars Jacobssen. And I'm just really impressed by what this tool can do. So you need a\nCloudFormation template. That's what you feed it. So if you're using Serverless Framework, CDK,\nCloudFormation, any of those tools, SAM, you can use this. And it supports different output types.\nSo it supports like ASCII output. It can generate Mermaid diagrams. It can generate draw.io output.\n\n\nSo it can even like, you can sync it up with your VS code so that you can view the diagrams there.\nSo it's got ASCII art, draw.io, Mermaid diagrams, and I think GraphViz as well.\nSo all in all, I think it's pretty powerful. The diagrams look really good that it generates.\nOf course, with all generated diagrams, ultimately, if your code is complex and your\ninfrastructure is complex, the resulting diagram is going to be complex. And the more resources\nyou have in there, the more difficult it will be to read. But I think I've been impressed by the\noutput so far. And if you've got an existing code base and you don't have any diagrams for it, you\ncould use either something like X-Ray, like use your service map, like your actual dependency\ngraph to start off, or you could use a tool like this. So I'd really recommend checking that one\nout. And if you're using CDK, people might already be familiar with another one by Tom\nRushko called CDK-Dia. And it does a similar job. It generates, I think it's got a few different\nformats. I've just basically generated PNGs from it. And it'll include like bounding boxes.\n\n\nSo different stacks within your architecture and different constructs will be grouped together.\nSo again, if your code is nicely structured, your diagram will look well. So it's good incentive\nactually to break your application down into small stacks that are nicely structured.\nOutside of CloudFormation, I'm probably not the best person to give advice on the Terraform\necosystem. I know you can do Terraform graph, and I have used that from time to time. And that'll\nbasically generate a dot format for graph is that will generate a, basically a dependency diagram\nfor your infrastructure. And I guess a new one in this space is the AWS application Composer.\n\n\nSo that's the new tool that was announced at\nreInvent last year. The team that built this is essentially the team that built another tool that\nwas very similar to this in the past called Stackery. And AWS acquired that company and\nthey've come up with some really nice tooling in AWS Application Composer. So as this adds more\nand more services, it's going to get really compelling because it can sync the code and\nthe diagram in your console using the file system API in your browser. So that's a pretty good set,\nAWS applications. Those are really good for starting off to get a diagram. You can even\ngenerate them as part of your build system and include them in your readme then as part of an\nautomated process. We mentioned that these are ultimately a tool for communication between\nmembers of a team and people in the company. So what do we do about sharing them and keeping them\nup to date and all that kind of stuff?\n\n\nLuciano: So we already said that programmatic tools are better because you can keep them in a repository. So when you are sharing the code, if you have\nthese diagrams in line with the code, they are immediately available to at least people that\nare trying to understand the code or contribute to the code. So that's definitely one way you can\nuse programmatic tools. Manual tools are I think that they have kind of a lower barrier to entry\nbecause it's easier to just you open this kind of white canvas and it's easier to understand\nhow to drag and drop things and connect them. But then you have the problem that how do you share\nthem? Sometimes you need to give people access to that particular tool. For instance, we mentioned\nalready Lucidchart, which has that particular problem. And the tool kind of becomes effectively\na repository for you to host all your diagrams, which might get very messy.\n\n\nAnd if you are a company that deals with different projects and you want to share the diagrams with your customers,\nhow do you do that? So sometimes you just end up exporting pictures and putting these pictures\nsomewhere. And then there is the problem of how do you keep things in sync? Every time you change\nthe diagram, you need to re-export the picture and make sure to copy it in the right place, which\nI've seen is kind of a common issue and you generally end up with pictures that are out\nof sync with the actual architecture diagram. So I don't necessarily have a solution for that\nproblem except trying to be diligent with this process, but just be aware that when you use\nprogrammatic tools, you can kind of create pipelines that render the pictures and put them in the right places.\n\n\nWhen you're doing things manually using a manual tool, it's much harder to achieve that.\nSo you have to be more diligent and trying to propagate the rendering of that diagram\ncorrectly in the right place. And then the other point is where is the place that people are\nexpected to go in that particular organization to consume these architecture diagrams?\nSometimes certain companies rely a lot on wikis, something like Confluence and there are specific sections\nfor architecture, so they do expect to go there and be able to find some kind of visualization\nof this architecture. So in that case, you don't really have a lot of options. You still need to\nexport pictures and embed them in the wiki system. I don't know if certain wiki systems maybe have\nplugins that can integrate with specific tools like Lucidchart or Visio or other things.\n\n\nMaybe there could be an option to try to keep things a little bit more in sync, but generally it gets\nmore complicated to work with these kind of integrations. Another thing that I really like\nis to have diagrams in the readme. So again, if you work in a repository, don't just put the code\nthere, but try to come up with a rendered version so that you can just click and see it straight\naway without having to clone the code and run some kind of script in your own machine to actually see\nthe effect of that particular code-generated diagram. And for that, I really like the tool\nthat we mentioned before, kroki.io, because you don't necessarily even have to create a pipeline,\nit's just literally the URL itself will become the picture that you can easily embed in your markdown.\nSo I think this is all we have for today. I hope that you found some useful information in this\nchat, but also I'm really curious to hear if we did miss anything. If you have other tools that\nwe were not aware about, we'd love to learn from you, we'd love to know if you have any\nbest practice that you would recommend, so definitely reach out to us on Twitter or leave\nus a comment and we'll make sure to get back to you. So thank you very much and we'll see you in\nthe next episode.\n"
    },
    {
      "title": "71. Should I be worried about Serverless?",
      "url": "https://awsbites.com/71-should-i-be-worried-about-serverless/",
      "publish_date": "2023-03-10T00:00:00.000Z",
      "abstract": "Join us as we explore the controversy surrounding serverless computing in this week's video! We'll be discussing David Heinemeier Hansson's recent blog post where he argues that serverless is a trap that only benefits cloud providers.\nWhile we respect DHH's opinion, we'll be providing an alternative perspective by analyzing his major points and discussing the benefits of using serverless computing, including Total Cost of Ownership (TCO) and increased agility. We'll also be highlighting how serverless can help teams focus on business logic instead of infrastructure management and enable easier integration with other cloud services, making it more efficient to build and deploy applications.\nDon't miss out on this informative and thought-provoking discussion!\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\n&quot;Don't be fooled by Serverless&quot; (Original post by DHH)\n&quot;Why are we leaving the cloud&quot; (Previous post by DHH)\n&quot;Why AWS Lambda pricing has to change for the enterprise&quot; (our article on AWS Lambda pricing comparison)\nOur previous episode &quot;What will serverless 2.0 look like&quot;\nKelsey Hightower's tweet on vendor lock-in\nJeremy Daly's article &quot;The cloud isn't the issue, you are using it wrong&quot;\n\n",
      "transcript": "Eoin: Last week, DHH, creator of Ruby on Rails, founder of Basecamp and Hey, and general internet\nopinionist, released a bit of a controversial blog post titled, \"Don't be fooled by serverless\".\nIn this article, DHH states that serverless is effectively a trap that only benefits cloud\nproviders.\nToday, we want to analyze the major points in this opinion and provide an alternative\npoint of view.\nMy name is Eoin, I'm joined by Luciano and this is the AWS Bites podcast.\n\n\nAWS Bites is sponsored by fourTheorem and fourTheorem is your AWS partner for migration, architecture\nand training.\nFind out more at fourtheorem.com and you'll find that link in the show notes.\nSo let's get started by analyzing the main points of this opinion.\nBut before we do, for the sake of transparency and for your own benefit, you might want to\nthink of the article, read for yourself.\nIt's a relatively short one.\nSo if you want a more unfiltered view, pause the episode, check out the link in the show\nnotes, read the article and come back in a few minutes.\nSo welcome back.\nLuciano, do you want to summarize the salient points in DHH's takedown of serverless?\nI'll try my best, yeah.\n\n\nLuciano: So the main thing is that serverless is something that gets promoted a lot by cloud aficionados.\nAnd I think there is a word, something like it is kind of a mantra that people keep kind\nof repeating.\nAnd this makes me think that the author immediately wants to take distance from the serverless\nworld of people who believe in serverless.\nThe other comment is that serverless is magic enough and the users don't question how serverless\nworks while they should.\n\n\nThen there is another part, which is a little bit of a crude financial example, where DHH\nshows how cloud providers maximize their income using serverless.\nAnd the gist of it is basically they can sell smaller portion of compute for more money.\nAnd the idea is that, of course, they can pack more users into less hardware.\nSo that kind of shows how this model is convenient for cloud providers.\nThen the other opinion is that if you are an occasional user, that financial approach\ndoesn't really affect you that much, because if you have low rate of function invocation,\nyou're going to pay very little or no money just because there are like three tiers and\nso on.\n\n\nSo this doesn't really affect occasional users of the cloud.\nBut when you start to become an heavy user, so if you start to scale up a specific project,\nthe price gets very high and unnecessarily high sometimes.\nSo the next point is that there is a massive technology and vendor lock-in because effectively\nevery cloud provider implemented serverless in a different way.\nThere isn't really a common interface.\n\n\nSo whatever cloud provider you pick, you are stuck with that.\nAnd the author at the end kind of suggests that you should own the cloud computer you\nare operating with.\nIt's literally called the cloud computer.\nSo meaning you should have your own data center.\nAnd the quote is that you should own the donkey rather than renting hundreds of slices of\nthat donkey, which is a little bit of a weird image in my mind.\n\n\nBut I suppose that the conclusion is that if you are...\nI think that the author suggested that the cloud is only good for two very specific kinds\nof companies that are very extreme with each other.\nEither you are a very, very big player, something like Amazon or some other big e-commerce with\nvery high swings in traffic.\nFor instance, when you launch a Black Friday campaign, then the cloud is going to be very\nconvenient for you because you can have the kind of scalability that justifies all that\nother complexity and cost.\n\n\nOr if you are a very, very small one where basically all this economics don't really\naffect you because you're not going to use the cloud that much to really incur any significant\ncost.\nAnd the last piece is that serverless doesn't change this point of view because it doesn't\nbasically open up.\nThis is my interpretation that the author said that it doesn't open up for other kinds\nof companies to take full advantage of the cloud.\n\n\nSo the conclusion is that serverless is just a trap and you should be aware of this trap\nand you should avoid it.\nNow for more context, I think it's worth mentioning that the author, DHH, has been transitioning\nhis companies off the cloud.\nAnd there is a previous blog post called \"Why We Are Leaving the Cloud\".\nWe will have the link in the show notes as well.\nAnd basically the idea is that the cloud comes with very big disadvantages and it might be\nmore convenient to go back and own your own data center.\nAnd this article explains all of that point of view as well.\nSo probably a good idea to also in this context to read that article as well to get the bigger\npicture of the author's opinion.\nNow that we went through a summary of the opinion, and I hope I gave justice to this\nopinion.\nI didn't butcher it too much.\nEoin., should we go through what do we believe in our opinion that actually it is right in\nDHH's opinion?\nWhat do we think it's fair?\nAnd then maybe after that we'll discuss what do we actually think that is not that fair\nand where we have different views.\nYeah, let's do that.\n\n\nEoin: Maybe first I just want to say like we're all biased when we discuss these topics.\nAnd it's interesting to kind of analyze the bias on each side a little bit because I've\nfollowed the content that DHH and his co-founder Jason Fried have created like the \"Remote\" book\nand the \"It doesn't have to be crazy at work\" book.\nI really like those.\nThere's a similar style here, which is that they're really compelling, very interesting,\nthought provoking and good at kind of defying conventional thinking around work practices,\nbut not necessarily backed up by data or facts and figures.\n\n\nSo there's an interesting strategy I think that's working well for them and their companies\nin that they create this content.\nIt gets a lot of discussion going that increases popularity, which increases the draw to their\nproducts and it's a similar thing with their latest message around leaving the cloud.\nI mean, it's all good publicity at the end of the day.\nSo even if the original logic for leaving the cloud doesn't necessarily stack up in\nterms of facts and figures, it's still generating clicks and inbound leads for the companies.\n\n\nSo it's kind of win-win in a way.\nBut like, I mean, I think there's a fair point here.\nWe're also biased, right?\nWe are working on the cloud.\nMost of our work involves cloud consulting.\nUltimately, we would say that we're solving business problems for people.\nWe're not just selling cloud, right?\nThat's not what we do.\nWe don't sell the cloud to people.\nWe sell solutions to people to actual problems that they have.\n\n\nAnd if the cloud isn't the right solution, we try not to give it to them.\nBut at the same time, this is a part of our toolkit and we have a certain fondness for\nit and we have biases here.\nSo let's get that disclaimer out of the way.\nNow, what do we think is correct in this article then from DHH?\nI think the pricing analogy, even though it's a bit crude, it does highlight a significant\npoint with serverless that people should look at seriously, especially if you take the definition\nof serverless as Functions as a Service only, which seems to be what's being done in this\narticle.\n\n\nIt doesn't really talk about serverless in the general sense as we normally would, where\nyou're just talking about trying to remove lots of infrastructure and use things like\nSQS, S3, DynamoDB, not just Lambda.\nBut we have talked before in the realm of compute and comparing fast EC2 and Fargate\nabout how the price of serverless kind of has to change.\nAnd we can link back to that article again.\nSince that article was published, the price of Lambda has kind of changed for bulk pricing.\n\n\nSo it may be heading in the right direction.\nBut as of today, if you were to design a workload that perfectly maximizes utilization of an\nEC2 machine, which is the example that DHH uses in the article.\nAnd then compare that price with Fargate, Fargate will cost you about three times more.\nAnd even if you were to run the same workload on Lambda, it could be two and a half times\nmore expensive than Fargate or up to seven times more expensive than EC2.\nSo that's if you're just looking at raw compute cost and not factoring in all the other equations.\nSo there is a point there.\nAnd we should seriously look at that, make the calculations and ask ourselves some tough\nquestions if things are going to be significantly pricey for our business.\nIt's not the case for everybody, but you definitely have to do the calculations.\nWhat do you think is wrong or missing from this article that you know?\n\n\nLuciano: Yeah, I think in general that the main thing that I found was missing that maybe gets a little mention in the other article, but not in this one about serverless is total cost\nof ownership.\nBecause when we talk about cloud total cost of ownership, we mean the costs required to\nhost, run, integrate, secure and manage your workloads.\nAnd of course, if you are managing your own data center, that's significant cost that\nneeds to be taken into account when trying to compare the cost of a serverless solution\nwith the cost of that same solution running on premise or maybe even just in an EC2.\n\n\nYou'll need to have a lot more maintenance when you're running more like VM style workloads.\nAnd that's cost that needs to be taken into account to have a more fair comparison.\nAlso generally when you manage your own VMs or you manage your own data center, you probably\nneed a lot of specialized stuff that can operate this infrastructure.\nYou also need to take care of electricity cooling, physical security of the buildings,\nracking things up, making sure all the cables are connected correctly, UPS.\n\n\nAnd also there is an element of opportunity cost because if you're focusing so much energies\nof your business into this kind of elements, you're probably getting distracted from what\nis it your business needs to deliver to customers.\nSo that's also something that needs to be taken in mind.\nAnd so, yeah, it's fair to say that calculating total cost of ownership is really, really\nhard to do correctly.\nI don't even know if there is a definition for what correctly means, but everyone I think\nshould do a genuine effort to try to understand this particular choice.\nLike if you go with one option or the other, you cannot just look at one dimension and\nnot consider the other.\nYou need to be consciously look at both types of cost structure and decide which one might\nbe more convenient for you.\nSo yeah, I don't know if there are other points that you have in mind where you would disagree\nwith the article?\n\n\nEoin: Well, I think total cost of ownership is the big one and I think you're completely right.\nThat's a difficult thing to quantify.\nSo it's something that both sides of the argument can hide behind for sure.\nBut the other one is just around the agility you get with serverless.\nI would say in our experience in general, serverless helps teams to be more agile and\nship products faster.\nThat's not true for everybody.\n\n\nIf you're learning serverless for the first time, I think in one of our very first episodes,\n\"Is serverless good for startups?\",\nwe talked about how, if you've got a skill set and it isn't serverless, you might be\nbetter off not using it at a certain point.\nSo it's not a one size fits all, but I would say in general, if you can invest in it and\nyou've got the existing expertise somewhere within your organization, you can benefit\nmassively during the innovation phases because of the low cost of experimentation with serverless.\n\n\nYou can try things, swap them out, quickly adapt and move on.\nSo I would say it also makes it easier to reverse a decision and create that two-way\ndoor where, if you build a system and it does end up being expensive or suboptimal for any\nother reason, you can switch things off and immediately stop paying for it.\nAnd you don't have to worry about the capital expenditure upfront and the sunk cost fallacy\nthat comes with investing a lot of money in one particular decision.\n\n\nThat's one of the really massive benefits of serverless that isn't spoken of quite enough,\nI think.\nA lot of this is because a lot of the responsibilities that traditionally would fall under the umbrella\nof different development teams are just delegated to the cloud provider itself.\nSo we're talking about security, high availability, reliability, scalability, et cetera.\nAnd that should allow teams to be more focused on the business logic and the application.\nThere's always a trade off there as well.\nYou also have to understand that with new paradigms, there's new learning that has to\noccur with new challenges.\nSo you have to bear that in mind too, and keep going with your eyes open.\nCertainly, with massively distributed serverless applications, event driven applications.\nYou need that investment in observability and operations.\nIn response to DHH's comment around lock-in, is there really an insurmountable lock-in\nwith serverless?\n\n\nLuciano: Yeah, I have mixed feelings about that because I think it's true that every cloud vendor\nhas different APIs for Functions as a Service, at least.\nAnd I remember back in 2018, I think, the Cloud Native Foundation was trying to bring\nall the cloud vendors together, at least to try to define a common set of events and provide\nkind of a unified interface.\nI don't think that there was any progress in that initiative, or at least up to this\nday.\n\n\nI think all the cloud providers are very different in the way you are supposed to write your\nown function as a service, the events that you get, and different capabilities that you\ncan have within your Function as a Service.\nSo definitely, that's something that we cannot negate.\nIt's definitely there and it can be a problem.\nThat creates an effect that if you want to change provider, for instance, it's not something\nthat you can do as a lift and shift kind of process.\n\n\nYou will probably need to rewrite some stuff.\nAnd this is true not just for FaaS, but also for serverless compute in the broader sense.\nEven if you use containers, for instance, if you use Fargate, if you look at Fargate\nor container serverless in the context of AWS, it is still very specific to AWS.\nHow do you configure, for instance, networking, storage, permissions?\nAnd again, even if that container probably is something that you don't need to change\nin the event of migration, you still need to change a lot of stuff around the configuration\nof how that container is supposed to run in another environment.\n\n\nSo I would say that in general, lock-in is something that maybe cannot really be avoided.\nIt's something that always exists because every technology choice you make in one way\nor another causes some degree of lock-in.\nAnd we can go deeper and start to look at web frameworks, libraries, hardware vendors,\nlike every one of these choices.\nIt's something that is going to lock you in to some extent.\nAnd if you need to change, you'll need to incur some kind of cost and redesign some\nelements of your solution to adapt to a different approach.\n\n\nSo yeah, I suppose that another question we might ask ourselves is, OK, let's admit that\nthere is lock-in, but is the serverless lock-in bigger than a lock-in that you would get with\nanother approach, maybe, I don't know, managing things in your own data center?\nAnd in a way, I feel that this will be a very fair question to ask, but also a very difficult\none to answer to properly.\nBecause I think it really depends on how you build your own applications.\n\n\nAnd that can be, of course, in both ways.\nYou can build a very good application on premises and a very bad application on a serverless\nenvironment.\nAnd therefore, one might be easier to move than the other, but also the vice versa applies\nhere.\nSo I would say in general, trying to think about how to build applications that will\nhave less lock-in, it's an exercise that you should do anyway, regardless if you go for\na serverless approach or for a different kind of approach, maybe your own data center on\npremises.\n\n\nAnd, of course, there are some common suggestions there that you can follow.\nFor instance, you should try to decouple your business logic from vendor-specific APIs,\nwhether they are coming from, I don't know, your own FaaS provider on the cloud, or maybe\nthey are coming from the hardware that you picked on premises.\nYou should try to isolate that code into libraries that are easy to swap so you can keep the\ncore of your application as, let's say, pure as possible so that you don't need to change\nanything there.\n\n\nBut then all these integration layers, you should be able to write them in a way that\nthey can be swapped easily in case you decide to change that kind of integration.\nAnd this is a common practice in software.\nYou've probably seen this in many, many books or talks.\nIt's not something new and I think it's just best practice that you should follow anyway,\neven if you are not considering the option of switching your environment in the future.\n\n\nThe other thing that I think goes a little bit in favor of serverless is that serverless\nby nature forces you to write very granular units of code, like the function unit itself.\nSo that's something that in the event of a migration can be actually convenient because\nit doesn't force you to go for a big bang migration where you have to take an entire\nsolution and move it in one go.\nYou can actually have the freedom to move very specific functions.\n\n\nMaybe you realize one function is very expensive and it's not convenient to run it in the cloud.\nYou can just move that one function.\nOr maybe you can decide, I want to rewrite this one function in another language because\nmaybe that language can be more efficient, can be cheaper, can be easier to maintain.\nYou can do it with that one function.\nYou don't have to rewrite everything.\nSo actually, in a way, I think serverless can be, at least from this particular point\nof view, can be easier to migrate incrementally than other solutions.\n\n\nAnd then there is another great quote about vendor lock-in that I really like by Kelsey\nHightower who is saying, this is actually an old tweet, probably around 2017 or something\nlike that.\nAnd he was saying that instead of trying to avoid vendor lock-in, concentrate on switching\ncosts, which basically is, you need to try to answer the question, how easy it is to\ncreate a new solution.\nHow much does it cost as opposed to how much will it cost to migrate away from that solution\nlater.\n\n\nSo if for you it's very, very easy to build something, that alone might justify maybe\nhigher cost of switching in the future.\nOf course, it's not an hard and fast rule.\nEveryone needs to try to understand what are these costs and kind of make a balance and\ndecide, okay, this is convenient for me.\nI'm going to go for this.\nOr this is probably too worrying.\nI'm not going to go for this and keep going with maybe a more traditional approach that\nis very well known within the company.\nSo I think, yeah, it's definitely important to consider this conversation with more data\npoints and more dimensions.\nIt's of course easy to say that serverless doesn't work with just one data point.\nI think that there is a lot more that needs to be put on the table to decide whether serverless\ncan be convenient or not for your particular project and your particular company.\nNow I don't know if you want to add any final take on serverless and what we think about\nthe future of serverless in general?\n\n\nEoin: We believe that serverless does bring significant benefits to the table.\nHopefully we've managed to convince people.\nI think everybody needs to embrace it and figure out where it works and where it doesn't\nwork for them.\nIf you take it as your default choice but have a healthy degree of skepticism, I think\nthat's a good approach.\nIt always comes with its own trade-offs.\nYou just need to understand and evaluate them, but that goes for every choice.\nAnd that choice should be made based on looking at the data and the facts a little bit deeper\nand not just based on some well-written opinion piece.\nIt's a trend that isn't going to stop with the serverless.\nSo businesses need to deliver products fast, have to get going quickly.\nServerless can really help there, so don't lose out.\n\n\nLuciano: Yeah, I think this probably covers what we wanted to share and I think we did a good\njob at giving another point of view.\nAnd again, it's just another point of view.\nSo at the end of the day, you have to bring all the different opinions together and decide\nfor yourself what do you believe.\nBut we are really curious to know what do you believe.\nSo definitely let us know in the comments here if you think that serverless is actually\ngoing to be more and more prevalent in the future of the cloud, or maybe it's just something\nthat is going to fade away and will come back to more traditional approaches.\nOr maybe there is something else that we are not considering that is neither serverless,\nneither on premise, and maybe that's something we should be focusing more on.\nAnd before leaving you, I want to mention there is actually a very interesting resource,\nwhich is an article by Jeremy Daly, which we will have the link in the show notes.\nAnd it's a very good response to the first article by DHH, which is \"Why Are We Leaving\nthe Cloud?\"\nAnd I think that's, again, if you're interested in this kind of conversation, it brings yet\nanother opinion into the table.\nAnd it can give you more things to think about so that you can form your own opinion.\nSo yeah, thank you very much for being with us and we will see you in the next episode.\n"
    },
    {
      "title": "72. How do you save cost with ECS?",
      "url": "https://awsbites.com/72-how-do-you-save-cost-with-ecs/",
      "publish_date": "2023-03-17T00:00:00.000Z",
      "abstract": "AWS ECS is a powerful service that allows you to run containerized applications at scale. It's suitable for a variety of use cases, including web applications, microservices, and background processing.\nIn this episode, we'll provide an introduction to the main concepts of ECS and then dive into cost-optimization strategies. We'll explore the different options for running containers on ECS, including EC2, Fargate, and ECS Anywhere.\nWe'll discuss various opportunities for saving money, such as using Arm (Graviton) instances, Spot instances, Compute Savings Plans, and RIs or EC2 Saving Plans.\nFinally, we'll cover how to set up ECS to use Spot instances, including how to create capacity providers and specify a capacity provider strategy. We'll also discuss whether it's always best to use EC2 instead of Fargate for cost optimization and recommend some tools that can help you find other opportunities to save on container costs.\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nSaving Plan calculator\nEC2 instance selector\nSpot prices dashboard by Vantage\nFargate Right Sizing dashboard\nAWS Cost Explorer rightsizing recommender for EC2\n\n",
      "transcript": "Luciano: ECS, or Elastic Container Service,\nlets you run containerized applications at scale on AWS.\nIt's suitable for web applications, microservices,\nand background processing.\nWith ECS, you can use Fargate to make it simpler,\nor the EC2 version for more advanced control.\nYou can even extend the workload to your own data center\nif that's your thing.\nAnd given all this option, how do you get a handle on cost\nand optimize to get the biggest bang for your buck?\n\n\nIf you stay until the end of this episode,\nwe will tell you some tricks and tips that you can follow\nto try to optimize cost for your Fargate\nor ECS-backed EC2 deployments.\nI am Luciano, and I'm here with Eoin,\nand this is AWS Bites podcast.\nThe AWS Bites Podcast is sponsored by fourTheorem.\nfourTheorem is an AWS partner for migration, architecture,\nand training, including ECS.\nFind out more at fourtheorem.com.\nYou'll find the link in the\nshow notes.\nSo first of all, it's probably a good idea to start to explain\nwhat ECS is and what are the main concepts.\nEoin, do you want to try to do that?\nI'll give that a go.\n\n\nEoin: Yeah, so you have two primary modes.\nYou have ECS on EC2, which you mentioned.\nThat's the original flavor of it,\nwhere you're running containers,\nand you're actually running EC2 instances,\nand you can see those instances\nand manage those instances underneath.\nAnd then you've got Fargate, which is the so-called\nserverless flavor of ECS,\nand that allows you to run containers without worrying\nabout EC2 or the operating system underneath too much.\n\n\nThat mode is a bit simpler.\nI suppose the trade-off there is that you have less choice\non the underlying hardware, network speed, disk,\nand the ratio between CPU and memory,\nand you also have no GPU with Fargate, at least yet.\nThere is a third option, which is ECS Anywhere,\nand that's where the instances are external.\nSo if you've got a data center there with lots of hardware\nlying idle, you can run ECS tasks there as well.\n\n\nYou just have to run the ECS agent\nand the systems manager agent on your on-premises instances.\nThere's a couple of terms and concepts\nwe probably want to describe with ECS.\nSo before we get into cost optimization,\nwe'll talk about them because we're going to mention them\nas we go through the different ways of saving cost.\nSo the primary grouping within ECS is a Cluster,\nand if you go back to the original EC2-backed flavor of ECS,\nthat's what the cluster was for.\n\n\nIt was for grouping your instances together\ninto a logical grouping,\nand they would be able to run tasks for you.\nNow, you still use clusters if you're running Fargate,\nbut you don't really have to configure very much there,\nand a cluster can combine different workloads\nthat are running EC2 and Fargate,\nwhich is kind of interesting, and we'll get into that.\nAnd then the smallest, I suppose, unit on ECS is a Task,\nand a task is one or more containers,\nso it's a bit like a pod,\nand you've got multiple containers running in it.\n\n\nIt could be one.\nIt could be one container with a kind of a sidecar,\nand in order to run those tasks,\nyou need something called a Task Definition,\nwhich is where you declare what's the container image,\nis it compatible with Fargate or EC2,\nand lots of other parameters like your networking setup,\nmemory, and CPU.\nSo that's your task and task definition,\nand then if you're running groups of tasks\nor pools of tasks together,\nyou can use the ECS Service concept,\nand a service is basically where you say,\nI want to run a desired number of tasks,\nand I want you to scale it up and down for me.\n\n\nSo you're leveraging the ECS managed service capability\nto scale up and down for you,\nand it includes a feature called application auto scaling,\nwhich will scale up and down tasks,\nand if you're using EC2 under the hood,\nyou can then combine that with cluster auto scaling\nso that your tasks are scaling up and down,\nbut also the instances are scaling up and down\nto be able to run that number of tasks as well,\nand a service gives you a couple of other neat features\nlike deployment configurations,\nso what happens when you upgrade a new version\nof that task definition,\nand what happens when things stop working?\nDo you want to be able to do a circuit breaker\nand roll back from that?\nIt includes health checks,\nand it also includes integration to load balancers as well\nso that when you scale up,\nyou're adding tasks into that load balancer target group.\nSo that's really the set of concepts\nyou kind of need to learn if you're getting started with ECS.\nA lot of people might be familiar with them already,\nbut pricing is sometimes a little bit more\nof a nebulous area, Luciano,\nwhich you want to try and describe.\nHow does ECS pricing work?\n\n\nLuciano: Yes, so you mentioned that there are pretty much two different ways of running containers on ECS.\nEither you provide your own EC2 instances,\nor you basically delegate the task of figuring out\nwhere the compute is by using Fargate,\nand AWS will try to figure out\nhow to allocate compute resources to you.\nSo when you use EC2, it's actually pretty interesting\nhow that affects the pricing model,\nbecause you basically just pay for the EC2 instances\nthat you are putting into Fargate.\n\n\nSo you just pick whatever set of EC2 instances\nsuits your needs, and then you know\nthat's going to affect your cost somehow,\nand there is no extra cost for ECS at that point.\nIf you decide to use Fargate,\nbecause it's on AWS to find the compute resources,\nthere is, of course, a pricing model,\nand that pricing model is based on two different dimensions,\nwhich is how much memory are you giving\nto the specific containers that are running,\nand how many VCPUs you are giving to the container as well.\n\n\nAnd of course, there is like a unit cost,\nbut then there are unit costs for both of these two dimensions.\nFor instance, if you use one VCPU and one gigabyte of memory,\nyou might be paying something like, I don't know,\nfour cents for the one VCPU,\nand another something for the one gigabyte of memory\nthat you are giving to the container.\nAnd this makes sense, of course, in a specific region,\nand if you're using Linux x86,\nbecause if you're using Windows,\nthe same way of calculating cost applies,\nexcept that the pricing is different\nbecause you are paying for a Windows instance.\n\n\nAnd there is also an additional charge,\nwhich is the license which gets included.\nSo on top of what you pay per VCPU and per gigabyte,\nyou also pay something around five cents per hour\njust to cover that Windows licensing cost.\nSo keep in mind that if you use Linux,\nyou're not paying the extra licensing cost.\nIf you use Windows, you have to pay an additional charge\nfor covering the license effectively.\n\n\nSo the next question might be, given this model,\nwhat are our options to save on cost?\nAnd there could be different ideas\nthat we are just going to try to put on the table,\nbut the first one is that you can use a different processor,\nbecause if you use ARM or Graviton,\nit can get up to 20% cheaper when you use Fargate.\nIt can be a little bit more variable with EC2,\nbecause depending on the type of instance that you pick,\nthere might be differences in cost,\nbut generally gets cheaper by using ARM or Graviton processors.\n\n\nThere is also another aspect there,\nbecause you could get better performances with ARM,\nso maybe it doesn't necessarily affect your cost per se,\nbut you might need less compute overall if you switch to ARM.\nSo maybe that can be another thing\nthat ends up affecting your cost,\nbecause you can reduce the number of instances\nor the amount of compute that you actually need\nto perform your specific workloads.\n\n\nAnother option is to use Spot,\nand you can use Spot either if you go with Fargate\nor if you go with EC2.\nAnd the idea of Spot in general, if you never used\nor if you never heard of what Spot actually is,\nit's basically an idea that AWS offers you\nto try to sell all the spare capacity\nthat they have available in their data centers.\nSo at any given moment in time, it looks like an auction.\nYou can just try to get additional compute at a better cost\njust because you are trying to buy the leftover from AWS,\nwhich is very interesting,\nbecause it makes it for a very variable market,\nwhere there isn't really a fixed cost\nthat you can easily predict.\n\n\nIt's something that shifts all the time,\nso you're saving, even if most of the time\nyou can get significant saving by going to the Spot market,\nbut it's very variable,\nso it's not something that you can predict\nwith extreme accuracy.\nAnd of course, you have different costs\nin regards if you go with Fargate or EC2,\nand in general, what AWS tells you\nis that you can get up to 70% saving\nif you go with Fargate\nand up to 90% saving if you go with EC2.\n\n\nBut again, that really depends on the price\nthat you have available at the moment\nwhen you decide to get those instances.\nAnd we will talk more about other trade-offs\nthat are there when you go for Spot.\nSo it looks like such a good deal,\nso of course there are trade-offs there\nthat we will need to keep in mind.\nSo stay tuned, because in the end,\nwe'll cover more about what are those trade-offs there.\n\n\nAnother option to save money is Compute Savings Plan,\nwhich is basically a commitment that you make to AWS\nto pay a certain amount of money upfront for Compute,\nand basically you will be able to use\nthese computes that you are buying upfront\nat a discount.\nAnd it doesn't really lock you in\non the type of instances that you can use.\nYou have some flexibility to shift the compute as you go.\nSo you are making this commitment\nfor at least one year, up to three years.\n\n\nSo of course you cannot predict\nwhat is your use case going forward in the future,\nso AWS gives you some room to maneuver that,\nto try to figure out if you want to switch\nto different compute, you can do that.\nAnd it is actually quite interesting,\nso we will have a link to the specific page\nwith the Saving Plan Calculator,\nif you want to play around with this idea.\nBut basically you can get from 20 to 52% saving\nwith Fargate, and up to 66% with EC2,\nand always depending on instance type, region, and so on.\n\n\nNow another similar concept,\nand another way that you might be using\nto save money with ECS, is to use Reserved Instances,\nwhich is something where basically you are literally\nbuying upfront instances,\nand you get locked into that specific type of instance,\nand that will give you up to 72% saving\nwith fixed distance classes.\nAnd finally, another idea that we have here is ECS Anywhere,\nwhich is basically the option that we mentioned before\nwhere you can run ECS on your own hardware.\n\n\nSo basically you are running containers on your own data center,\non your own spare hardware\nthat you have available somewhere else outside AWS,\nand you don't pay, of course, for that hardware\nbecause we assume you already have it,\nso you probably send you already purchased,\nbut there is a charge that AWS adds for every single instance\nthat is connected to ECS.\nIt's very low, it's about $0.01 per hour,\nbut it's something that you need to keep in mind,\nmaybe if you have thousands of smaller instances\nin regards instead if you have a few very big instances.\n\n\nSo the next question is, can you mix cost savings?\nAnd meaning, can you mix these strategies of cost saving?\nOf course you can.\nFor instance, you can mix using a Graviton processor with Savings Plans,\nbut you cannot use Savings Plans with Spot.\nSo something to keep in mind that there is some flexibility there\nto use different strategies and combine them,\nbut not every single strategy can be used together with the other.\n\n\nAnother point is if you are doing cost optimization,\nyou might have this question.\nDo you always go for EC2 or do you go for Fargate?\nBecause for what we said so far, EC2 seems to be cheaper in general,\nso you might be thinking,\nshould I just use EC2 and not Fargate and save money that way?\nIt is actually a very interesting question.\nIt's very hard to answer in absolute,\nbut in general we feel that it's not necessarily a good idea\nbecause the price difference is not that much,\nand we need to remember that EC2, you still have to manage your instances,\nwhile Fargate is doing all that work for you.\n\n\nWhen you need compute capacity,\nAWS will figure out on which EC2s to run that compute capacity,\nand it's not up to you to provision those instances\nand make sure that they are secure,\nthat they are connected to the network in the right way,\nand all these kind of things that you will need to do on your own.\nSo you definitely get slightly cheaper prices\nwhen you go with your own EC2\nif you just look at the cost of literally running that particular workload,\nbut at the same time you have to take on board on you\nall the additional cost of managing those instances.\nSo from a total cost of ownership perspective,\nFargate still seems a lot more convenient than EC2,\nconsidering that the price difference is not that high after all.\nSo I suppose that the question that we can try to address next is\nhow do you set up ECS to use Spot instances?\n\n\nEoin: We said that you can get up to 70% saving with Fargate Spot.\nIt's interesting that with Fargate Spot,\nyou get actually pretty close to 70% almost all the time with Fargate Spot.\nIt's generally somewhere in the region of 60, 67, 68% saving,\nwhereas with EC2 you're paying individual Spot prices\nfor individual instance types.\nSo it's actually per instance type a lot more variables\nso it can sway between like 50% and 90%.\n\n\nThere's some tools you can use actually for EC2\nwhich make it a little bit more transparent\nwhat the historical Spot prices are.\nYou can actually go into the EC2 console and look at the historical pricing.\nThere's an API for it and you can use the AWS CLI\nto look at the historical Spot prices,\nbut there's also a couple of tools that are worth mentioning.\nOne is an open source tool from Amazon called Amazon EC2 Instance Selector,\nwhich is just a really nice CLI tool that will allow you to select instance types\nand filter for the different criteria you want\nand then give you a report back saying these are the instance types\nthat are suitable for you.\n\n\nAnd then there's a website from Vantage.\nIt's ec2instances.info.\nWe'll give all those links in the show notes\nand that tool will show you the min and max Spot prices.\nSo when it comes to actually setting up these Spot,\nwe mentioned that for a cluster you can choose Fargate or EC2 or mix them.\nSo what you basically do there is you have this concept of capacity providers.\nSo within your cluster everybody gets Fargate\nand Fargate Spot capacity providers out of the box\nbecause they're always there. You don't have to set anything up.\n\n\nAnd then you can create your own capacity providers for EC2\nby creating auto scaling groups in EC2 and referencing them.\nSo you basically say I'm creating a capacity provider\nand it's using an auto scaling group that uses a launch template for Spot instances\nand that launch template specifies that I'm going to use this different,\nthis set of classes of EC2 instance and that's one auto scaling group.\n\n\nBut then you can create an on demand auto scaling group as well\nand you can add that as a capacity provider.\nI think you can add up to like 10 capacity providers.\nBut then you need to combine them in what's called a capacity provider strategy\nwhere you say okay I'm going to use some from this capacity provider\nand some from this other capacity provider\nand that could be a mixture of EC2 and EC2 Spot\nand then you give a weight to each of them.\n\n\nSo you can say for every one on demand instance I want four Spot instances.\nAnd but you can also specify a baseline.\nSo you could say I want a steady baseline of two on demand instances as well.\nSo you could say my first two instances will be on demand\nand for the rest beyond that there's a ratio between on demand and Spot.\nAnd you can actually mix multiple capacity providers in that.\nIt is worth stating that you, while you can mix Fargate and EC2 providers in your cluster,\nyou can't really use them at the same time.\n\n\nSo if you set up a service that scales up to a certain number of desired tasks,\nyou can't have a mix of Fargate and EC2 in that.\nYou can only use Fargate together with Fargate Spot or EC2, together with EC2 Spot.\nYou can always use run task API directly so you don't have to use a service.\nYou could just launch tasks yourself using the run task API\nand every time you call it run task API you can launch up to 10 tasks\nand you can pick whichever capacity provider you want.\n\n\nSo that's another way you could potentially mix and match between Fargate and EC2\nand get a blend.\nBut obviously the more you do, the more complex it can become there.\nSo you have to trade off between the complexity of basically managing your own scheduler\nand orchestrator and your cost saving.\nSo Spot is pretty advantageous.\nI mean, if we just go back to our pricing example, we've run a few calculations there today.\n\n\nYou have to kind of look at how many CPUs you want and how much memory you want.\nLook at the available EC2 instances.\nWith EC2 you always want to pick a ratio that matches your task definitions CPU to memory ratio\nand then you can do a pricing comparison.\nAnd if we look at generally the, if you look at a one vCPU configuration\nwith two or four gigabytes of memory, you're talking about the range of like four cents or five cents per hour\nfor a CPU and memory.\n\n\nAnd when you compare Fargate and EC2, you get a bit of a,\nyou pay a little bit extra for Fargate, but it's not completely miles away or anything.\nSo you might say, okay, well Spot is giving me up to 70% saving in here,\nso why can't I just use Spot all the time?\nSo what are the disadvantages of Spot?\nWell, you mentioned that they use spare capacity, so that means that when,\nthere will come a point where the spare capacity isn't spare anymore\nbecause somebody actually wants to run an on-demand instance.\n\n\nAnd in that case, you'll get a notification, a 120 second warning from AWS\nthat they're going to shut your instance or your Fargate container down\nand you have to gracefully shut down and move your work somewhere else\nor just be satisfied that you're going to have to wait for a while.\nThe other disadvantage is that prices fluctuate, so it's not exactly deterministic.\nAnd sometimes instances won't be available for your instance class or in Fargate even.\n\n\nIt depends on your region, the time and the demand,\nand of course, there are Linux and x86 only.\nSo you don't get ARM, Spot, Fargate containers at the moment\nand you don't get Windows, Windows Spot.\nIf you're using something that's on demand, has to be predictable.\nIt's really a good idea to look at compute savings plans instead.\nIt's really flexible, but if you know you're going to be using a certain amount of compute\nfor a year or three years, then it's a significant save that you can get there.\nOn this topic, what else do we need to bear in mind, Luciano?\nIs there anything else we need to cover off?\n\n\nLuciano: I think in general, we focus a lot here on the compute cost itself.\nAnd it's definitely the biggest chunk of the cost when you run ECS workloads.\nOf course, when you run ECS workloads, you will have additional cost coming from your logs,\nyour metrics, data transfer, disk I/O, and we can even extend that if you're using databases,\nprobably how you interact with the database, how much data do you store and so on,\nand the cost of, I don't know whether it's Dynamo or RDS.\n\n\nSo in general, it's very hard to make just cost optimization only looking at the compute itself.\nSo I would suggest to expand a little bit the horizon there\nand see if there are other opportunities for cost optimization.\nFor instance, maybe you can reduce the retention of logs if you are storing logs indefinitely.\nYou can put a retention time, or if you have some retention time that is very high,\nmaybe you can reduce that and that can give you significant cost saving\nalmost immediately with a very small change.\n\n\nSo I guess the suggestion there is try to look for opportunities to do cost saving,\nnot just outside the realm of ECS itself, but just looking at what else ECS is kind of forcing you to use\nbecause you need to run very specific workloads.\nAnd there are some other tools that we might want to recommend there\njust to have other ideas on where there can be opportunities to save cost.\nOne is the Fargate Right Sizing Dashboard, which basically is a tool that you can just deploy in your own account\nand use as container insights metrics to identify if there is any waste or optimization opportunities.\n\n\nSo we will have the link in the show notes, but again, it's just something you can decide to deploy in your own account\nand use it as an additional tool to get more visibility.\nAnd similarly, there is a tool already in AWS, in AWS Cost Explorer specifically,\nthat is something that can give you a recommendation for right sizing EC2 instances.\nSo something that you can just have a look at if you're using a lot of EC2 instances to back your ECS\nand just make sure that those instances are actually used well enough.\n\n\nMaybe you can just get away with a smaller instance or just a different kind of instance.\nSo that's everything we have for today.\nAnd because this is a very big topic and I think cost optimization, as you say, is a little bit nebulous.\nIt's a little bit of an art.\nWe would be really curious to know if you have other tricks that you learned along the way.\nPlease share them with us and we'll be sure to share them on Twitter, on YouTube or in a next future episode.\nAnd this is definitely a topic where you never learn enough.\nYou always need to discover something new.\nSo I think it's very, very important that as a community, we keep sharing what we learn so we can all learn from each other.\nSo thank you very much for being with us today.\nI hope you found this informative and we look forward to seeing you in the next episode.\nBye.\n"
    },
    {
      "title": "73. What is AWS Application Composer?",
      "url": "https://awsbites.com/73-what-is-aws-application-composer/",
      "publish_date": "2023-03-24T00:00:00.000Z",
      "abstract": "In this episode, we're going to be talking about AWS Application Composer - a FREE service that promises to help you build serverless applications with ease. With its simple drag-and-drop interface, it's supposed to make Infrastructure as Code a breeze. But the real question is - does it live up to the hype?\nWe know a lot of you are probably struggling with building applications using CloudFormation. It's a real pain, right? So, we decided to take Application Composer for a spin and see if it's worth adding to your toolkit or giving it a hard pass.\nAfter covering a generic overview of the service, how it works, and the main concepts, we discuss our experience in creating a new simple serverless application from scratch only using API Gateway, Lambda, and S3. Then we cover what it looks like to import an existing project (a slightly more complicated one) into Application Composer and find out what works and what doesn't.\nWe conclude by discussing some other things that didn't work as expected and by providing our general recommendation on whether you should be using this service today.\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\n⁠Web platform filesystem Access API\nThe current status of cross-browser support for the File System Access API\nOur first Application Composer demo source code\nEarthquake notifier serverless project\nOur previous episode on Fargate and how to optimize cost for it\n\n",
      "transcript": "Eoin: AWS Application Composer is a free service that helps you architect and build serverless applications.\nWith a simple drag and drop interface, it promises to lower the barrier to entry\nin building applications using infrastructure as code.\nBut does it live up to that promise?\nIn this episode, we're going to unbox AWS Application Composer,\ntell you what it's like to use,\nand let you know whether you should add it to your toolkit or give it a hard pass.\n\n\nI'm Eoin, I'm here with Luciano and this is the AWS Bites podcast.\nAWS Bites is sponsored by fourTheorem,\nand fourTheorem is an AWS partner for migration, architecture and training.\nFind out more at fourtheorem.com and you can find the link in the show notes.\nRight, let's get down to it, Luciano.\nWe covered Application Composer briefly when it was announced at reInvent late last year.\nSince then, you've taken it for a proper test drive\nand really tried to see where it works and where it doesn't work.\nSo maybe we can just start by telling everybody who doesn't know\nwhat is Application Composer and what kind of problem is it actually designed to solve in the first place?\n\n\nLuciano: Of course, yeah. So let's get started.\nAnd as you said in the introduction, Application Composer is a free service.\nLet's put a little bit of an effort there in that free keyword,\nbecause I think it's going to be very important towards the end when we try to draw our conclusions.\nAnd the idea is that this service is going to help developers, cloud architects,\nto streamline and accelerate the architecture configuration of serverless applications.\n\n\nSo the idea is that it's built on top of infrastructure as code.\nSo it uses some of CloudFormation syntax in a more general sense.\nSo it kind of helps you out if you are not familiar with that syntax,\ngives you a very visual drag and drop interface where you can drag different resources,\nconnect them and behind the scenes, it keeps that infrastructure as code representation\nof everything that you are drawing up to date with the latest changes.\n\n\nSo the idea is that if you don't know infrastructure as code,\nCloudFormation specifically, or maybe you know a little bit of it,\nmaybe you don't know specific resources, again, it's that idea that it tries to lower down the barrier to end\nand trying to speed up achieving something mostly in the serverless space as of today,\nthat you can quickly build and use it and put it in your own AWS account.\nThe way it works is that it is an application that is running on your browser,\nspecifically, you just log in in the AWS console,\nand it's going to be one of the many services that are available through the AWS console.\n\n\nAnd when you start that particular service on the AWS console,\nyou immediately see that there is this concept of a canvas,\nwhich looks like one of the resources that you can manage with this service.\nBut it's important to mention that it's not really an actual resource in the classic sense,\nbecause it's not something that is persisted long term in your AWS account.\nIt's just the idea that you can start a new session in the browser,\nbut that session is totally lost when you leave that browser window.\n\n\nAnd this is an important detail that we will cover a little bit more in the rest of this episode.\nSo the idea is that you have to persist files somewhere\nbecause you are creating templates in YAML for your infrastructure as code through this experience,\nvisual experience.\nSo what the application composer gives you is the opportunity to synchronize\na local folder in your file system with everything that you are doing in the browser.\n\n\nAnd this is what they call connected mode.\nSo when you start a new canvas,\nthe first thing that you see is literally a screen that tells you,\ndo you want to work in connected mode?\nIf you want to do that, you need to give me permission.\nYou need to give the browser permission to read a specific folder in a file system.\nNow, this is another important detail because being able to give a browser file system permission\nis something relatively new in the web platform.\nAnd at this time, only Chrome and Edge, I believe, implemented that.\nDefinitely doesn't work for Firefox,\neven though if I was researching a little bit this API called File System Access API,\nand it seems that Firefox now has an experimental support for that.\nSo maybe eventually this is something that also Firefox will be able to support.\nSo again, at this point, what you can do is start the browser, start the application composer,\nconnect your local folder, give permission, and at that point, you can work on this canvas.\n\n\nEoin: OK, so from what I understand, then it's something that you get through the AWS console.\nIt gives you a nice looking UI for building applications, and it works in both directions.\nSo you can generate a visual representation from code,\nbut you can also generate code by dragging and dropping in this application.\nI know that like many other people, it took me a while to get to grips with CloudFormation\nwhen I started many years ago and figuring out how to define components and connect them together.\nSo it's not something you can pick up that quickly.\nAnd for that reason, I think the idea of application composer makes sense.\nAnd you've mentioned that phrase, lowering the barrier to entry a few times.\nSo it seems like, in theory, at least a good fit.\nIn practice, what is it like?\nSo you've built some applications with it.\nWhat are your first impressions?\nDid you encounter any major roadblocks?\n\n\nLuciano: Yeah, I think it would be beneficial to try to describe what I try to build, because it's a relatively simple project.\nBut I think there is also enough complexity there,\nif you were going to do all of that infrastructure as code manually,\nto have a good feeling for whether this kind of tool can really help us for more realistic projects.\nSo everything that I'm about to say, by the way, is available in our repository.\n\n\nWe will have the link in the show notes.\nAnd also the repository details how to basically clone that solution and deploy it,\nif you want to see it running yourself, and all the steps that you can follow,\nif you want to replicate everything that I am about to describe yourself.\nSo the idea is that it's a very simple application,\nwhere there is an API gateway with just one endpoint.\nThis endpoint will trigger a Lambda, which is responsible for basically doing\nthat Lambda proxy integration and generating an HTTP response.\n\n\nAnd before doing that, it's going to go into an S3 bucket\nand create a new file. So basically for every request,\nit's going to create a new file with a timestamp,\njust to simulate some interaction between the Lambda and another service like S3,\nto answer that particular HTTP request.\nSo effectively, we have three different services, API gateway, Lambda and S3.\nThey are connected together and there is a very specific workflow\ntriggered by an HTTP request, starts the Lambda, the Lambda writes to S3,\nand then the Lambda responds back to API gateway.\n\n\nSo these are the steps that I follow to try to implement all of this.\nI started a new project in Application Composer, started in synced mode.\nSo I had to select a folder in my file system.\nAnd as soon as I said, okay, keep going,\nthe first thing that I noticed is that I didn't have anything in that folder,\nbut as soon as I clicked okay in Application Composer,\na new template.yaml appeared in that folder.\n\n\nAnd initially, it's totally emptied.\nActually, there is literally like an empty object in the YAML syntax.\nSo that kind of tells you that an empty canvas is equivalent to an empty object in YAML\nfor Application Composer.\nAt this point, you have an Application Composer on the left side,\na list of all the resources that are supported.\nAnd one of these is API gateway.\nSo I just drag and dropped that resource into the canvas.\n\n\nAnd immediately what you could see is that it starts with one endpoint.\nThis is already preconfigured for you, which is a GET endpoint to the slash path, so the root path.\nOf course, it's something that you can change.\nYou can double click in the canvas object that you just dropped there,\nand you will see a panel with a bunch of configuration options.\nSo you can add more endpoints if you want to.\nBut for the sake of this project, I was happy with that default endpoint.\n\n\nAnd then I started to look at the template and I realized that all of that stuff,\nso that this new API gateway resource was already codified in Infrastructure as Code.\nAnd that was actually something already interesting because that is, I don't know,\nprobably around 15, 20 lines of code.\nSo it's definitely not necessarily simple if you want to write it yourself.\nAnd I was able to get all of that in just a few seconds by drag and dropping something in the browser.\n\n\nSo the next step was to start to provision, well,\nstart to create the code related to the Lambda function.\nSo same idea, go to the bar with all the resources supported,\ndrag the Lambda icon into the canvas and immediately you have the definition of a Lambda,\nboth in the canvas visible, but also in your template YAML.\nAnd with Lambda is actually very interesting.\nI feel like it's one of the resources where they spend the most time trying to\ngive you ways to configure it.\n\n\nIn fact, you can double click that Lambda object in the canvas\nand you will have such a big panel with like many configurations options.\nAnd one of these options, the first one that kind of captured my attention was the runtime option.\nSo you can click there and you have a long list of all the possible runtimes that you can use.\nAnd actually even more interesting, if you're interested in Node.js,\nyou have runtimes that are not just Node.js,\nbut you can have the same runtime for instance Node.js 18 plus TypeScript.\n\n\nSo I was really curious to try that and I picked up Node.js 18 with TypeScript.\nAnd I immediately realized that I didn't just get all the YAML related to that Lambda\nwith that particular runtime, with the metadata that SAM understands to be able to\nbuild TypeScript for you using ES build.\nBut also there was a skeleton folder that was created for me called source slash function,\nwhere inside of it I already had an handler in TypeScript and a package JSON.\n\n\nSo all of that stuff was very convenient.\nAnd again, something else that lowers the barrier to end to even understand\nhow do you organize files with your source code when you're using a template with SAM.\nAll of this stuff was nicely connected together.\nThen at this point, the next step was S3.\nSo again, drag the S3 icon into the canvas and you get a YAML with the definition of a bucket.\nAnd this was also very interesting because this is something if you want to do it yourself,\nit's actually the simplest of the three resources.\n\n\nBut then you never remember that there are a bunch of configuration best practices\nthat you can do when it comes to S3 buckets.\nFor instance, encryption, for instance, making sure that you don't have communication\nwithout encryption when you use the APIs to connect to S3.\nAnd all that stuff, all these best practices were already provided\nby the YAML auto-generated by Application Composer.\nSo it wasn't just the S3 bucket on its own, but there was also all the configuration for\nencrypting at rest and encrypting in transit, which is, I think, a really good best practice\nand something that you would easily forget to do it yourself,\nespecially if you are learning CloudFormation for the first time.\n\n\nSo the next step is to connect things together because right now we only have an API gateway.\nWe have a Lambda and we have an S3 bucket.\nThey are not connected with each other and we can easily connect them by dragging.\nThere are like some dots that you can drag on every source to connect with the other one.\nSo basically you create a line between them.\nAnd the first line that I created was from API gateway to the Lambda.\n\n\nAnd what that did was basically understand, OK, you want to trigger this Lambda from API gateway.\nSo basically the YAML was updated, creating an event description in the specific Lambda\nsaying this Lambda can be triggered by an HTTP event.\nSo that was really convenient as well, especially if you don't really know what's the right syntax.\nShould they put something on the API gateway side?\nShould I put something on the Lambda side?\n\n\nThat kind of helps you out to really figure out how to describe that integration\nin CloudFormation and some.\nAnd finally, I did something similar for the bucket.\nSo basically I connected the bucket to the Lambda and that understood that I wanted to use\nthat particular bucket in the context of my Lambda code.\nSo what happened is that it was created a policy for me.\nSo for the Lambda to actually access the S3 bucket, but also this was really cool.\n\n\nIt created two environment variables in the Lambda, referencing the name of the bucket\nand the ARN of the bucket, which is something very convenient when you need to start to add your code.\nYou don't necessarily know what's going to be the name of the bucket.\nSo generally you would use an environment variable to create a reference,\nand then you have an abstract way to connect to a specific bucket in your Lambda code.\n\n\nNow, the last step was to try to deploy all of this.\nI thought at that point I had, of course, I had to write a little bit more code in my own Lambda\nbecause the handler that was generated was just an Hello World.\nI actually wanted to write code to use the SDK,\nconnect to S3 and create that file and then respond back to the API Gateway request.\nSo for all of that, I needed to install a few extra dependencies because the SDK was not included.\n\n\nAnd also there is a dev dependency that you can use to get\ntypes for the different events and responses that you can use with the API integration.\nThere was some code there, but the dependencies were not installed.\nSo I needed to install those myself, but eventually I had all the code ready.\nIt's probably like 12 lines of code. You can check that in the repository.\nAnd I was able to deploy.\nI wasn't able to deploy directly from Application Composer itself,\nbut I had all the files locally.\nSo basically locally you can just run SAM, you can do SAM build to build all the TypeScript\nand convert it into proper JavaScript. And then you can do SAM deploy to initialize\na deploy. And after a few minutes, you should have everything running in your AWS account.\n\n\nEoin: It's pretty nice the way you can also then use SAM Accelerate.\nSo you could run 'sam sync --watch' and have it just live update.\nSo when you drag and drop components in your architecture,\nit's actually deploying that infrastructure under the hood and updating the function\nconfiguration under the hood. And it could be a fairly productive workflow\nfor the kind of applications that Application Composer supports right now.\nSo you've kind of explained how you start with the visual version,\nbut we talked about this being bi-directional.\nSo what happens if you've got an existing application?\nCan you import any application you've got that supports CloudFormation or SAM?\nDoes it work flawlessly? Are there any limitations?\nYeah, by the way, I didn't try to do that sync idea.\n\n\nLuciano: I think it's a pretty cool idea. We should try it at some point.\nAnd I'd be curious to see what happens if you like break the YAML at some point,\nbecause another note that I have is that it's very easy to break the YAML if you create new\nresources, remove them, disconnect them, reconnect them.\nYou might end up in a state where the YAML is not necessarily 100% correct\nuntil you fix the issues. So it would be interesting to try exactly what happens\nwhen you use the sync mode and then it tries to deploy continuously at every change.\n\n\nAnyway, with that aside, I did try to import an existing project that I worked on\nthat is a little bit more... It's not too complicated, but there is a little bit more to it.\nAnd this is a project we will also have the repository link in the show notes.\nIt's something that I call earthquake notifier, because it's a small personal project that I built\njust to be notified when a significantly interesting earthquake is happening close\nto my family in Sicily. So I can immediately know that that's happening and I can reach out to them\nto see if everything is fine. And it's a serverless project. Basically, it's built using a Lambda that\nruns on a schedule. This Lambda will call a specific API every hour that this API has a\nlist of recent earthquakes in Italy and it will check if there is a set of conditions that will\nmatch mostly based on position and magnitude of the latest earthquakes. And if there is an\nearthquake that matches what it does, it creates an event bridge event. Then there is an event bridge\nrule that captures this kind of events and will deliver them to a CloudWatch log stream,\njust for me to see all of them historically. And also to an SNS topic where I can easily\ntrigger notifications, for instance, to my email using an email subscription to that SNS topic.\n\n\nSo again, it's not an extremely complicated project. It's actually a very simple one,\nbut there are enough resources there to try to put application composers to test.\nAnd I did all of that using some pretty much manually. So it was also a good use case,\nI think, to try to see that reverse approach when you have something already created manually,\nwhat happens when you import it in Application Composer.\n\n\nSo the result was actually a little bit disappointing because when I imported,\nwell, basically I created a new canvas and I connected it to my local folder. But this time\nthe local folder had all the files for this project, including the template.yaml. And what\nappeared in the canvas was some of the resources, not all of them, and they were not all connected\ncorrectly. I had my own architecture diagram and I was expecting to see something very close to it.\n\n\nInstead, it was very different. There were only a few resources that were connected together,\nand most of the connections were missing. And so that was a little bit disappointing.\nAnd I started to try to fill the gaps myself. For instance, I was trying to connect an event\nbridge rule to SNS and I got an error message popping up saying this feature is not supported\nyet. So this is probably one of the reasons why the Application Composer wasn't able to build\na correct representation of the architecture or at least the connections between different\nresources correctly, because probably some of these things are not supported yet.\nSo in general, yeah, I wasn't really happy with the result. I don't think it was giving me value\nat that point to continue with that approach. I think for that particular project, I was probably\nbetter off to just continue with the manual approach.\n\n\nEoin: Okay, so there's a mixture of good and bad there. Talk about, you mentioned that it's for creating serverless applications, right? So\nI'm sure then it supports all of the serverless offerings from AWS, including, I don't know,\nFargate, MSK serverless, Redshift serverless, EMR serverless, right? They're all in the same\nserverless family. Obviously my tongue is in my cheek there, but what is the extent of it? Where\ndo the boundaries exist for this Application Composer serverless world?\n\n\nLuciano: Yeah, it definitely doesn't support many of the serverless or at least the ones labeled with serverless offering in AWS.\nFor instance, Fargate is the one that I wish was there because it requires significant\ninfrastructure as code to even just provision one container running on Fargate and it takes\nsome time to learn all the concepts. We have another episode on that if you're curious.\nSo definitely missing Fargate, I think it's a major feature that I wish was there. And of course,\nyou mentioned other serverless services like Redshift, EMR and so on. It would be nice to\nhave those also integrated in this tool. So the reality is that they are not there yet.\n\n\nThere are, the last time I counted was I think yesterday, there were 13 resources, but I also counted them\na couple of weeks ago and there were 12. So that's maybe an indicator that the team is working hard\non this product and we will see more and more resources being added over time.\nSo that's, in the negative, there is a positive trend that we see that there is cost and development happening\non this tool. So things might change drastically in a few months. Then there were a couple of weird\nthings that I noticed that I think are just confusing. I don't know if I will consider\nthem missing features or bugs. I'm just going to try to give you a quick list of things that I\nfound a little bit confusing or not really working as I expected. The first one is that you see step\nfunctions there. And I was like, okay, maybe this is a good alternative or a more integrated tool\nthat I can use to design my Step Functions visually without, rather than using the step\nfunction designer tool. And then I realized that you can create this Step Function resource,\nbut basically it only manages one state. It doesn't allow you to really create a complicated\nworkflow. So you can just connect a Step Function to one Lambda, for instance, and that's it.\n\n\nYou cannot create all the other logical connections that you can use with Step Functions, like if\nstatements or parallel execution and all these kinds of things. So that was a little bit annoying\nbecause really there isn't a lot of value to have a Step Function resource if it doesn't do\nall, well, not necessarily all, but at least some of the main things that you're probably\ngoing to be using a Step Function for. The other one is that SQS queues are a little bit confusing\nbecause you can use an SQS queue as a, for instance, as something that triggers event on Lambdas.\n\n\nFor instance, if you want to consume messages with a pool of Lambdas from a queue,\nbut also you can use a queue as a dead letter queue for a Lambda. And it seems like these two\noptions are supported, but I wasn't able to use the designer and really connect things in the\nway I wanted. So again, it feels more like there is some missing feature there or some bug, and it\ndoesn't really seem to work in the way you want it to work, even though you know, or at least I\nwas aware of the concept and I was trying to do something that was very clear in my mind, I wasn't\nable to use this tool to achieve that particular goal. The other thing, there are some other small things.\n\n\nLike for instance, it's really cool that it creates environment variables in Lambdas for\nyou. But if you are experimenting heavily, like creating new resources, connecting them, then\nmaybe removing them because you change your mind, you realize you can do something differently,\nall these environment variables are not cleaned up for you. So you will end up with a lot of\nreferences to resources that maybe are not relevant anymore to your Lambda, maybe you even\ndeleted and your template at that point is wrong because it's going to fail to reference the\nspecific resource when you try to deploy. So again, probably in between a bug and a missing\nfeature there, so there is definitely work to do to polish it. Very similarly, if you change your\nown runtime, for instance, if you switch from JavaScript to TypeScript, it will create a\nJavaScript file first and then a TypeScript file, but it doesn't clean up the original JavaScript file.\n\n\nNow this is maybe a safety thing because if you have already written code in that JavaScript\nfile, of course you don't want to lose that code, but you might end up with, for instance, I noticed\nthat the package.json was not getting updated correctly, and if you manage to create a new\nLambda with TypeScript straight away, the package.json will contain things that you need\nfor TypeScript, like the type definitions, but if you started with JavaScript and then you create,\nyou switch to TypeScript, it's not going to update your package.json anymore, so it doesn't match\nexactly your expectations at that point, and you need to manually fix all of that.\n\n\nWell, you need to realize first and then manually fix it. So yeah, that's another bugbear that I have with it,\nthat it could be a little bit better and will give much more value. And last point I have is,\nactually last two points I have is one that for TypeScript, it is really cool that generates a\nskeleton for you with types already built in, but for instance, when you connect API Gateway,\nI was expecting to create a skeleton that will contain proper types for the LambdaProxy\nintegration. Instead, it just gives you a generic handler object object, which at that point,\nit doesn't give you too much value for TypeScript. Of course, you can fix it manually, but it would\nhave been so easy to add that extra step and you will get so much value for it, because you,\neven having to know exactly which types are the right one to use with API Gateway and the\nLambdaProxy integration takes a while, because there are so many different types of events\nin the TypeScript library that you will be spending probably five minutes just to figure\nout which ones are the right ones for the incoming event and the response that you have to provide.\nAnd finally, the last one is that I needed to go into the API Gateway console just to figure out\nthe URL of my provision API Gateway, because it didn't create an output block for me, which is\nsomething that could have been very easy to do and very convenient. Just a small feature request,\nif anyone from AWS is listening. And the last thing, and I think we kind of mentioned this\nalready, is that the Canvas resource is not persistent. So basically, that means that if\nyou forget to copy paste your files, if you are working in a non-synchronized mode, you're going\nto lose everything you just did. It doesn't even ask you, like, when you're closing the tab, be\ncareful because you're losing everything there. It just closes the tab and everything is gone.\nSo if you were working in sync mode, you still have the files locally, you get everything that way.\nBut if you didn't work in sync mode, it can be very dangerous that you might be losing everything,\nwhile I think the expectation in AWS is that everything is persistent.\nSo that could be a little bit misleading if you are used to use the web console in AWS.\nSo it sounds like there's quite a lot of work to do.\n\n\nEoin: I mean, on Application Composer, I think I'm still on the optimistic side because I like the fact that the designer user interface,\nit seems and feels like a bit of a step forward from other AWS user interfaces.\nIf there was better support for all of the different configuration options for\neach of the services and some of the other things you point out, it could become very useful.\nBut what's your opinion in general? Would you recommend for people listening,\nthey should give it a chance now, wait for a while or just ditch Application Composer and\nforget about it altogether? It's a very good question.\n\n\nLuciano: I honestly have a bit of mixed feelings. I see a lot of potential there. This might become a really\ngood tool in the future. Right now, it is a little bit not there yet. It could be still useful if you\nare just doing your very first steps in AWS and you are trying to learn serverless and you want\nto use infrastructure as code, by the way, you should. So it could be really convenient for just\ndoing some very quick experiments and trying to see if I do this, what kind of infrastructure\nas code do I get? And that could be a way to kind of train yourself to understand CloudFormation\nin a more practical way, rather than just reading the documentation and then copy pasting and trying\nto build things from scratch by yourself. This can speed up a little bit. You're building a\nmental model for our CloudFormation and some work. And when I was trying to think if I like this tool\nor not, I realized that probably like 20 years ago when I started my tech career, I was learning HTML\nand the way I learned it was through tools like FrontPage and Dreamweaver, which were giving me\nthis kind of visual feeling. And then I was really curious and I was constantly switching to the\nHTML tab of everything that I was drawing. And it was horrible HTML, retrospective, like it wasn't\nclean at all. I wouldn't write HTML that way today, but that helped me a lot to learn the HTML\nlanguage on itself. Like I was able to, for instance, draw a table with columns that were\nmerged and very complicated things, and then just switch to the HTML tab and stuff to learn all the\ndifferent tags and attributes that I could use to rebuild the thing myself. And eventually, of\ncourse, I stopped using these tools because I was able to just write HTML myself.\nBut I think these tools were very instrumental for me to speed up that learning journey and make it more interactive\nand probably more rewarding, at least for the way of learning that I have. So hopefully Application\nComposer can become that kind of tool for infrastructure as code in AWS.\nThat's at least my positive take on it.\n\n\nEoin: That sounds like a fair assessment. So it's worth a shot if you're getting started. And also maybe if you just want to prototype something up quickly based on all these\nserverless resources and then switch to code from that point on. I suppose we should probably say\nthat it's always good advice to prepare to learn what the generated template syntax is anyway,\nand don't try to completely hide from infrastructure as code because it's a really\nvaluable cloud skill. Maybe one day we'll be able to use Application Composer to import very advanced\napplications with loads of resources and keep that bi-directional flow going. But it's a difficult\nambition to achieve. So maybe everyone out there, if you have tried it, let us know what you found\nabout it and what you might expect from it in the future. Otherwise, thank you very much for joining\nus again and we'll see you in the next episode.\n"
    },
    {
      "title": "74. Function URLs vs API Gateway",
      "url": "https://awsbites.com/74-function-urls-vs-api-gateway/",
      "publish_date": "2023-03-31T00:00:00.000Z",
      "abstract": "How can you use a Lambda to respond to an HTTP request? There are more ways than ever to do it. We have API Gateway REST APIs, Lambda support for Application Load Balancer, and now Function URLs. But which one should you use, and when?\nIn this episode of AWS Bites podcast, we will give you a quick and simple guide to picking the best way to build APIs with Lambda. In this video, we're going to pitch Function URLs against API Gateway in a battle for the ages!\nFunction URLs offer a simple and quick way to get a public URL to invoke a Lambda function, with fewer configuration options and cheaper pricing. They are suitable for private webhooks, simple backend functions, and machine learning inference backend. However, they lack authorization and DDoS protection, making them unsuitable for like public webhooks. On the other hand, API Gateway offers more features and control, making it suitable for public APIs. API Gateway comes in two flavors: REST and HTTP with some subtle differences.\nFinally, we will also cover Application Load balancer and explore when and why it can be a convenient alternative to both Function URLs and API Gateway.\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nArticle by AJ Stuyvenberg reporting that Function URLs have a latency of 8.35ms: introducing-lambda-function-urls-4ahd\nGitHub repository with all the material we used in our evaluation\n\n",
      "transcript": "Luciano: People have been building APIs and web books on AWS Lambda for many years now.\nThere used to be one way to do this with API Gateway REST APIs.\nThen came Lambda support for Application Load Balancer\nand then the new HTTP APIs in API Gateway.\nAnd the latest way to build simple APIs backed by Lambda is Function URLs.\nAnd you don't even need API Gateway or load balancer for that one.\nBut should you use it? And if so, when?\n\n\nToday we will give you a quick and simple guide\npicking the best way to build APIs with Lambda and pitch Function URLs\nin a battle against API Gateway.\nAnd also we're going to mention the pros and cons of load balancers in between.\nI am Luciano and I'm here with Eoin and this is AWS Bites podcast.\nAWS Bites is sponsored by fourTheorem.\nfourTheorem is an AWS partner for migration, architecture and training,\nincluding APIs for application and system integration.\nFind out more at fourTheorem.com.\nThere is a link in the show notes.\nSo since Function URLs is the new feature here,\nwhat is it and why it's something that we should be excited about?\n\n\nEoin: With Function URLs, the focus is on trying to make almost any configuration go away.\nSo if you have a function built in Lambda,\nthe goal here is to give you the quickest way of getting a public URL to invoke that function.\nAll of the other methods you talked about, like API Gateway and load balancer,\nthey require you to create and configure lots of new resources\nand configure them and understand how those configurations are syntaxed.\n\n\nAnd you have to then integrate them with the Lambda service.\nWith tools like SAM and the Serverless Framework, it makes that process easier,\nbut there's still all those knobs under the hood\nand you kind of have to understand how they work.\nSo Function URLs are a different approach.\nIt's not even a simple separate service.\nIt's just a feature of the Lambda service itself.\nSo you only have to enable it with one property.\n\n\nAnd once you enable it, you get a generated URL right away for the function\nthat can be invoked publicly.\nSo the format of that URL you get is like HTTPS,\na generated ID, dot Lambda URL, dot whatever region, dot on dot AWS.\nSo it's different to the ones you've seen in API Gateway.\nNow, I did say that the aim was for zero configuration.\nThere are a few parameters you can add if you want.\nSo one is you can enable course.\n\n\nSo if you've got a front end and you need to have course enabled for your API, you can do that.\nYou can also enable IAM authorization.\nSo while the URLs are public, you can restrict access to them by mandating IAM privileges.\nAnd that's the only supported authorization method for Function URLs.\nAnother one you could do is you can actually map your Function URL\nto a specific Lambda function alias.\nSo that means you have a label that points to a specific version of that Lambda function.\n\n\nAnd if you have safe deployments where you're shifting traffic from one version to another,\nyou can do that using aliases in a controlled way and then point your Function URL to a specific alias.\nSo what this simple configuration means is that you don't get a lot of the features\nthat are supported with API Gateway.\nWe could probably just quickly list through the features that you're going to be missing out on\nthat you get with API Gateway, but don't come with Function URLs.\n\n\nSo API keys are out.\nYou can also don't have the option of using Cognito User Pool or Lambda Authorizers or\nJWT Authorizers.\nYou have no support for custom domains, so you are restricted to using the generated names\nthat you get.\nYou don't get request or response validation.\nYou don't get any caching.\nAnd if you want a web application firewall, there's no support with Function URLs directly.\nNow, you can use CloudFront in front of a Function URL if you want and get caching from CloudFront.\n\n\nYou could also use that for WAF as well.\nSo that's another way of overcoming the caching and WAF limitations in Function URLs.\nYou don't get HTTP access logs with Function URLs, which might be a bit of a deal breaker\nfor some people.\nAnd you have no way of doing throttling or usage plans.\nAnd of course, there's no WebSockets support like there is with API Gateway.\nSo those are all the things you're missing out on.\nSo given that that's quite a long list, what do you think?\nWhy would you use it?\nWhat are the actual advantages of Function URLs, Luciano?\nAnd what kind of use cases might people say, OK, they don't have all the features, but\nI could still use Function URLs here?\n\n\nLuciano: Yeah, I think as you said, if you don't need most of these features, there are some trade-offs for which I think it might be beneficial to consider something like functional URLs.\nFirst of all, it is much simpler.\nSo if you just need a quick way to trigger a Lambda based on a URL, that's probably the\nsimplest way that there is right now.\nIt is also cheaper.\nIt is actually free compared to all the other options where you have an extra cost other\nthan the cost of the Lambda invocation.\n\n\nIn the case of Function URLs, you don't have that extra cost.\nYou just pay for the Lambda duration and the invocation.\nAlso, there is an interesting article.\nWe're going to put the link in the show notes that claims that this integration with function\nURL, it is the fastest one.\nAnd it claims something around 8.30 something milliseconds with a warm Lambda.\nThat of additional overhead just in doing that HTTP request.\n\n\nBut we were not actually able to replicate that number.\nAnd I think the best that we got in our tests was 100 milliseconds.\nMaybe that really depends on the testing methodology.\nIt depends on the region.\nThere might be many factors to affect really that particular result.\nWe'd be curious to know more.\nSo if you have any more data that can prove or disprove this hypothesis, definitely let\nus know in the comments.\n\n\nAnother thing, and this is actually a really interesting one that may enable some interesting\nuse cases, is that timeout is much longer than API Gateway.\nIn fact, in API Gateway, you have 30 seconds, meaning that if your Lambda doesn't respond\nbetween 30 seconds, API Gateway will just stop the connection there and respond with\nan error.\nWith Function URLs, instead, you can leverage the entire maximum direction of a Lambda.\n\n\nBecause if your Lambda is lasting the maximum amount of time, which is 15 minutes,\nthat connection is still going to be open.\nSo you could use this particular feature if you have APIs that, for whatever reason, need\nto take more than 30 seconds to create a response for the user.\nAnd actually, with that, let's move on to some use cases.\nSo we can try to think a little bit more practically what we can use this feature for.\n\n\nAnd the first use case, and probably the one that will come to mind first for most people,\nis Webhooks.\nBecause Webhooks, traditionally, you just need a URL and that creates an integration\npoint between different systems.\nAnd for instance, I do not.\nThe common ones I've seen is CRM systems.\nThey generally support Webhooks.\nSo maybe you would want a way to know when a new deal is created in the system, because\nmaybe you want to do some, I don't know, data enhancement.\n\n\nMaybe you want to use another system or another API to try to enrich the data.\nYou could create your own Lambda with a Function URL and then create the Webhook integration\nusing that Function URL.\nAnd very similarly, I've seen newsletter systems doing the same thing.\nBasically, every time somebody signs up for your newsletter, you can have the option to\nspecify a Webhook URL there, and then you can run your own backend logic to do something\nwith it.\n\n\nMaybe you can insert that email address into a specific database, maybe a marketing database\nor something like that, or maybe you can just send a custom newsletter to welcome the person\ninto the system.\nAnother one that I've seen is shipping providers.\nSo if you are running an e-commerce that needs to send physical products, very, very often\nthey will provide a Webhook-based interface to give you updates every time there is a\nstatus update on one of your shippings.\n\n\nSo you could create a Lambda with a Function URL, and then you can receive those kinds\nof events and use those events to update your own database and maybe send notifications\nto your customers to tell them what's the status of their shipping.\nAnd finally, another one that I've seen a lot for Webhooks is integration with CI systems\nor repositories, where they also can trigger Webhooks based on different events.\n\n\nSo if you want to do something custom, again, you can just build your own Lambda very quickly,\nFunction URL and integrate it that way.\nAnother use case could be outside Webhooks when you are building quick prototypes.\nAnd this can be also something that long term, maybe you want to do something like API Gateway,\nbut you are just sketching out a Lambda just to see if the idea works.\nAnd maybe you are even using the web console.\n\n\nIt could be very beneficial if you already have your cURL request ready or maybe post\nmap request ready there to just quickly enable the Function URL just to get a URL that can\ninvoke your Lambda, do the request, test that everything is actually working as you expect,\nand then you can switch it off and maybe before going to production, actually migrate to API\nGateway to leverage all the other features.\n\n\nAnother use case, and this is something that actually we found in the blog post announcement\nfor Function URLs, is what they call single function microservices, which actually in\nthe blog post didn't really explain what they meant with that example.\nBut later on in the same blog post, they mentioned the idea that you could create a payment endpoint\nfor a mobile application.\nSo maybe that's what they were referring to as a single function microservice.\n\n\nAnd I think that makes a lot of sense if you also consider that you can use IAM authentication\nthere, so you could create a very small service.\nIt's just an endpoint.\nYou don't need to create an API Gateway dedicated to that because you can secure it with IAM\nand then you can call it from other services.\nAnd then I was trying to think for other possible use cases and I was thinking the different\nfeatures that you have with Lambda Function URLs.\n\n\nAnd since you can enable CORS, I think that enables a bunch of use cases where you have\nvery simple front ends, maybe static websites, and you want to create some server-side functionality.\nAgain, that could be a contact form or maybe a form to sign up for a newsletter and you\ncan build all the server-side logic using a Lambda with a Function URL.\nAnd finally, again, thinking about that 15 minutes time, extra time that you can use\ncompared to API Gateway, I was thinking that that could be a good use case if you have\nan API that runs some machine learning inference algorithm that maybe can take longer than\n30 seconds, maybe can take a few minutes, and you still want to use that HTTP protocol there.\n\n\nIf you use Function URLs, then you can still use a Lambda to do all this work behind the\nscenes.\nSo, of course, these are just some examples.\nIf you have other ideas, we'd be curious to hear what you're thinking about in the comments,\nso definitely let us know.\nBut it's always worth keeping in mind what are the limitations there, because in general,\nyou can build all these different use cases, but you need to think that you have no authorizations,\nat least outside if you use IAM, but most importantly, you don't have DDoS protection.\n\n\nSo that basically means that if somebody manages to figure out what's your Function URL, and\nif you're using that, for instance, BI in the frontend application, it will be there\nin the HTML, so it's not something that you can easily keep secret.\nSo what they could do is they can start to generate requests against that particular URL\nand basically do a denial of wallet attack to you, because you might end up paying for\nall of those invocations.\n\n\nNow, we were trying to think, okay, are there strategies there that you can put in places\nto protect yourself against that?\nAnd maybe to some degree, what you could do, you could create an alarm to detect excessive\ninvocations and then basically respond to that alarm by editing the Lambda function\nand setting the throttling to basically forbid additional invocations of that Lambda.\nAnd that could work, but you have to keep in mind that under the alarm, you have to\nand that could work, but you have to keep in mind that at that point you are stopping\nthat Lambda entirely, even for legitimate invocations.\n\n\nSo you are literally deciding, let's sacrifice this feature for now in exchange for not getting\nthe denial of wallet attack.\nSo again, if you are really worried about DDoS, probably Function URLs are not the best\noption there.\nThe final point that I have is in terms of drawbacks of Function URLs is that there is\nno routes or HTTP methods.\nAgain, it's just a URL.\nYou get a custom domain and every request there will invoke the Lambda.\n\n\nSo if you are expecting a post and somebody does a GET, you still get the Lambda invocation.\nNow, of course, you can do validation inside the Lambda function, but you have to remember\nthat the Lambda gets invoked.\nSo you're going to be paying for that, which is something that you can avoid instead with\nAPI Gateway because you can be very specific on which methods and paths will actually invoke\nthe specific Lambda.\nAnd finally, the Function URL, since it's generated randomly, if for any reason you\nhave to delete the stack, redeploy, so you're basically removing the Lambda and recreating\nit, you will end up with a new URL.\nSo if you are assuming that that URL is going to be stable across the entire deletion of\nthe stack and recreation, that's not going to be the case.\nSo if you happen to do that, you will need to reconfigure whatever integration because\nyou will have to provide a new Function URL.\nYeah, that's a good point.\n\n\nEoin: And there's no real way around that.\nI think you can't use a CNAME because I presume the service uses the host header to do the\nrouting back to the individual function.\nSo given these drawbacks, let's kind of pitch Function URLs in a battle against the rest.\nWe don't want to cover API Gateway in extensive detail, but we can remind ourselves of the\noptions and the trade-offs.\nNow, API Gateway has two flavors.\n\n\nWe have the classic REST API mode and the newer HTTP API.\nHTTP API is about 30% of the price of REST API roughly, but there are some feature gaps.\nSo it's a bit leaner.\nIt doesn't have all the features of REST API.\nWe should mention you can also do WebSockets with API Gateway, but we'll leave that out\nsince that's not really part of our use cases here.\nAnd we'll assume that you're dealing with GETs, POSTs, PUTs, patches, and that sort\nof typical stuff.\n\n\nSo HTTP API, I would say, is the nearest competitor to Function URLs because it's a bit simpler\nand the price and performance are better than REST API.\nIn terms of authorization, we mentioned that Function URLs supports IAM.\nHTTP API will support IAM Lambda authorizers as well and JWT authorizers.\nAnd those JWT authorizers can also be used with Cognito User Pools, whereas the REST\nAPI has a dedicated authorizer type just for Cognito User Pools.\n\n\nThe older REST API also has a private mode, so you can use it for internal APIs.\nBut that's not always plain sailing.\nIf you want to use custom domains with private internal APIs, that's not very trivial and\nit requires a workaround with a load balancer in front.\nOtherwise, if it's not a private API, HTTP APIs and REST APIs both support custom domains\nwith your own domain names.\nSo that handles the case you mentioned, Luciano, where you delete the function, redeploy it.\n\n\nIf you've got custom domains, then you can map that to the underlying API and you don't\nhave to worry about regenerating URLs.\nBut HTTP API and REST API both have throttling support.\nThey both have the concept of stages, so you can have like a production stage and a development\nstage, QA stages, and they both have some level of open API or swagger support.\nNow, REST API does have a bunch of extra features that HTTP API does not have, like edge deployments.\n\n\nrequest response validation with JSON schemas, and also request response transformation with\nvelocity template language or VTL.\nThis actually gives you the option of excluding Lambda altogether and having a functionless\nAPI.\nAnd you can proxy your API into a backend service like DynamoDB or Kinesis or some other\nunderlying service.\nAnd that can be optimal in terms of cost and performance for some cases.\n\n\nREST API also has caching and it has X-Ray tracing support.\nHTTP API doesn't have X-Ray support yet.\nSo that's a pretty important one, I think, for a lot of people.\nI would say that the general guidance is use HTTP APIs if you don't need anything in that\nlist, because REST API has a lot of those features, but they're not required in all\ncases.\nEdge deployments, not particularly.\nYou can always use CloudFront as an option instead of edge deployments anyway.\nAnd request response validation and transformation.\nI don't see it used a lot in the wild, really.\nThey're a little bit of niche in terms of their usage.\nSo I would say also as a point of note on observability, both of these API gateway modes\nwill give you logs, access logs, and you can configure that log format.\nIt's worth mentioning that although you don't get any access logs with function, you're\nalso you do get CloudWatch metrics.\nSo you get the request count, request latency, and metrics for 400 hours and 500 hours as\nwell.\n\n\nLuciano: Yeah, so my understanding is that the general guidance seems to be that you should be using Function URLs if your use case is very, very simple and you don't really have a lot of\npatience for configuring a bunch of other resources just to route traffic to a Lambda.\nAnd that's probably a good use case if you are just learning AWS and these are your first\nexperiments with Lambda and you just want something simple that just works.\n\n\nYou are not too concerned about extremely production ready deployments at that stage.\nIf that's not your case, probably your second best choice is to use HTTP APIs.\nBut of course, this is the case when you don't need caching, you don't need private endpoints,\nX-Ray validation, transformations, and all the other extra features that are not currently\nsupported by HTTP APIs.\nBecause if you need those, of course, you need to fall back to more traditional REST\nAPIs on API Gateway.\n\n\nBut we still have left out another contender there, which is using an Application Load\nBalancer.\nSo let's try to have a quick overview on why that might be an interesting option.\nAnd to be fair, in general, it's not too common to see load balancers being used with Lambda.\nThey are more used for traditional EC2 deployments or Fargate deployments.\nBut it is definitely possible to use load balancer to forward traffic to Lambda, basically\nto Invocate Lambdas with an HTTP event.\n\n\nAnd there are some advantages actually in doing that.\nFor instance, if you use API Gateway, something that I don't think is very commonly known,\nthere is a limit of 10,000 requests per second.\nThere is some burst capacity there, but there is a limit anyway.\nWhile if you use a load balancer, you can literally have millions of requests per second.\nNow, I don't know if that's really that useful in the context of a Lambda, because in the\ncontext of a Lambda, you always need to keep in mind that if you have many concurrent requests,\nthat would probably trigger many concurrent invocations of different Lambda instances.\n\n\nSo basically, you might run out of concurrency in Lambda anyway at some point.\nSo I don't think you can easily leverage millions of requests per second by just using Lambda\nbehind the scene, unless you really had a lot of conversations with AWS and they managed\nto increase your quota significantly.\nBut that probably only happens if you really have a very large business running lots of\ninfrastructure in AWS.\n\n\nInteresting enough, ALB and general use with EC2, as we said, so you have options to route\ntraffic in many different ways.\nFor instance, you can look at the hostname, you can look at headers, you can look at query\nstrings.\nSo in the case where you have complex microservices, where you have different routing logic, I\nthink ALBs are the most flexible option there to decide where to send specific requests.\n\n\nAnd we will talk more about an example that can be relevant there.\nAnd finally, another one is about pricing, because if you have very little traffic with\nload balancer, there is a specific pricing model where basically it might be a little\nbit more expensive to just have an ALB to route traffic if you don't really have a lot\nof requests.\nBut when you start to have a significant amount of requests, I think that price is actually\nmuch more justified and overall you might end up with a better pricing compared, for\ninstance, to just having API Gateway.\n\n\nSo keep in mind that also pricing might be interesting depending on what's your particular\nneed there.\nJust to mention that particular example that I was saying before, one good use case is\nwhen you are trying to migrate a monolith application, maybe behind an Application Load\nBalancer, to something that looks like microservices and maybe you want to use Lambda as part of\nthis new microservice architecture.\n\n\nWhat you could do, you could still keep your own monolith and apply the strangler pattern\nto slowly migrate bits and pieces of that application to microservices.\nSo basically what you could do as soon as you implement a new endpoint with a Lambda,\nyou can just change the routing logic at the load balancer level and use the load balancer\nitself to map that particular endpoint to a Lambda invocation.\n\n\nSo that way you basically can do everything at the load balancer level, manage all that\nrouting, keep using the monolith as long as we have features to migrate over to serverless\nfunctions and then at some point you will have only serverless functions in a microservices\nway.\nSo just to summarize, we have four options.\nWe have Function URLs, which are the simplest and the quickest.\nVery good for webhooks, very good for quick experimentation, very good if you are learning\nLambda and you want a simple option just to trigger HTTP requests and invoke Lambdas.\n\n\nThen we have HTTP APIs where you can leverage some of the more advanced features of API\nGateway, but they are not as feature complete as REST API Gateways.\nSo if you need all the additional features, of course, go to REST API Gateway.\nAnd finally, we have Application Load Balancer, which are generally very good either when\nyou are migrating from monoliths or where you have a lot of traffic and therefore it\nmight be more beneficial in terms of pricing and scalability to use an Application Load\nBalancer.\nWe also have a repository where we put some of these notes and some of the evaluation\ntests that we made to prepare this episode, so we will have the link in the show notes.\nAnd this is everything we have for today.\nSo really looking forward to your opinion, which HTTP integration method to use.\nAnd if you have any other suggestions on the we missed some other reason to use one approach\nrather than another, please let us know in the chat.\nSorry, in the comments and we'll be back to you and trying to debate whether we like this\nopinion or not, or whether we are missing something there.\nThank you very much and we'll see you in the next episode.\n"
    },
    {
      "title": "75. GitHub Copilot and ChatGPT: Game Changers for Developers?",
      "url": "https://awsbites.com/75-github-copilot-and-chatgpt-game-changers-for-developers/",
      "publish_date": "2023-04-07T00:00:00.000Z",
      "abstract": "In this special episode of AWS Bites, we drop all our opinions about the sudden growth of AI and how it is going to change the future as we know it!\nWe begin by taking a trip down memory lane and discovering the types of AI tools that have been used in the past and how they have helped us. Then, we'll dive into ChatGPT, a language model that can assist us in writing and even creating code. We're especially excited to discuss how ChatGPT can be used to create slide decks or even write a book or a blog post.\nBut wait, there's more! We'll also explore the utility of other AI tools such as Grammarly and OpenAI Whisper for improving our writing and transcribing spoken words into text.\nMoving forward, we'll examine how we tried to use AI to develop cloud applications on platforms like AWS. We'll also consider the impact of AI on the education system and how it can be used to modernize complex systems, or for learning, including programming languages that are new to developers.\nNow, we know there might be some concerns about using AI, such as whether it takes away the fun of software engineering or reduces creativity. But fear not! We'll address these concerns head-on and explore how AI can actually make us more productive and lead to exciting new discoveries.\nFinally, we'll discuss the exciting possibilities for AI and its potential to democratize access to the job market and society in general.\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nOur episode about OpenAI Whisper for generating transcripts\nDavid Boyne's AI-powered story generation tool (AWS Blog post)\nThe Fission project for simplifying monolith to microservices migrations\n\n🎁 Bonus content\nA Limerick by ChatGPT\nOn the Amazon Cloud far away,\nWhere businesses went to play,\nThe costs grew so vast,\nTheir budgets were trashed,\nAs their dollars all floated away!\n",
      "transcript": "Eoin: If you haven't been living under a rock lately, you have heard a lot of noise and excitement about AI tools.\nGitHub Copilot, DALL-E and ChatGPT are just some of the latest tools.\nChatGPT and other large language models in particular are being heralded as game changers for the future of work across all industries.\nSo how do we escape the noise and make a measured assessment of where this is all going and what it means for software engineering?\n\n\nWell today we're going to try and do exactly that.\nWe'll talk about what we've been using, what works and what doesn't work, and give some thoughts for the future.\nStay tuned to find out how you can use all these tools to your advantage.\nI'm Eoin, I'm joined by Luciano, and this is the AWS Bites Podcast.\nAWS Bites is sponsored by fourTheorem.\nfourTheorem is an AWS partner for migration, architecture and training.\nFind out more at fourtheorem.com. The link is in the show notes.\nLuciano, maybe we can just chat through this and talk about what we've been using first.\nIt's not that new in so many ways because for a while we've been using things like autocomplete and autocorrect on our mobile phone keyboards.\nWe've had smart AI-assisted tools in Google Docs and Gmail Smart Reply.\nEven Google Translate is something probably anybody who's been abroad has used once or twice.\nWhat else have you been using recently, or what do you find is particularly cool?\n\n\nLuciano: Yeah, absolutely. I think now there is just an acceleration of new tools with more overwhelming and unexpected capabilities.\nAnd especially the ones that I've been using the most are GitHub Copilot, a bit of ChatGPT, Notion as well, some interesting tools.\nSo maybe we can talk a little bit more in detail about why are we using them and for what.\nYeah, it's interesting. With Copilot in particular, I used it when it was part of a developer early access preview.\n\n\nEoin: I used it quite extensively and really loved it actually, and I was pretty impressed by it.\nBut I stopped using it after a while, and I'm not sure why exactly, but I think a little bit of it was I was afraid of getting a little bit lazy and not thinking for myself anymore.\nSo I just tried to just turn it off for a while and see how I'd get on.\nI haven't had the major urge to go back to it, but at the same time, when we're pairing together and pairing with other people who are using Copilot, I do have a bit of FOMO.\nSo what do you think it's good for then, or where does it really shine?\n\n\nLuciano: Yeah, to be fair, I also started to use it almost as a joke. Like I didn't expect it to be useful.\nBut then I immediately saw value and I didn't go back to not using something like Copilot.\nSo maybe I'm biased because I'm really enjoying using it and I'm seeing lots of good use cases for it.\nFor instance, recently I was writing some unit test and I needed to create an object that was necessary for the test that was a little bit convoluted.\n\n\nIt's like complex objects with lots of like nested substructures and everything needed to be generated in a way that was making some sense for the test.\nAnd then of course, I needed to generate some assertion after I was doing some operations in this object.\nAnd as soon as I started creating the first line of that object definition, Copilot immediately completed like 10 lines of code.\nAnd when that happens, I'm always a little bit skeptical because when you write too much code, I am always afraid that I'm going to be missing some important detail, maybe a comma there or maybe some name somewhere else.\n\n\nSo I did check everything and it was actually quite good.\nAnd then it also generated a bunch of assertions which were like 99% close to what I was trying to do.\nSo I still needed to review everything and fix a few small things, but I think it did save me a lot of time.\nAnd that's something that I mean, it's happening all the time that I feel that I'm becoming a bit more productive just because it gives me that suggestion and also gives me that suggestion in a way that it's easy for me to decide, do I want the suggestion or not?\nAnd if I don't want it, I just keep writing and ignore whatever is saying.\nSo definitely useful and I'm seeing value basically using it every day for coding.\nYeah, I've seen a few people then use the AWS to code Whisperer. I did try it for a little while, but I found that the responses are just very slow to arrive, which slows down your development flow.\n\n\nEoin: And that's a real problem, but I'm sure that's going to improve in the future. AWS does tend to release things early on and see how people get on and then improve them over time.\nI've also seen that you've got Copilot X, which I think is the future generation of Copilot that has been previewed.\nI think you can get access to preview. There's a wait list.\nAnd from what I see, that's going to allow you to do a lot of other developer tasks like create PRs and documentation easily and maybe more like ChatGPT, use a conversational interface.\nAnd you can also use it from your terminal. So if you're trying to find out what's the command to search for a certain string and extract the third column and sort them and find unique values, you could do that from the terminal.\nAnother example I've seen is like generate the FFmpeg command to extract the audio from this and convert it to MP3 or something like that.\nSo that all sounds pretty useful. Again, I wonder, will everybody forget how to do these things for real and what does that actually mean?\n\n\nLuciano: I should try to see if it helps me with Git rebases.\n\n\nEoin: Yeah, yeah. What could go wrong?\nSo let's talk about maybe the headline grabber ChatGPT.\nI found it pretty useful, but I've had also very mixed results. What do you think it's good for?\n\n\nLuciano: Yeah, I've been using it quite a bit, especially for this podcast. I find it extremely useful for coming up with good video descriptions or episode descriptions, starting from our notes or our transcripts.\nAlso for generating, for instance, the YouTube tags for every episode, which is a very daunting task.\nSo just trying to think what are 20 different keywords that are somewhat related to what we just mentioned today.\nAnd I think that ChatGPT can very quickly give you suggestions that, of course, you can take or not take, take some of that.\nSo I think it's still important to have some degree of human intervention in these things, but it's just making your life easier and making you more productive when you use these tools in a very targeted way.\n\n\nEoin: I completely agree with using it for summaries and even also for idea generation, and you mentioned Notion. And I think ChatGPT 3 is integrated into Notion now.\nSo if you're a Notion user, you can use ChatGPT just from within your documents. So you can select text, get it to summarize it, get it to rewrite it, get it to change the tone or language of it.\nJust expand it out with adding even more detail. But you can also use it for kind of brainstorming or coming up with ideas. Like one thing I tried was give me a list of ideas for company activities around Dublin.\nAnd it gave me like 10 to 20 very reasonable suggestions, actually. So that stuff is really good for when you're just stuck for inspiration and just need to unblock yourself a little bit.\n\n\nLuciano: Yeah, I have another cool example, which I was actually really impressed with the result. I tried to ask something like, I want to create a course on Node.js Streams.\nWhat are the main topics that I should cover? And it literally gave me 10 different chapters. And the order was making a lot of sense.\nIt was like a progression on the level of complexity of the different topics. And I think it was covering all the basics.\nNow, it's probably not too different from doing a Google search, looking at four or five different pages and then coming up with your own summary of all of that.\nBut I have a feeling that this is just much faster to get a decent result this way.\nOne of the things that ChatGPT is pretty good at doing is if you have text, you want to rewrite it or change the style of that text.\n\n\nEoin: It's pretty good. But I've actually also heard from people working at universities as researchers and writing a lot of academic papers.\nAnd it's quite common that you've got lots of international collaborators and people who are not native English speakers who are burdened with the task of creating formal academic English in papers.\nAnd this is obviously a difficult enough challenge on top of doing the research itself and that universities are actually starting to recommend people to run the text through ChatGPT before they submit it for review,\nbecause it can actually get rid of that cycle of people just commenting on language and small wording issues and just accelerate that whole process.\nSo I think that's quite an interesting benefit as well. And I've also seen it used for just changing the tone, you know, make this less formal, make it more formal.\nI like that a lot. And it's something I'll definitely try to do in the future with my blog posts is just try to improve the quality of English, because everybody has their own style that they're accustomed to, I guess, when they're writing.\nAnd you don't sometimes see that maybe you're just a little bit verbose or you have a tendency to repeat yourself. And even just by asking ChatGPT to make paragraphs more succinct, it might help a lot there.\nYeah, I think there is on one side the risk that is going to make basically creativity fade out a little bit because you're just going to take whatever the tool is giving you and not question if there is an alternative.\n\n\nLuciano: But at the same time, it can be an opportunity for actually exploring different styles of things. Like for instance, I often try to do to take a piece of text that I have already written myself and say, can you try to make this more engaging?\nCan you try to change the style to be, I don't know, more mysterious? And I don't always get good results. But sometimes I get inspiration for changing what I was doing in a way that maybe I was not anticipating before.\nSo I think you, on one side is true that there is a risk of losing creativity. On the other side, I think you can still use this tool to kind of foster more of your own creativity by just getting some ins and then developing from there.\nYeah, that's really interesting because you would worry that it's going to take the personality out of writing or the individualism. But I suppose whether that happens or not depends on what you do with the response you get back.\n\n\nEoin: I think everybody, whether you're a native English speaker or not, or whatever language you're writing in or speaking, everybody's using AI to some degree almost at this point for grammar checking and spelling. Like in Google Docs, it's already built in there.\nOr if you're using Grammarly, it's using AI to suggest grammar. It should level the playing field a little bit for people.\n\n\nLuciano: On that topic, I actually recently discovered a VS Code plugin that allows you to use Grammarly for markdown directly in your VS Code. And I was recently writing an article for my blog.\nAnd basically, this tool works a little bit like ESLint. Basically all the errors, you will see the file on the sidebar becoming red. And then if you go on that file, you see like a list of things to fix.\nSo I immediately saw that all my older blog posts were like all red everywhere. And I did realize that there were a lot of small issues that I wouldn't have done today just because today I have this kind of tools to help me out and make me spot the issues straight away.\nSo that's just another, I guess, confirmation of what you've suggested that, yeah, we have been using these tools for a while now. But if you compare maybe with a few years ago when we were not using these tools,\nthere was a little bit more disadvantage for people that were not English speakers to come up with like decent written content.\n\n\nEoin: Yeah, it's interesting. I mean, when the advent of SMS back in the early 2000s, when people started using short messages, there was this grave concern from a lot of people that it would kill the English language or any other language because of all the abbreviations and acronyms and everything.\nSo maybe this is going to go to the other extreme and kind of set this new standard for grammar which is unrealistic for humans and almost takes kind of idiomatic individual language out of the equation altogether.\nJust another disadvantage. But on the subject of language, we have used, if people have been listening to the podcast for a while, we have used the other open AI model, the whisper model for generating transcripts, and we did a whole episode on this.\nBut if you go to awsbites.com you can also see transcripts for our almost 80 episodes so far all there and I think this has really been useful for us in a number of different ways. What do you think Luciano?\n\n\nLuciano: Yeah, absolutely. I think it's first of all making the content a bit more accessible because if people want to read through rather than listening to it or watching the videos, they can do that now.\nBut also for us, if we were to do that in a more, let's say manual way where we would either do the transcript ourselves or pay somebody else to come up with good transcripts, that would probably been something a little bit prohibited for us either for cost reasons or time reasons.\nSo I think we found a very good trade-off there that is allowing us to make our content more accessible without really investing too much into it.\nSo that's another, I guess, proof that these tools can be beneficial in kind of business-related context or content creation in general.\nI haven't seen this yet, but we both use Canva from time to time and you're a bit of a Canva expert at creating those amazing thumbnails for the podcast. But you mentioned that Canva is now getting a few AI features. What's that like?\n\n\nEoin: I haven't used most of them yet. Only the image generation one which looks a little bit like DALL-E so you can just write a prompt and it will generate an image for you.\n\n\nLuciano: Now the level of quality I've seen so far wasn't really anything amazing. I wouldn't have used those for creating anything professional, I suppose.\nBut who knows, maybe that's going to improve over time. Maybe they will start to use other models and get better results.\nBut I also saw that they recently announced a bunch of additional AI power features more geared towards editing.\nSo if you have, for instance, already a picture that you're working on and there is maybe a part of that picture that you don't like, you can easily highlight a portion of it and maybe say something on the line, can you please replace this part with something else?\nAnd I mean, the video I saw, they were getting amazing results. Now, of course, they probably tried to create a video that was somehow amazing and would push people to use the product.\nSo I still need to use it in real life and see what are the results, but it seems very promising. And I saw that Adobe announced something similar, if not this week, last week.\nSo there is definitely a push even in the space of graphics and art and design to use more and more AI features and AI power tools.\n\n\nEoin: Yeah, I've played around with DALL-E and Stable Diffusion and I've seen great things with Midjourney examples.\nIt seems to be that in the image generation space, there's a lot of different models and a lot of different tools out there.\nThere's also a lot of more controversy so far, I think, around, you know, using copyrighted images to train these models and then who ultimately is responsible for the intellectual property and there's some lawsuits and everything going on.\n\n\nBut there's also pretty interesting use cases. There's a nice mix of both of them. There's a serverless application that David Boyne put together.\nI don't know if you saw that Luciano but it's an interesting one where he can generate bedtime stories for his kids every night using an automated schedule of the serverless application.\nHe uses ChatGPT to generate a story, then DALL-E to generate some images, and he gets like a children's story every day.\nYeah, okay we'll try and throw that link in the show notes for people who want to check it out.\nWhy is this happening now? What's the sudden acceleration? Or is it a sudden acceleration or is it just that the mass consumer market has suddenly awoken to the fact that all of this has been going on?\n\n\nLuciano: Probably the latter. I mean, there is definitely an acceleration in the technology itself.\nProbably this technology now is more, I don't know, fine-tuned, more accessible. I assume also cheaper because if it was possible to make it so accessible, probably now training the models but also running the inference is, I assume, much cheaper than it used to be a few years ago.\nAnd on the other side, definitely there is an education piece of the market that is starting to realize that this tool exists, how people can use them, how can they maybe help people to accomplish specific tasks that would have been much more time consuming before.\nAnd plus all the news that are catching up and promoting in a way or another that these things exist and people can use them. So everything is kind of creating a vicious circle where this technology is kind of promoting itself almost, right?\n\n\nEoin: We've mentioned a few things already about what it's good for in terms of generating readable English, summarizing content, generating documentation. I think we mentioned that in the context of Copilot X, but I think I've also been using a ChatGPT to do that.\nGenerating a readme based on basic information or some code, it's pretty good at starting that off for you. But everything you generate tends to need a bit of scrutiny.\nWhat about generating code then in general? So we mentioned that Copilot is how it's useful, but you also need to check it if it's long enough. What about using ChatGPT to generate code? How have you got on with that?\n\n\nLuciano: I actually tried something recently. I was trying to rewrite something I had in Python in Rust. And just for fun, I basically said to Copilot, to ChatGPT, can you do this for me?\nAnd I was actually impressed of one specific detail that in the process I forgot to copy part of the code. And ChatGPT somehow realized that and basically gave me a placeholder saying, you didn't really give me this particular function, so somewhere you need to write the implementation of this function.\n\n\nSo basically in the place where I was calling the function, there was a comment on the side saying, remember to implement this function or something like that, which I thought was quite impressive. So I'm not too sure how that like these are the cases where it might seem that ChatGPT is actually understanding and is kind of having some kind of logic process there, which we know is not really the case.\n\n\nSo I'm not really sure what was happening there in the model in order to figure out that some piece of information was missing and then generating some kind of note there that was telling me, be aware that you are missing some important information there.\nAnd then in general, the rest of the code was actually quite good for that particular example. But I also had other examples where it was totally hallucinating. For instance, I said, can you convert this React component into Solid.js?\n\n\nAnd because I think Solid.js is a much newer technology, maybe there isn't really a lot of examples out there. It was kind of getting some things right, like the imports and so on. But then like importing the right libraries, but then it was importing functions that didn't exist in Solid.js, but they were existing in React.\nSo it was kind of getting there, but not quite. So it was making a lot of false assumptions there. So at the end of the day, the code that was generated was totally rubbish, even though it looked legit.\nSo that's probably another risk there where you need to be really careful that if you assume the code is always going to be right, be prepared for disappointment or for nasty surprises.\nI think it's good to have a starting point, but always validate it yourself and make sure that everything that is generated makes sense to you. Maybe write some tests or at least test it manually. Don't just trust it blindly.\n\n\nEoin: Yeah, I like those two examples, because the first one you mentioned about it, like recognizing that you'd missed part of the code when you pasted it in.\nThat's the kind of thing that leads people to say, oh, this is getting close to artificial general intelligence. And then when you see the second example, you realize, OK, this is just basically a really advanced search engine.\nThat's what it is. It's a language model. It's not general intelligence. And it's pretty good at spoofing as well. So that's one of the unfortunate drawbacks of GPT is that it doesn't really tell you very well when it doesn't know.\nInstead, it just tries to make something up. Yeah, in many ways, it's like a software developer's when we're at our worst.\n\n\nLuciano: That's true. We should probably use it for coding interviews.\n\n\nEoin: I'm sure it would do pretty well. It's already passed, GPT4 has passed the Google coding interview, I believe.\nYeah, I have used it for content creation. And I think that's a really good one. Like if you want to create a slide deck presentation about something, you could say, give me enough slides for one hour presentation and the topics and the titles and the bullet points.\nYou can even say, suggest me some images, visuals I could use. So it won't generate images for you, but it can describe images maybe that you could put in.\n\n\nWhen you're faced with a blank slide deck and you have to create a deck for a new talk or to explain a concept or whatever, there's a bit of friction and inertia when you're just getting going and creating that initial structure and the format.\nAnd if you have something like this generated to start, then it gets over that. You can always reshape it and customize it and personalize it.\nI do kind of think like you've been in the process of writing a book. So have I in the past and you have that same problem, right?\nIt's very difficult to know how to structure it and what to write. Would you use, if you're writing another book starting today, would you use GPT to help in the process?\nBecause I think I would really find it difficult to avoid using GPT now that I've seen what it can do.\n\n\nLuciano: I think I would use it to some extent, definitely not going to write the entire book just with whatever ChatGPT write me this book.\nI don't think that will give you value. Like you're not going to give the readers lots of value. I think at least at the level of quality we have today, it still needs a lot of checking.\nIt still needs a lot of human input. So I see that more as an assistant where maybe you have written down a lot of notes and you want to just, I don't know, somebody to help you break it down or figure out what are the main chapters.\nWhat is the progression of topics or maybe even just rewrite something that you wrote in a very verbose way into something that is a little bit more digestible.\nSo I think in all these cases you can definitely get value. But again, it's nothing more than an assistant and you still need to put the work in and make sure that everything that is generated makes sense and fits with the rest of the content that you have there.\n\n\nEoin: But it is like an assistant at the same time, right? So it is like, you know, sometimes when you are in the process of writing, you'd love to have somebody there who you could just whenever you wanted to turn to them and say, what do you think of this?\nWhat do you think of this structure with ChatGPT? You kind of got something equating or approximating that, I think.\n\n\nLuciano: Another example is that I recently wrote an article on my blog, which was based off of a presentation that I already had. I had the slides. I was actually writing the slides in markdown. So it was even quite easy to just copy paste everything and say, convert this into an article.\nAnd the result was very mixed. There were lots of parts of the article that I liked and I kind of took and reshaped them a little bit and other parts that I had to rewrite entirely because I didn't like the generated output at all.\n\n\nIt was either too verbose or it was missing out the important bits. So I think overall I saved some time anyway because I wrote that article, which is something you can probably read in half an hour.\nSo it's a relatively long article in probably four hours. And I think in the past it would have taken me, I don't know, a couple of days to do the same thing.\nSo again, there is definitely value, but don't just take the output and use it. I always encourage everyone to try to make sense of the output and decide for yourself what's good, what's not good and what needs to be changed.\nWhen it comes to the quality of stuff, one thing I've seen is that there's going to be, I suppose, an interface between a lot of these tools and non-I services.\n\n\nEoin: So ChatGPT in itself, because it has to make up something and always give you a response, it can hallucinate and give you suggestions that are completely off the wall.\nBut they are also working on these plugin ecosystems so you will be able to integrate into proper sources of information and also follow up on actions.\nSo the ability to book a flight or a restaurant through ChatGPT or do your shopping or perform some computation.\n\n\nAnother one is they're going to integrate with Zapier so you can run Zapier workflows and then you can integrate with thousands of different services.\nSo I think that's actually quite good because it just finally opened, like for a while we had this conversational AI hype bubbling around and we had Amazon Lex and Alexa and all these voice enabled systems, but they're kind of limited in what they can do and they have to have very structured menus and options and they're always a bit hit and miss.\n\n\nBut I think when you combine proper voice recognition with ChatGPT and these other integrations that it will bring all of that to a new level.\nWhen it comes to the code quality, I mean, I've seen ChatGPT do some amazing things.\nSince we're talking about this on an AWS podcast, I did try and see what it would be like to try and build a complete AWS serverless application with ChatGPT.\nAnd it started off pretty well.\nSo I asked it to build a skeleton CRUD API.\nActually, it was a shopping list application, I think.\nAnd it generated serverless.yaml and it generated five Lambda functions for me.\nAnd all of the Lambda functions code was completely perfect, I would say.\nCompletely reasonable.\nI had to make some minor tweaks that it would deploy, but it did deploy and the API worked.\nSo it had deployed a DynamoDB table, Lambda functions, IAM roles, API gateway.\n\n\nLuciano: By the way, did you ask it explicitly to use serverless framework or was?\n\n\nEoin: Yeah, I asked it to use serverless framework.\nSo I asked it to use generator serverless.yaml and the Lambda functions.\nThen I said, okay, well, this is a very, very common serverless 101 example out there.\nSo I'm sure it's got lots of examples of that in its training data.\nI asked it to add custom domains, DNS.\nAnd at that point it went a bit haywire.\nLike the output looked reasonable, but it was like somebody really trying to fake it.\n\n\nYou know, somebody who's just like really trying to convince you that they know what they're talking about, but they don't.\nAnd I could tell by looking at the cloud formation that it wasn't going to work.\nSo then I said, okay, forget about custom domains.\nBut I thought it would be interesting to try something a little bit more, you know, specialist.\nSo I said, okay, can you reduce the code duplication by introducing Middy?\n\n\nAnd first of all, I asked it to do validation as well using Middy.\nSo at first it gave me like a mix of using the Joi library and Middy that didn't really make that much sense.\nMustn't be the canonical way to do it.\nBut then I said, okay, just use a Middy approach rather than mixing in Joi.\nIt started to give me really weird syntax and then started to add in weird serverless framework plugins that I didn't need.\nAnd it had like 12 different Middy middlewares in there for no reason.\nIt was also using like old versions of Middy syntax.\nSo I said, please, could you use Middy 1?\nIt like upgraded the package.json version, but the code was the same.\nSo I thought it was interesting.\nThis was the GPT-3 engine, by the way.\nSo maybe GPT-4 is going to be better at this.\nBut I think it also points to the fact that as you get into more and more specific technology questions where you know the training data volume is going to be lower, it will struggle.\n\n\nLuciano: That's an interesting example.\nBut you also mentioned that Jeremy Daly, I remember we were chatting about this, has written about using AI tools.\nIs it worth summarizing all of that?\n\n\nEoin: Yeah, yeah, I think this is really kind of insightful article.\nIt's actually in the premium version of the Off By None newsletter.\nSo I think he mentions it in the free version.\nBut basically the point that Jeremy was making, like he's been using a fair few AI tools lately and is finding them all useful, as we have been as well.\nBut his approach was to think about university education and what the impact will be on university education, particularly in the US where the college fees and everything is really expensive and really it's a massive investment.\n\n\nAnd yeah, he is suggesting that universities will really need to think about reevaluating the curriculum for all these courses, and as well that we kind of need to reevaluate our learning and focus on areas where human creativity is really indispensable instead of, I suppose, what are now becoming easily automatable tasks.\nAnd yeah, he's basically questioning that maybe that we need to think about the value of a computer science degree as it currently exists, given that AI can do so much with this for us.\nSo I think it was very interesting. You're just saying like everyone, we need to think about adapting how we work to focus on areas less likely to be automated.\nAnd I thought that was quite interesting. I mean, it does bring up the question and maybe we can go into some of the drawbacks of all of these tools but does it, is it really automating away these skills, or is, are we just able to use them, if you've already got the knowledge,\nare they allowing you to accelerate your workflow, does it help you if you are at the start of your learning journey about to embark on a college degree, will it short circuit all of that in some way?\n\n\nLuciano: Honestly, I don't know because I have a feeling that education will have to change at some point just because there are a lot of tasks that now, they don't make too much sense anymore, maybe tasks that are too mnemonic, or that are easy automatable with tools.\nFor instance, I remember that ages ago, people would have their logarithmic table. That's something that's been obsolete for quite a while. Maybe we will see something similar in the education where a lot of things that we used to do become less relevant, so we focus on other areas.\nAnd definitely, it is going to be interesting to see if we are going to be able to focus more on areas that require more kind of deep engineering or creativity, rather than things that just require sheer knowledge and mnemonic skills, which they don't really require AI today.\nI think it's just computing in general, it's, and the web are solving that problem already for a while. So with AI is just a little bit of the next step in trying to automate the research and giving you easier access to the results.\n\n\nEoin: I think there's probably a couple of ways to look at it. I mean, if you think about it, if you were to believe that everybody will eventually move to cloud computing, or most people will, then the number of roles I would imagine for, you know, data center engineers, you would expect will concentrate just on cloud providers.\nSo they may become more specialist niche roles. It might also be the same for software engineering in general, that if these tools allow people to do a lot of what we currently do in a more automated way, then there will still be a need for specialists who are more rare but also understand what this generated code is doing and where it can go wrong and troubleshoot it and understanding the underpinnings.\n\n\nI mean, this is probably always the case as new generations of technology emerge. If you were learning software development in the 60s, you would have to learn a low level programming language, and you would need to really understand about how CPU and memory works.\nThat hasn't been the case really for a while and CPU and memory are so cheap, and also have become more complicated that it is, it's something people don't really think about that much anymore. Not in general, I would say.\n\n\nBut at the same time, people who do know those things and retain that knowledge and do dive deep, become increasingly valuable then, as it becomes like a rarer skill.\nSo that's one way to look at it. But, yeah, I also think that in general the education system does need to adapt quickly to this sort of thing and kind of introduce.\nAnother thing you could say is that understanding how AI works and AI models work and how to interact with them and really getting into that area as a specialty is also somewhere, an area where you could take advantage today because if this is going to change everything into the future,\nthen it will need a lot more expertise and people who can maintain those systems and work on them. So, every challenge becomes new opportunities as well.\n\n\nLuciano: Definitely a growing market so there will be opportunities there.\n\n\nEoin: Another area I'm kind of interested in is, like, what we do quite a lot is in understanding existing legacy systems and this is a skill that's actually quite difficult to get and to find in the market, you know, being a people who are willing to go into\na legacy system and not just look at it and go oh my god this is awful let's just rewrite it from scratch. But the ability to understand what's happening in it, capture the value in it, capture the history and all the tasks that knowledge has been built up and retain\nthe arts, as you migrate to whatever the target is, be it a serverless architecture or microservices architecture in the cloud.\nThis is a challenging thing to do. And a couple of our colleagues at fourTheorem have been working on this Fission project in collaboration with DCU University here, and have had really good results in like using AI and other techniques to actually\nanalyze code bases and do refactoring on them. All of these new tools introduce that capability or the possibility at least that you could use language models, point them at an existing code base that humans really don't want to look at.\nAnd it can tell you okay well this is what it's doing, these are the domains, these are the bounded contexts and the entities in this application. And here's what a microservices architecture for the system might look like, and even generate code templates or part of the code for you.\n\n\nLuciano: Yeah, and I think we've been using this and some case studies with some good degree of success, so it's definitely not rocket science, it is feasible today to do this kind of things using AI.\n\n\nEoin: And I suppose the interesting thing about that is that what we've learned from our experience using these tools is that it really helps you to accelerate a lot of that forensic analysis of legacy code bases, but you still have to use your\nlogic and reasoning to figure out what the target architecture would look like, and then how you get there.\nAnd figure out how you do it in a way that doesn't disrupt the business and is incremental and works and doesn't fall over the first time you run it.\n\n\nLuciano: Yeah, I suppose even just deciding which service you should create first is not trivial at all, like you need to have so much business context that probably still going to be a human decision for a while to decide which one is that.\nWould you trust ChatGPT to generate code for you in an area where you don't have so much expertise? So if you were asked to, I don't know, I'm going to pick a language that I haven't used very much at all, which is Golang.\n\n\nEoin: So if somebody asked you in the morning to write a CRUD application in Golang, would you use ChatGPT for it? I'm pretty entrusted to generate good output if you didn't have the knowledge to scrutinize it like you do with the many languages you do understand.\n\n\nLuciano: That's a good question. I'm always a little bit skeptical of using these tools in general, not just for code, in areas where I don't feel I have enough expertise, just because I am afraid that eventually I am responsible for that content.\nAnd if I don't know what I'm doing, like AI is not necessarily going to give me something that I can rely on blindly. I guess I always need to double check.\nI find it just easier to use these tools to make my life easier when I know something and I just want to speed up the process.\n\n\nAnd I had so many instances of the generated code being far off from reality that I learned not to trust it and not to use it in cases where I wouldn't feel confident myself doing all the work.\nBut maybe that's just me. Maybe that's something that is going to get better over time, like the quality of the generated output, and then it could make sense to use it even in areas where you don't have knowledge and then you can use the generated output as an excuse to learn something.\nAnd actually trying to validate the generated output and see, okay, it generated this, why? Let me go and figure out what this function means and why this function actually does what I'm trying to do.\nYeah, it might change the perception. And I think a good question is, if I was a junior engineer, would they use it? Would they have a different perspective?\nAnd I generally don't know the answers. I don't know if you have a special point of view there, but it's an interesting question.\n\n\nEoin: If I was at the start of my learning process and career, I think it would be very appealing because there's so many frustrating times when you're trying to figure things out and you are lacking knowledge and skills in so many different areas and experiences.\nThat it's a real shortcut. So that becomes what you mentioned just there is actually quite an optimistic point is that you can use these tools to get started quickly, but also find out what you need to start being curious about in a way.\nAnd it can give you a template that can help you get stuff done. But if you have the right mindset, you can take what it gives you and try to understand it yourself.\nAnd it just possibly reduces the amount of endless trolling the web and getting distracted and finding articles that are not really written in a way that suits what you're trying to look for.\nMaybe they can give it, condense that information for you and help you to absorb it better.\nThere's pros and cons to how it applies to junior developers, but I think what you mentioned there gives me a little bit more optimism than I had before.\n\n\nLuciano: Yeah, the other point of view is, is this better than just copy pasting from, I don't know, Stack Overflow or articles that you find by searching on Google?\nAnd I kind of have mixed feelings there as well, because from just a perspective of getting stuff done quickly, of course, this is more appealing because it does most of the work for you.\nBut on the other side, I think when you go off and do your own research and you try to read different examples and make sense of them, and then copy paste different parts from different places and glue everything together in some way that should make sense.\nI think all of that is a learning journey. And you have many opportunities to learn related things that now, I don't know if that just the amount of opportunities for learning are decreasing just because these AIs are doing so much work for you.\nAnd kind of pros and cons, of course, the more manual approach is lower, but I think you have more opportunities to learn, while the automated approach is much faster, but you might just give everything for granted and move on and lose a lot of important details there.\nYeah, it throws up so many questions and kind of potential drawbacks and open unanswered questions that might give you a little bit uncertainty about where the future is going.\n\n\nEoin: We mentioned that it's useful for content generation, but if that becomes the norm, then where does the content come from that trains these models in the first place?\nIs this stuff going to drown out authentic original content generated by humans with creativity and real intelligence?\nIn the code context, even if an AI is developing an application, who runs it? Who maintains it? Who's operating the system?\nDoes it make it easy for fake content to just proliferate the internet?\n\n\nLuciano: That's definitely the risk. And I think you're certainly right, when that happens, if it happens, like what content means anymore, right?\nWe're not going to be able to say, I don't know, where is the value of content anymore? If everything is auto-generated, you can have endless content, but it's going to be very hard to figure out what's valuable and what's not valuable.\nSo maybe there is an opportunity for people to try to be even more creative, just to try to beat this huge flood of generated content.\nProbably there is an opportunity there to stand out even more if you can put the effort to come up with more original content, maybe to have ways of doing things that, of course, is going to be a lot more personal, something that is going to be very hard to be automatically generated by an AI.\n\n\nEoin: Plagiarism is already a concern for a lot of content creators on the internet, but then if your content gets picked up for training data and gets repurposed in answers or generated content, how do you protect that if you want to protect it?\nIt's okay, I think, in general for you and I, we want our content to be shared widely and consumed as widely as possible, but for some people, maybe they're relying on the revenue from the content itself directly, and that's a little bit more of a concern.\n\n\nIt's a worry. And also for companies, companies will probably know that lots of people are using this today, maybe they're not being totally transparent about it, and I think companies need to figure out what is their policy for these tools really, really quickly.\nAnd there's, again, strengths, weaknesses, opportunities and threats in all of this, right? So that analysis is definitely worth doing.\n\n\nWhat happens when people are pasting sensitive information into the ChatGPT? If they're pasting in a sensitive document in order to get a summary of it, that can help to communicate a message to the company at larger, which is a great benefit, but potentially you're leaking sensitive company information at the same time into the public domain.\nThere's a whole question about AI as a game changer, right? So many people are predicting that this is a game changer and will change the way work is done irreversibly.\n\n\nWe first of all have a challenge for us as individuals, which is maybe a bit of an existential threat, or maybe like what opportunities, how do I change how I work?\nBut also then for our companies, because if you're in a company with a large team of people, it's okay to say, well, how does this change me as an individual? But more importantly, how does it change your company and how you all work together?\nBecause it's probably much better that you're tackling this as a group rather than individually thinking, how do I get an edge here?\nIf one person is secretly using ChatGPT to look more productive than their colleagues, I don't really like the sound of that very much, but if you can look at it as a company and say, okay, let's really assess what we do every day, where we spend our time or where we can be more productive.\nThat's probably where you can really get an edge because the whole organization can potentially benefit.\n\n\nLuciano: Yeah, and I think that relates to something we mentioned before that in some cases, people are afraid to ask questions to colleagues and therefore it's easier to just ask AI.\nSo probably I would like to take the opportunity to invite people to refrain from doing that as much as possible because I think the opportunities you get by asking a colleague are still much higher than what you can get from AI.\n\n\nNot just because you can build a relationship, you can get chances to work with other people, but I think there is a much more in-depth exchange when you try to communicate with somebody else, especially if they are people with either they work in a different area of the business, so you can have an opportunity to learn more from their experience and what they do in the company.\nOr maybe somebody that has just more seniority and then you can use that excuse to learn other things that you didn't anticipate, or even if it's somebody with less seniority, then you can still use that opportunity to learn what kind of challenges are they facing, if you can help them in any way, or if there's something in the system that should be improved.\nSo yeah, I think if we end up letting AI do everything, we will miss out so many opportunities and everything will become kind of flat and standard, and instead we should be looking for opportunities not to do that as much as possible.\nI find it interesting when people say, oh, like this is already making me two times more productive, and when you dig into it a little bit more, it's because they have a sounding board and they have something to ask questions of.\n\n\nEoin: But this benefit, this 2x or 10x improvement in productivity, I think you can also achieve it pretty well by having a more collaborative spirit with colleagues and friends.\nThe barrier there is just ultimately ego and pride, and I recognize it in myself and I see it in myself and try to stamp it out wherever possible.\nBut when you see yourself thinking, I'm just going to keep working on this and figure it out myself because then I have, as an individual, achieved something, but you could also equally just ask a colleague, admit that you don't know, and learn from them.\nThe cost is just a little bit of humility, but the benefits, I think, when you end up achieving something together as a pair then or as a team, and I found when that works well in organizations and in teams I work with,\nthe benefit in terms of work satisfaction, ethos and mood in the team, and then the knock-on effects on productivity are far greater than some of the benefits that people report from just getting from interacting with ChatGPT.\nSo I think there's a lot to learn from it, and I think it's an interesting parallel.\n\n\nLuciano: Yeah, I agree 100%.\n\n\nEoin: Will AI take the fun out of software development?\n\n\nLuciano: That's a good question. I don't think that today there is a risk of that, I mean, in the short term, just because we have seen with all the many examples we mentioned today and that we have seen online that it doesn't really understand what software development is about.\nIt's just taking data from the web and reshaping it in a way that sometimes is correct, some other times is close to correct, other times is totally hallucinating and doesn't make any sense.\nSo I think until there is more of a general intelligence that can make logic reasoning and connect different thoughts and understand the problem and try to come up with solutions for these problems, I think until then we still will have a lot of fun trying to do the work ourselves and building solutions ourselves.\nSo again, it's just going to be an enabler, something that can make our work a little bit faster, but I don't think it's going to take all of that fun away of building things, thinking about problems, solutions, architectures, trade-offs and so on.\n\n\nEoin: I guess the part where I think potentially you might lose out on the fun is when you really have to figure something out yourself and you don't have almost like the crutch of your language model to lean on to help you.\nAnd you have to go deep and explore it and it's like a little bit of exploring uncharted territory and you emerge with an answer at the end of it.\nWhether you do that as an individual or in a group, I worry that some of the challenge will be gone from it and while that has obvious productivity gains, then it might also hinder enjoyment as well.\n\n\nIf you've seen an open letter published asking these large model developers to halt development on the more advanced versions until we kind of figure out what they can do because of the risk, the potential existential threat, the risk to jobs, all of that.\nI also think there's a lot to that and I've seen a lot of analysis on that saying there's other more subtle threats out there like tools like this can potentially expand the rich-poor divide by just giving the privilege more access to more tools that allows them to be more productive and earn more money and be more profitable.\nAnd it just makes existing societal issues worse in that sense.\nOn the other hand, I'm wondering can the fact that AI allows people to come up to the level of people who either have the privilege of being native English speakers in the software development world or having more experience or more access to education, can these tools be a leveler for people?\nCan it be a democratizing effect?\n\n\nLuciano: Yeah, that's a good question that I don't know if I have the answer, but in general I've seen that technology has been helping to get to a more democratized society.\nLike just if you look at the web today, you can have so much information that I think 20, 30 years ago maybe would have been accessible to only very few people in very specific roles.\nAnd for them it would be of course even much slower in order to extrapolate the information.\n\n\nToday it's not that anyone in their pockets they have a device that can basically give answers to most of the human questions that you can ask today, right?\nSo I think in general technology can have that kind of positive impact, but of course there are always lots of negatives and they can be very hard to predict.\nSo I think we always need to be vigilant there and make sure that we always try to use technology in the right way.\nProbably one of the key things is if these kind of technologies are pretty much accessible to everyone, maybe they can become a leveler.\nIf they are kind of gate kept, maybe they are very expensive to use, very complicated to run, and only few people or organizations can use them, then there is a risk.\nThat is that there is going to be more divide in society making the richer even richer and the poorer even poorer.\n\n\nEoin: And I guess there are so many potential knock on effects as well. I mean I've been even trying to find out from the point of view of power consumption in training these massive models and running inference on these massive models at scale.\nIf they are to become accessible to everybody, what is then the sustainability, environmental impact of these models?\nAnd it's quite difficult to get access to data making this very clear, but it's all very well to be increasing productivity.\n\n\nEven with a democratizing leveling effect for everybody, but on the other hand it's just kind of another mindless growth advancement in technology that really hurts the environment that we need to sustain ourselves, then it might be also a step towards shooting ourselves in the foot.\nMaybe we've come to a natural end at this point. I think we've probably agreed that this is a game changer and has already been changed things irreversibly to some degree.\nThere's so many open questions here that I think we need everybody out there watching and listening to contribute your ideas and let us know what do you think about the future of AI?\nWhat tools have you been using? What has really blown your mind? And also perhaps even more interesting, have you had any AI generated disasters?\nPlease let us know and we really appreciate you listening to this special episode on AI tools in the software industry. Until next time, we'll see you.\n"
    },
    {
      "title": "76. Unboxing AWS Copilot!",
      "url": "https://awsbites.com/76-unboxing-aws-copilot/",
      "publish_date": "2023-04-14T00:00:00.000Z",
      "abstract": "In this episode, we're doing something different! Join us for a special screen-sharing edition of our podcast series, as we take a deep dive into AWS Copilot, a service designed to simplify container application deployment on AWS.\nDuring this video, we'll be sharing our screens as we walk through the AWS Copilot landing page and documentation, and demonstrate how to use the service to deploy a container application. We highly recommend watching the video version of this episode, as we'll be providing a lot of visual guidance and examples.\nStarting with the basics, we'll learn about the differences between copilot init and copilot app init, and how to prepare our environment using a custom domain. We'll then walk through the deployment process step-by-step, examining the generated configuration file, manifest.yml, and testing our deployed application.\nNext, we'll explore the networking resources created by AWS Copilot, including a VPC, subnets, and a load balancer, and review the automation capabilities of CodePipeline. We'll also discuss the options available for rolling out new changes, and demonstrate how to make changes and re-deploy through the pipeline.\nThroughout the video, we will share their thoughts and opinions on AWS Copilot, including a failed attempt with AppRunner and a review of the pipeline execution and timing.\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nAWS Copilot landing page\nAWS Copilot documentation\nAWS App2Container tool\nAWS AppRunner\nOur previous episode &quot;Do you use CodePipeline or GitHub Actions?&quot;\nGurarpit Singh's blog post &quot;Blue/Green Deployments with AWS CodeDeploy and Terraform&quot;\nAdditional guides and resources on AWS Copilot\n\n",
      "transcript": "Luciano: Hello everyone and welcome back to another episode of AWS Bites.\nToday we have an incredibly exciting episode in store for you.\nWe are here to do something a little bit different than usual.\nWe want to unbox an AWS product and specifically we want to unbox AWS Copilot.\nThis is going to be a very visual episode and we will be screen sharing.\nSo if you are listening to the audio only version,\nwe will be doing our best to describe what's happening\nbut you might be better off watching the video version on YouTube or Spotify.\n\n\nNow I mentioned that in this episode we will be exploring Copilot.\nCopilot is not to be confused with GitHub Copilot.\nWe are talking about AWS Copilot which is a CLI helper\nthat helps you to create container-based applications and deploy them on AWS.\nWe will work you through the entire process.\nWe will show you how to install it, how to configure it\nand we will create a container application and deploy it on AWS.\n\n\nSo I hope you are as excited as we are to explore AWS Copilot.\nHopefully we will make the best of it.\nSo sit back, enjoy and relax and let's have fun together.\nAWS Bites is sponsored by fourTheorem.\nfourTheorem is a cloud consulting firm that helps businesses to migrate to AWS\nand optimize their cloud infrastructure.\nWith a theme of experience cloud architects and engineers,\nfourTheorem provides end-to-end solutions for cloud migrations,\napplication development and infrastructure management.\nIf you're interested in finding out more, check out fourtheorem.com.\nThe link is in the show notes.\nSo today we are here to explore AWS Copilot.\nEoin, what do you have in store for us?\n\n\nEoin: Well, when you take it out of the box, the first place you might look\nis the Copilot landing page on the AWS site.\nSo there's a quick overview here.\nYou might be wondering, okay, where does this fit into\nthe 17 different ways to run containers on AWS?\nSo I suppose it's not really a separate way to run containers.\nIt's just like a tool that you can use to run them on the existing services.\nSo we've already talked on the podcast about ECS and we've talked about Fargate.\n\n\nWe haven't talked a lot about AppRunner yet, but AppRunner is a relatively new way\nto get up and running with running containers on AWS for fairly simple applications.\nNow those could be like background processing or APIs or front-end applications,\nbut it hides a lot of the stuff you normally have to deal with,\nlike task definitions and services and load balancers.\nAppRunner makes that a lot simpler, but it also is a bit limited.\n\n\nYou can only run so many parallel containers of certain sizes,\nbut for a lot of use cases, it would be perfectly okay.\nSo what Copilot is doing is it's allowing you to run stuff on ECS and Fargate,\nbut you avoid having to write loads of cloud information yourself\nor Terraform or CDK code.\nYou don't have to worry about all of the well-architected pieces\nbecause Copilot, I think, is trying to sort that for you,\ntry to make it observable, make it easy to set up deployment pipelines, safe deployments.\nIt's a command line interface only, so there's no AWS service here or anything like that.\nSo we're going to give it a try.\n\n\nLuciano: So is it fair to say that Copilot is basically a guided experience into trying to deploy containers in AWS the right way, hopefully?\n\n\nEoin: Yeah, I think that's a good way to put it.\nIf you go to the Copilot CLI documentation, you've got this GitHub page.\nSo it's hosted on GitHub pages, and it's got a good getting started guide,\nwhich we're going to have a look at to take you through.\nIf we go to getting started for the first time, you install the Copilot CLI.\nI'm on Mac OS here, so I've already brew installed it,\nso I've already followed this part.\n\n\nAnd then you can see the commands you get.\nThere's a couple of concepts here in Copilot, which we might cover quickly.\nWhen you start initially, you do this `copilot init`.\nIf I go `copilot init`, it's going to ask me if I want to create an application,\nand then it's going to ask me for my Dockerfile,\nand then it's going to go ahead and deploy everything.\nAnd it's going to ask me, give me a few options here.\n\n\nSo let me just give an example here.\nSo I'm just going to say test, application name test.\nIt asks me what kind of workload represents my architecture.\nSo it's showing me here that I can have like a web service backed by AppRunner\nor a web service backed by ECS on Fargate with a load balancer in front of it.\nOr I can have a back end service, which is ECS on Fargate.\nSo it doesn't have an internet facing connectivity.\n\n\nAnd then it can have like background services,\nlike something pulling off a queue or a scheduled job.\nSo if I run one of these things, it'll ask me for a name.\nBy the way, this wizard is also, it has this question mark for help, which is pretty good.\nAt every step, it'll give you more details about all these configuration options and what they do.\nLet's say I call this service one.\nAnd then it asks me which Dockerfile I want.\n\n\nSo there's a couple of concepts here.\nIt's got application.\nWithin the application, there's a service and you can have multiple services within your application.\nAnd those could be microservices.\nSo it could be a front end and then you might have an API gateway.\nAnd then you might have other services that talk to each other and it will set up all that for us.\nSo an application is almost like a workspace, right?\n\n\nYeah, exactly.\nThat's a good way to look at it.\nCancel out of that.\nBecause we did a quick dry run for this earlier on, actually, it was more than a dry run.\nIt was a bit of a failed attempt where we explored it, a few different AppRunner concepts.\nAnd one of the things we realized is that if you want to use custom domains for an internet facing\napplication, the default setup doesn't work out of the box.\n\n\nSo if you just do copilot in it, it'll create that application for you, but it doesn't give you the\noption of using a custom domain and it's difficult to apply it after the effect.\nSo there's this, the better way to set up a copilot application for the first time\nseems to be to use `copilot app init` first.\nAnd once you do that, it will allow you to specify your domain.\nWhen you deploy an application to it, you can use your custom domain.\n\n\nSo it's more of a step by step because when you do `copilot app init`, it doesn't create any\nservices, it just creates the workspace as you call it.\nAnd then you later have to go and add services in it.\nSo you can add services into that workspace.\nSo it takes a little bit of time to get those concepts in your head, but you've got your\napplication, your services in your application.\nAnd then the third thing is you've got environments.\n\n\nSo you have your test environment, your QA environment, production environment.\nWe can start with our `copilot app init` and let's put a domain in.\nWe're using an AWS account that has a domain registered with route 53 and the hosted zone\nfor that is in the same account.\nSo that makes it easier when it comes to creating certificates and we'll see how that works.\nSo let's get cracking then.\nSo we've got an awsbites.click domain, which we can use.\nSo let's give that a go.\nIt's going to check that we own it first.\nLet's just call our application copilot app.\nAnd now it says it's proposing infrastructure changes, which is not a term that you would\nunderstand by just from normal AWS usage.\nWhat it's basically doing is creating CloudFormation templates.\nIt says it's creating an admin role for CloudFormation, which is good.\nAnd then it's adding name server records for our domain.\n\n\nLuciano: So basically it requires us to have an hosted zone on route 53 with that particular domain, right?\n\n\nEoin: If we go into AWS, we can actually look into CloudFormation and see what is happening under the hood.\nSo here's one stack being created by copilot app.\nAnd in here, we've got the admin role it mentioned.\nIt's creating a hosted zone and it's creating an execution role.\nSo we can look, click into that hosted zone even.\nOkay.\nThis hosted zone is basically a sub domain over our main domain.\nYeah.\nSo we've also got our main domains hosted zone in here in awsbites.click.\n\n\nIt hasn't created a delegate NS record in here yet, but I expect that it will.\nThese name servers are publicly available and anyone who's doing DNS queries against\nthis domain, the records will come from these name servers.\nBut this new one, our copilot-app.awsbites.click, that's in a different hosted\nzone.\nSo that uses a different set of name servers.\nSo we need to delegate from this one to the subdomains name servers.\n\n\nOkay.\nSo this create is still in progress.\nIt's weird the way it says stack set.\nI saw that it's creating stack sets and I'm wondering why does it need stack sets because\nstack sets are normally when you need to deploy a stack across multiple regions in AWS.\nAnd it seems like it has support for creating some resources across region, like your ECR\nrepository, where you're going to put container images.\nBut I couldn't find anywhere in the documentation that explains why it was doing multi-region\nconfiguration by default or where the set of supported regions would be defined or how\nyou would actually proceed and do a multi-region deployment with copilot.\nSo it's a little bit strange.\nI wonder if it's just like future proofing in some way so that eventually they can support\nmulti-region disaster recovery type deployments.\nYeah, that's an interesting detail.\n\n\nLuciano: Okay.\n\n\nEoin: It says it's complete and it also says it has added the NS records to delegate responsibility.\nWhat I like about this is that it gives you much more human readable output than just\ngiving you the CloudFormation events raw.\nAnd I also like that it told you what to do next.\n\n\nLuciano: Yeah, I think it's the usability and the developer experience.\n\n\nEoin: Somebody has put a lot of thought in here.\nSo well done and thank you.\nSo this is done.\nLet's have a quick look in our hosted zone.\nSo the hosted zone, which was there before, it has been updated to add the delegate NS\nrecords.\nAnd that's it.\nSo we don't have any application yet.\nAll we've got is some foundational stuff set up.\nBut now, as you said, Luciano, it's told us that the next step is to add the host\nand the next step is to run copilot init.\nSo let's try that.\nOh, by the way, this application we have, I just based it on the one that comes with\nthe getting started documentation.\nThat's what it looks like.\nOkay.\n\n\nLuciano: Should we have a look at the Dockerfile as well very quickly?\nYeah, let's do that.\n\n\nEoin: Yeah.\nSo this is the Dockerfile.\nSo it's almost as simple as you can get.\nIt's using an Nginx web server.\nIt's exposing port 80 and it's using all the default config in Nginx.\nAnd it's just overriding the index.html and adding an image.\nSo it's literally just a static website backed by Nginx.\nYep.\nSo I think we can try and init.\nOkay.\nSo we're going to go for a load balanced web service.\nSo this is one that will use a load balancer rather than using AppRunner.\nI think this is a kind of a more interesting example and probably more common given that\nthis is our static application.\nLet's call this front end.\nAnd it's asking us whether we want to use a Dockerfile or an existing container image.\nWe want to use our Dockerfile because we haven't built the image yet.\nSo let's try that.\nAha.\nIt's interesting that it says that it's detected that I am on ARM 64, but it's going to set\nthe platform to Linux x86 64 instead.\nYeah, that's a very common problem.\n\n\nLuciano: Yeah, that's a very common mistake that it's always very hard to troubleshoot because of course you need to build the container for the target architecture, but by default it's\ngoing to be the container for your system architecture, the one you are working on.\nSo if there is a mismatch, you might end up with a container that is going to fail in\nvery weird ways in production.\n\n\nEoin: We're going to do x86, but it says here that we can update a manifest if we want to change that.\nOkay.\nPlease change the platform field in your workload manifest.\nSo maybe we can have a look at that while it's doing the next step because it says,\nwould you like to deploy a test environment?\nAnd I would say yes.\n\n\nLuciano: By the way, that's another interesting concept that it supports this idea of environments out of the box.\nBy environment, we mean a development environment, QA staging, whatever you want to call it,\nand production or more custom environments if you want to.\nYeah.\nAnd my understanding is that it will give you a set of best practices there as well,\nlike isolate them in different domains.\nYeah.\nI think it creates maybe different VPCs as well.\nYeah, that's a good point actually.\n\n\nEoin: We can switch back to the docs actually, because we didn't really focus enough on what's\nin the docs.\nSo the docs will show you what the different concepts are, the things we talked about,\napplications, environments, services.\nIt mentions that it has application-wide resources that are not specific to environments, like\nyour ECR repositories, buckets.\nAnd by the way, you also have these handy utilities like Copilot App Show or Copilot\nService Show, which would just give you a text summary of an application or an environment,\nI think, or a service.\nSo if we look at developing, we've got the ability to add additional resources to the\nadditional resources.\nSo I was kind of surprised to see this, but you can add a bucket or a DBDB table or an\nAurora serverless cluster using the copilot storage init command.\nI haven't tried this.\nBut you could also add custom resources, right?\n\n\nLuciano: If you want to do your own CloudFormation, row, just add whatever you need, right?\n\n\nEoin: Yeah.\nSo we've got here the concept of add-on templates, which allow you to put in essentially raw\nCloudFormation.\nSo it supports then properties, resources, outputs, parameters.\nInterestingly, there is another section in here on overrides.\nIt talks here about CloudFormation overrides.\nThere's also the concept of CDK overrides.\nSo CDK overrides actually allow you to put CDK components into your application as well.\n\n\nAnd then there's task definition overrides, which I guess allow you, if you discover that\nthe generated ECS task definition isn't exactly what you want, you can go in and customize\nit at a more granular level.\nSo this manifest was mentioned, so let's have a quick look at it.\nSo the manifest, it looks like this is just the, this is the declaration for our whole\napplication.\nAnd if you were doing ECS from scratch or Fargate from scratch, you'd have to do a lot\nof CloudFormation or Terraform.\n\n\nBut this is like a very concise, higher level declaration of an application that avoids\nyou having to do that.\nSo the pieces in here are the HTTP setup.\nSo you could set up the path health checks.\nYou specify your image config and then the container config.\nSo this one is using a quarter of a CPU, half a gig of RAM, it's x86, and it's going to\nrun one task by default, it looks like.\nIt's going to support ECS exec, which is nice because then we can shell into containers\nif we need to.\n\n\nOh yeah, and for microservice fans, the way it supports communication between microservices\nis using ECS service connect.\nSo you turn that on, it allows you to address other microservices and it will use ECS service\nconnect, which uses Cloud Map, which is like internal DNS and a load balancer.\nThe one also supports HTTPS, right?\nAcross server.\nThis is with self-signed certificates or something like that?\n\n\nI don't think that's provided by service connect out of the box, but yeah, there is something in here.\nYeah, it may be under the load balance web service.\nHere we go.\nYeah, if you look under load balance web service, there's some things that are quite nice about\nthe documentation.\nSo when you go in here, you can see sample configurations for lots of different workload\ntypes.\nSo you've got basic configuration and then a manifest with a domain and then examples\nof configuring larger containers, auto scaling.\nAnd if we go further to the right, we've got end to end encryption.\nSo I think this is what you were getting at.\nRight.\nOkay.\nSo you still need to provide a little bit more configuration.\n\n\nLuciano: It seems to be using Envoy as a sidecar.\nOkay.\nInteresting.\n\n\nEoin: Our deployment, meanwhile, is still chugging away, but we have this very nice summary of what it's doing.\n\n\nLuciano: Yeah, I don't know how I feel about this YAML because on one side is another type of, I don't know, infrastructure as code.\nLet's call it this way, another specification that you will need to learn and get familiar\nwith, but at the same time, it's much more friendly than it would be if you just go with\nCloudFormation, especially if it's the first time that you're trying to do something like\nthis.\nI wonder where this applies.\n\n\nEoin: Yeah.\nIs it for people who just don't want to get bogged down in CloudFormation all the time?\nI can certainly understand why people wouldn't want to, but there's also a lot of options\nfor containers.\nLike I've deployed lots of different ECS clusters and services, and it's still not trivial to\nconfigure and deploy.\nThere's always something that's subtly different between setups.\nSo I can understand the appeal of making that a bit simpler.\n\n\nAlso, I can imagine if you were migrating a load of on-premises workloads to the cloud\nwith AWS, you don't want to have to configure all that syntax yourself.\nIf you could use something like this, it could make the job a little bit easier.\nAnd maybe a good question, maybe somebody out there knows, but there is another tool\nthat AWS has called App2Container.\nI don't know if you've seen that one.\nI haven't.\nBut it's partly designed for if you've got on-premises workloads like Java applications\nor.NET applications, it allows you to containerize those in order to move them to AWS.\nOkay.\nSo I think it's doing it a little bit like Copilot, but it's more aimed at detecting\nyour Spring Boot application configuration, packaging it, and launching it into ECS.\n\n\nLuciano: Okay, interesting.\nSo it's probably more of a migration tool than a more general part was stored.\n\n\nEoin: What has it actually done?\nIt talks about Fargate.\nIt's creating load balancer resources.\nIt's created HTTP and HTTPS listener rules, CloudWatch logs, and it's created the ECS\nservice.\nSo the ECS service, it's in the middle of creating that and waiting for the desired\ntask to be running.\nSo it has a target group.\nYeah, so it's almost there, I would say.\nWhat about...\nDNS?\nDidn't see anything relating to DNS.\nI'm curious to look in the console and see what it looks like from the raw CloudFormation\nview.\nIf I understand correctly, what we expect to have at the end is something like test.copilot.\n\n\nLuciano: awsbites, what did we call it?\n.clicks?\n.click, yeah.\nRight, so it needs to create that domain as well and map it to the AWS.\nIt needs to create that domain as well and map it to this particular environment, right?\n\n\nEoin: Yeah, I think so.\nYeah, well, I would expect an alias pointing to that load balancer.\nSo this is our stack.\nMaybe we can have a quick look at the template.\nSo does it have Route 53 resources?\nYes, it has a record set.\nAnd does it have a certificate?\nNo, it doesn't.\n\n\nLuciano: Okay, this is going to be interesting then to see what we get in the end.\nMaybe because it did create a star certificate.\n\n\nEoin: Did it create a star certificate?\nYes.\nWell, no, it created test.\nIt has one for the test environment created.\nOkay, but it's not part of that stack that we are developing.\n\n\nLuciano: Must be.\n\n\nEoin: Let's see what the CLI says.\nOkay, it says it's done.\nOh yeah, and it's giving us the address.\nSo our address is pretty much what you said.\nIt's a bit of a mouthful, but front end.\nSo that's the service name.\n.test, which is the environment name.\n.copilot-app, which is the application name.\nAnd then the domain, awsbites.click.\nLet's click on it.\n\n\nLuciano: It's working.\nNice.\n\n\nEoin: Why are we surprised?\n\n\nLuciano: I am surprised that it is probably creating some kind of star certificate because this\nis called frontend.test, right?\nSo it didn't create a TLS certificate for frontend.test, but just test.\nSo I wonder if this certificate also contains asterisk.test.\nYou're right.\nIt does.\nOkay, perfect.\nYeah.\nI guess that's, I'm sure that certificate must have been in this template.\n\n\nEoin: If I look in resources and search for cert.\nSo was it in the previous stack?\n\n\nLuciano: So probably there is a stack for the environment and then a stack for the specific.\nYeah, you're completely correct.\n\n\nEoin: There's a stack for the environment and a stack for the service.\nAnd it was created in the previous stack five minutes ago.\nWe didn't even notice.\nOkay, good.\nI'm curious to have a quick look at the load balancer just to see what it looks like.\n\n\nLuciano: So it did create a significant number of resources for us, which is probably worth highlighting, right?\nThat we got a VPC fully created from scratch with all the subnets and everything else.\nAnd maybe there are questionees, we might check in a second.\nIf it did create a NAT gateway or not.\nLet's have a look at that.\nThen it did create a load balancer.\nIt did create DNS records, certificates, Fargate cluster, Fargate tasks, built our container,\ndeployed it into a registry and kind of connected all the dots, even supporting a multi-environment\nsetup.\nRight now we just deployed a test, we call it, but we could deploy QA in production,\nright?\nYeah, I guess so.\n\n\nEoin: And then we could change the number of containers in production so that it's more than just\none runs across multiple availability zones, handles.\nWe've even put in auto scaling if we want to.\nWhat I'm slightly confused by here is that it adds another listener.\nSo it has a HTTP port 80 listener as well, which also seems to forward to the same target\ngroup as the HTTPS listener.\nAll right.\nIt doesn't do a redirect.\nIt doesn't do a redirect, but it gave me a 503.\nOur other one seems still to be working.\n\n\nLuciano: And if you use this one, but on HTTP, what happens if you use that domain?\nBut HTTP.\n\n\nEoin: Well, good question.\nYep.\nIs it using like host header?\nOkay.\n\n\nLuciano: Does it redirect it somehow?\n\n\nEoin: Is NGINX doing the redirect in that case?\n\n\nLuciano: I don't think so because I think NGINX is just on port 80 for what I could tell from that Dockerfile.\nYes, it is.\n\n\nEoin: But I'm just wondering if it has, if it can check the X headers to see where it came from and redirect like the forwarded by headers.\nI don't know.\nI suppose one thing I'd like to do is turn off port 80 on the load balancer or on the\nport 80 on the load balancer or else ensure that it's just a redirect.\nShould we try creating a pipeline?\nWe should check the NAT gateway first.\nOh, the NAT.\nBut anyway, I want to remark that we did a little bit of preparation.\n\n\nLuciano: So that's of course to be mentioned.\nBut other than that, we spent slightly more than 30 minutes to have all these things configured.\nAnd we have a container up and running in AWS with a fairly decent setup that we could\nprobably take into like production with confidence.\nSo that's fairly impressive.\nIt's not bad.\n\n\nEoin: Not bad at all.\nIn the VPC resources, we can see the internet gateway that it created.\nWe can also see if there's a NAT gateway.\nAnd I'm happy to report that there's no NAT gateway.\nSo you don't have to worry about mortgaging the house to pay for it.\n\n\nLuciano: But that also means that our container is not going to be able to reach out to the internet if it needs to, I don't know, download anything or call an API, right?\nYeah, which is a nice default.\n\n\nEoin: But I guess if we need to, then yeah, that would involve extra configuration.\nBut you can provide if you do your VPC configuration separately, and lots of people will, or they'll\nhave existing VPCs, you can provide those inputs.\nYou don't have to get Copilot to create it.\nSo we can see here that we've got this new resource map is pretty handy.\nIt's showing us that we've got a public subnet and a private subnet, two availability zones.\nAnd yeah, it's from the public, so it must be the internet gateway route.\nYeah, that's what it is.\nOkay, anything else we want to check?\nOr we try to automate this with a beautiful pipeline?\nLet's see.\nLet's have a look at the pipeline.\nOkay.\nSo one thing I guess to understand is that it allows you to use Copilot to set up a code\npipeline.\nAnd it supports GitHub, Bitbucket, or AWS CodeCommit.\nAnd it will deploy to your environments and runs automated testing.\nSo it's code pipeline only.\nIf you're using GitHub actions, you're out of luck.\nYou got to do that yourself.\n\n\nLuciano: But you can still use GitHub for repository, basically.\n\n\nEoin: Copilot pipeline init is the first step.\nSo let's try this.\nLet's call this container front end main.\nThat seems like a decent default.\nOkay, so what kind of continuous delivery pipeline is this?\nWorkloads or environments?\nDeploys the service or jobs in your workspace or deploys the environments in your workspace?\nThat's a bit obscure to me what it means.\n\n\nLuciano: What's the difference between the two?\nIt says we can do question mark for help.\n\n\nEoin: The help says a pipeline can be set up to deploy either your workloads or your environments.\nOkay.\nIt's not really telling us anything new.\nWhich would you pick?\nI don't know what the difference is.\nWe go back to the docs?\nYeah, let's have a quick look at the docs.\nWe must be desperate if we're checking the docs.\nOkay.\nPipeline configurations are created at a workspace level.\nIf your workspace has a single service, then your pipeline will be triggered only for that service.\nIf you've got multiple services, then the pipeline will build all the services in your workspace.\nThat makes sense.\nBut what's the thing about pipe applications versus environments?\nIt doesn't tell us.\nIt doesn't say anything about this question in the documentation.\nI have a feeling that you could probably have different pipelines for different environments.\n\n\nLuciano: Yeah.\nIt's not too obvious which one is which.\nWell, as opposed to have like one global pipeline that maybe allows you to promote things across environments.\nI wonder if maybe this is a new feature that isn't in the latest documentation yet or something.\n\n\nEoin: Okay.\nSo let's try this again.\nGo for workloads.\nAnd which environments would we like to add to the pipeline?\nOh, we've only got one at the moment.\nSo we'll just use that one.\nOkay.\nThat seems to be all good.\nAnd now we can deploy.\nOh, commit and push the copilot directory to your repository first.\nAll the copilot manifests gets pushed and has to be in your Git repo.\nOkay.\nSo now we can do copilot pipeline deploy.\nOkay.\nAnd it says action required.\nI have to go to here to update the status of connection from pending to available.\nOkay.\nSo this is for connecting to GitHub.\nSo this is probably a one-off operation, right?\n\n\nLuciano: For this particular pipeline.\nYeah.\nSo this is like basically the OAuth flow to connect to your GitHub repo.\n\n\nEoin: It's interesting that I pasted in this URL.\nIt goes to the CodeSuite service and this lands me in, redirects me to the Stockholm region.\nFor some region.\nVery random.\nOkay.\nOkay.\nLet's go back to home.\nAnd this one is pending.\nOkay.\nConnection.\nSo I got to update this pending connection, which means I have to do an OAuth dance with GitHub, I imagine.\nOkay.\nSo let's say install a new app, pick awsbites and do my multi-factor dance as well.\n\n\nOkay.\nSo now that I've got the GitHub app configured, I can just hit connect.\nAnd then it knows how to talk to GitHub and set up web hooks to trigger the pipeline and\nall of that good stuff.\nLet's see what our...\nOh, okay.\nSo it says it has created the pipeline.\nSo let's go into code pipelines, see what we've got there.\nOkay.\nSo it is in progress.\nAnd it's the latest one is my latest commit, which adds the copilot directory.\nSo this should be deploying.\nSo what does it actually do?\nSo it takes the source from GitHub.\nIt's got the commit there and it's running a CodeBuild job.\nWhat does the CodeBuild job do?\nProbably builds the container, I imagine, right?\n\n\nLuciano: So we'll look.\nOkay.\nBuild details.\nWe should be able to see the build spec here.\n\n\nEoin: I'm guessing.\nOkay.\nSo that's in the source code.\nSo we can go back to VS code to see that.\nPipelines build spec.\nOkay.\nSo what's this build spec?\nIt downloads Copilot itself.\nSpecific version.\nRuns the tests, but that's commented out right now.\nPost build.\nOkay.\nOoh.\nIt's converting.\nYAML to JSON, right?\n\n\nLuciano: YAML to JSON using Ruby.\n\n\nEoin: That's why it needs Ruby.\nOkay.\nIt's quite verbose.\nOkay.\nThis is going through all the environments.\nSo it's reading the environments from here, but we asked it to only run the test environment, didn't we?\nBut okay, let's go have a look at that.\nIt comes from the pipeline.\nThe pipeline manifest.\nOkay.\nSo I guess that's what dictates the environments, its stages, and then the name.\nSo it's only going to pick out test because there is only test.\nBut we can add more there to this array, I guess.\n\n\nLuciano: Yeah.\nOkay.\nSo here there is a similar concept where build spec is fairly complicated.\nSo they kind of provide this higher level, simpler interface, which is the manifest YAML.\nYeah.\n\n\nEoin: It's interesting, though, that this build spec is part of your source code then instead of all of this, all of these steps being somehow folded into the Copilot CLI itself.\nThat's true.\nGood point.\n\n\nLuciano: Probably because they want to allow you to change things in there, like enable the test or I don't know.\n\n\nEoin: So from what I can see, it's basically doing Copilot package, which is generating the cloud information and not doing any deployment.\nI don't see it doing Docker build, though, but maybe that is happening as part of the\npackage.\nLet's have a look, see how our pipeline is doing.\nBuild logs.\nIt's succeeded anyway.\nSo what did it say?\nOh, yeah, it does build the container image.\nAnd pushes it to ECR and it has also generated the stack, the cloud information stack and\nthe parameters.\nCool.\nBut it did not deploy, did it?\n\n\nLuciano: This was just the build step.\n\n\nEoin: So I think there's an additional step for deployment.\nSo here is the deployment step.\n\n\nLuciano: Which is in progress, though.\nWe don't have to manually approve.\n\n\nEoin: We don't have to manually improve, but from what I can see by the manifest, you have the option for each stage to say whether it requires manual approval.\nThat's pretty good.\nYeah, nice.\nI still prefer GitHub actions, but it's pretty good.\n\n\nLuciano: One thing we could test is to change that index HTML and I don't know, add a title or whatever.\nCommit again and then we should see another run of the pipeline and eventually we should\nland that change into the test environment.\nAha.\n\n\nEoin: OK, so this is updating the CloudFormation stack.\nSo this, OK, the deploy action is actually an AWS CloudFormation integration.\nIt's not a CodeBuild job.\nThat's why it took us directly to it.\nSo it's taking the CloudFormation template that was generated in the previous step and\nthe parameters and it's using the CloudFormation integration directly.\nOK.\nWhile that's happening, should we talk about our failed attempt or disappointing attempt\nwith AppRunner?\nYes.\nEarlier, because we've deliberately decided to use the load balancer here because we tried\nthis earlier with AppRunner and we were recording and everything and it all went a little bit\nsour.\nTo say the least.\nIt went well to start with because it created AppRunner.\nWe were keen to see AppRunner because we haven't used it very much.\nWe were keen to see how it would work.\nBut it has a couple of flaws, right?\nAt least the copilot experience with AppRunner.\nHow would you describe it?\nYeah, I think it wasn't too bad.\n\n\nLuciano: Initially, I think it got a little bit tricky when we said, what do we do about domains?\nYeah.\nAnd I think because AppRunner is, it almost looks like a PaaS, like a Heroku that tries\nto remove all the details.\nLike you don't get to see where is the load balancer, where is ACM, how does it generate\ncertificates, how does it manage DNS and all that kind of stuff because they are happening\nin some kind of global AWS account that you don't get to control.\n\n\nYeah.\nSo a lot of the automation that we were expecting was not actually happening.\nIt was more, for instance, OK, if you want to use this custom domain, you definitely\ncan, but you need to basically create all these DNS records yourself.\nYeah.\nWhich, first of all, we didn't expect it.\nSo it took us a while to figure out that literally was blocked waiting for us to creating\nthese DNS records manually.\nAnd then it did take a while to actually validate the DNS records themselves.\nI think it took like 15 minutes.\nSo we were wondering, are we doing something wrong or is it normal that it takes so long?\nPlus, we did a few mistakes with DNS ourselves because, of course, with DNS, there is always\nsomething that goes wrong.\nRight.\nSo I don't know if I'm missing anything, but this is my recollection of what went wrong.\n\n\nEoin: Yeah, exactly.\nI think we just made an assumption, like at the start of this exercise, we use this\n`--domain` when creating the application to associate the Route 53 domain.\nAnd that worked so well for this one.\nBut when we tried it with AppRunner, you'll notice in the documentation, this is all specific\nto the load balancer, well, it's our web service application type.\nAnd this is all nicely documented and it tells you how to use different domains for different\nenvironments, et cetera, what it works like under the hood.\n\n\nThat's really nicely documented.\nYou can also import your own existing certificate.\nBut if you go down to this section, admittedly, we didn't read this before we tried it.\nThe request-driven web service, that's the AppRunner version.\nIt says that you can also add a custom domain for this.\nBut the way it works is that you specify a subdomain in your alias here, but it's unrelated\nto your dash dash domain in your app.\n\n\nThis is just per service.\nAnd you get one domain name per service and you can't use different domain names for different\nenvironments.\nSo there's this info here, which should be maybe warning, but it says for now, they only\nsupport one level subdomains and environment level subdomains or application level domains\nare not support or route domains are not supported yet.\nSo there's a few caveats there.\nIt's a pity because AppRunner, I think, is really nice.\nI think our summary at the end of that, Luciano, it was more like AppRunner is already pretty\nsimple to set up, even with CloudFormation or with console or whatever you're using.\nSo maybe Copilot doesn't really add that much, especially given that you've got these restrictions.\nYeah, that seems like a fair conclusion.\n\n\nLuciano: I don't know why they are trying to support it.\nMaybe there are more advanced use cases that they want to support.\nBut in general, it feels that for ECS, Fargate, there is a lot of value there just because\ndoing things manually and by manually writing your own CloudFormation or Terraform, even\nif you are experienced, it's still going to take you probably an entire day just to set\nup everything.\nIf you are not experienced, probably we're talking about weeks.\nSo here, this tool is probably going to take the time down by a lot, like probably in the\norder of hours or even less.\nShould we make an aesthetic change to our application?\n\n\nEoin: Yeah, let's just add an H1 tag or let's change the background code.\n\n\nLuciano: That sound good?\nSounds good.\n\n\nEoin: It's not going to be very pretty, but it's a very visible change.\n\n\nLuciano: Lovely.\nIt's attention grabbing.\n\n\nEoin: Okay, so let's add this.\nYou should add like a CSS animation and make it flash.\n\n\nLuciano: All right, let's push this and see if CodePipeline does its thing.\nAlso, this can be an interesting test to see how long does it take from commit to actual production.\nWell, it's CodePipeline, so it's not fast is the default answer in that case.\n\n\nEoin: But I'm curious to see, are we talking about one minute or like 10 minutes?\n\n\nLuciano: The first execution was eight minutes, 49 seconds end to end.\n\n\nEoin: Okay, let's see if it has some kind of cache or whatever.\n\n\nLuciano: If it's faster than that, no.\n\n\nEoin: Probably a good time to promote or refer people to our CodePipeline versus GitHub actions episode, where the performance of CodePipeline is a hot topic.\nIndeed.\nYeah, because here what we are effectively doing is not allowed, right?\n\n\nLuciano: It is building a new version of the container, but hopefully it uses the Docker cache effectively.\nAlso, it's a relatively small container, like we literally have two files, one is the\nstarting from NGINX, and then it's created the CloudFormation using the CLL, the Copilot\nCLI, and then it is using the CloudFormation integrations to actually deploy that.\nYeah, and in the last pipeline execution, that was the part that took the time, five\nminutes and 12 seconds.\nOkay.\n\n\nEoin: So I'm guessing that the reason for that is because it's a relatively small container,\nokay, so I'm guessing that the reason for that is not for CloudFormation, because I\ndon't think CloudFormation itself is at fault for that performance.\nI think it's just ECS, load balancer, target group, updates, all of that, all those shenanigans.\nWaiting for the health checks and so on.\n\n\nLuciano: That's actually maybe something worth looking at while the deployment is happening.\nWhat happens to the service?\nIt's not going to go down, right?\nI imagine it's going to kind of soft roll out to the new version in a way or another.\nThis is a good question, actually.\n\n\nEoin: I think we have options there.\n\n\nLuciano: Because this is also one of the things that when you have to decide for yourself, it's a difficult choice and it's difficult to configure it correctly, so I'm curious to see\nwhat's the default here.\n\n\nEoin: I'm guessing it's not doing the blue-green.\n\n\nLuciano: It's probably just waiting for the new containers to be up and receiving traffic and being healthy and then starting to drain the older ones.\nWill it add it as a target to our load balancer before it removes the old target?\nMaybe an interesting thing to do is to keep doing requests on the website and see if it\nflashes back and forward for a while in the background.\nOh, I got that.\nSo you see that you have both containers running right now for a while until I suppose it's\ngoing to start to drain the old one.\nWhich I mean is not necessarily too bad.\nBut I do want to go back to the docs because...\n\n\nEoin: Okay, here we go.\nSo in the deployment section, you can specify a rolling deployment strategy.\nWhich is probably what we have right now, right?\n\n\nLuciano: Yeah.\n\n\nEoin: The valid values are default, which creates new tasks before stopping the old ones.\nSo you have a moment in time where you have both versions running.\n\n\nLuciano: Yeah.\n\n\nEoin: Or you can recreate.\nBut there's no blue-green deployment or canary deployment.\nAnything like that.\nSo what is recreate doing?\nRecreate...\nSorry, where did it go?\nStops all running tasks and then spins up the new tasks.\nSo you have some kind of downtime, I imagine.\n\n\nLuciano: Yeah.\n\n\nEoin: Minimum healthy percent is zero, which is...\nI guess this is faster to get your new container up and running completely, but more dangerous\nif you've got a service that people rely on.\n\n\nLuciano: Okay.\nInteresting to see that there is no other strategy there available.\n\n\nEoin: Yeah.\nI think for people's benefit, there is a good blog post on the fourTheorem Blog by Gurarpit,\nwhich is about blue-green deployments with AWS CodeDeploy on ECS.\nAnd this is using Terraform, but I think the same kind of strategy still apply because it's using\nCloudFormation, ECS integration for the blue-green part anyway.\nBut I think if you read this, like there's a very thorough article that you'll realize\nhow much complexity and how much goes into thinking about safe deployments on ECS.\nAnd it's probably understandable then why Copilot doesn't support it out of the box.\nYeah.\nWe will have the link in the show notes for you too.\nIt looks like we're all yellow.\nOkay.\nAnd I'm curious to see if the pipeline says that everything is completed.\n\n\nLuciano: It looks like it took at least five minutes, right?\nYeah.\nNot as long as I thought.\n\n\nEoin: Let's have a look.\nSix minutes.\nIt was six minutes, 50 seconds.\nAnd the breakdown was also five minutes for CloudFormation.\nIt's just that the...\nWhat is the difference with the previous one?\nProbably build it a container was faster because...\n\n\nLuciano: Five, 1.5.\n\n\nEoin: Previous one is 5, 1.53.\nIt's the exact same.\nI think the difference is probably just it could be the amount of time it was waiting\nfor CodeBuild to provide a container.\nRight.\n\n\nLuciano: So transitioning between the steps.\n\n\nEoin: Yeah.\nIt was pretty successful.\nThis unboxing.\nI think we saw that there's some gotchas with this, but hopefully it's fairly obvious to\npeople that...\nLike the documentation, I think we've all seen mixed quality AWS documents.\nI think this is a good example, one that we should probably call out as a nice document.\nThe tool itself, I think provides good onboarding.\nThere's a couple of concepts you have to get used to, like what's an app, what's a service,\nwhat do you do first, how do you add things?\nIn general, it provides a nice guided experience.\nIt is developer friendly.\nSo I think that's off.\n\n\nLuciano: Indeed.\nYeah, I agree with that.\nI'm just surprised that this tool seems to have been around for a while.\nI think you mentioned like 4 years.\nYeah.\nAnd it's not something that I see often being mentioned.\n\n\nEoin: I mean, it's definitely worth using.\nI would use it again.\nThat's my impression from having used it today.\nIf we look at it, it has been used, it has been committed to since I think 2019 is when\nthe repo starts, but it's continually maintained.\nIssues are being opened and closed all the time.\nIt's long enough time that we can say like, this is definitely being maintained.\nBut yeah, it's not going to go away anytime soon, probably.\n\n\nLuciano: I get one of the things like it provides an abstraction over ECS, Fargate, AppRunner.\n\n\nEoin: I think it's probably more valuable for ECS and Fargate, but I'm interested to know if\nother people have tried it with AppRunner or have opinions on it.\nMaybe people who've got experience of running co-pilot based workloads in production and\nhow it compares to the other options.\n\n\nLuciano: Yeah, or also if you use it for other kinds of workloads, like I don't know, task based from, I don't know, workers processing tasks from a queue or something like that, which\nseems to be an option that it is support.\nSo I'd be curious if people tried that, what is your feedback?\n\n\nEoin: Yeah, and also if people have tried it from microservices applications with service connect for inter-process communication, maybe some of the more advanced stuff like auto-scaling.\nAnd also there's some concepts we didn't even touch on here altogether.\nMaybe it's under developing, but you've got the ability to put a cloud front CDN in front\nof it as well.\nAdd additional sidecars for whatever you want.\nSo I'm interested to know what's the most extreme co-pilot example out there in the\nwild.\n\n\nLuciano: Okay, so I suppose that's all we have for today.\nThis has been a little bit of a long episode.\nI hope you will like this new format that we're trying to explore.\nSo definitely give us your feedback if there is something you particularly liked or you\ndidn't like, or maybe another product that you would like us to unbox.\nLet us know in the comments and we'll take your feedback very seriously and hopefully\nwe'll be able to give you value in the next episode.\nSo thank you very much for being with us today and we look forward to seeing you in the next\nepisode.\n"
    },
    {
      "title": "77. How do you use Lambda Response Streaming?",
      "url": "https://awsbites.com/77-how-do-you-use-lambda-response-streaming/",
      "publish_date": "2023-04-21T00:00:00.000Z",
      "abstract": "Are you tired of waiting for your Lambda functions to finish before getting a response? Well, now you don't have to! In this episode of the AWS Bites podcast, we will talk about Lambda Response Streaming, a new feature recently added by AWS that lets you stream responses from your Lambda functions in real time.\nWe'll start by explaining what Lambda Response Streaming is and how it differs from buffering. We'll also discuss HTTP Chunking and other benefits of streaming. If you're a Node.js developer, you'll be happy to know that we'll cover how to work with streams in Node.js and how the new Lambda Response Streaming API works with the Node.js runtime.\nBut that's not all! We'll also discuss how to consume Lambda Response Streaming responses and compare that with S3 Object Response. And if you're wondering about pricing and quotas, we'll cover that too.\nFinally, we'll answer the question on everyone's mind: will we get streaming requests as well? You'll have to watch the video to find out!\nSo if you're interested in learning more about Lambda Response Streaming and how it can improve the performance of your serverless applications, make sure to tune in. We promise it'll be worth your time.\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nOfficial announcement blog post for Lambda Response Streaming\nOur previous episode about Lambda function URLs vs API GW vs LB\nHTTP Chunked transfer encoding protocol\nLuciano's free Node.js streams workshop on GitHub\nNode.js design patterns (the book)\nStreamify response functionality in Middy\nLambda Rust Runtime codebase (support for Response Streaming)\nSimilar evidence of Response Streaming feature support in the GoLang Runtime\nOur previous episode about S3 pre-signed URLs\nLambda Response Streaming pricing\nEoin's article about S3 Object Response\nExperimental Node.js custom Node.js streaming runtime\n\n",
      "transcript": "Eoin: The AWS Lambda team recently announced an exciting new feature that might be interesting\nto all of the JavaScript and TypeScript developers out there using Lambda.\nWe're talking about Lambda response streaming, which allows for sending data incrementally from\na Lambda function, reducing the time to first byte. I'm Eoin, joined by Luciano, and in this\nepisode of AWS Bites, we will tell you everything there is to know about this new Lambda feature.\n\n\nWe will discuss the benefits of streaming over buffering, talk about the provided API,\nwhat we like and what we don't like about it, and we'll mention quotas and pricing. And finally,\nwe're going to speculate on whether we can expect a more streamable future for Lambda. So stick\naround for the full episode to learn more about Lambda response streaming and how it can benefit\nyour JavaScript Lambda functions.\n\n\nAWS Bites is sponsored by fourTheorem. fourTheorem is an AWS partner for migration, architecture,\nand training. Find out more at fourtheorem.com. That link is in the show notes.\nOkay, Luciano, for Lambda response streaming, let's talk a little bit about what it is. It was\nannounced, I think, just about a week ago, and it's a Lambda feature and it's currently only\npossible for Node.js runtimes and custom runtimes as well. And the idea here is that it allows you\nto send a response from a Lambda function incrementally. Previously, you had to kind\nof build up your whole response and then send it back in one go. So instead of sending it back in\none block, you can now start to send some bytes as soon as you have them and your client can start\nreceiving them. So this is useful for streaming data, like let's think about some use cases,\nlet's say records from a CSV file if you're generating them on the fly, or even server\nside rendering. I think that's a big use case for this kind of functionality. So the client can\nstart receiving the data straight away, even while processing is still happening. Now, how do you\nactually integrate this into your client? So it currently works with Lambda function URLs. This\nis something we talked about in a very recent episode, link is in the show notes, but you can\nalso use that then with a CloudFront distribution in front of it if you want to get all the benefits\nof caching and edge locations with CloudFront CDN. It doesn't really work with API Gateway or\nApplication Load Balancer. I mean, it kind of works, but it doesn't give you the streaming\nbenefit. So you can send use it with an API Gateway in front of it, but the API Gateway\nis still going to buffer it up for you. But there are actually some subtle differences when you use\nit with API Gateway that give you some benefits and we'll cover that later. On support, it's\nsupported in CloudFormation and SAM. I mean, you don't actually have to change the Lambda function\nitself, only the code. But if you want to use it with function URLs, there is a new CloudFormation\nfeature. It's not available in CDK yet. It is available in SAM and it's not yet available in\nthe serverless framework, although there is a pull request open for it. Luciano, you're a bit of a\nstreaming guru, if you don't mind me saying so. What is a stream really and how does it differ\nfrom buffering? Now I feel the pressure.\n\n\nLuciano: Okay, let me try to explain what is the difference between streaming and not streaming. Because on one side, if you think about sending data over the wire,\nlike networking is inherently streaming, like you keep appending bytes in a channel. So it's more\nof a conceptual model on how do you actually produce the data before sending it to the wire\nor while sending it to the wire. So when we talk about buffering, basically the idea is that you\nare creating some kind of response object or some kind of response data. And of course, it's a\nsequence of bytes, represents some serialization, represents some kind of structure. And you might\nbe accumulating all of that information in memory for a while until you actually have the entire\nmessage ready. And this is kind of what we mean when we say buffering. You buffer all of that\ninformation in memory and only when you complete that, you start to send the bytes over the wire.\n\n\nAs opposed to streaming where maybe you can have meaningful chunks of that information, maybe,\nagain, your example of CSV records, I think it's a very good one. So you might be able to send\npartial information over the wire as you are accumulating that information. Actually,\nyou don't even accumulate. You keep kind of passing the bytes as they are becoming available.\nAnd your client can start to make sense of that information as soon as the first row, for instance,\nif the CSV record arrives, you can actually start to take advantage of that information.\n\n\nSo again, in a CSV file example, what that can be useful for is the client can start to do analysis\non that CSV data. Maybe it's doing a query so I can filter out the records that are relevant\nand keep only the ones that display to the user the ones that match the query. Or maybe if it\nneeds to do some kind of streaming aggregation, I don't know, maybe you're calculating a sum or\nan average or something like that, you can start to do all the computation real time. And even with\npartial data, you can start to show something useful to the user. Other examples are you could,\nfor instance, if you are plotting a chart that has thousands or millions of points,\nif those points are sent to you in a streaming fashion, you can start to display them straight\naway as soon as they arrive rather than waiting for the entire response to arrive. Similarly,\nyou could render pictures that are encoding formats that are progressive, so you could\nstart to render the picture as it arrives, even before it's all received. Or even web pages,\nyou mentioned the case of server-side rendering pages, and I think this is kind of a common trend\nin the front-end world. Lots of single-pages application frameworks now have capabilities\nto server-side render to make sure that you get the advantage of that time to first byte,\nwhich is a very important metric on the web today. Search engines will prioritize web pages that\nwill start to render as soon as possible. So this is probably the most interesting use case,\nand I would bet that this is the main motivation why AWS spent energies enabling this feature,\nbecause I've seen a lot of other competitors trying to support the story of if you're using\nSvelte, if you're using Vue.js, if you're using React, if you're using Solid or Quick,\nthey all offer today ways to server-side render and be able to send dynamic content to the users,\nand probably Lambda needs to be on par with these other engines and make sure that this use case is\nsupported as well. Now, another interesting technical detail is that if you're really curious\nto understand what is the actual protocol that gets used, this is just using plain HTTP chunked\nencoding behind the scenes, and we will have a link in the show notes if you're curious,\nbut the idea is that because you need to send small frames of data, you need to tell the\nreceiving side how big is every frame, so there is a protocol that kind of documents how do you\nencode that information, so not just the raw data, but encapsulated in a frame that also specifies\nthe length of that frame, and then of course there is a way to mention, well, to specify when the\nentire stream is over and the entire request can be considered completed, so I hope that kind of\nclarifies what do we mean when we say buffering and streaming and what are the benefits there\nand the use cases, but I don't know, maybe there is something more to add on why one model could\nbe better than the other or when one is more convenient than the other.\n\n\nEoin: I think you already mentioned the main benefit, which is the time to first byte metric, and you also mentioned that\nthis is useful for search engine optimization and also for performance metrics like Lighthouse,\nso it's pretty important for web applications and mobile applications these days, but the other\nthing I suppose is that with streaming you can handle potentially infinite amounts of data in a\npretty efficient way, so you've already got the alternatives of doing frequent polling if you\nwanted to get periodic data like to render a chart based on real-time data, but it's much more\nefficient to do with streaming. You don't have the request overhead every time and you don't have to\nspecify the content length up front with junked encoding. It can dynamically evolve in that way.\n\n\nIt could also be used for say streaming footage from a video camera, a security camera for\nexample, but from an application development point of view the nice thing about streams I suppose is\nthat they are composable, so you can have streams within streams and if you're a Node.js developer\nyou'll probably see this quite often, but it's also possible with lots of other languages\nwhere you've got streaming abstractions that allow you to nest streams. For example, you can have a\nzip stream or an encryption stream and you might have then a base64 encoding on top of that and by\nwriting into one stream you get that automatically happening for you under the hood and the base64\ngzipped version is being streamed out at the other end. I suppose there is a disadvantage with\nstreaming as well in that it can be a little bit more complex and harder to reason about.\nBuffering is a little bit of a simpler model and easier to debug, but I still think that in modern\nweb applications it's hard to avoid streaming. So maybe Lutiano, you've done a lot of content,\na lot of speaking and writing about Node.js streams and all the different generations of\nhow streaming works in Node.js, so you're probably best place of anybody to talk about how this will\nwork in Lambda with response streaming. So how does it work under the hood?\n\n\nLuciano: Yeah, not going to go too much in depth, but I think it's worth giving a very quick summary of\nwhy Node.js is particularly suitable for streams and for this particular use case in Lambda.\nAnd I think it's because streams in Node.js are kind of a first-class citizen. They always have\nbeen, even from the very early versions of Node.js. And there are some primitives that get used in\ndifferent places in the Node.js core library and third-party libraries. And these are all different\nkinds of streams that you can use and compose in different ways. So the main ones are readable\nstreams, which are basically an abstraction that allows you to consume data from a source.\n\n\nThat can be a file from the file system, a network socket basically is how do you read data in a\nstreamable fashion. Similarly, there are writable streams. So again, if you are trying to send data\nto a file or send data over the wire or maybe write to standard output, you can use writable\nstreams, which again, is just an abstraction to basically allow you to write data incrementally\ninto whatever source of data. Then there are transform streams, which are kind of something\nin between. They basically allow you to take data from readable stream, do some transformation,\nand then send it to a writable stream on the other side. So these are generally, if you can imagine,\nstreams are like plumbing. These are generally something you put in between a readable and\na writable to change the data on the flight. And good use cases also in the standard library are\nencryption, compression, and yeah, basically you can use them even together. Like you can even\npipe two transform streams and add encryption and compression at the same time.\n\n\nAnd finally, there is duplex streams, which is kind of an abstraction that represents\nbidirectional channels, like a network socket. So for instance, when you have a communication\nchannel where you can both read and write and you are kind of creating a channel where two parts\ncan basically exchange information in both directions. So again, this is important just\nbecause these are all classes that you can use in Node.js core and they will be used to implement\nall the more advanced streams capability like file system, HTTP, compression, encryption, standard\ninputs, standard output, standard error. So once you get familiar with those primitives, you can\neven build your own custom streams, custom transformations. And if you, for instance, have to\ninteract with a new database that you're starting to use for the first time and it's not in the\nstandard library, you can use stream to also read and write data from that database if you want to\nin a streaming function. Now again, I don't want to go too much into detail, but we will have a link\nin the show notes with a workshop that I created some time ago that basically guides you through\nthe whole experience of understanding how stream works, there are exercises up to get into more\nadvanced topics where you can create your own custom streams. So if that's something you are\ncurious about, feel free to check the link and let me know if you like it or not.\n\n\nEoin: Okay, sounds good. Do you also want to shamelessly plug this book, which also covers Node.js streams?\nOh, that's a good one. Yes, we will also have the link to that one in the show notes. Thank you.\n\n\nLuciano: Thank you. Yeah, there is an entire chapter about streams and I think most of the material is also\ncovered in the workshop, but I think the book probably goes a little bit more in depth.\n\n\nEoin: Okay, great. Well, maybe we can talk a little bit then about how the world of Node.js streams meets\nLambda functions with this new feature. There's a new API that you can use in your Lambda functions\nfor Node.js. So you end up with a global object called 'awslambda', and then you call the 'streamifyResponse'\nfunction on it. So it's not a case of just returning your existing payload. You have\nto write your functions a little bit differently if you want them to work with response streaming.\n\n\nSo instead of your typical Lambda function signature with event and context parameters,\nyou do this streamifyResponse and then you use a function with three parameters. So you get an\nevent, a response stream and a context, and then you can start writing to this response stream\nin the way that you just described. So it's just a writable stream and it forwards bytes\nincrementally. So you can write to it or use any of the libraries out there that support Node.js\nstreams to write to it. Once it's finished, then you call.end to close the stream.\n\n\nSo you mentioned that there's lots of different cool things you can do with Node.js stream. So you\ncould use pipeline if you've got multiple steps in the pipeline to create those streams. Like we\ntalked about gzipping and base64 encoding, that would be one example you can do with it. Now,\nit is a bit of a weird thing, this global object, awslambda. We haven't had this before.\nI'm slightly surprised by this because globals are generally frowned upon. This is weird for\na few different reasons. Your linter or TypeScript will complain about this object because it's just\nnot the done thing these days to have globals. So you'll have to configure your linter to ignore\nthis global. You won't be able to run your Lambda function code globally. So you'll have to fix\nsomething for your unit tests. It does seem to work with local invoke, which is good news. But\nI did try the latest serverless framework version and the one from the PR that has this function\nstreaming URL support. It didn't work with local invoke. It didn't know what the AWS Lambda global\nwas at all. So it's a little bit of a strange kind of black box that you have to work with. The code\ndoesn't seem to be publicly available on GitHub. Although I think Luciana, you found that there\nis something you can find from the Lambda emulator container, the container image for Lambda. You can\nactually reverse engineer that a little bit and find out how it works. If you don't want to deal\nwith all of these problems, MIDI has actually been very quick and added support for response streams\nand added a nice interface that hides a lot of the complexity. There's a link to the documentation\nin the show notes. Now, Luciano, is there anything you'd like to add to that? And maybe you can talk\nabout how do you actually read the response of a Lambda using response streaming?\n\n\nLuciano: I think that's a good segue from what you just mentioned. Yeah, basically the idea is once you have a Lambda that\nuses this feature, sending responses in a streaming fashion, how do you consume that information?\nRight. Also from the client perspective, you need to be ready to consume information in a streaming\nway. So you will receive chunks of bytes rather than just the whole response straight away.\n\n\nSo the way they implemented this in AWS is we already mentioned the diffuse Lambda function URLs.\nYou basically can just use HTTP and you will see the HTTP response coming up in a streaming way. So\nfor instance, if you get a chunk every second, you should see, maybe you do a call or you open\nthe browser in a specific function URL, you should see the response being rendered basically over\ntime as the new chunks are available. That's one way, but if you want to use a more programmatic\nway, you can also use the SDK. There is a new functionality called Invoked with Response Stream.\n\n\nSo basically that I haven't checked exactly the specification in JavaScript, but my expectation\nis that you will get a readable stream so you can then keep using streams in Node.js if you're using\nthe Node.js SDK, the JavaScript SDK. Now, what about Node.js runtimes? It's probably the follow-up\nquestion there. And the answer is still a little bit, let's figure it out. I think that there is\nstill new documentation that will come up in the next few weeks. It seems that there is some support\ncoming up. For instance, we were able to dig into the Rust runtime, which is fully open source,\nand we found instances in the code where it looks like this feature is fully supported there,\neven though we couldn't really find an official piece of documentation for that.\nAnd we will have links in the show notes for all of that. Very similarly for Golang, we were able\nto find something for it. But again, we expect we will have news in the coming week about more\nofficial support for other runtimes because of course all these concepts, yeah, custom runtimes,\nall these concepts are not special for Node.js. It's just Node.js has a better support because\nit's a primitive that existed in Node.js for a long time, and people are more used to use these\nkind of concepts in Node.js. But of course, this concept, you can use them in any language. You\njust need to have an interface to work with that. Okay.\n\n\nEoin: So the official line there is that we have supported Node.js and custom runtimes, but not in any of the other provided runtimes. So you won't\nbe able to use this in the Python provided runtime or the Java one or.NET yet. You'll have to wait\nfor new features to come out in those actual runtimes. Yeah, that's a good clarification.\nOkay. Maybe then given that we can talk a little bit about some of the other limits and pricing,\nanything else that might stop you from plowing ahead with this feature. We talked about the\nability to do infinite streaming with streaming. Now that's practically of course not the case\nwith Lambda because you still have a 15 minute timeout limit. One of the advantages of using\nLambda response streaming though is that you can go over the traditional limit of a six megabyte\nresponse payload. And now you can stream up to 20 megabytes of data, but that's only a soft limit.\n\n\nSo you can ask for more if you needed to. And that will then allow you to go beyond six megabytes and\nwell beyond that with function URLs or with the direct invocation method. If you're using an API\ngateway, you still have a 10 megabyte limit. So it allows you to get up to 10 megabytes with\nAPI gateway, but it's not a massive increase. And if you do go over six megabytes, then you\nstart to incur an extra cost. So this is another pricing dimension for Lambda. So it used to be\nthat you only had requests and memory per second to worry about when you were thinking about Lambda.\n\n\nNow there's a few more pricing dimensions. So this is a new one and it's just less than a cent\nper gigabyte roughly for anything above the normal six megabyte limit.\nIt's also important to realize that there is a maximum bandwidth throughput limit of 16 megabits\nper second, otherwise known as two megabytes a second for streaming functions. So if that\ndoesn't sound like it really works for your use case, we should probably remind everybody that\nthere's a couple of other ways to do streaming with Lambda. And if you cast your memory back, you might\nremember that there was an S3 object Lambda feature released a couple of years back. I\nwrote an article about it at the time and did some example repos, which we can link in the show notes.\n\n\nBut this is one way to get a streaming response back to user. And this is where you basically\nintercept something like a get object request to the S3 service. And it will invoke a Lambda\nfunction that allow you to intercept the object and pass back some different response. So you can\neither generate a completely new response or you can mutate the response in some way like convert\nthe file format, do some filtering. And this allows you to stream back as well using a different\nstream interface. So this is something that might also work for you if you need to do streaming,\nbut it's only based on objects that exist in an S3 bucket. So an object still has to exist.\n\n\nYou could probably hack this a little bit and just have some dummy objects that you use and then use\nthe S3 interface to essentially give you a way to get a much longer stream back and then control\nthat stream using Lambda function code. And you can even do S3 presigned URLs on top of it as\nwell. So there's probably all sorts of cool hacks you could build on top of it. Also worth mentioning\nthat you can do WebSockets. So you don't necessarily have to do it in the streaming way.\n\n\nYou can use HTTP, WebSockets, and API Gateway has support for that. The disadvantage being that you\nhave to maintain the state yourself. So you generally need a DynamoDB table or something\nto keep track of all the connections you need to send messages back to. And the other option\nfor WebSockets is to use AWS IoT, which is the original way to do a serverless WebSockets on AWS.\nAnd that way it does the state management for you, but there's a little bit of other complexity\naround authorization, but it's not too bad. So I think those are all the response streaming topics\nwe have. Maybe we could speculate a bit about the future. Do you think we'll get request streaming\nsupport at some point? And why would we want it? What would it be good for?\n\n\nLuciano: That's a good question.\nAnd I think one of the reasons why this conversation comes up is because if you were dealing with kind of a more classic standalone web server in Node.js,\nlike Express or Fastify or Happy or something like that, almost all of them, they will give\nyou this abstraction where the request that is coming in into your handler, it's a stream.\nIt's a readable stream. So for instance, if you are implementing a piece of functionality\nthat receives an upload, you can actually start to consume that information and process it and\nsave it somewhere else without having to buffer that entire upload up front and then later on\npush it somewhere else. And this is one of the common limitations when you implement\nAPIs in Lambda that is not a great idea to do that with, for instance, for uploading files.\n\n\nYou probably will end up using something like a strict resigned URLs, which by the way,\nsomething else we talked about before, we will have the link in the show notes as well if you're\ncurious. So I think once we get now, since we got now the response streaming functionality, it's\nlegit to ask ourselves, are we going to get also requests streaming? So maybe we can support\nuploads or other use cases where even the request is streaming, or maybe when you need to send a lot\nof information to the Lambda and you want the Lambda to start to process that information as\nsoon as possible. Now, I don't really have an answer, of course, we are just speculating here,\nbut I think this was a couple of years ago, I was curious about the same question. We didn't even\nhave response streaming at the time. And I was just trying to understand how the Lambda runtime\nwas working behind the scenes. And the way I did the research was basically, okay, let me try to\nbuild a custom runtime myself for Node.js and see if I can make that custom runtime closer to an\nactual web framework in Node.js. So rather than receiving an event and the context, I actually\nwanted to be able in my handler to receive a request object and a response object where the\nrequest object was a readable stream and the response object was a writable stream, which is\nbasically the lower level interface that you get in any web framework, even the\nHTTP library that is built in with Node.js. So I was actually able to achieve that in a bit of\na hacky way. And the reason why that worked is because the Lambda runtime, when you want to\nbuild a custom runtime, AWS gives you two HTTP endpoints to work with. The first HTTP endpoint\nis like a polling endpoint where you have to constantly poll from your runtime to see,\nis there a new message there? Is there a new event that I need to handle?\n\n\nAnd of course, it's an HTTP request. So you can take that as a readable stream and that's\nbasically your input. And then also you get another endpoint, which is basically when you\nfinish and you have a response, send the response to this HTTP endpoint. So effectively you have\nan HTTP endpoint to read from and an HTTP endpoint where you send the response to,\nand what you have to run in between is your actual handler code. So you could build your own custom\nhandler in any way. You could even call something written in COBOL at that point in between, right?\n\n\nAnd this is basically the way you build custom runtimes. So at that point, what I did is basically\na very thin layer that just call a handler passing the request from the first endpoint and a response\nthat is already starting to the second endpoint. And then the handler can basically fill the gap\nby reading from the first endpoint and writing in the second endpoint. Now this is very sketchy and\nbarebone and it's probably exposing more that it should need to be exposing because it's actually\ngiving the handler full access to requests and responses that are actually part of the runtime.\n\n\nSo I'm not too sure it's the best way of doing this, but it kind of works and it demonstrates\nthat the idea makes sense in the context of Node.js, even though there is a big caveat.\nThis is assuming that you are streaming end to end, that basically you are not buffering anywhere\nelse outside the runtime, which we know is not the case when, for instance, you integrate\nthis solution with API Gateway. So we saw that for response streaming, AWS needed to kind of come up\nwith different ways and add support for different features to actually make this possible. So even\nthough we expect this is possible also for request streaming, I think AWS will also need to kind of\ncome up with some new feature that enables that end to end. So we still, yeah, big speculation\nhere. We cannot just say for sure that this is going to happen and when it's going to happen\nor how it's going to happen, but we just feel that it's technically possible and it could enable\nsome interesting use cases. So hopefully it's something that we will get in the future.\n\n\nEoin: I think we've covered all of the features and benefits and everything with function response streaming so far. So I'd just like to say thanks for joining us for\nthis episode of AWS Bites. Again, we hope you enjoyed learning about response streaming and\nhow it can benefit your JavaScript, TypeScript, and custom Lambda functions. If you want to learn\nmore, check out the links in the show notes and don't forget to subscribe to the podcast if you\nhaven't already. Hit like, hit the bell and stay tuned for more AWS news and updates.\nThanks for listening and we'll see you next time on AWS Bites.\n"
    },
    {
      "title": "78. When do you need a bastion host?",
      "url": "https://awsbites.com/78-when-do-you-need-a-bastion-host/",
      "publish_date": "2023-04-28T00:00:00.000Z",
      "abstract": "Harken, good sir! Art thou aware of the arcane art of safeguarding thy AWS instances from malevolent threats whilst keeping them accessible for thy travels? There exists a mighty tool for such purpose, and it is hight the &quot;bastion host.&quot;\nIn this pamphlet, we shalt unravel the mysteries of the bastion host and showeth thee how to useth it to safeguard thy web space. We shall commence by presenting a shadowy example architecture and introducing thee to the definition of a bastion host. We shalt then delve into the question of whether bastion hosts could be a security liability and explore the enigmatic concept of port-knocking.\nWe shalt also take thee on a valiant journey of how to provision a bastion host on AWS, and explaineth the cryptic basics of SSH and tunnels. Thou shalt discover the dark side of managing SSH keys and auditing SSH connections, and we shall reveal the secrets of AWS EC2 Instance Connect and AWS Session Manager (SSM) as solutions.\nThou shalt learn how to accept connections without exposing a port on the public internet, and we shall introduce thee to a mysterious tool called &quot;basti&quot; that can make it easier to provision SSM-based bastion hosts and connect to thy databases.\nWe shalt wrap up by revealing alternative security measures to the mysterious bastion host and provide thee with cryptic closing notes to summarize the key takeaways from this video. Heed our call to this intriguing guide to securing thy web space, and may the forces of the internet be in thy favor!\n\nHarken, good folk! We would like to offer our deepest gratitude to our noble sponsor, fourTheorem, an AWS Consulting Partner that doth offer training, cloud migration, and modern application architecture. Thanks to their generosity, we are able to continue on our journey of imparting wisdom and knowledge regarding AWS.\n\nVerily, in this episode, we hath made mention of the following resources:\n\nAn open-source implementation of the port-knocking technique\nThee official guide to set up EC2 Instance Connect\nA list of AWS IPs\nThee official docs on how to set up SSM\nSSM agent code on GitHub\nThee inlets project on GitHub\nBasti on GitHub\nTailscale\nWireguard\n\n",
      "transcript": "Luciano: When you use more traditional data storage services such as a MySQL or a Postgres database on RDS,\nor maybe you're using a Redis instance on ElastiCache, it is good practice to provision\nthese resources in a private subnet. In fact, only your applications running in your private\nnetwork should be able to access these sensitive resources. We don't want them to be publicly\naccessible on the internet, of course. But what do you do if you want to connect to those resources\nfrom your own desktop machine? Maybe you want to run some Ad-hoc queries on your database,\nmaybe you are investigating a bug and trying to figure out what's going on on the data layer,\nmaybe we simply want to make sure that the data is being persisted correctly because you just\nreleased a new application and you want to control that things are going as expected.\n\n\nSo how can we access a resource that is not reachable from the public internet?\nToday we will be talking about Bastion host, sometimes also known as jump boxes or jump servers,\nand these are basically well-known ways to create a secure bridge that can allow us to access\nthese private instances through the public internet when we need to do that.\nWe will also discuss some practical architecture in AWS and the trade-offs of these solutions.\n\n\nWe will explore the different approaches you can use to connect to a Bastion host,\nincluding plain SSH, EC2 instance connect, and AWS Session Manager. Finally, we will give you\nsome resources that can help you to create a Bastion host when you need them. My name is\nLuciano and today I'm joined by Eoin for another episode of AWS Bites podcast.\nAWS Bites is sponsored by fourTheorem. fourTheorem is an AWS partner for migration, architecture,\nand training. Find out more at fourtheorem.com. The link is in the show notes. So Eoin, should we maybe\nstart introducing this topic by providing an example architecture that we can use to wrap our\nminds around?\n\n\nEoin: Why don't we start with an example like a three-tier web application architecture? So you've got load balancer, which is public facing. You might have a web application,\nwhich is running in private subnet, and then you've got a database, relational database,\nalso running in private subnet. So for security reasons, we want to keep the network, the web\napplication and the database are running in private networks, and that load balancer is\npublic. So the subnet where the load balancer is deployed sometimes called a perimeter or DMZ,\nlike a demilitarized zone. And this is like a buffer zone between the internal network and the\npublic network. And it's not possible to reach the internal web application server from the public\ninternet. So you're reducing the attack surface there, basically. The only way you can access it\nis by the very specific routing that's possible from the load balancer to that web server.\n\n\nAnd equally, it's not possible to reach your database server. You don't want your database\nserver publicly exposed for many, many reasons on the public internet. So this is all great\nin practice. And when you're running in production until for some reason, you decide you want to\naccess that database from your local laptop. So either you've got a production instance,\nor you're just trying to troubleshoot some environment even in the staging environment,\nwhich is set up in the same way, or you want to run like a database user interface that\nconnects over ODBC, for example, to your database. So like while this is a three-tier web application,\nwe might describe this as a problem that only exists for traditional applications. This kind\nof challenge also comes up with serverless modern applications as well, because you can still have\nresources in a private VPC. You might have Lambda functions that access RDS or ElasticCache again,\nor ElasticSearch, or even an internal load balancer. So in all of those cases,\nthis is the kind of scenario where something like a bastion host can help.\nSo we've said bastion a few times Luciano, what is a bastion host?\n\n\nLuciano: Yes, I'll try my best to describe that. So it is basically a virtual machine.\nSo an instance that you run on AWS in this case, and it kind of works almost like you can imagine it as a bridge\nif we want to have the kind of mental model. So you can spin up this at the perimeter, so somewhere\nthat is reachable on the public internet, and then you can configure your networking to use that\nto effectively route connections from public internet, which in this case may be your own laptop,\nto your database server. So you just put it there in the middle and use it effectively as a jump box.\n\n\nThat's why some people like to call it jump box. Now you might be thinking, okay, we are trying to\nkeep our database in a private area because probably the main thinking there is security reasons.\nIf we have a jump box, isn't that a security liability at that point? Like, I mean, we're\nre-exposing everything again, right? And that's kind of true. You have to be really careful with\na bastion host or a jump box because if you don't do it correctly, it can become a security liability.\n\n\nSo there are some things that are very common that people would do to try to keep it as secure\nas possible. Of course, you need to use an up-to-date operative system and have things in\nplace to always update the operative system every time there is a new security patch available.\nYou need to install only the bare minimum software needed. Traditionally, that's just an SSH server.\nYou don't need much more than that. And you need to open only the ports that are really needed\nfor accepting connections on that SSH server. And you need to... Basically, if you can limit\nthe IP addresses that can connect to that machine, that's even better. Most likely you will want only\nyour own personal IP, so your own laptop, to be able to connect on that port. So you can create\na security group that is restrictive. Or maybe if you are using that and sharing it with your own\noffice, you can have a list of IPs that are authorized. But the idea there is don't make it\nopen to any possible IP. Just limit to the ones that you trust. And don't use SSH through password,\nbut use SSH keys. That's another common best practice. And one more thing is you don't\nnecessarily need to have these instances running all the time, because we know in AWS you can\nspin things up and down when you need them. So it would be nice if you have a process that allows\nyou to provision really quickly a bastion host only when you want to do a connection, and then\ntear it down when you don't need it anymore. I think that's an even better security practice,\nbecause then you are limiting the exposed surface only for the time when you are actually using it.\nAnd there are other tricks like port knocking, and maybe, oh, and you are more familiar with it,\nso you want to mention something about that.\n\n\nEoin: Port knocking is kind of just an interesting way of, I suppose, implementing a software-based combination lock for this bastion host. So the\nidea with port knocking is that by opening up some special ports on the machine, you don't open\nup SSH port 22 by default, or whatever port you're using. You have some special random ports that\nonly you know about. And by opening these ports in a certain sequence, you can have a daemon or\nserver on the machine which recognizes that you're essentially knocking on the door in the right\nsequence, and it will then enable the SSH socket. So it could use IP tables or something at that\npoint to enable SSH inbound. And there's a number of different implementations of port knocking out\nthere that will allow you to not leave SSH open all the time as a bastion host. So it just makes\nit more difficult. It's kind of an obscurity mechanism. It makes it more difficult for hackers\nto detect that you've got SSH running there. So maybe it's worthwhile just going through how you\nnormally provision a bastion host on AWS. So traditionally, the way you do that is by running an EC2 instance.\n\n\nYou'd usually use a very small one using a recent Linux AMI, and you give it\nvery minimal capabilities because all you want to be able to do is jump right to your destination\non the target VPC. And you will have to generate an SSH key pair for that machine. And this is\nwhere it gets a little bit challenging because if you've got key pairs that are long lived, then you\nknow you've already got a security problem because you've got issues with retention of those and also\npeople who have access to those when they shouldn't have. Then you configure the\nsecurity groups of that instance to accept traffic only on that SSH port.\n\n\nAnd you would also make sure that the instance gets a public IP and is reachable from the internet. So you might\nhave to update your routing tables and add internet gateway and all that kind of stuff.\nYou also should make sure that the security group on your database can accept connections\nfrom this bastion host. And then you should be able to connect to this instance using SSH and\ncreate a tunnel to your database. So you could open a shell on the bastion and then go to your\ndatabase, or you can tunnel. We might talk about that in a bit. If people aren't really familiar\nwith SSH, many people would have seen it, but maybe not used it that much. It stands for secure shell\nand it's a common tool that's existed since 1995. OpenSSH is pretty widespread implementation.\nIt allows you to manage a shell session in a remote machine and also to create tunnels.\nIf you've used GitHub and you've had a repository of your own and you've pushed code to it,\nyou've probably used SSH at some point, especially if you don't want to manage passwords all the\ntime. So you can have a server application running on the machine you're connecting to,\nand you also need a client application. So OpenSSH is generally available on most platforms these\ndays, including Windows, but you often see people using PuTTY on Windows as well. And it supports\nlots of different types of authentication. We mentioned username and password, but public and\nprivate keys with various levels of encryption are also possible. So that's SSH. And one of the cool\nthings about SSH is its ability to tunnel. So Luciano, what the heck is a tunnel?\n\n\nLuciano: Yeah, so a tunnel is also something that sometimes is referred to as SSH port forwarding. So the idea is that you\ncan create a secure connection between, let's say, your own local machine and a destination machine\nusing this kind of jump box. And the idea is in your own local machine, you will expose that\nconnection in a local port. So then you can connect to that remote endpoint by just using\nlocalhost and whatever is the port that you selected locally. So basically the idea is that\nthrough SSH you create this channel, this channel is encrypted, and it's exposed somehow locally,\nand all the bytes that you send to this local port will be forwarded to the destination system.\nSo maybe the database running on RDS in a private subnet. And of course all the data coming back\nfrom RDS will be channeled through your own local port, so you can also read and write all this\nstream of bytes this way. So it's effectively a secure way to create a channel on the public\ninternet when you need to do something like that, and you want to make sure it's stable because it's\nthrough TCP and it's encrypted, so it's not going to leak any information along the way.\nSo you also mentioned that managing SSH keys is a security risk, so is there any alternative?\nLike if you don't want to take this security risk, what can you possibly do?\n\n\nEoin: Yeah, once you have long-lived keys we know that's an issue.\nWhen we talk about IAM access keys, it's the same problem. Where do you store them? Who has access to them? Or how do you revoke them?\nBut also another challenge is that with SSH you also have to figure out how do you collect logs\nand maintain audit trail of access and commands executed. That's quite common for various\ncompliance and governance scenarios. So there are a couple of ways in AWS that make this a bit easier.\n\n\nThe first one, which is maybe less common, is EC2 instance connect, and that gives you a way\nto have short-lived native SSH connections with short-lived keys. So it works with Amazon Linux\n2 and I presume the newer versions as well, and also Ubuntu AMIs. It doesn't work with other\ndistros and OSs. If you want to install it on older versions you'll have to yum install EC2\ninstance connect, and then at that point you need to make sure that the principal has a policy that\nauthorizes this special action which is EC2 instance connect send SSH publish key.\n\n\nThere will be a link in the show notes with examples and everything you need to do to get up and running\nwith this. When you use this it basically means that when you want to initialize a new connection\na new SSH key pair is generated on the fly. The public part of it is pushed into the EC2 updating\nthe SSH server configuration and the private part is kept local and used to authenticate the new\nconnection. Then the public key is removed from the server after 60 seconds so you don't have to\nso you have to have establish your connection within that time frame. And then once you do that\nyou can connect from the browser from the EC2 instance console and also with the AWS CLI.\n\n\nIf you do use the CLI then you have to regenerate the key pair yourself and run the SSH\nconnection command. So the fact that it's using SSH means that the instance still needs to have\nSSH installed and the SSH port has to be reachable and if you don't want your SSH port reachable from\nany public IP you can actually download the list of AWS public IPs and look for the ones associated to EC2 instance connect.\n\n\nSo there's a bit of work involved in that but the link for the IP range\nwill also be in the show notes. So that's EC2 instance connect and that uses SSH so it might\nbe a bit more familiar to people how that works. The other one which I think I've used a lot more\noften and is much more exciting in my view is Session Manager and it's a bit more advanced and a broader solution.\nIt works on Linux and it works on Windows and it even works on premises and on\nedge devices like IoT Greengrass. And Session Manager what it does is it provides this a whole\nsuite of things so it's not just connecting but it provides like auditable node management without\nthe need to have sockets open and have SSH keys and all of that and you're just relying on IAM\nfor authorization for everything. So you don't have to run any ports no SSH no RDP the only thing you\nneed to have for your EC2 instances is to have the AWS Session Manager agent installed. So this\nis an agent-based approach and once you have it all set up you can create a connection from either\na web browser or from the AWS CLI. All of the commands you issue can be logged then and made\navailable in an S3 bucket or in cloud which logs for auditing purposes.\nThe connection creation is also logged in CloudTrail as well. So you need because it's IAM based you need to make sure\nthat the instance profile your EC2 instance is running has specific permissions there's a managed\npolicy for that and you can also put in the permissions yourself. There's actually three\nkind of resources you need to deal with so you'll see it when you put in your actions in the IAM\npolicy you've got SSM for systems manager EC2 messages and SSM messages so you actually have\nthree namespaces in your actions that you need to use so it's actually if you if you look at the\ntraffic it's actually talking to three different hosts in the background and once you have that\nset up you can actually create SSH tunnels through Session Manager sessions as well so all of the\nstuff we talked about with SSH port forwarding can also work over Session Manager as well.\nSo it might seem like okay how does this work it doesn't use like normal TCP stuff it just uses\nIAM and AWS actions what kind of black magic is this how does it work in practice do you have any\nidea?\n\n\nLuciano: Yeah so that's something actually that a few years ago I was trying to do something similar and I bumped into this tool called Inlets and I got curious about like what is the black magic when\nlike you you don't have any inbound connection because you cannot literally reach that machine\nfrom the outside. How is it possible that you still can create a channel and connect to that\nmachine when you set up this kind of thing? So there is some kind of networking trick there\nthat I need to figure it out to really understand how is this possible.\n\n\nSo I investigated this Inlets project. It's an open source project you can check out on GitHub we'll have the link in the show notes.\nAnd I tried to figure out how it works and in practice what it does it's something really clever.\nBut also later on I figured out it's really common it's something that's been used for a while\nand even when you use tools like ngrok they do something similar right to expose your own laptop\nto the rest of the world if you want to showcase as like a website you are running locally.\n\n\nAnd the idea is that basically you don't accept any connection from the outside like starting from\nthe outside but you can start a connection from your own blockchain machine to the outside world\nand then create a channel that way. And then on that channel you can basically keep sending bytes.\nSo the trick is that you have to do the opposite. You have to initialize a connection from the\nmachine that is in the private subnet and then with that connection you can start to accept traffic.\n\n\nBut that means that the machine in the private network needs to initialize the connection\nwith some instance that has the capability to receive traffic from the public internet.\nSo you have to have this kind of, let's call it a service, that runs somewhere in the public internet and\nyour local machine is connected to that service and this intermediate public service is the one\nthat receives connection from the outer world and then creates this kind of tunnel.\n\n\nSo in the case of SSM, this is the way it works because even if you expect the traffic on your machine you will see\nthis machine reaching out to a domain that starts with ec2messages.something. AWS, so definitely\nthey are creating a tunnel this way by initializing a connection from inside the machine itself and in\nthe case of AWS of course they can use all the AWS ecosystem, they can use IAM as you mentioned Eoin,\nand make sure that you are authorizing all the kind of connection to happen.\n\n\nAt that point you basically have effectively this service being aware of all the connections that are\nestablished and you can also use that in SSM to track keep track of all the instances that you\nare running to distribute patches to all these instances, to see the health check, and basically\nit becomes kind of an overlay network that allows you to keep track of all the instances you are running and manage them.\nSo I suppose that kind of brings another question. If this is such a generic\nsolution to solve this kind of problem of exposing private resources in the public internet, is this something that we can use only for EC2 or can we use it outside EC2?\n\n\nEoin: There's probably a clue you've given already because we've said it's an open source agent so and that all it\nneeds is you know a connection to AWS services like ec2messages, SSMmessages, and then you can\nprobably imagine that it probably works in other environments and that's the case.\nIt works on ECS as well, so you can get it working on ECS containers and Fargate containers as well and\nthen not just shell into the host machine but actually into the containers as well.\n\n\nAnd that feature has a different name. It's called ECS Exec in that context and it has a slightly different\ninterface but it's still just SSM Session Manager. You always need to enable these things in advance.\nIf you need to troubleshoot and you haven't set this up in advance, you're kind of out of luck.\nOr you have to go back and make sure these things are turned on, so you have to enable ECS Exec in\nyour Fargate service and you'll also make sure you need those IAM permissions. Now Fargate doesn't\nneed all the permissions that EC2 needs because again, its host is managed, so you just need to\nmake sure you can do a certain number of SSM message actions from your task role in Fargate\nand then it can access SSM and then you can shell into containers to debug and troubleshoot and all sorts of good stuff.\n\n\nActually you can also run it outside of AWS as well, so that means you can run,\nif you've got instances on premises or if you've got hosts on other cloud providers or even on\nembedded devices, you can register these as managed instances in SSM and you can run the SSM agent on these hosts.\nThere's a special kind of sequence you need to do to activate and register these just to\nset up your security. You have like an activation code and then you register the instance and then\nit assumes an IAM role from this instance and it can run SSM agent and then you can connect via\nthe AWS console into your even your own laptop, for example, if you run the SSM agent on that.\nThere a cost actually associated with running that. Just because it's your machine, once you're running SSM\nagent on it there is a certain cost and you also need to enable a special advanced tier of\nSSM to do all of that. But you can imagine that if you're using SSM for some of the other things\nnot just SSH and bastion kind of stuff, but you're using RunCommand to be able to run the same\ncommand or a set of commands like documents across a fleet of machines that this becomes useful or\nif you're using patch manager to apply patches to thousands of machines, including on-premises machines, then this could be an advantage.\n\n\nLuciano: Yeah, so you are definitely settling Session Manager as the solution you would want to use if you are thinking to set up a bastion host.\nBut we also saw that it's not trivial, like there are a bunch of steps involved and you need to do many things\nright for that to work it's not just like one click and everything works.\nSo when we were reviewing the notes for this episode we were thinking is there any tool that would help you to\ndo all these things right? And we discovered this tool open source tool called basti by Bohdan Petryshyn.\n\n\nI hope i'm pronouncing the name correctly, and what basti does is basically a CLI tool\nthat allows you to provision a bastion host in the simplest possible way.\nSo it tries to reduce the amount of knowledge that you need to have to basically be able to provision\na bastion host and also create a tunnel for an RDS instance or for an ElastiCache instance.\nAnd the way works is it's basically a tool written in Node.js, so you can easily install it with 'npm\ninstall --global' and then the first thing that you can do once you have it installed is run the CLI\nwith the command basti init and what init does, it will start a guided procedure.\n\n\nOn the CLI, it will ask you for a bunch of questions. For instance, it's gonna list all your RDS databases\nand tell you which one do you want to connect to and based on that choice it will figure out\nin which VPC is that instance running and it will let you select a subnet where the instance\nis going to be provisioned. Also that instance is going to be provisioned with the right policies\nand it's going to create the right security groups so that you can do all the connection using SSM.\n\n\nAnd at that point, what you can do when you have everything provisioned, you can run a second command\ncalled basti connect and basti connect effectively is the part that connects to the instance that you\njust provisioned and creates a tunnel on your local machine.\nSo the only thing that you need to do at that point is select a local port and then you can basically use localhost on that port\nto connect with, I don't know, another CLI or maybe using a graphic client whatever you want to use to connect to your database.\n\n\nI did try to use this tool and I was very pleased with the developer\nexperience, like using it seemed very simple. I didn't even have to think much about what I was\ndoing, but unfortunately it didn't work the first time and this is definitely my fault.\nI did two very dumb mistakes that basti cannot really protect you against and the first mistake was\nthat I selected a private subnet rather than a public one. So basically my instance didn't have\nany connection from from the public internet. And of course I fixed the problem.\n\n\nStill didn't work and then I realized looking at the security group of my RDS instance that I didn't\nconfigure that security group to accept traffic from instances running on the public subnet.\nI was accepting traffic only from instances running on the private subnet, so I also needed to fix that.\nAnd at that point, basti connect worked straight away and I was able to connect with my graphic\nclient to the database and inspect the data. Another interesting thing that is worth mentioning\nabout basti is that it tries to keep the costs down even though it's running a very small machine.\n\n\nSo the cost would be minimal anyway. What it tries to do is it's something very clever what they do is\nwhile you run the basti connect command, they keep tagging your instance with like a timestamp.\nAnd then there is a cron job running on the instance itself that basically scan the instance\ntags and if you have been running well if the latest tag was older than a certain threshold\nthey will automatically shut down the instance which reduces the cost a bit more.\nAlso it makes it more secure because you are not always running that instance all the time but you run it\nonly when you need it so that's a really clever trick that they figured out and I was\nreally pleased to to look at the code and see how they implemented that.\nYeah so definitely checkout basti and let us know if you like it as well and maybe if you are into open sources and other\nprojects worth contributing to. But are there alternatives to Bastion hosts?\n\n\nEoin: You already mentioned inlets. You might want to look at other kind of similar tools in that realm. Tailscale is one that I use quite a lot for not necessarily for like production deployments on\nAWS but between my own machines and hosts. And you can even connect from your phone to your host.\nI actually use this sometimes when I'm building docker images on x86 hosts and I don't want to do\nit on my Mac and cross compile. I'm using a remote docker host and I can use TailScale to do all the\ntunneling for that. It's really nice has a really good user interface and manages all your devices.\n\n\nThat's built on top of a technology called WireGuard which is more or less a VPN solution.\nYou could also use a more traditional VPN like OpenVPN, or you on AWS you could use like a Client VPN.\nSo those are other ways of securing access to environments. You could also like really just\ndo a basic solution if you wanted to access something like ElastiCache or RDS, you could create\na Lambda function that accepts custom commands and executes them. I know that's\nsomething I've seen in the past for very simple access scenarios\nbut really I don't think it's really worth the investment and I would recommend that\npeople go with something like an SSM based solution and set that up in your developer\n, staging, production environments from the get-go. It makes it a lot easier in the long run.\nYou wouldn't be able to run a graphical client or have proper open sockets through a Lambda function of course. It's still it's a simple solution for very simple tasks.\n\n\nLuciano: Yeah i'm definitely guilty of implementing the solution many times and I think just because I always found until now recently that provisioning\na bastion host the right way was more complicated than I wanted it to be the moment where I needed it but\nhopefully with all the research we did in this episode and trying all these different tools now\nit is much easier and I won't need to go for a Lambda anymore to satisfy these kinds of use cases.\nSo I think that's everything we have for today. I hope you enjoyed this episode. If you did please\nremember to like this on YouTube and subscribe and if you have listened to the audio podcast, please\nleave us a review, and if you have any comment, if you have been using other solutions that maybe we\ndidn't mention, please let us know because of course. We'd like to hear from you and we'd like\nto learn from you and then share these learnings with other people. So thank you very much and we'll\nsee you in the next episode.\n"
    },
    {
      "title": "79. How do you do Functionless APIs?",
      "url": "https://awsbites.com/79-how-do-you-do-functionless-apis/",
      "publish_date": "2023-05-05T00:00:00.000Z",
      "abstract": "In this exciting episode of the AWS Bites podcast, we're diving into the fascinating world of functionless applications. Yes, you heard it right! We'll be exploring how reducing the number of lambda functions can simplify your applications, resulting in lower latency, no cold starts, and cheaper costs.\nBut don't worry, we still love lambda! We'll be explaining the pros and cons of this approach, taking you through a step-by-step guide on how to use service proxies and manipulate the input for the target service using VTL.\nAnd that's not all! We also share some helpful resources for those interested in learning more about this approach, including blog posts from some of the brightest minds in the field like Alex DeBrie, Sheen Brisals, and Paul Swail.\nSo, tune in and learn how to simplify your applications, reduce costs, and take your AWS game to the next level with functionless applications!\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nBlog post by Alex DeBrie on API Gateway Service Proxy\nAnother blog post on Service Proxy by Sheen Brisals\n&quot;Some code is more equal than others&quot; by Paul Swail\n\n",
      "transcript": "Eoin: In today's episode, we'll be discussing a fascinating topic,\nthe benefits of functionless applications.\nYep, you heard that right.\nWe'll be exploring how to reduce the number of Lambda functions.\nDon't worry, we still love Lambda.\nBut let's be fair, reducing the number of Lambda functions\ncan simplify your applications, resulting in lower latency,\nno cold starts, and cheaper costs.\nSo today we'll learn how to use service proxies to forward requests\ndirectly to AWS services without the need for a Lambda function.\n\n\nMy name is Eoin, and I'm here with Luciano\nfor another episode of the AWS Bites podcast.\nAWS Bites is sponsored by fourTheorem.\nfourTheorem is an AWS partner for migration, architecture, and training.\nFind out more at fortheorem.com.\nThe link is in the show notes.\nSo what are we talking about today?\nWell, let's say you've built an API with API Gateway,\nand you want to forward the request to another AWS service like SQS.\n\n\nThis is a really common pattern.\nYour Lambda function isn't necessarily doing that much\nexcept wiring the database to the other server.\nSo let's say you're building an API with API Gateway,\nyour Lambda function isn't necessarily doing that much except wiring the data\nyou get from the API into the service at the backend.\nSo you don't necessarily need to use a Lambda function to do this at all.\nYou can use service proxies instead.\nYou're still using API Gateway, but you're skipping the Lambda step\nand forwarding the request directly to the destination service.\nSo Luciano, are there some example use cases\nthat this is particularly good for?\n\n\nLuciano: Yeah, this seems pretty exciting, and I'm already thinking about some potentially interesting use cases.\nFor instance, I think a very common one is you want to store click data\non an e-commerce is one we always mention, right?\nAnd if you do that through an API,\nbecause you want to collect this data through the web,\nmost often you create a Lambda, and then you send that data to Kinesis.\nSo why do we have that Lambda in the first place just to do this mapping, right?\n\n\nIf you could remove that Lambda, it would be nice if you could just send the data\nfrom API Gateway directly to Kinesis.\nSo that would be a really cool example, I believe.\nSimilarly, other examples we've mentioned many times\nis you just need to put records to DynamoDB.\nSo there is a very specific type of request you receive through an API call\nand probably want to store most of it in a DynamoDB type as a record\nwith some maybe minimal data manipulation.\n\n\nWe generally used to do that with a Lambda. Do we really need that Lambda?\nMaybe we can do the data manipulation by using this new feature.\nAnd another one could be if you want to do some kind of event-based system,\nmaybe you are receiving, I don't know, jobs that you can process from an SQS queue.\nSo you receive that request through the web, API Gateway receives the request,\nyou just want to send that payload to SQS.\n\n\nThat's one example. Or similarly, you might be using SNS or Event Bridge\njust to propagate an event.\nAnd I think it's important to call out what are the pros and cons of this approach,\nbecause of course it's not a silver bullet for everything.\nI think we still use Lambda for many other use cases.\nSo let's try to figure out when this kind of approach is useful\nand when it gets a little bit more challenging.\n\n\nSo in terms of pros, for sure, lower latency,\nbecause you have a direct integration between two services.\nYou are not waiting for a Lambda to spin up, to execute and do all the API calls,\nbut that API call will be done directly by AWS.\nSo most likely much lower latency if you use this approach.\nAnd again, that means no call starts.\nSo, and not creating a new Lambda for that specific use case.\nSo that's probably giving you more capacity available in your Lambda pool to do other stuff.\n\n\nSo if you're not in your account, you have lots of Lambdas,\nthat's kind of giving you more space to do more useful work with your Lambdas.\nIt's going to be cheaper because again, you are not executing that compute\nand you're going to be paying for that compute.\nSo that's going to lower down your cost.\nAnd in general, I think there is a concept of you are going to be maintaining less code.\nYou will have less code to understand, write, deploy.\n\n\nSo I think in general, the cognitive load of the team\nor people working in this project in general, it's going to be much lower.\nNow, it seems amazing. So where does it fall down?\nWhat are the cons of this approach?\nWell, the first one and the most obvious one is that you have to use this VTL templating language,\nwhich I haven't heard anyone actually loving it so far.\nSo that's going to be definitely on the con side.\n\n\nSo it's basically a language that allows you to do some kind of templating to map\nhow are you going to reshape the input data to send the request to the downstream AWS service.\nAnd then also you have to send a response back to the client.\nSo how do you manipulate data to create that response as well?\nSo to define all this mapping, you need to use VTL.\nAnd again, it's not the most intuitive language ever.\n\n\nIt's not super easy to learn. People complain a lot about that.\nAnd it's also a little bit tricky to find a good example.\nSo that's maybe one of the reasons why nobody's really loving VTL.\nIt might be a bit complicated to set up and it's not well supported by some serverless framework.\nSo you might need to be spending a little bit of time there just to figure out how do you make it work.\nAnd the other bit is not going to be super straightforward to debug\nbecause maybe you set up something, then it doesn't really work.\nWhere do you even start looking for trying to figure out what did you do wrong?\nAnd we'll have some suggestions later on on this.\n\n\nEoin: I remember using VTL or Velocity template language,\nI think it was like back in 2006 or something like that,\nlike building a custom reporting solution.\nSo there's lots of templating languages out there.\nVelocity is one of the older ones.\nSo it's an interesting choice.\nIt's based on an Apache project from the Java ecosystem.\nAnd it's okay when you're just using it as a templating system.\nYou have your inputs, you can run the template, generate the outputs.\n\n\nBut when it's tightly integrated into API Gateway,\nit can definitely be difficult to troubleshoot.\nSo the whole experience of building this end to end, let's give an example.\nLet's say you want to build an API to receive some data from a sensor.\nSo for example, let's say in my greenhouse and growing tomatoes,\nand I want to monitor the temperature of this greenhouse.\nSo I've got a sensor in there.\nAnd this supports sending the data from the sensor to a HTTPS endpoint.\n\n\nSo I want to bring up a HTTPS endpoint where it can send the data,\nthen I can monitor the temperature of the greenhouse,\nfind out it's not getting too hot and turning my tomatoes into pizza sauce.\nSo how would we do this?\nWell, you'd start off by creating your REST API resource in CloudFormation,\nas we would normally do.\nYou normally have to create the API and a stage and then a resource type,\nall this usual kind of hierarchy of resources\nuntil you get down to the actual HTTP method.\n\n\nSo at that point, you want to say, okay, here's my post action\nfor this sensor readings resource.\nAnd it's in this method where you configure this AWS service proxy integration.\nSo with these integrations, there are different types you can choose from.\nAnd then one you'd normally use for a Lambda function is AWS proxy.\nBut you can also have a HTTP type that's just used for proxying the request\nto another web server.\n\n\nYou have the Mock one, which is also quite commonly used,\nespecially for just generating static responses like options,\nmethod responses for cores.\nThat's quite common.\nIf you use the serverless framework or SAM,\nit'll normally generate those ones for you\nand you don't even have to worry about it.\nAnd then there's the AWS integration type.\nSo when you use the AWS integration type, that's the one that says,\nokay, I want you to integrate this API gateway method into an AWS service.\n\n\nAnd then you specify a URI, and that's essentially an ARN for the AWS API.\nSo it's something like SQS actions send message.\nBecause you're making that API request,\nyou need to give this method a role that it can assume.\nSo it's a role with a trust policy that trusts API gateway\nand allows you to perform whatever action you want, like a SQS send message.\nAnd then you could basically use your velocity templating language, your VTL,\nto map the input you get into the input that is required by that API.\n\n\nAnd that's usually a little bit where it starts to get difficult,\ndifficult to develop, difficult to troubleshoot,\nespecially because sometimes you need to basically generate\nform encoded syntax.\nSo it's not as easy as just generating JSON always.\nBut once you've done that, you can also do the same thing with the response.\nSo if you get a response back from this API,\nyou can either return a static response for different error codes\nand different even content type headers,\nyou can return different responses back to your client.\n\n\nSo there's nothing you can shield the underlying implementation completely\nfrom your API consumers.\nThey don't have to know SQS or Kinesis or whatever it is under the hood.\nSo in a lot of ways, many people who've worked with AppSync\nare actually a lot more familiar with this\nthan people who are working with API gateway\nbecause with AppSync resolvers traditionally are also using VTL.\nThe difference is now in AppSync,\nyou have the great new option of using JavaScript resolvers.\n\n\nThat option isn't there yet in API gateway,\nbut it's something that might come, who knows?\nAnd by the way, this kind of integration is supported\nin the version one and version two of API gateway.\nSo the rest API method and in the HTTP API method.\nWe did mention that debugging all of this stuff can be tricky,\nbut you can use the test feature in API gateway console,\nwhich can give you some details on all the steps of your API\nas it goes through the different life cycles and gives you logs as well.\n\n\nAnd also you'd want to probably set up some good local development tooling\nwhere you can run these templates with sample inputs as well.\nThat just makes sense.\nAlmost the same as if you're doing a step function development really.\nI think that's probably everything we can cover on the topic,\nbut there are some really good resources out there actually.\nSo there are some people who have been talking about this functionless approach\nto developing serverless applications.\nThere's a really good blog post from a few years ago from Alex DeBrie\nwith a great example of doing this with SNS.\nSheen Brisals also has a great article on writing functionless applications,\nwriting less code and covers a lot of ways to reduce the number of lambda functions\nin your architecture, including a service proxy example.\nAnd there's a great article on this style of development as well from Paul Swail\ncalled Some Code is More Equal than Others.\nSo we'll link all of those in the show notes.\nAnd we're very interested to hear if anybody there has done something really cool\nwith service proxies as well.\n\n\nLuciano: 🍅 Like growing tomatoes, right?\n\n\nEoin: 🤣 Mm-hmm.\nSo that's all we have for this episode.\nThanks very much. Don't forget to like and subscribe.\nAnd we'll see you in the next episode.\n"
    },
    {
      "title": "80. Can you do private static websites on AWS?",
      "url": "https://awsbites.com/80-can-you-do-private-static-websites-on-aws/",
      "publish_date": "2023-05-12T00:00:00.000Z",
      "abstract": "In this episode of the AWS Bites podcast, we discuss the challenges of hosting private static websites on AWS.\nWe explore why it's important to host internal corporate applications and line of business applications only for internal consumption, and the requirements for doing so.\nWe also evaluate different options for hosting private static websites, including S3 with CloudFront, containers on ECS/Fargate with ALB, API Gateway, and AppRunner.\nFinally, we summarize the pros and cons of each option and provide a rating for each.\nIf you're looking to host a private static website on AWS, this episode is a must-listen!\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nOur previous episode &quot;How do you deploy a static website in AWS?&quot;\nOur previous episode &quot;How do you use Lambda response streaming&quot;\nOur previous episode &quot;How do you do Functionless APIs?&quot;\nOpen issue on GitHub for private hosted zones support for App Runner\n\n",
      "transcript": "Luciano: Hosting a public website on the internet is something pretty easy.\nYou know we love static websites, we've been talking about them quite a lot\nand you have seen probably some of our work around them. But we also work with a lot of\nenterprises and they often want to do static sites but they want to keep them private for\ninternet consumption, which means no connectivity from the outside and only connectivity from the\nenterprise network. So if hosting static website publicly is very easy, doing it for private ones\nis a completely different story. And today we're going to talk with you about why something that\nseems very easy, it becomes actually very very hard when you try to do it in a private way.\n\n\nThere are actually many solutions so we want to discover today what can you do and what are the\npros and cons of every single solution. I am Luciano and I'm here today with Eoin and this\nis another episode of the AWS Bites podcast.\nAWS Bites is sponsored by fourTheorem. fourTheorem is an\nAWS consulting partner offering training, cloud migration and architecture consulting. Find out\nmore at fourTheorem.com. You'll find this link in the show notes. Okay so let's start by summarizing\nwhy you might want to use a static website in a private mode, right? Because maybe if you haven't\nworked with enterprises it's not something that can be necessarily too obvious.\n\n\nAnd I think that the main thing to understand is that companies of a certain size they will often have what they\nwould call internal applications or internal corporate applications and those are applications\nthat they don't need to expose to external customers or external stakeholders but something\nthat they use internally to fulfill specific tasks. And you can imagine something like\ndocumentation, wiki, so things, information that they want to make available inside the\ncorporate network but it can also be a little bit more advanced than that.\n\n\nSo if they want to expose some kind of functionalities, some kind of interactive website they can do that using a\nsingle page application and this single page application will be totally static.\nIt's just HTML, CSS and JavaScript and it will be relying on some internal APIs for more dynamic functionality.\nAnd one example for instance could be, I don't know, you can imagine a bank that\ninternal needs to have a way to browse, I don't know, mortgage application maybe and they can do\nall of that using an SPA and that SPA can call internal APIs and they can make it available only\nfor internal consumption. Now again this just emphasizes how it is very common for web application\nto have a static part and a dynamic part. So we are trying to figure out how do we host that\nstatic part in the most efficient way or in the simplest way and the idea is that it's going to\nbe only files, specifically HTML, JS, CSS, maybe some images, maybe some other assets like JSON,\nJSON, whatever and we need to have an easy way to serve those assets using HTTP.\nAnd for public websites, I know you are probably already thinking this is very easy, you will expect to be able to\ndo that in like minutes just taking a repository from GitHub and deploying from there and if you\nhave used Netlify or Vercel, these are the kind of experience you will get and we actually have\nspoken about that in a dedicated episode, check it out, episode 3, link is in the show notes if\nyou are curious. How does it get difficult if you want to do it for private websites?\nWhat are the requirements there I guess is the question.\n\n\nEoin: A lot of the requirements are the same as when you're doing a public app so you want HTTPS, normally you want a custom domain so people can find it easily. You probably want a low latency\nas possible for fast load times, sometimes that matters less in the corporate context but not\nreally, people's expectations for web applications is generally just going in one direction and you\nwant support for large and small assets so not too limited in terms of asset size. Where it gets\ndifferent is that unlike a public app, like you said, it should only be accessible from a corporate\nnetwork, it should not leave the network boundary, often you also have a requirement to use an\ninternal DNS service, also sometimes a third party HTTPS certificate vendor or provisioning mechanism\nand sometimes even you have a requirement that even for DNS records there's no public DNS trace\nso even though it might be acceptable for a lot of organisations to have some public DNS resources\nthat resolve to private IP addresses, that's not always the case and sometimes it's just a hard no\non things like that. So I guess why can't we just use something like S3 then? That's probably\nsomebody's first answer when you think about static websites on AWS.\nYeah and I mean this is something that was also my first idea, right?\n\n\nLuciano: Sounds almost obvious that you just want to serve files in AWS, that's a string, right? That's what S3 is for. But there\nis a slight caveat there which is S3 is really good because it kind of offers that website feature\nbut it's not quite there because it only works on HTTP so extremely annoying that you have like 90%\nof what you need and that 10% becomes kind of the hard no that kind of stops you from using\nthat feature. So that's maybe my wishlist request for AWS to please, please, please support HTTPS\non S3 websites that will make some of these things much easier and make S3 a viable option here.\n\n\nNow the next best solution that we might think of if we still want to use S3 because S3 is\nreally good for hosting files and keeping them around versioning, storing them in a reliable\nway and all the different things we know about S3. The next thing is probably, okay, can we just\nput CloudFront in front of it? And this is actually the most common way of serving public\nstatic websites. In AWS you put the files in S3 and you create a CloudFront distribution\nto distribute the files. And with CloudFront you get HTTPS, you even get cache, you get\ncustom domains. So it looks like a good solution and it is definitely for public websites.\n\n\nNow when you want to make it private, there are some things to consider there. First of all,\nyou are already buying into a level of complexity that you probably don't need. Like why do you need\na CDN? I think unless you are like such a huge organization with offices all around the world\nand you want to distribute that internal application in a way that can be performant\nfor everyone, maybe even in different continents, maybe okay. It could make sense to think about a\nCDN, but for most use cases probably that's a bit of an overkill.\n\n\nThe other thing is that some assets somehow still need to be unsecured. Like let's say that you have a login page,\nmost likely you would want to have a login page. How do you do all of that? Maybe you can use some\nkind of IDP and then MFA and all that good stuff. But once you have all of that, how do you connect\nit with the CDN distribution? And there you could be using Lambda at Edge and do kind of a dynamic\nlayer of authentication there to make sure the user is authenticated before you serve that\nstatic content through CloudFront. Another thing is that you need to actually keep it private.\n\n\nSo how do you make it possible that if I am accessing from the internal network, I am authorized to see\nthat content, but if I am somewhere else, I shouldn't be able to see anything? So one way is that\nyou could be setting up a web application firewall in front of CloudFront and with a web application\nfirewall you can define IP filtering. So you could say this class of IPs will be able to pass,\nanybody else is going to be blocked. And if you use private classes of IPs, that should be a\nsecure enough approach because I think it's not possible or if it is, I think it's easy to spoof\nprivate IPs. So that should be a good way to create that kind of security boundary.\n\n\nNow there is a next issue though, which is companies will tell you, okay, but then the\ntraffic is not flowing through my VPC because effectively we have to think as CloudFront as\nlocation at the edge, which means that these are servers that live outside the data centers, the\nAWS data centers. So you cannot really have kind of a VPC only flow of data. You need to go around\nand that's not always desired because companies would want to track all the traffic in VPCs. They\noften use VPC flow logs. So they are not too happy if you tell them, well, some of your traffic is\ngoing to go in the public internet anyway. So that's another big no-no.\nthis approach is way more complicated than we wanted it to be. It looks feasible.\nIt looks nice because you can use S3, but then there is a lot going on there and setting it up correctly and\nsecurely. It is a lot of work and still there are some areas where it's still not perfect.\nSo what's the next idea Eoin?\n\n\nEoin: I think after that, at this point you'd probably think, okay, well, the way we used to do this is just to run it in a container and have something like Nginx server\nstatic content. So let's have a look at that. And again, this is probably more complex than you\nmight think. And the way you would normally do it is like, let's say take ECS or Fargate,\nyou'd package your Nginx application, maybe with the static content in it, or else the Nginx is\npulling and streaming from S3 in the background. And you can put an application load balancer in\nfront of that. Then of course you have to make sure it's connected to your VPC correctly.\n\n\nAnd then you're running your container in multiple availability zones. And with the load balancer,\nyou can attach your HTTPS certificate. You can integrate it with Route 53 private hosted zones\nor third-party DNS, and you're ready to go. I still think it's like quite a lot of work\nfor what you're trying to do here. If we go back to one of your first ideas, if we just had HTTPS\nin S3 and a private option there, you could use your resource policy and restrict it to your VPC.\n\n\nThat would simplify the thing a lot, but it seems like if you were to compare\nthis kind of container setup to just running a very simple API with Lambda, it's a lot of work.\nAnd since we're mentioning API Gateway, I mean, there is a workaround here that maybe some people\nwould think of in this case, which could be a bit easier, which is you could say, well,\nlet's just use our API Gateway to serve our static content instead. And that's something that can\ndefinitely work. You can imagine an API Gateway endpoint with a Lambda function behind it, and it\nwill take the content from your S3 bucket and push it through back to your response.\n\n\nYou could also do like a service proxy integration, like we talked about in the last episode,\nand fetch data directly from S3 and go back through the API Gateway. But there's a couple\nof limits there, and we talked about these limits recently when we were talking about response\nstreaming with Lambda. You have a 10 megabyte payload limit in API Gateway. So that could be a\nbit of a blocking issue. You can, of course, if you're using Lambda, you can stream the response,\nlike we mentioned with response streaming. That gives you the time to first byte benefit,\nwhich is important for this kind of scenario. And it will also give you the opportunity to go over\nthe 10 megabyte limit. But I don't think response streaming really helps you for this case, because\nyou only get that benefit with Lambda function URLs. And Lambda function URLs are not recommended\nfor production by AWS, really. They are for testing Lambda functions, really, and not for\nthis kind of highly secure corporate environment. So the other thing is that private custom domain\nnames in API Gateway, you still need a load balancer anyway. So it seems like no matter what\ndirection we turn to, things are just getting more complex instead of simpler. Is there any hope\nfor a simpler approach, Luciano?\n\n\nLuciano: There is maybe one. I mean, recently, we've been looking into AppRunner, which is a relatively new service from AWS. And the nice thing with AppRunner is that\nit tries to be a lot more like a SaaS offering, where in a way similar to what Vercell or Netify\nwould be doing for you, where you just provide a repository or a container image, and you don't\nhave to worry too much about everything else. AWS will take care of load balancers, network,\nscaling groups, and all the different things that we generally have to create ourselves when\ndeploying things on AWS. So it's kind of trading convenience for less control. You don't see a lot\nof the stuff that is going on, but you get a lot more convenience. And you can definitely run\nstatic websites on AppRunner publicly. You just do a container that will serve your static assets.\n\n\nBut what it looks like for a private alternative, can we do that? And until very recently, you\ncouldn't really do private containers using AppRunner. But this has just changed because\nnow you can create private endpoints in AppRunner, which are only accessible from a specific VPC and\nuses the VPC interface endpoints feature. So we are going to have a link in the show notes if you\nwant to deep dive on this feature. But this is what is kind of making us reconsider AppRunner for\nthis particular use case. And we could get this up and running by following a few steps. So definitely\nwe need to define the container image. So what I could think is the simplest way to serve static\nassets is to just spin up an NGINX container. You make sure to bundle all your static assets in the\nNGINX HTTP folder. And then that container should be a good enough starting point for serving those\nstatic assets on the web. Now we need to, the way that you expose this stuff to AppRunner, again,\nis either to a repository, for instance, on GitHub, or through an ECR registry. So you can decide\nwhether you want to publish stuff directly to a registry or keep a repository and then AppRunner\nwill abstract the publishing process for you. You also need to create a VPC endpoint, interface\nendpoint, and something that is called ingress connection. That is what is used to link that\nendpoint to the AppRunner service. Finally, you need to set up a custom domain. And that is where\nthings get a little bit hairy. And because that forces you to use a public DNS record.\nAt least it doesn't support yet private hosted zones, but there is an open issue on GitHub. So\nhopefully that's something that is going to change soon enough in the future. So I think that's\nquite a reasonable option, but is it what we would recommend today or do we have a different\nfinal recommendation?\n\n\nEoin: I think given everything we've said, it depends on what category you fall into, but let's assume that in a strict corporate environment, you can't let anything be visible\noutside your organization. So no public DNS, everything is completely strict, all within the\nnetwork boundary. You're not going for a zero trust policy on this at all. So if you have that\ncase, then I think the most pragmatic solution is to go with that Fargate container and load\nbalancer approach that you described. The way I do it though, is I say, okay, within a corporate\nenvironment, don't make every team do that every time they need to deploy a static website.\n\n\nInstead, provide centralized platform that where you do this once and pretty much leave it running\nand you just monitor it and support it like you do everything else in your platform. Just have\na centralized bit of infrastructure with your Fargate container application load balancer,\nand allow people to just publish containers somewhere to start that content, or just put\ntheir content into a bucket and automate the process of making that available then as a static\nwebsite with a certificate and a domain and all of that good stuff. Just as if S3 already had that\nout of the box for a private corporate network. And you can also use application scaling at Fargate\nto make sure it scales to your needs. So while this is a complex enough setup for just a single\nweb application, if you've got some strict applications, strict requirements, and you've got\nmultiple teams that you want to allow them to publish with very minimal manual effort, static\nwebsites for different line of business applications, then I think this is a good compromise\nsolution.\n\n\nLuciano: I like this approach because I think it kind of removes that on one side it still gives you the S3 option, which is the simplest interface I could think of if I just want to publish static\nassets. On the other side, if you have a platform that takes care of updates and making sure\neverything is always up and running with the most secure setup, and you avoid duplicating all of\nthat work for every single team for every single project, I think that seems like a very good\ncompromise. So it would be nice to be able to suggest some open source solution that does that,\nbut I don't think I'm aware of one. So if you know of something like this, please leave us a comment\nin the comments box below. So let's maybe try to wrap up this episode by summarizing all the\noptions we mentioned today. So we have S3 with CloudFront, which is good performance,\nglobal distribution, HTTPS support, but it's definitely an overkill. The CDN is still public,\nand you are relying on IP whitelisting. So good enough, but not quite. Then we discuss containers\non ECS Fargate with ALB, which is good because it meets all the strict requirements. It's probably\neasy enough for new teams to add web applications when they need it, but it would be a bit too\ncomplex for a single web application because you need to set up a lot of stuff up front.\n\n\nThen we have API Gateway, which could be used with Lambda to fetch from S3. It doesn't need\nto have a multi-xid setup and load balancers, but then it's limited to 10 megabytes, which could be\na very strict requirement, maybe if you're serving big files, I don't know, maybe you have big images\nor other kind of big enough payloads. Also doing private custom domains is not supported yet, so\nthat's another big issue. Finally, we have AppRunner, which is an interesting solution. It is\nprobably something that we will need to revisit more in the future. It might become one of the\nbest solutions if they enable certain features. As of today, it's simple enough to set up, it's\nmulti-xid, it can give you private endpoints, but then you still are running public DNS records,\nso that's something that some companies might not be entirely happy with, and they might just\ndisregard the solution because of that. Hopefully, that issue will be eventually closed and we will\nhave that support, which might make AppRunner one of the best options out there for AWS.\nSo that's everything we have for today, and again, if you think that we missed any idea that you\nmaybe have solved this problem in a different way and you want to share your approach, we'd love to\nhear that. I think we spent significant amount of time thinking about this problem. For sure, we are\nmissing some option there. I'm sure there are many other combinations of AWS services that you can\nuse to achieve something like this, so if you know any one of them, please share it with us. We'd love\nto hear from you. Thank you very much, and we'll see you in the next episode.\n"
    },
    {
      "title": "81. Remote VSCode with EC2 and Cloud9",
      "url": "https://awsbites.com/81-remote-vscode-with-ec2-and-cloud9/",
      "publish_date": "2023-05-19T00:00:00.000Z",
      "abstract": "Are you tired of being stuck in your local development environment? Do you dream of coding from a beach in Sicily? Well, get ready to make those dreams a reality with this episode of AWS Bites!\nToday we are here to show you how to use VSCode to develop against a remote Cloud9 instance on AWS.\nYou'll learn how to edit in VSCode instead of using the Cloud9 editor, so you can take advantage of the power of the cloud and code from anywhere while staying in the comfort of your favourite code editor.\nWe'll cover two ways to edit in VSCode: with SSM and with VSCode Tunnels. With these setups, you can code from home, a coffee shop, or even a beach in Sicily (if you like granitas and sunshine). Plus, you'll get to use that fancy iPad you spent all that money on (also) for coding!\nGet comfortable and let’s learn how to set up our next remote coding environment!\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nVisual Studio Code Server\nOur previous episode &quot;When do you need a bastion host?&quot;\nInstructions on how to set up your own VS Code integration with Cloud9\nOur live series stream &quot;Coding a serverless file transfer app on AWS&quot;\n\n",
      "transcript": "Luciano: Are you tired of being tied to your local development environment when working on your cloud projects?\nWell, today we'll talk about how to use VS Code to develop against a remote cloud 9 instance running on AWS.\nWith this setup, you can go from anywhere and take advantage of the power of the cloud.\nSo whether you're working from home, a coffee shop, or maybe on a beach in Sicily,\nyou can continue developing your project without worrying about hardware limitations.\n\n\nIf you are like me, you probably spend a lot of money to get an iPad,\nand you might be happy to know that you can finally use it also for coding.\nSo in this episode, we'll talk you through the process of launching a cloud 9 IDE\nand two different ways that you can use to basically edit the code using VS Code\ninstead of using directly cloud 9 as an editor.\nMy name is Luciano and I'm here with Eoin, and this is another episode of AWS Bites podcast.\n\n\nAWS Bites is sponsored by fourTheorem.\nfourTheorem is an AWS consulting partner offering training, cloud migrations, and architecture.\nFind out more at fourtheorem.com. You'll find the link in the show notes.\nSo let's start with the question, why would you want to remotely develop with VS Code?\nAnd I think there are some use cases here that are interesting,\nand maybe not just the fact that you might be in Sicily enjoying some sunshine.\n\n\nMaybe somebody calls you because there is an urgent bug to fix,\nand you have to struggle to fix it without leaving the seaside, right?\nSo that could be one use case, but let's think about other use cases.\nMaybe a simple one that actually is something that happens to us quite commonly in our line\nof work is when we have to do training or when we have to deliver a demo.\nMaybe you want to have some kind of isolated development environment\nto give people the opportunity to play around, but without having them to think,\nokay, I need to set up an entire development machine.\n\n\nSo maybe we can give them something a little bit easier.\nThat's actually, I think, one of the most representative use cases here.\nAnother one could be when you have many developers in an organization,\nand it might be tricky to set up the development environment for everyone in kind of a uniform way.\nSo if you can set up something in the cloud,\nthat could be a solution that gets people started very quickly,\nand they are not also limited by specific hardware.\nAnother one is that if you cannot access the network,\nthe specific AWS network resources from your laptop,\nthat client needs to be in AWS.\nSo by setting up the development environment directly in AWS,\nyou can easily work around that limitation of being physically present in that network\nwith your development machine.\nSo what kind of options do we have here to start to set things up?\n\n\nEoin: Yeah, so maybe just for a bit of context first.\nSo in case people haven't used Cloud9,\nit's basically an AWS service that gives you a development environment.\nAnd how it does that is you can either use an existing EC2 instance,\nor it'll spin one up for you,\nand then it gives you a code editor and a terminal in the browser\nthat you can use to do your development.\nAnd it's quite common to use this for training and once-off environments.\n\n\nBut the Cloud9 editor is pretty good,\nbut it's not as good as all of the best editors out there like VS Code or Vim,\nwhich is obviously the best.\nThere's a couple of things you can now do if you want to get around that limitation.\nAnd in fact, these methods will also work with just an EC2 instance.\nIt doesn't have to be Cloud9.\nSo what you need to do to get started,\nif you want to use VS Code on your local machine,\nis install the VS Code remote development extension.\n\n\nAnd then there's two methods here.\nOne is using SessionManager.\nAnd we just talked about SessionManager as part of the Bastion host episode.\nAnd one of the things we mentioned that SessionManager can do in SSM\nis it can provide you with SSH tunnels.\nAnd that's the secret ingredient here to making this work.\nSo if you've got AWS CLI installed,\nand then you've got the SessionManager plugin for AWS CLI installed,\nthen you can set up this tunnel.\n\n\nAnd the process for that, there's a few steps,\nand I'll just run through them really quickly.\nYou need to have an SSH key.\nSo you might have one already,\nor you might create one just for this purpose.\nSo you do your SSH keygen to generate that.\nOn your Cloud9 instance, then you can go to the shell in that instance.\nYou need to find out your instance ID.\nSo you can get that from the EC2 console,\nor you can curl it from the metadata service in EC2.\n\n\nOnce you have that instance ID,\nwhich is the one that begins with `i-`,\nthen you also need to add the public key\nthat you generated to your authorized keys on the Cloud9 instance.\nSo that's basically authorizing your client to connect to this SSH server.\nAnd the SSH server is already running on the Cloud9 instance.\nSo you don't have to start it or configure it.\nThen on your client machine, this is where the magic happens.\n\n\nSo you edit the SSH config on your local machine,\nand you need to add in basically a rule that says,\nwhenever my host matches the syntax, which is `i-*`,\nthen create this proxy locally.\nAnd by the way, the instructions for doing this are in the SSM documentation.\nSo this isn't unique to what we're trying to do here.\nIf you search for how to create an SSH tunnel with SSM,\nwith SSM, you'll find these instructions.\n\n\nAnd we can also put a link in the show notes.\nAnd that means that every time you do SSH to an `i-` host,\nthat it will basically intercept that\nand create an AWS SSM start session command with the CLI.\nAnd it will create this local tunnel that goes over SSM,\nuses IAM authentication to do the SSM.\nAnd suddenly you've got a socket between your host and the remote Cloud9 instance.\nSo then you can just SSH directly into your Cloud9 instance.\n\n\nFirst check that that works.\nThen you can also create a few additional parameters in this SSH config,\nwhich VS Code can then pick up.\nSo you basically make sure you specify your Cloud9 host\nand the user you want to connect as, which is usually EC2 user.\nAnd that's it. That's all your setup done.\nNow you can just go VS Code, do control shift P or command shift P\nand select the connect to host option.\nAnd this host then, VS Code will automatically pick up this entry from your SSH config\nand allow you to connect to it.\nAnd right away, you're up and coding on this remote instance.\nYou've got your VS Code environment on your laptop\nwith everything you've configured on.\nYou can, you get your autocompletion and your IntelliSense.\nYou've got the VS Code terminal.\nYou can do debugging, everything just works out of the box.\nNice.\n\n\nLuciano: I haven't seen before this proxy command thing with SSH, but sounds pretty clever and pretty useful in this,\nbut in this particular case, I think it's useful also to summarize all the steps\nother than just linking the official documentation,\nwe'll summarize all the steps and make them available in a gist on GitHub.\nAnd the link will be in the show notes.\nSo you can easily just copy paste the commands of everything we just described\nand be running very quickly.\nSo what's the second option?\nBecause you mentioned there are two options there.\nThe second option is a lot simpler.\n\n\nEoin: It's a lot easier.\nMaybe we should have gone with it first,\nbut the second option isn't maybe isn't available to everybody,\ndepending on what kind of environment you're in.\nAgain, if it's an enterprise corporate restrictions, who knows?\nBut the other option is using VS Code tunnels,\nwhich is their inbuilt remote development mode,\nwhich doesn't use SSH.\nIt basically it's like doing its own version of how we described SSM working.\n\n\nAnd you're basically establishing a connection from both hosts\ninto VS Code's proxy layer.\nSo you could do this by just having two VS Codes running.\nYou can connect to one from the other if you're signed in with your GitHub account.\nIt's a bit like VS Code live sharing.\nIf anyone has used that, it's a similar mechanism.\nBut on your Cloud9 instance, you don't need to run a full VS Code.\nYou can run the headless binary.\n\n\nSo you can just download the VS Code CLI from the Microsoft website.\nYou can run that on any platform, including x64 Linux or ARM Linux.\nAnd then you just get a code binary and you run the command code tunnel.\nAnd right away, it'll allow you to log in using the OAuth device code flow.\nSome people might remember that from our live stream episode,\nwhere we built the WeShare product, the WeTransfer clone.\n\n\nWe actually implemented that device code flow.\nSo that'll allow you to log in with your GitHub account on your VS Code instance,\njust using your browser.\nAnd then you just have to give it a machine name.\nSo you just give it a name like my Cloud9 instance.\nAnd you immediately get a vscode.dev link.\nSo you can click on that link and then you get a VS Code editor in your browser.\nSo this is where you might finally, you know, you might be able to pay off your iPad\nbecause you can suddenly start developing on your iPad with this vscode.dev link.\nAnd you don't have to install anything locally.\nYou can do this on any machine you're in.\nYou know, even if it's a coffee shop library, you can potentially do this.\nBut you could also then go back to your VS Code on your host\nif you want to use your local environment.\nAnd you can do control shift P, command shift P, and then connect to tunnel\ninstead of connect to server.\nAnd it'll give you, because you're logged in on GitHub on both sides,\nit'll allow you to pick the machine name that you just specified\nand you're up and coding again in VS Code.\nNice.\n\n\nLuciano: I'll give you an extra tip there that if you use GitHub for login and you have enabled GitHub, so saving your VS Code settings through GitHub,\nbasically,\nwhen you bootstrap this vscode.dev web instance of VS Code,\nit's actually going to start to inherit all your themes and plugins.\nSo you don't even have to think about reinstalling everything in this environment.\nIt kind of starts as if it was your main development environment,\nwhich is pretty good.\nThis really feels like magic, this being able to do this.\n\n\nEoin: I can imagine using this in lots of different situations.\nThe only thing is I'd love to know more about what it's sharing under the hood,\nwhat is being exposed and being copied from one instance to the other.\nBecause if you imagine, if you're working on a customer application,\nI wouldn't do this without understanding exactly what's being shared.\nFor example, when I launched my vscode.dev,\nI could see that it had picked up that I was using AWS toolkit locally.\nIt installed this.\nI could select various AWS credentials that I had on my local machine,\neven though this was running on the remote machine.\nNow these credentials didn't actually work,\nbut it seemed like it made me think,\nokay, well, what is actually being shared here between my client machine and my server machine?\nAre my local AWS credentials possibly being copied up onto this Cloud9 instance?\nI don't think they are,\nbut it's worth investigating more thoroughly before you would adopt this widely\nfor sensitive applications or code bases\nwhere you don't want the code to be leaked outside the environment.\n\n\nLuciano: Awesome.\nSo again, we'll make all the links and all the resources available in the show notes.\nMake sure to check them out if you are curious to try out some of these ideas.\nAnd we are also curious to know from you, as usual,\nif you have solved this problem in different ways, maybe using other tools.\nSo definitely let us know in the comments.\nAnd if you liked the episode, remember to give us a thumbs up,\nsubscribe, do a review of the podcast.\nIf you are listening to the audio podcast,\nall of that stuff is going to help us a lot to keep promoting the podcast\nand understand what you like the most so we can produce more content like that.\nThank you very much again and we will see you in the next episode.\n"
    },
    {
      "title": "82. Redis on AWS: Is ElastiCache the Right Choice?",
      "url": "https://awsbites.com/82-redis-on-aws-is-elasticache-the-right-choice/",
      "publish_date": "2023-05-26T00:00:00.000Z",
      "abstract": "Who is the king of all databases when it comes to performance? Yes, Redis! Of course!\nIn this episode of AWS Bites, we talk about Redis on ElastiCache, one of the most essential instruments in the cloud architect's toolbox.\nWe explore the joys and woes of Redis on AWS and share some exciting alternatives regarding in-memory databases and caching systems.\nWe discuss the use cases of Redis, including session storage, web page caching, database cache, cost optimization, queues and pub/sub messaging, and distributed applications state.\nWe extensively talk about ElastiCache, the managed cache solution on AWS based on either Redis or Memcache, and its features such as replication groups, auto-scaling, and monitoring.\nFinally, we discuss potential alternatives, such as DynamoDB (with DAX), Upstash, or Momento, a serverless cache built on Pelikan.\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nElasticache\nUpstash\nMomento\nThe story of Antirez and Redis\n\n",
      "transcript": "Eoin: For a long time, if you asked a lot of architects,\n\"How do I make my system faster?\", the answer was \"Redis\"!\nIt's still one of the most important instruments\nthere in the architect's toolbox.\nOn AWS, the recommended way to run Redis has been ElastiCache,\nbut that's not without its problems.\nWe're going to talk about the joys and woes of Redis on AWS\nand share with you some exciting alternatives.\nI'm Eoin, I'm here with Luciano and this is the AWS Bites podcast.\nOur sponsor is fourTheorem, an AWS consulting partner\nthat helps your organization get excited about building software again.\nFind out more at fourtheorem.com. You'll find that link in the show notes.\nLuciano, what is Redis and how does it work?\n\n\nLuciano: Let's start with the name.\nAlso, because before we prepare for this episode,\nI actually didn't even know what the name of Redis stands for.\nWe figured it out by looking on Wikipedia\nthat stands for remote dictionary server, which I don't know.\nDidn't expect that meaning. Let's move on.\nLet's talk a little bit about the story of Redis\nand then try to understand how it works.\nRedis was born from an Italian startup\nthat was building effectively a real-time web log analyzer type of application.\n\n\nI imagine that like some kind of Google Analytics alternative.\nI don't know if it's too accurate,\nbut you can imagine the kind of data problems,\nnumber crunching that you have to do\nto keep aggregating all the type of information coming through very quickly.\nI believe initially this company was using MySQL for all the persistence,\nbut they, of course, if at the point of scale\nwhere they needed to do all these operations fast for large volumes of data,\nMySQL wasn't really enough.\n\n\nAntirez, Salvatore Sanfilippo is the real name,\nwas the developer of Redis, which was initially written in Tcl\nto try to address this problem.\nSo how do we figure out a different kind of storage\nand data persistent layer that allows us to aggregate\nall this information fast enough for this specific problem?\nNow, after the first prototype in Tcl,\nthey realized that this solution could work\nand they rewrote that in C and open-sourced.\n\n\nAnd it was actually quite successful, especially in the Ruby space.\nAnd it was quite quickly adopted by GitHub, later on by Instagram.\nI believe also Twitter was a heavy user of Redis as well,\nat least for the very initial period.\nNow, how does it work and why it is faster than MySQL?\nAnd the main difference is that while MySQL is a database\nthat tries to give you the best guarantees\nthat when your writes are acknowledged, they are actually persisted on disk,\nand you can trust that you're not going to lose your data,\nRedis works in a totally different way\nbecause performance is the main concern.\n\n\nSo what they do, they just store everything in memory.\nAnd of course, it's a little bit less reliable\nwhen it comes to making sure that the data is there,\nbut it will give you very fast times\nand it will be doing very, very fast operations.\nSo we'll talk more about the trade-off\nand what can you do to mitigate the risk of losing data.\nSo the idea is that you store everything in memory,\nbut it's also a distributed data store\nand it works with that key value kind of mindset.\n\n\nSo the main primitive is that you just say,\nin this key, I want to store this information,\nand you don't really have a lot of flexibility\nlike you would have with a relational database.\nSo you really need to think about your key value pairs there.\nYou get very fast sub-millisecond latency.\nVery commonly, of course, it might depend on networking,\nbut if you have a good networking connection with the client,\nyou get very, very fast times.\n\n\nIt is mostly used for caching.\nSo that's a case where if you lose some of the data,\nit's not a big deal,\nbut you're going to get very, very fast round trips,\nreading the data and writing data.\nIt can also be used as a message broker.\nIt does support natively kind of a pub-sub mechanism.\nAnd then coming to the durability piece,\nthere are some options there that you can enable\nto try to mitigate the risk of losing data.\n\n\nLike you can create snapshots and do different things.\nWe'll spend a little bit more time later on that.\nNow, another cool thing is that every time you write something on a key,\nyou can decide which data type you want to use for that particular key.\nAnd then based on that data type, you can do different operations.\nSo in a way, this could be like your own introduction\nto algorithms and data structures.\n\n\nIf you read the documentation where you see all the data types supported\nand all the operations for every different data type,\nyou also get what is the time and space complexity\nof each and every one of them,\nwhich is really cool if you come from a computer science background\nto see in real life how all these different algorithms\nand data structure affect things that you can actually use\nin production to build products.\n\n\nAnd just to give you some examples, you can write, of course, strings,\nbut you can also write lists, maps,\nwhich are effectively ash maps or dictionaries.\nYou can also write sets, sorted set.\nYou can even have more advanced data types, for instance, streams,\nor you can even use the special indexes\nif you are trying to index points in space\nand do queries on geographical problems.\nAnd other data structures like that.\n\n\nIt is also extensible because it supports out-of-the-box Lua scripting.\nSo if you want to build, let's say, your own kind of operation\nby using the basic APIs,\nyou could provision into a Redis instance your own Lua script\nand then you can invoke it later to do more complicated stuff.\nIn a way, it kind of reminds me of when you start functions in a database\nand then you call this function.\nIt's probably kind of a very similar idea.\n\n\nSo stored procedure, that kind of thing.\nAnd you can also create pipelines.\nSo if you want to do a series of commands,\nyou can just send this kind of pipeline of commands to Redis\nand Redis will execute them in order.\nAnd there is kind of a structure in place\nthat allows you to define this kind of pipelines.\nAnd finally, one really cool thing, this is more recent.\nIn recent years, the developers of Redis\nspend a lot of time trying to make it extensible.\n\n\nAnd now there is a quite large ecosystem of modules\nthat you can add on top of Redis to just extend its functionality.\nAnd some interesting use cases are there are modules for full-text search,\nfor converting Redis into kind of a graph database.\nSo for all the kind of graph problems, you can store the information Redis\nand the module will give you query functionality\nto use Redis as a graph database.\nAnd there are also some modules that will give you some ML capabilities.\nSo given all this introduction,\nand now you should be understanding all the capability of Redis,\nwhat is it good for? What are some use cases?\nSession storage is probably the number one use case.\n\n\nEoin: So if you go back to the early days of web applications,\nit was more common in the beginning to not have any high availability\nand to have single servers with state in them.\nAnd eventually people realised that we needed to be able to scale\nfor high availability and also for performance, horizontally scale.\nThen the question became, how would you store distributed state\nlike session storage?\nAnd Redis is, I think it's probably the number one use case there.\n\n\nAlso with the web applications, web page caching,\nlike pre-rendering, pre-rendered server-side content\nis something you can also store in Redis.\nAnd then you can just use it as a database cache.\nAnd I think this is probably one of the areas\nwhere it became popular in the Rails community,\nwhere you've got a relational database in the back end,\nbut you don't want to hit the database for every single query,\nespecially for reads.\n\n\nSo you could use it as a database cache\nbecause it's only coming from memory.\nYou could save a lot of latency.\nYou can reduce the query load on your database\nand that can internally save you a lot of money.\nSo Redis is also a cost optimisation tool in that sense.\nIt's not just a cache as well.\nSo it does have support for pub-sub messaging,\nwhich means that it has become quite common\nfor low latency microservice communication as well.\n\n\nI've also been using ElastiCache and Redis\nfor things like application stage.\nSo if you've got, especially in a serverless environment\nwhere you've got lots of Lambda functions\nthat need some sort of shared state,\nbut you want really low latency for that state,\nyou realise you can't just have point-to-point communication\nor networking between functions because that doesn't exist.\nInstead, you use Redis as your state store.\n\n\nAnother example, which is kind of similar and related,\nis when you need to be able to query lots of keys in S3.\nS3 isn't a file system. It's an object store, as we know.\nSo one of the disadvantages there is that doing file lookup operations\ncan be very expensive on S3,\nespecially if you want to do a list of all your keys in S3.\nSo what I've done in the past in a number of different cases\nis every time an object is put into S3,\nyou can capture that event with event bridge,\nand then you can have a function or some downstream process\nthat takes that event and registers the presence of the object in Redis.\nAnd then if you want to do a lookup of all objects with a certain prefix,\nyou could just do a list operation in Redis or whatever.\nSo that makes it incredibly fast.\nIf you were to do this with the S3 API\nand do pagination with list objects,\nyou could be there for days reading a large bucket with lots of objects.\n\n\nLuciano: Yeah, I do remember that use case, and it was pretty cool to be able to solve it with Redis.\nAnd yeah, the performance difference was impressed.\n\n\nEoin: I think the trade-off there is always that when it comes to Redis, you just need to make sure you have the right memory.\nAnd also if you've got lots of objects arriving at a fast period,\nyou need to be able to scale it correctly.\nSo maybe a little bit later on,\nwe'll talk a little bit more about the non-serverless nature\nand how we might overcome that.\nBut maybe first let's talk about the persistence.\nSome people like to use Redis as a database.\nIt's something I would be very fearful of using as a database myself.\nThere are things that will give you ACID compliance.\nRedis will not.\nWhat persistence options do we have?\n\n\nLuciano: Yeah, so if we just take a vanilla installation of Redis, not necessarily in AWS,\nthere are some interesting options worth exploring.\nAnd the main one is that you have point-in-time snapshots.\nSo you can imagine those as a backup\nthat you can do every once in a while,\nand that will give you a full snapshot of all the data\nthat is currently stored in memory.\nThat's one thing that you can do\nand is definitely something recommended to do anyway,\nunless you really don't care about losing the data.\n\n\nMaybe it's very cheap for you to rebuild all that information in memory\nif you happen to lose all of it.\nThe other option is you can use Redis as an append-only file.\nYou can enable this append-only file log,\nwhich basically is going to write a transaction log in the background\nand you can configure the frequency.\nAnd I think the recommended setup that I saw somewhere was one second.\nSo it's going to flash that transaction log into disk every one second.\n\n\nAnd that way you have a little bit more guarantee\nthat if you lose data,\nit's not going to be more than one second worth of data.\nNow, this is still maybe something that could not be acceptable for you.\nMaybe you want really not to lose any record.\nYou can actually even configure the system to flash into disk\nfor every single record.\nBut I think that kind of defeats the point of Redis a little bit\nbecause then you are converting what is effectively an in-memory database\ninto something that needs to write to disk for every single write operation.\n\n\nSo you're probably going to lose most of the benefits\nin terms of performance that Redis can give you.\nSo this is definitely an option.\nIt could be interesting for you to explore that option,\nbut just keep in mind that at that point,\nyou are almost having the same constraints\nthat you have with a regular relational database\nminus all the features that a relational database would give you.\nSo worth considering the kind of trade-off\nif you want to go down that path.\n\n\nSo in general, it's probably worth enabling both of these options.\nYou can have the snapshots just in case, I don't know,\nsomething fails or you are restarting your machine.\nMaybe you are doing, I don't know, an upgrade.\nIt's going to be very easy for you to rehydrate all that memory\nwith the information coming from the backup.\nAnd then you can use this append-only file\njust to kind of protect you from data loss.\nIt's not going to be a perfect protection,\nbut it can be quite good for most use cases.\nNow, all of this makes sense in the sense\nof a kind of generic context\nwhere we are not specifically talking about AWS.\nSo maybe it's worth spending a little bit of time\ntrying to figure out what is AWS giving us out of the box,\nwhat kind of services can we use to provision Redis in AWS,\nand what are the features available if we do that.\n\n\nEoin: I mean, you can always run Redis on EC2 or in ECS or EKS, manage it yourself and get full control and flexibility,\nbut we tend to always look for ways of saving ourselves, heavy lifting,\nand all sorts of upgrades and patching.\nSo we turn to the managed version, which is ElastiCache.\nElastiCache is managed cache,\nand it's based on either Redis or Memcache.\nSo you can kind of choose your flavor\nwhen you set up an ElastiCache cluster.\n\n\nNow, despite the fact that it's called ElastiCache,\nit's not just a cache, you get all the features pretty much of Redis\nwhen you have an ElastiCache Redis instance.\nSo you get PubSub, you get streams.\nOf course, those aren't necessarily natively integrated with AWS services.\nIt's not possible yet, but it would be interesting\nif you could trigger like EventBridge or Lambda from a Redis PubSub.\nIt's in memory only, really, so no persistence.\n\n\nThe append only file mode, the AOF,\nit's not supported anymore on ElastiCache.\nSo you can just use the snapshots,\nand you can also do backups to S3.\nYou can also do replication groups.\nSo there's two different ways you can set up\na high availability.\nYou can have vertical scaling,\nwhere you just have read replicas in your cluster,\nbut you can also set up what's called cluster mode,\nwhich is where you start sharding your data over multiple nodes,\nand you can scale horizontally as well.\n\n\nI mentioned that you can do backups to S3,\nso minimum of one an hour.\nIf you want to do more granular backups,\nyou'll have to rely on snapshots or some other mechanism.\nI think the most important message to take away\nfrom using ElastiCache on Redis\nis it's not a serverless service.\nYou need to right-size it.\nSo that means you have to monitor things like your latency,\nthe performance, CPU, and memory in particular.\n\n\nEverything is stored in memory, so you need to keep an eye on that.\nThere's CloudWatch metrics for all of that.\nYou should have alarms on them,\nand you should be constantly revisiting that.\nRedis as well is single-threaded and uses an event loop.\nSo if you've got expensive commands,\nlike you mentioned Lua scripts,\nyou could have Lua scripts in your event loop.\nThat can tie up your cluster for a long time.\nSo you should look at the latency metric to help you spot that\nand see if you need to spread your workload in some other way\nor just increase the size of the instance\nthat's underpinning your ElastiCache.\nShould we talk about ElastiCache in a serverless context?\n\n\nLuciano: Because you mentioned that, and I think it's really interesting\nto double down on that and see how serverless it is, ElastiCache.\nAnd you already spoiled it a bit that it is not really that serverless.\nWe need to do cluster management, high availability.\nWe need to figure out what's the right size,\nwhat kind of...\nHow much memory do we need effectively for the given workload\nthat we want to use Redis for?\n\n\nSo in a way, it reminds me of RDS in terms of experience.\nYou need... Yeah, you don't necessarily need to host\nand make sure everything is running all the time,\nbut you need to know a lot about how Redis works\nand how to configure your workload upfront\nif you want to make sure everything is going to work well in production.\nAnd that's, of course, also in terms of networking,\nit requires a VPC.\nYou need to make sure that any instance that wants to connect to Redis\nhas network access to that VPC.\n\n\nFor instance, if you use Lambda,\nthat Lambda needs to be provisioned in a subnet\nthat has access to that ElastiCache VPC.\nSo you might argue there's no necessarily great fit for serverless,\neven though we have been using it\nand the performance is still pretty good.\nIt's just that the amount of effort in managing it\nis probably way more than what we wanted it to be.\nSo if we can have an additional wishlist item for reInvent 2023,\nthat would be to have a real serverless Redis available in AWS\nthat we can just click and it's available and it auto scales,\neven scales to zero if you're not using it\nand pricing should be kind of, of course, proportional to that.\nNow, AWS actually announced MemoryDB quite recently,\nand we were excited,\nbut that excitement didn't last too long\nbecause by just reading the announcement,\nwe realized, okay, Redis is compatible, is durable, is in memory.\nAll of that were very good tick boxes and made us very happy.\nBut then we saw, okay, still requires a VPC,\nstill requires us to specify an instance size.\nAlso, it's not open source.\nWhile Redis, it is open source,\nso we don't really know exactly what's going on there.\nAnd it was also in terms of pricing quite expensive,\nor at least that's what it seemed\nfrom just looking at the table of prices.\nSo the next question will be,\nare there alternatives that maybe are a bit more easier to use\nwhen you have a serverless setup?\n\n\nEoin: Well, if we think about the distributed state example\nwe mentioned earlier,\nit would actually probably try and go for DynamoDB first before last catch\nbecause of the amount of extra management\nand the whole non-serverless nature of ElastiCache.\nAnd it might work for you, but DynamoDB,\nwe know that you get single millisecond reads for records on DynamoDB,\nbut your writes can be slower and queries,\nlike if you're trying to do a query like in Redis,\nyou can do a wildcard lookup of a bunch of keys with a specific prefix\nand its nanosecond responses.\n\n\nWith DynamoDB, that's going to be multiple tens,\nhundreds of milliseconds potentially,\nand you have pagination and all that kind of stuff.\nNow you can use DynamoDB DAX,\nwhich is their cache layer that is on top of DynamoDB,\nbut that's only going to help if you have a read-heavy workload.\nIf you've got lots of writes happening, as many writes as you have reads,\nthen that read cache isn't really going to help you\nbecause you still have to write it to DynamoDB,\nit's still going to make sure that it's committed to at least part two nodes\nbefore you get a response.\n\n\nSo if you really need that low latency,\nthen you might look at some of the hosted Redis solutions.\nI think we've mentioned Upstash on the podcast quite a few times.\nThey're not sponsors, but we like to point people\nin the right direction from time to time.\nThat's a much more serverless option, at least at the pricing level.\nSo you've got Upstash Redis,\nand you also have Redis Enterprise Cloud\nfrom the company that's managing Redis as well.\n\n\nNow there's been a fairly new player on the block as well,\nwhich I think is a very interesting one to watch,\nand I've been watching it fairly closely, which is Momento.\nAnd this is not hosted Redis or even a Redis compatible cache,\nbut it's a completely new SaaS offering.\nAnd it's aimed at a similar space, at least for caching,\nand they do actually have a new PubSub offering as well.\nSo this is built on Pelikan, which is an open source caching engine\nthat came from a lot of the ideas in Twitter.\n\n\nAnd this Pelikan open source caching engine\nhas recently been rewritten in Rust,\nso we can expect very low latency and good performance and security there.\nAnd they have their own SDKs, so they have like Java, Node,.NET,\nPython, PHP, Go, Rust.\nAnd they do actually have a Redis compatible kind of drop-in library\nfor Node.js, which will work for some of the Redis commands,\nnot all of them, I believe.\n\n\nAnd the idea of it is it runs in AWS or GCP,\nso you can pick your cloud host and your region\nto make sure that you get low latency\nand you avoid data transfer costs as well.\nAnd then the pricing on that, at the moment,\nit's like 50 cents per gigabyte.\nSo this is the kind of thing where you could,\nsome pricing models will work well for your workload,\ndepending on your patterns, your read and write patterns.\n\n\nIt could be expensive for data-heavy operations,\ncould be very cheap for lower volume operations,\nbut you get a pretty good free tier actually,\nso 50 gigabytes every month is free.\nSo I think that sounds like a pretty nice incentive\nto start using Momento.\nIt's pretty new, but we suddenly see them everywhere.\nAnd I certainly like the idea of having\na completely serverless, lightweight, simple caching\nthat can work with your AWS deployment.\n\n\nI think that they've got funding as well,\nand I think part of that significant funding they got\nis towards supporting other clouds as well.\nSo we can probably expect Azure support down the line.\nI was actually impressed to find\nthat they got a cloud formation provider\nfor their caches as well on GitHub.\nSo that you can put that provider in your account,\nand then you can create a cache just like you can\nwith a Redis cluster.\n\n\nAnd you can expect the management overhead\nwill be significantly less.\nIf you're looking to find them, by the way,\ntheir website is at gomomento.com,\nand that link is in the show notes.\nI mention this because they seem to have a bit of an SEO problem,\nsince there's also a Memento database with an E.\nMomento, the one we were talking about, is with an O.\nBut definitely check them out.\nSo if you have any other alternatives, please let us know.\nThat's it for today's episode of AWS Bites.\nWhether you watch on YouTube or you listen on your podcast player,\nif you like it, please subscribe, leave a review,\nand share AWS Bites with your friends and colleagues.\nAnd we really appreciate that.\nWe'll see you in the next episode.\n"
    },
    {
      "title": "83. Bucket List of S3 Best Practices",
      "url": "https://awsbites.com/83-bucket-list-of-s3-best-practices/",
      "publish_date": "2023-06-02T00:00:00.000Z",
      "abstract": "In this episode of the AWS Bites podcast, we explore the best practices for creating and configuring S3 Buckets, Amazon Web Services' popular object storage service. We will learn how to set up buckets correctly from the start, avoiding common pitfalls and ensuring efficient management.\nWe provide a quick recap of Amazon S3, covering buckets, objects, and various use cases. Discover the importance of globally unique bucket names, versioning, and observability through logging and metrics. We will tell you how to ensure the security of your buckets with encryption options and proper access controls.\nFinally, we discuss S3 integrations and additional settings you might consider for your workload.\nDon't miss this insightful episode packed with practical tips and expert advice. Tune in now and optimize your S3 Bucket setup for success!\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nOur previous episode on How to deploy static websites on AWS\nOur previous episode on How to deploy private static websites on AWS\nOfficial documentation for S3 buckets server logs\nAWS S3 Storage Lens\nYan Cui's article on S3 Object encryption\n\n",
      "transcript": "Luciano: S3, an object storage service, is one of the oldest and most used AWS services.\nEven if you don't use AWS regularly, chances are that you have been working with a project\nthat relies on S3.\nIn this episode, we will cover some of the best practices to adopt when creating and\nconfiguring S3 packets.\nWith the list that we are going to cover today, you'll be able to start with the right setup\nstraight away and won't have to go back and revisit all your packets.\n\n\nI am Luciano and I'm here with Eoin for another episode of AWS Bites podcast.\nToday's sponsor is fourTheorem, an AWS consulting partner that helps your organization to get\nexcited again about building software.\nFind more at fourtheorem.com, you'll find the link in the show notes.\nSo, I would like to start today with a very quick recap of what is S3, because maybe it's\nthe first time that you are approaching this topic and I think it's good to get the basics\nnailed down, or maybe you haven't used S3 in a while, and again, it's a good practice\nto just review what are the main concepts that we will be relying on for the rest of\nthe episode.\n\n\nSo, S3 is a storage service.\nThe name actually means simple storage service, and the idea is that basically it allows you\nto store files in a scalable, reliable, and somewhat cheap way.\nThere are two main concepts.\nThe first one is bucket.\nSo the idea of a bucket is pretty much, it is a container where you can store your files.\nSo you can think about that as a folder or a drive in a kind of a file system, parallel\nword.\n\n\nBut of course, it's in the cloud and it scales very different.\nIt scales to different sizes than you can just scale your local drive.\nAnd what do you put in a bucket?\nYou put objects.\nSo this is the actual term that S3 uses for the idea is files, pretty much.\nAnd every file is identified univocally by a key that needs to be unique in that particular\nbucket.\nNow, again, this is an AWS service, so you can use it from the web console, you can use\nit from the CLI, or even programmatically with one of the many SDKs that are available\nfor different programming languages.\n\n\nJust to give you some common use cases, when you use S3, what you could do with it, you\ncould store images, videos, or other assets that you need, for instance, for a web application\nor a mobile application.\nYou can use it as a way to store backups in case of disaster recovery.\nYou can also use for long term archival.\nThere is another service that is often used with S3 called Glacier and you can easily\ntransition your files from S3 to Glacier for long term archival.\n\n\nYou can also use it to implement big data solutions or data lakes where you are going\nto be storing lots of structured files and then you can query those files directly from\nS3.\nYou can also use it for application hosting, which means that if you're building a static\nweb frontend, you can put it in S3 and expose it as a website.\nThis is a topic that we covered a couple of times already.\nWe will add the links in the previous episode talking about this in more details in the\nshow notes.\n\n\nNow why are we talking today about configuring buckets correctly?\nBecause this is a common topic and there are very common mistakes that happen all the time.\nYou will read them all the time in the news.\nSome of the common issues are accidentally leaving buckets publicly accessible, which\nthere have been some very big industry failures.\nSo if you're curious, you can search for the Equifax and the Capital One instances of this\nincident just to get an idea of the magnitude of the problem.\n\n\nBut the idea is that you just forget to make it private.\nSo whoever figures out the bucket name can read any file in that bucket and all the sensitive\ninformation that might be stored in that bucket.\nOr other problems might be if you don't use a very good naming convention.\nThat is because the names are unique across every account and every region.\nYou might end up having a conflict with somebody else's bucket.\n\n\nSo you are not in a position where you can automatically provision that bucket maybe\nif you're using some script.\nSo you'll need to figure out some workaround.\nSo having good convention there will help you out to avoid conflicts.\nOther issues are accidental deletion.\nSo you might end up deleting files and you maybe didn't have a way to kind of have copies\nof this file somewhere else.\nSo you basically lost information that might be very vital for your business.\nAnd we will see how you can protect yourself from that.\nAnd the last one is missing encryption or in general misconfiguration that might force\nyou to eventually just to fix the problem to have to migrate all the data to another\nbucket that is configured correctly.\nAnd therefore all the files copied there will be correctly provisioned.\nSo shall we start by discussing bucket naming?\nEoin, what do you think?\n\n\nEoin: Bucket naming shouldn't be that much of a big deal, but in fact, what you choose for\nyour bucket names, especially if you want to have nice consistent bucket names across\nthe whole organization, it is something important.\nAnd one of the important factors there is that while buckets are created in a specific\nregion, the names themselves are globally unique.\nSo avoiding collisions is important.\nYou can think of them like domain names and you might even have people squatting on them\nlike domain names.\n\n\nSo there's a couple of recommendations there.\nSo one thing is to avoid personally identifiable information.\nLike if you're having a multi-tenanted app, don't include the customer's name or a tenant's\nname in the bucket.\nYou know, that makes it very difficult to delete that information afterwards.\nThe bucket name could also be then be exposed in like various cases like API calls, signed\nURLs, or even DNS requests.\n\n\nSo then when it comes to making them globally unique, well, you can include a unique portion\nand that could be a hash or it could be a combination of like the region, the account\nID and the environment.\nSo one example would be like ACME, project one, production and the account ID, and then\nthe region name.\nAnd that will generally make it pretty difficult to have a collision in bucket name, but you\nmight not like this because it publicly exposes some information like the account ID and that\nmight be for your tastes or for your compliance.\n\n\nSo instead of that, you could just have a hash of all of those elements.\nI still think it's kind of nice to have some of those things visible just for readability\nand troubleshooting if you can kind of look at a bucket name and see where it lives, but\nthat mightn't suit your organization.\nBut I think either approach is generally good and it is a good idea to have a random part\nin it just in case there's that small chance that somebody else happens to have taken or\nmaliciously squats on a bucket name that has an account ID that you have and your region.\n\n\nThe reason for including a region, by the way, is that you might start off with a bucket\nin one region, but later decide that you want cross-region replication.\nAnd then you can just follow the same convention and have a similarly named bucket just with\na different region identifier in it.\nNow if you're using CloudFormation to generate your bucket, if you don't put the bucket name\nin the template, it will just create one for you and it will do that random identifier\npart for you as well.\n\n\nSo it'll take the stack name that you're using in CloudFormation and then just append something\nlike a 12 character suffix onto the end.\nSo it doesn't, it isn't very readable, but it does help to automatically avoid those\nnaming collisions.\nAnd then you can just publish that bucket name somewhere like in an SSM parameter or\na CloudFormation output where other people can use it.\nThat's bucket naming.\nSo it's a good idea to know exactly what your bucket name in convention is, have it written\ndown, enforce it, and then you don't really have to worry about it anymore and you don't\nend up with different kind of name buckets all over the place and a bit of a mess.\nBeyond that, what other best practices do we need to think about Luciano when we're\nsetting up buckets for the first time?\n\n\nLuciano: One of the things I really like is versioning.\nSo S3 supports this concept where basically if you do changes on a file, you won't override\nthe previous version of the file, but it will just create a new latest version.\nSo at any point in time, you can go back and see all the previous version.\nAnd that is something that works even if you are deleting a file.\nIt doesn't really delete it.\nIt just creates like a soft delete mark and you can kind of revert it and restore the\nfile if you have versioning enabled.\n\n\nSo it is a really good practice to avoid either accidental deletion or accidental overrides\nof files.\nAnd it gives you that additional peace of mind, especially if you're storing useful\ninformation that you're not just going to lose it accidentally by, I don't know, maybe\na bug in your code or maybe by doing something very quickly on the CLI and accidentally deleting\nstuff that you were not supposed to delete.\n\n\nSo this is a good one, but there is a caveat there that of course, if you are storing multiple\nversions of an object, you are increasing the amount of storage that you are using.\nSo that will affect your cost.\nSo this is something to keep in mind.\nAnd it might not be worth to enable this in every single environment.\nMaybe you want to do it just in production.\nYou might not want to do it in other development environments or testing environments where\nthe data is not going to be as important as production.\nSo this is just something good to use and enable almost all the time.\nBut caveat, it might make more sense in production than in other environments.\nAnd always keep an eye on cost.\nWhat about observability instead?\nIs there any way for people to see what's going on in a bucket and what kind of settings\nwould they have to enable to do that?\n\n\nEoin: Yeah, there's definitely a good few tips here we can share for creating buckets for the first time or even changing the configuration of existing buckets you have if you don't\nhave some level of observability.\nSo one is to enable logging.\nAnd there's a couple of ways of doing this.\nYou can with CloudTrail enable data events on S3, and then you'll get an audit log in\nCloudTrail of things like gets and puts on an S3 bucket.\n\n\nSo that gives you lots of detail, user identity information, all that stuff you get with the\ncontrol events in CloudTrail.\nBut data events have an additional cost.\nAnd if you've got lots of read and writes from your buckets, that can be very costly,\nactually.\nIt can be more significant than the S3 storage cost itself, depending on your usage.\nSo it isn't something I would recommend turning on by default, but it is powerful.\n\n\nAnd you can just enable it for a subset of a bucket, like a prefix, or you can just enable\nit for write actions if you don't want to log all of the reads.\nIf you want a similar level of audit logging capability but don't want to pay through the\nnose, the other option is to turn on server access logging.\nAnd then you just get a simple HTTP common log format access log, which gets placed into\nanother bucket.\nAnd it just gives you simpler information, like where the request or IP is, HTTP status\ncodes, the time, the object size, it doesn't give you as much detail as CloudTrail data\nevents, but it is a lot cheaper.\nSo I think that's a good one to turn on by default.\nSo that's one to add to the list.\nThen we've got metrics.\nWhat can we do when it comes to getting insight through CloudWatch metrics?\n\n\nLuciano: Yeah, the nice thing is, as many other AWS services, is that you get some metrics out of the box.\nYou don't need to configure anything.\nThey are just there for you to use when you need them.\nAnd some of these metrics for S3 are the daily storage metrics.\nSo basically the number of objects and the bucket storage size.\nThere are additional metrics that you can enable, for instance, the request metrics.\n\n\nAnd you can create different configuration also for different prefixes.\nSo it's not global for a bucket.\nYou can be more fine grained if you want to.\nAnd basically the idea is that you will get a one minute level granularity where you can\nsee the number of head, get, put and delete requests, where you can see the number of\n500 or 400 types of errors.\nYou can also see the total latency and the first byte latency, which can be very useful\nif you're trying to troubleshoot more kind of performance oriented rather than just error\ntype of things.\nAnd you can also, there is a relatively new addition to S3, which is called S3 Storage\nLens, which is something that you can use and it will give you kind of an overview across\nall the buckets in your organization and show you useful metrics for kind of an aggregated\nview of all the buckets.\nLet's move to security then.\nWhat can we recommend there?\nYeah.\n\n\nEoin: Okay.\nWell, this is the really important stuff and there's really two things you need to think\nabout.\nFirst one is public access and the second one is encryption.\nSo when we talk about public access, this is where a lot of the horror stories have\ncome out in the media.\nNow new buckets prevent public access by default.\nSo now we've got good sensible defaults.\nYou can also turn off the facility for people to create public buckets at an account level.\n\n\nSo that is something I would definitely recommend.\nIt makes it a lot easier.\nYou can also put in alerting, of course.\nWhen you disable public access, when you block public access, what you're doing is you're\nessentially preventing users from doing things like creating public ACLs.\nSo with buckets, you have IAM controls like with every other AWS service, but ACLs are\nkind of an older access control mechanism that came with buckets initially.\n\n\nAnd they're less commonly used these days and somewhat deprecated, I would say, but\nthey are still used for various specific scenarios because ACLs still allow you to have access\non an individual object level.\nSo you can associate an ACL with an individual object for very fine grain control.\nBut if you don't need that, you generally don't need them.\nSo you can generally avoid ACLs in general these days.\n\n\nThen you have bucket policies.\nSo bucket policies, you should also think about, okay, what's my boundary here?\nHow far do I want people to be able to be from my bucket when they access it?\nSo it's what you can use to enable cross account access, but you can also use it to restrict\naccess.\nSo it can restrict access to users within a VPC or accessing the bucket from a VPC endpoint\nor accessing it from a specific organization.\n\n\nSo with IAM condition keys, you can say, okay, allow everybody within my organization to\nread from this bucket.\nAnd of course you can do all the usual fine grained access control you can with IAM.\nSo that helps you to avoid public access and just keep your request boundary to make sure\npeople can access it from only within your network or whatever else you need.\nThe other important thing is an encryption.\n\n\nNow since I think January of this year, bucket encryption is now on by default and new objects\nare encrypted with server-side encryption.\nThere's three options when it comes to encryption on AWS S3.\nThe first one is the simplest one and that's SSE S3 for server-side encryption S3.\nAnd that's when AWS manages the key and the encryption for you and you don't have to think\nabout it.\nNow then you have the other extreme, which is customer key encryption, which is when\nyou manage and store your key and you give the key to AWS every time you make a request\nand it will do the encryption and decryption for you.\n\n\nBut that has a lot of burden associated with it because you have to manage distribution,\nstorage and rotation of those keys yourself.\nSo the middle ground is SSE KMS where you have control over the keys, but AWS still\nstores them.\nSo you can have a customer managed key or the AWS managed key.\nNow I think in general, a customer managed key is the preferred option since you have\ncontrol and additional security, but you don't have the overhead of storing and distributing\nthat key yourself as you do with SSE customer encryption.\n\n\nAnd the thing about using SSE KMS, it means that even if your bucket is compromised, so\nsomebody gets credentials that allow access to the bucket, they would also need credentials\nwith access to the KMS key if they were actually going to read that data.\nSo this is the really important point here.\nSo I think KMS with a customer managed key and SSE KMS is the best balance here.\nAnd there was actually a good article by Yan Shui recently who mentions the fact that just\nbecause public encryption by default is now an option in AWS, your job isn't done.\nWe'll provide the link to that link in the show notes as well, because that was a good\none and well worth pointing out.\nSo that's another one for our list, SSE KMS.\nWhat else have we got?\n\n\nLuciano: I think another interesting point could be integration because very often you don't use\nS3 like standalone.\nYou will use it as an integration point for other things that you are building.\nAnd there are different ways to trigger events or to interact with the lifecycle of objects\nin S3.\nSo let's try to cover some of them because some of these will have an impact on the configuration\nof the packet.\nSo for instance, one way is what is called S3 notifications, probably one of the oldest\nnotification mechanisms in S3, which basically allows you to trigger a notification to either\nLambda or SQS every time that there is a change in a bucket, like a new file being added.\n\n\nAnd that requires you to explicitly configure and enable that notification mechanism.\nSo something that you might want to do if you are thinking as that kind of use case\nfor your particular bucket.\nThere is another alternative, which is CloudTrail Data Events.\nThen there is another one, which is probably one of the newest, which is through EventBridge.\nSo you can enable EventBridge notification.\nAnd this is probably the most recommended way these days.\n\n\nSo once you turn this feature on, basically you don't have to do any change.\nYou can just listen to events in EventBridge and you don't pay additional charges either\nfor processing because you can, for instance, you're just going to configure a Lambda to\nbe triggered.\nYou are only going to pay for the execution of that Lambda.\nAlso it's very interesting that you can use EventBridge content filtering if you want\nto do more in-depth matching for specific events.\nFor instance, maybe you're not interested in all the files, maybe only the files with\na specific prefix.\nSo you can do that through EventBridge content filtering.\n\n\nEoin: It's nice that they added that one because in the past we had three notifications, which meant you had to go and modify the bucket, which wasn't always possible.\nEspecially if you were just deploying a new CloudFormation stack.\nAnd then the CloudTrail Data Events was like, okay, it could be costly as well.\nYou could still access them through EventBridge, but it could be slow and then you\nhad the additional cost of data events, which caught me out a few times, I have to say.\n\n\nSo I definitely think adding the new EventBridge method to the checklist is a good no-brainer\nfor creating new buckets.\nSo there are a couple of other settings that we might consider.\nI wouldn't necessarily put them on the must-have list, but there's some nice to haves that\nyou might think about depending on your context and the workload.\nOne of them is, well, a couple of them, I suppose, in the area of compliance and security\nare MFA delete.\n\n\nSo you can enforce that multi-factor authentication is enabled for the principal trying to delete\nan object.\nIt's a little bit cumbersome to enforce in some cases, because now with things like SSO\nidentities, you don't even have the MFA flag in some cases, but there are ways around that.\nAnd another one is object locking.\nSo you can enable an object lock to prevent objects being deleted for a period as well.\n\n\nThat's often a compliance situation.\nWe mentioned replication.\nI think you mentioned it a few times, Luciano, and it is something to think about from the\nget-go.\nWill I be replicating this to another region, another bucket?\nHow much data will I be replicating?\nDo I need to set this up from day one?\nJust so I have all that data there from the start and to test it out and see how replication\nworks, how long it takes, and to understand all of the different nuances with it.\n\n\nLifecycle policies.\nThere's other different storage tiers with different costs associated with them.\nAnd you can move data between the tiers.\nIt can get a bit complicated, but you can also save a lot of money.\nThere are some good cases of people saving a significant amount of cost.\nI think Canva was a recent case study I saw come out where they saved a lot of money by\nusing lifecycle policies.\nSo again, do your calculations.\n\n\nIf you intend on using S3 for a lot of your data, you might save significantly.\nAnd you can even turn on intelligent tiering from day one.\nThat might give you a good balance between complexity and cost savings from the start.\nThe last one is access points.\nAnd these are being used more and more for different S3 scenarios.\nWe talked about them for S3 object lambdas, which leverage access points.\nBut fundamentally, it's just a way of having another way to access S3 buckets without using\nthe bucket name.\n\n\nInstead you use a different access point name, which is generated for you.\nAnd it allows you to have a dedicated resource policy with specific security controls around\nthat access point.\nSo it allows you to have different access modes for different people.\nAnd this is something that's also leveraged in the new global access point support.\nSo if you do have replication across two different regions with buckets in separate regions,\nyou can have a global access point, which allows users to access from the region with\nlowest latency.\nSo those are all things to research and consider as part of setting up S3 buckets.\nNot necessarily must haves, but maybe before we wrap up Luciano, could we summarize our\nchecklist of must dos for bucket creation?\nSure.\n\n\nLuciano: So the first one that we have is once you create a new bucket, you should turn on request\nserver access logging because it's easy and it should be relatively cheap and it will\ngive you that bit of visibility that you might want to have to see what's going on in a bucket.\nThen you might want to turn on request metrics just to get a better, more detailed set of\nmetrics that you can use, again, for building dashboards and for troubleshooting latency\nissues or things like that.\n\n\nThen we have enable versioning, again, extra peace of mind in case you might be worried\nthat some file might be deleted accidentally.\nThis one might be more worth it in production than in other environments just because of\nthe cost implications of basically having multiple copies of the same object as you\nevolve it.\nUse global unique collision.\nWell, basically use names that will help you to avoid collisions with other bucket names.\n\n\nAgain, remember that bucket names are global across all accounts or regions.\nSo make sure to figure out a way that is consistent, but at the same time reduces the chances of\nhaving collisions with either in the same organization and even with other organization\nthat you don't even have control on.\nYou can turn on EventBridge notifications because it's probably the simplest way to\ncreate integrations on top of files being created or deleted or modified in S3.\n\n\nThen in terms of encryption, probably the easiest approach is to use SSE KMS with a\ncustomer managed key and finally make sure to disable public access.\nThat should happen by default with new buckets, but just be sure if you are revisiting older\nbuckets, just make sure that that's a setting that is there.\nSo that's everything we have for this episode.\nI don't know if you have any other suggestion or best practices that you have been using.\nPlease let us know, share it with us in a comment down below or reach out to us on Twitter\nor on LinkedIn and we will be more than happy to chat with you and learn from you.\nIf you found value in this episode, also please remember to give us some kind of feedback,\nto write a review of the podcast or if you're watching it on YouTube to give us a thumbs\nup and subscribe.\nThank you very much and we will see you in the next episode.\n"
    },
    {
      "title": "84. Are VPCs an anti-pattern?",
      "url": "https://awsbites.com/84-are-vpcs-an-anti-pattern/",
      "publish_date": "2023-06-09T00:00:00.000Z",
      "abstract": "In this episode of AWS Bites, we explore the future of Virtual Private Clouds (VPCs) in the context of the zero-trust security trend.\nWe'll dive into the pros and cons of using VPCs, including their usefulness when dealing with sensitive data or when you need fine-grained control over your network environment. But let's be real, sometimes VPCs can be a bit of a headache. We'll discuss why you might want to avoid them, including the added complexity they can bring to your network environment.\nFear not, we'll also provide a summary of when to use and when not to use VPCs, as well as alternatives to using VPCs, such as services that don't require them.\nSo, are ready to talk VPCs!?\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nOur previous episode on Bastion hosts\nAWS announcement for improved Lambda VPC connection (2019)\n\n",
      "transcript": "Eoin: Zero trust is the major security trend of the last five years.\nThese days, it feels like everything is public and more open,\nbut you have to authenticate every connection.\nIn this episode, we're going to delve into the topic of virtual private cloud or VPCs\nand whether they should be avoided or not as we evolve to this zero trust future.\nIf you stick until the end, we'll share our strategy for when to use and when not to use them.\n\n\nI'm Eoin and I'm here with Luciano and this is AWS Bites.\nAWS Bites is sponsored by fourTheorem, an advanced AWS consulting partner that works together with\nyou on architecture, migration and cost optimization. Find out more at fourTheorem.com.\nYou'll find the link in the show notes. Zero trust is a concept that moves away from relying on\nstrong network controls alone to strong authorization,\nfine grained access control and generally fewer network restrictions.\n\n\nAWS credentials and IAM are a good example of this since you can access these resources\nover the internet even though they might be very powerful, very private and require strong security.\nBut instead of enforcing network controls, AWS provides AWS v4 signatures for every API call\nand very fine grained policies with IAM and continuous monitoring and detection on top of that.\nEven if you've ever built an API with JWT or JWT tokens for authorization of a publicly\naccessible API, this can also be part of a zero trust approach.\nSo this is all very practical in the context when an ever increasing number of corporate\nenvironments would be comprised of cloud deployments and on-premises systems and third\nparty SaaS applications. There's just too much corporate footprint now living outside the\nboundaries of the private corporate network. So you might be wondering what does this mean\nfor architecting systems on AWS? Do we still need VPCs? What's the story? Where are they relevant?\nAre they overly complex? Luciano, do you want to start off talking about what is a VPC first of\nall? What does it mean for architects or developers working on AWS?\n\n\nLuciano: Yeah, I think it's a good idea to start by defining what is a VPC. And in the context of Amazon, VPC means virtual private\ncloud and the idea is that you are defining a network but in software. So you're not just\nconnecting cables around, I don't know, somewhere in your storage place but no, you are actually\ndoing all of that remotely and you are configuring a virtual network. But it's not too different from\nlike a real network that you might have seen in a data center or somewhere else. The idea is that\nit's something that is logically isolated even if it lives in the cloud together with all the other\naccounts that live in AWS, it is isolated from all the others. So you need to define that and\nconfigure it so that you can start launching AWS resources in that particular virtual network.\n\n\nAnd we can imagine that, again, it's like you build your own network so that you can start\nconnecting things and provide services to whoever needs to be able to access those services,\nexcept you're doing all of that in the cloud. And when you start creating your own VPC,\nwhat you can do is you can manage a range of different IP addresses and these are private\nIP addresses. You can define subnets and place them in different availability zones.\n\n\nYou can define your own internal routing rules, inbound and outbound traffic. You can use internet\ngateways and NAT distances to basically define whether that network needs to have public internet\naccess or it needs to be able to reach out to public IPs that live somewhere else in other\nnetworks in the public internet. And you can also do things that are a little bit more advanced.\nFor instance, you could be connecting different VPCs together using a feature called VPC peering\nor using something like Transit Gateway. And finally, other things that you could be doing,\nfor instance, you could be provisioning VPN access so that you could be able to join that VPC,\nfor instance, from your own laptop, development laptop, and access resources that way.\nOr alternative things that you could be doing is you could be provisioning Bastion host.\nSo there are different things that as soon as you start to think in terms of creating\na virtual private network, you need to think about, define, architect, and then start to\nactually provision in AWS. So I suppose with that introduction, which I hope was good enough,\nwhen do we want to use VPCs?\n\n\nEoin: Well, historically creating a VPC was like the starting point for pretty much every new cloud project on AWS. You needed to configure the place where you put all\nthis cloud infrastructure before you can start provisioning anything. And the rise of service\nand to manage serverless has changed this in a lot of different ways, effectively kind of making it\neasier for developers and architects to adopt cloud services without needing to think about\nnetworking, because it's all just part of AWS's managed network and you don't have to worry too\nmuch about IP addresses or routing or security groups. You're just relying on those kind of\nzero trust pieces like the IAM policies and roles and service linked roles and all that kind of\nstuff. So why do we still need VPCs then? Well, VPCs can still be incredibly useful when dealing\nwith sensitive data or when you have the need to have fine grained control over the network\nenvironment. They can also be used to provide a secure connection between your on-premises\ninfrastructure or another cloud and AWS resources. So they're not only useful for non-serverless\napplications, I would say, but they can be used in serverless applications as well. So\nan example is if you're using AWS Lambda, you can deploy your Lambda functions inside of VPC.\n\n\nThat's an optional configuration, which will give them access to resources inside that VPC,\nsuch as a database or some other services. So in general, VPCs and serverless applications\ncan be used for a few different use cases. So we mentioned accessing resources in a VPC\nor in an on-premises network. Another one is preventing outbound internet access to prevent\ndata exfiltration in the event of a vulnerability. So you can imagine if there's a supply chain attack\nand one of your modules is kind of rogue or has a vulnerability and an attacker gets access to that\nenvironment, they might need to exfiltrate some keys or some data. If they don't have internet\naccess, you're making it very difficult for them to do that. So having a network boundary on top\nof your zero trust mechanism makes it, it just gives you more defense in depth there. Of course,\nfrom a compliance point of view, or just like a really strict, well thought through network\narchitecture, you might want to do traffic analysis. And if it's all on AWS's managed network,\nyou can't do that. But if everything is in a VPC, you can use things like VPC flow logs to monitor\nyour traffic. And then you can also get fine-grained access control over access to AWS services with\nrouting, security groups, and VPC endpoints as well, which can have their own policies.\nFinally, I'd say service discovery through private DNS is another thing you can get if you have\nVPCs. So if you've got a hybrid of instances, containers, and things like Lambda functions,\nit might make sense to think about service discovery and DNS as well. So there's still\nsome valid cases for thinking about VPCs. I wouldn't dismiss them just yet, but it's not\nnecessarily, you would have to think through the trade-offs and think if you could have avoided\nVPCs until now, you might think, okay, should I go and start creating network resources just because\nI need a database? Or should I continue to try and use serverless options where I don't have to think\nabout networking? So why would you avoid VPCs, Luciano?\n\n\nLuciano: Yeah, my main reason is that VPCs can be quite complicated, even though they're useful for all the reasons that you described,\nthey are not really that easy to get right. And even when you are just starting, there are so many\nconcepts that you need to learn. I remember the first time that I started to work with AWS and I\nwas just trying to deploy a simple application. I did this one day course just to learn, I think,\n11 different concepts that are the ones that you just described. And I was overwhelmed and it felt\na bit unnecessary to having to go through all this pain just to deploy an application in the cloud.\n\n\nOf course, in retrospective, I think it was very valuable to learn all these things. And I feel\nlike now I understand the cloud much better and I can use it better. But I think the point still\nstands. When is it really worth it? Where should you go through all these layers and learning\nproperty properly while maybe other times you can just avoid all this complexity and focus on your\napplication? So the complexity bit is probably the main reason. And kind of a consequence to that is\nthat because it can be complex to understand setup correctly, sometimes you can do silly mistakes.\n\n\nI remember one time I created a subnet, I allocated a Lambda in that subnet, and then I didn't realize\nthat that subnet had very few IP addresses. So when that Lambda was running at scale,\nit was basically starving to get more IP addresses. The Lambda runtime wasn't able anymore to provision\nmore Lambdas. And therefore, at some point, we hit a ceiling and we couldn't really serve the\nusers that we were trying to serve anymore. And that was just a very silly mistake at the VPC\nlevel, configured in the subnets. And it was actually a bit complex to fix because then we\nneeded to redefine all the VPC and it was a bit messy to fix that kind of issue. This is just to\ngive you an example of stuff that can happen. If you want to do it, you need to learn it properly\nand you need to spend some time making sure you understand all the implications. And similarly,\nyou can think about routing firewall security groups, so many things that can go wrong there.\n\n\nThe other thing is that you might be wanting to provision bastion hosts or VPNs. Again,\nbecause if you start to have things behind, basically in a private network, in a way that\nthey are not very easy to access. Sometimes if you want to access from your own laptop,\nbecause you are trying to debug something, maybe access another base, for instance, or Redis,\nyou need to figure out how do I do that? How do I enable that? And again, that requires provisioning\nmore infrastructure and thinking more about security. And you are potentially creating\na backdoor that is useful to yourself. But if you don't make it secure enough, you are actually\ncreating a security risk there. We actually have been speaking about some of these topics,\nspecifically around the concept of bastion hosts in another episode. So we'll have the link in the\nshow notes if you're curious to explore that topic as well. And again, monitoring and intrusion\ndetection. It's something you probably want to do. You want to have ways to understand what's\nhappening in that VPC. And if somebody is having access and that somebody is not supposed to do\nthat, how do you spot that and how do you react to that? You need to put all these kinds of things in\nplace somehow. So again, I think that just to boil it down is not an easy feat. It's something you\nneed to spend a significant amount of time learning, trying things before you can feel\nconfident that you are doing it correctly and you can go to production without any big surprise.\nSo I suppose if all of that sounds scary enough, how realistic it is that you just go VPC-less and\nbe able to deploy even significantly complicated application without having to think about VPCs.\nIs that actually possible today or not? Yeah, definitely possible.\n\n\nEoin: I've been part of teams, I think you have as well, where we've built significantly complex applications that\nare completely VPC-less. And I think there's plenty of public examples out there of companies\nwho are able to do it. There's a lot of people advocating for this VPC-less kind of architecture.\nAnd I think a lot of people believe that it's kind of a core tenant of serverless applications,\nthat you should try and avoid VPCs. And I don't necessarily agree that you have to avoid them.\n\n\nLet's just think about some of the services that don't require VPCs and we might be able\nto extrapolate what you could do with them. So DynamoDB, for example, I think that's one of its\nadvantages and appeals is that you don't have to set up any networking in advance. API Gateway,\nyou don't need it. Kinesis. Also, most of the event services, SQS, SNS, EventBridge,\nStep Functions as well, you don't need to associate it with any of the tasks within your\nStep Function. Can connect with VPC, of course, if you want to. CloudFront even doesn't use VPCs.\n\n\nCognito. And then you have the compute services like Lambda, AppRunner, and Glue. So these are\nall capable of running without a VPC. And you also have lots of third-party services that can\nintegrate with AWS without needing private link. So you can think about, I suppose, any service\nthat uses IAM or even services that are integrated with EventBridge. So all of these services,\nthey use the AWS network and don't require you to set up a VPC. And Glue, AppRunner, and Lambda,\nthose compute services we mentioned, while they don't require a VPC, they allow you to specify\nVPC subnets so you can connect them to network-connected resources. So you have that\noption there. I think it's very possible then to build large, complex applications without using\nVPCs. But it begs the question, is this a utopian vision we should all strive for?\nIs it a good practice? Is it a well-architected thing in 2023 to be trying to avoid VPCs?\nAre they some sort of anti-pattern?\n\n\nLuciano: I wouldn't say, at least in the context of serverless, that VPCs are an anti-pattern. As you said, it's something that you can avoid in many situations.\nBut I think there are situations where if you weight the benefits and the complexity,\nthere might be a trade-off there where it could be worth it doing serverless and also do VPCs.\nAnd just to give you some examples, very simply, probably a use case we've been talking about\nbefore is if you start to use RDS, that's kind of a service that forces you to go down the path of\ncreating your own VPCs. And then if you want to connect a Lambda, for instance, to your own RDS\ndatabase, the easiest way is probably to just put the Lambda in the same VPC where you have your own\ndatabase and make sure that the whole routing and subnets is configured correctly so that you can\nhave access from the Lambda directory to the database. And very similarly, if you use ECS or\nEKS or Elasticash or Kafka or OpenSearch, all these services are built around the concept you\nneed to effectively provision instances behind the scenes in a specific VPC.\nSo you need to think about how am I going to structure this VPC and how I'm going to connect things together. So this is\ndefinitely a very good case where if you use some of these services, you are a bit forced down the\npath of thinking about VPCs. But again, as you said before, it's not necessarily a bad thing\nbecause you gain additional control in the sense of being able to add more in-depth security.\nSo you can add more things around your application that makes it a little bit more secure.\nSo it might not be just extra work for you. It gives you some advantages as well. In summary,\nI will say that VPCs are not necessarily an anti-pattern, but it's just something that you\nmight or might not need to use. And if you need to use, of course, you need to be careful in terms of\njust not discount the amount of work and the amount of complexity that there is and make sure\nyou take enough time to absorb all the concept and to try different things and to test that you're\nset up is actually doing what you think it needs to be doing. So again, anything that you would\nadd in terms of how to think about VPCs in the context of Lambda?\n\n\nEoin: Well, if you want to access resources in VPC or in an on-premises network, you need to define VPC subnets and security groups for the Lambda function. It's actually interesting with security groups and Lambda function, it can\nbe a little bit misleading because normally with security groups, you can say what ports are\nallowed in, what ports are allowed out, but Lambda doesn't really work like that because it doesn't\nhave that stateful socket connection. So the only reason you define security groups really with\nLambda is that so you can use those security groups in the rules of other security groups\nand allow that Lambda function to access network resources. The VPC with Lambda used to make a\nmassive difference for cold start times. So Lambda used to create and attach an elastic network\ninterface or an ENI for the cold start of each container. I think that was what led to the\nproblem you mentioned having experienced when your IP CIDR block was too small. But this is no\nlonger the case. So in the end of 2019, AWS changed this. So an ENI is now provisioned for the\nfunction when you deploy it and each Lambda container routes through this interface using a\nVPC to VPC NAT instead. So it's much more efficient. It doesn't impact cold start times like\nit used to and just makes VPCs with Lambda way more useful than they used to be. So that's\nwhat it used to be. So there are some advantages then of using VPCs for Lambda that are really\nworthwhile to state. And I think one really interesting one is something we alluded to\nearlier. You can prevent outbound internet access. There's no other way to do this.\n\n\nThere used to be this product called Function Shield, which doesn't really exist anymore, which was\nlike a low level interface that would try and prevent outbound internet access to prevent\nexfiltration. But the only way to do this really now effectively is with a VPC and ensuring that\nthere's no route to the internet from that VPC. So it's a useful security perimeter then. And if\nyou've got that supply chain attack problem, you don't have to worry as much. You can also then\nanalyze traffic with VPC flow logs and you've got fine grain network control, right? So in a\ncorporate environment or if you've got other network resources, it can make a lot of sense\nto use Lambda with VPC. Now I would still in a lot of cases, even when you've got a corporate\nenvironment end up with a hybrid approach where you've got lots of Lambda functions that don't\nneed to access those network resources. So those ones don't get assigned a subnet.\nAnd then the ones that do, do get to assign a subnet. So you can mix it like that. So I would definitely think,\nokay, don't worry so much about VPCs for Lambda. If you've got VPCs and private networks,\nit makes sense to just start using it. And we've got plenty of very serverless applications out\nthere now that use ElastiCache and RDS and other VPC connected resources. And it's just becomes part\nof the architecture that you have to use VPCs at least until we get into a situation where maybe\nAWS will start to think about more of a zero trust model for all services. Like with RDS,\nyou've got the data API now, maybe in the future, we'll have other examples where you're just using\nmore like HTTP based interaction for all of these services using IAM instead of network controls.\nI'm going to try to do a summary of everything we just said today.\n\n\nLuciano: And I suppose that the first point is that applications without VPCs are possible, that that's a given, and they can\nbe simpler if you are able to build something like that, you don't have the extra overhead\nof thinking about VPCs. But VPCs are not going away, at least not in the short term. And they\nare an important feature that I think is good, especially if you are an architect, if you learn\nthe principles and if you're able to use them, and if you're able to understand, especially the\ntrade off, like when it makes sense to use them, when you can just keep things simple and not\nhaving to bother with VPCs. And if you're using VPCs, again, very important to do them, like take\nyour own time to do them correctly. There are a lot of templates that you can find online that\njust create VPCs magically for you. They can be useful, but at least make sure you check out what\nthey are actually doing and you understand what's happening, you are okay with that particular setup.\n\n\nDon't just give it for granted that there is a matching default that would work in every case.\nI think you need to, again, spend the time, understand it, and then make the right choices\nfor the specific context. It can be complex, but I feel that things are going in a direction where\nproviders are giving simpler abstractions. So I think over time we'll be able to use things that\nare a little bit simpler, even though I think if you know the theory, if you know what's going on\nbehind the scene, that's always something powerful to have, because I think you can understand those\ninterfaces a little bit better and you can avoid some of the pitfalls that might still happen,\neven though you might have a simpler interface to deal with. There is an interesting tool called\nVPC lattice, I think it's pronounced, and we'll be doing probably another episode where we talk\na little bit more in depth, but the promise of this particular service is that it makes it easier\nto define networks that are a little bit more advanced and where you can connect things in a\nvery dynamic way. So it could be a very interesting topic to explore a bit more, so we'll probably\nleave that for another episode in the future. I'm looking forward to that one, actually.\n\n\nEoin: We're getting lots of great feedback on AWS Bites online, and when we meet listeners and viewers in\nperson, which is always great as well. If you do get something from these episodes, please do leave\na review and rating wherever you listen. It helps other people to discover to the show. And if you\nwatch on YouTube, like our episodes, subscribe and share with all your friends and colleagues.\nWe really appreciate that, so thanks for listening again, and we'll see you next week.\n"
    },
    {
      "title": "85. Tips for Attending AWS Events",
      "url": "https://awsbites.com/85-tips-for-attending-aws-events/",
      "publish_date": "2023-06-16T00:00:00.000Z",
      "abstract": "In the latest episode of AWS Bites Podcast, Luciano and Eoin share their insider tips on how to get the most out of in-person AWS events like summits, re:Inforce, or re:Invent.\nFrom networking to swag hoarding, they cover everything you need to know to make the most of these conferences. Learn how to convince your employer to let you attend and how to plan ahead to get the most out of the event. Plus, hear about the fun activities and after-parties you won't want to miss.\nDon't miss out on this must-listen episode if you're attending an AWS event soon!\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nAWS Events app for Android\nAWS Events app for iOS\n\n",
      "transcript": "Luciano: Last week, we attended the AWS Summit Conference in London.\nIt was a great event and we came back with tons of learnings\nand to be fair, even some good amount of swag.\nBut it was an event with close to 100 talks and thousands of participants.\nSo in this kind of events, it might be very easy to get lost\nand miss the opportunity to take the most value out of that particular day\nyou spent at the conference.\n\n\nIn this episode, we want to share some tips\nto make sure that you can enjoy this kind of events\nand feel like they were really worth your time and energies.\nMy name is Luciano and today I'm joined by Eoin\nand this is AWS Bites podcast.\nAWS Bites is sponsored by fourTheorem,\nan advanced AWS consulting partner that works together with you\non architecture, migration and cost optimization.\nYou can find out more at fourtheorem.com.\n\n\nYou'll find this link in the show notes.\nSo I want to start by giving a very quick description\nof the kind of events we're talking about\nbecause there is definitely no shortage of AWS related events.\nThere are lots of community activities, there are meetups\nand these events can be at very different scales.\nThe ones we want to focus about today are the ones that tend to be\na little bit more on the large side of this scale.\n\n\nSo where we have thousands of participants\nand they might have multiple tracks happening at the same time.\nYou might have lots of space for companies showcasing their products.\nAnd generally you are in this very large space filled with thousands of people\nand it might be very complicated to figure out exactly how should I spend my time.\nAnd those kind of events are the summits like the one we just mentioned in London.\n\n\nAnd there are other events that are quite similar.\nFor instance, the reinforce and reinvent which to be fair,\nI personally haven't been in those ones.\nSo I'm just extrapolated that they would be similar if not even more chaotic.\nSo I expect that some of the advices we are going to share today\nare also very valuable if you attend some of these other events.\nBut I guess we can be proven wrong.\nSo if you think you have been at those events\nand you have a different experience, hopefully you'll let us know\nand that will be a topic to discuss more for the next episodes.\nSo one question that we often get from people we talk to\nthat are interested in AWS is I really like to attend,\nbut it's sometimes a bit difficult to convince my employer to let me go for the day\nand travel and maybe refund the cost of all of that.\nSo how can people actually justify the value?\nWhat do you think, Eoin?\n\n\nEoin: Well, it's kind of an event because they're quite a big event,\nbut they're also just typically one day events\nand not too far away because they're in various different locations around the world.\nWhile we were in London on the same day, there was another one in Washington DC.\nSo they're quite regular at this point.\nAnd we've also had like recent events here as well,\nlike the AWS Community Day in Dublin and before that was AWS Cloud Day in Dublin\nand we got an opportunity to go to all of them\nand they're all a little bit different, but they're all just one day commitments.\n\n\nSo it's not too bad, because you're not traveling very far, it's not too expensive.\nSo it shouldn't take that much to convince yourself and your employer.\nI guess the main thing is just taking a day out of whatever else you're doing,\ngetting your head out of the weeds.\nIt really helps a lot to do that.\nSo the two things you're really getting out of it is learnings\nand then just meeting people, people you know and new people as well.\n\n\nBut if you're working with AWS, attending this kind of event\nis going to have a positive impact in general in your career and your day-to-day job.\nSo it should be a benefit for your employer.\nIt should help you to come back with ideas and ways of working better\nand improve solving challenges you have.\nSo you can share with your employer what kind of topics will be covered\nand show how they're relevant to your day-to-day job\nand the challenges you might have with them.\n\n\nIt's also an opportunity to meet vendors and vendors are there to sell something.\nSo sometimes you might be a bit allergic to that\nand be a bit afraid of being targeted for sales.\nBut at the end of the day, most of these vendors are trying to solve problems\nthat are commonly relevant for AWS practitioners.\nSo you should keep an open mind and be open to learning something\nand maybe replacing something you've built in-house with a third-party solution\nor just meeting vendors you already have and figuring out how to do things better.\nI'd say it's an opportunity to meet industry experts like as well\nand people you only know from Twitter and YouTube,\nincluding your favorite podcast hosts.\nBut the other advantage is that these events,\nespecially the vendor events organized by AWS are free.\nApart from all the data you have to give away when you go around the stands,\nbut in monetary terms, they're free.\nSo even though they take time and energy from you,\nthey don't have a sign-up cost like a lot of conferences,\nwhich can be more expensive.\nSo let's say you've convinced your employer,\nwhat do you need to do next, Luciano?\n\n\nLuciano: So this is just before the event.\nProbably you want to start to think about that a bit in advance,\nmaybe a month or so, because generally these events are planned way ahead\nso you can actually plan as well yourself.\nAnd there are a few reasons why it is important to just don't do it the very last minute.\nSometimes there is limited availability.\nSo of course, the sooner you register your spot,\nthe more peace of mind you have that you secure your presence at the event\nand you can just go.\n\n\nIf you do the very last day, maybe there is no more availability\nand you lost that opportunity.\nVery similarly, if you have to book flights and hotels\nbecause you have to travel a little bit, that could be another problem.\nOf course, the sooner you do it, the cheaper and the easier is to find places for staying.\nIt's also really important to check the agenda.\nNow, the agenda doesn't always come too early.\n\n\nIt is actually updated very close to the event in my experience,\nbut at least a couple of days before the event,\nit should be there and it should be quite complete.\nI would recommend people to check the agenda\nand bookmark the things that are more interesting to you.\nAs we said in London, there were, I think, something around 70 or more talks.\nSo there is a lot of talks and they happen all in parallel.\n\n\nIt's important to just don't randomly go and attend talks.\nTalks you need to have a plan.\nThe good news is that there is a mobile application called AWS Events\nthat can simplify organizing the talks you want to attend,\ncreating some kind of virtual agenda that then you can easily visualize\nto understand which talks are happening next in which room,\nand you can go there in time.\nThis app is available for both Android and iOS.\n\n\nWe will put the links in the show notes.\nI have a very small feedback.\nIf somebody from AWS is listening, please make it work offline\nbecause sometimes the connectivity at these events is not great.\nAnd yeah, if it doesn't work offline,\nyou might not be able to rely on this application sometimes.\nNow, one advice that I have is that it's very tempting to put,\nto squeeze a lot of talks in your agenda\nbecause they are effectively one after the other,\nbut they don't necessarily happen in the same room.\n\n\nSo you might have to walk sometimes even quite a bit\nto just go from one talk to the other.\nAnd therefore, my suggestion is try to put gaps in between sessions\njust because you have to probably spend sometimes going to one session to the other,\nbut also sometimes you find long queues to just get in\nand you might just be left out of the talk\nbecause the room fills in and there is no more space.\nSo my advice would be don't try to be too eager\nand try to attend all the talks,\nbut just pick the ones that you really care about\nand try to allow yourself a little bit of time between one talk and the next one.\n\n\nAnother last thing that I have on this topic\nis there are levels of how much in-depth is that particular talk.\nAWS uses this kind of scheme where you have 100, 200, 300, and 400,\nand they are increasingly more advanced levels.\nSo really, depending on your level of expertise,\nyou can also filter out the talks that are more relevant to you.\nAnd for instance, the 100 ones are very informative.\nIt's more with this service, you can do this kind of things\nand address this kind of problem.\n\n\nSo they are really good if you are just starting with AWS\nto just understand what's the landscape,\nwhat kind of services you can focus about,\nand get some use cases out of it.\nBut if you already have experience with AWS,\nsometimes they are really not adding much value to what you already know.\nSo you might want to look for the 200, 300, or 400 type of talks.\nAnd again, that might change depending on what is your area expertise.\nSometimes you are very experienced in one particular subject,\nbut you might want to explore other subjects.\nSo of course, try to just understand which levels apply the most\nbased on your specific expertise.\nNow, once you get at the event,\nwhat are some of the suggestions that we can share?\n\n\nEoin: Yeah, I like to try and travel the night before actually,\nso you can get there early and you're not exhausted.\nBecause if you're traveling the morning of the event\nand traveling back the evening of the event,\nthe events are tiring enough as they are\nbecause you're standing a lot, talking a lot, walking a lot.\nAnd so for that reason, it's nice to be close by the night before\nand then get there earlier\nbecause the registration can be really long queues.\n\n\nAt the London event, it got really busy in mid-morning.\nOnce, like, I think registration was open at eight,\nit was quiet at the start, but then it just gets a bit crazy.\nSo if you can get there, get registered,\nsit down and have a cup of coffee\nand start meeting people who've done the same,\nit's a nice way to start the day and then it sets you up.\nBecause sometimes, like, the networking part is hard going sometimes,\ndepending on what kind of mood you're in\nand how open you are to that kind of thing.\nBut if you do it, if you start when it's quiet,\nit's often easier to keep going that way.\nSo once you have everything ready, you have to bring like photo ID.\nYou can bring your QR code, get there, get registered,\nand then you're pretty good to go.\nAnd then you have a more relaxed approach to the day.\nYeah, and then I think the main thing is to focus on meeting\nas many people as possible.\nSo is there any trips you can offer Luciano?\nYou're a super networker.\nHow can people do more to meet more people?\n\n\nLuciano: Yeah, that's definitely for me the most important thing.\nI generally feel that even if I learn a lot with the talks,\nmeeting people has a special value,\nbecause I can still find a lot of content from the comfort of my home\nor my office, watching YouTube videos, reading articles,\nlistening to podcasts.\nThere is definitely no shortage of material where you can learn from\nor even just trying things on your own.\n\n\nIt's a good way, but you don't get as many opportunities\nto meet people and talk with them.\nAnd I think you can still learn a lot by just talking to people,\nlistening to their stories and understand what kind of problems\nthey are trying to solve, how did they solve them,\nand then comparing different approaches.\nAnd you can also retain those connections long-term\nso you can reach out to people later.\n\n\nSo I think it definitely adds value to you as a professional\nto try to meet as many people as possible and share expertise\nand build a connection with them.\nNow again, it's important to understand that these events can be massive\nin the order of thousands of participants.\nSo if you know that there are specific people you want to talk to at that event,\nmake sure to talk with them in advance before the event and book a meeting.\n\n\nAnd sometimes we already said that there can be connectivity issues.\nYou don't always get connection.\nThe Wi-Fi might be pretty saturated.\nAlso, I found that mobile operators sometimes can be pretty saturated\nif you are in an area with hundreds of people at that moment in time.\nSo sometimes you literally don't have any connection at all.\nSo if you really want to meet people, make sure to plan in advance also a time\nand a place where it might be the best place to have a chat.\n\n\nTherefore, even if you have no connection, you can just go there\nand most likely you are going to meet that particular person.\nAnd another thing, and this also happens to me,\nI know I might look like an extrovert, but sometimes I'm also an introvert.\nSo it might be a little bit scary to just start a conversation with random people.\nThere are some obvious things that you can do there.\nFirst of all, I think the best moments are when you have either lunch breaks or coffee breaks\nor even when you're simply standing in a queue, maybe to go into a room for a talk\nor maybe to get your lunch or maybe to do the registration.\n\n\nI think those are great moments to just exchange a few words.\nAnd I think if you just ask people next to you or in front of you,\nhey, what's your name and what do you do, that's already a great way to start a conversation.\nSo it doesn't have to be anything extremely smart or complicated.\nThat's the easiest question.\nAnd I think everyone will be happy to reply to that and have a conversation.\nSome other times there are people that are clearly there trying to promote something,\neither their own companies or maybe the company they work for\nor maybe they are trying to build a product or maybe they have a podcast.\n\n\nSo they might wear something that tells you,\nI am the person behind this in a way or another.\nSo they might have logos or I don't know names of companies in their t-shirt or in their backpack.\nAnd so another excuse to just start a conversation.\nIf you think what they do might be interesting, you can just ask,\nhey, what's about this name? What do you do?\nTell me a bit more, I'm curious.\nAnd that's another thing that people will be extremely happy to tell you more about\nand that can start a conversation and that you can compare maybe different experiences.\n\n\nI think if you engage with somebody that you feel like it would be great\nto keep as a long-term contact, it's important to make sure you exchange some kind of contact.\nNow, I'm not suggesting you still print business cards.\nI think that's not a practice that most people will do anymore.\nEven though it might be nice to kind of give away a business card.\nI think these days you can just get on with just using LinkedIn or Twitter or an email.\n\n\nSo just do that and on that I would really recommend,\nthis is actually a tip that I think it really goes a long way.\nIf you reach out to the people you engage with after one day or maybe two days,\njust saying, hey, it was great meeting you and I don't know,\njust make an effort to send a custom message.\nMaybe just say it was great to talk about X\nand I'd really like to learn about your experience.\nI think that will really help you to consolidate that connection\nand make sure that if you want to reach out to them in the future,\nthat connection is still there and it's going to be available for you.\nSo don't discount how important it could be to just send a simple message like this.\nNow, I know that in these events, of course, you can meet a lot of people from AWS itself.\nSo do you have any suggestion around that?\n\n\nEoin: Because we're AWS partners, we know a lot of people at AWS at this stage.\nSo we talk to a lot of people we know,\nbut we also get to meet people that we haven't in person and just try to increase our network.\nI always find like when you're working with AWS,\nit's a major vendor for everybody if you're adopting AWS,\nyou really benefit from knowing as many people as possible\nacross like technology and account management and everything.\n\n\nSo it is always good to meet as many as you can.\nI think the first time I went to these conferences,\nI always had questions about AWS,\nI was always struggling with something\nand all of these events tend to have an ask an architect booth\nwhere you can go up and a solutions architect will be available\nwith a whiteboard and a marker and a laptop and there to answer your questions.\nAnd I've always found that really beneficial.\n\n\nYou also find like this AWS stands with demos and community area as well.\nSo there's plenty of people from AWS and their job is to meet people like you,\nno matter where you're coming from\nand they're not just there to meet major vendors with multi-million spend on AWS.\nThey're there to meet everybody.\nSo you tend to get a lot from trying to trying to meet them.\nAnd then when you have needs for support in the future,\nhaving contacts will really help you.\nAnd as well, if you decide that you want to create content around AWS,\nif you're connected to people within the AWS community\nand working for AWS itself, this already helps you in the future like that.\nSo it's always good idea to make as many contacts as possible.\nWould you think it's worthwhile spending a lot of time meeting some of the companies\nwho are really invested in selling to you like sponsors,\npartners at these events, Luciano?\n\n\nLuciano: Yeah, for instance, in the case of the London event,\nthere were so many of them, probably the order of like hundreds of booths\nthat where you can just walk in and talk.\nSo I would recommend to try to talk with all of them\nbecause that probably wouldn't even have the time in an entire day to talk to all of them.\nSo of course, be selective will be the first suggestion.\nSo if there are companies that you are particularly interested in the products\nor maybe companies that you already have worked with,\neither you use their products or you're thinking to use their products,\nI think it might be really well worth it to go there and talk to them.\n\n\nAnd I'll give you an example that happened to me in this particular London conference.\nI'm not going to mention the vendor just because this,\nI don't want to that the episode looks like it's sponsored by them.\nBut there is a particular vendor that I've been working with for in the context of a project\nthat we are doing with our customer of ours.\nAnd I have been engaging with some of the people that were actually in person there at the booth.\n\n\nSo I just went by to say, hey, hello, it's finally nice to meeting you in person\nbecause before we just engaged in video calls and emails.\nAnd what actually happened and it was a bit unexpected is that there was there an architect for that particular product\nand I was able to have a conversation with them\nand talk about some of the things that we were going to do next with that product.\nAnd it turned out that they already had a bunch of examples and a bunch of code material\nready to be shared with us to actually implement all the things that we need to implement next week.\n\n\nSo unexpectedly this is making my job easier for the next week\nbecause I will have a lot of supporting material that I can just use rather than reinventing the wheel.\nSo again, this is maybe a very specific example.\nIt's not necessarily guaranteed that something like this is going to happen to you,\nbut it just goes to show that the more you engage with companies\nand people the more it can affect your work positively.\n\n\nThen another reason might be you just like to collect swag and every company has tons of swag\nand actually I'm impressed how more competitive it becomes from a conference to the next.\nYeah, I really like your sunglasses.\nAnd we are not just we are not going to mention the company again just because it's going to look sponsored,\nbut that's one of the swag we got from that particular conference.\nSo yeah, definitely that could be another reason to just go\nand talk with the companies that you like the most because they might give you a t-shirt,\nthey might give you other stuff that is of course branded,\nbut that you might just enjoy adding for yourself.\n\n\nNow, it's funny that because they're becoming more and more competitive,\nI'm seeing very expensive prices. Like I've seen very expensive Lego sets or Nintendo Switch\nor PlayStations or keyboards. And of course, you don't just walk in and get those kind of things\nbecause of course, they're very expensive.\nThey will try to lure you into join this competition or maybe some raffle or something like that.\n\n\nSo you can also play the kind of game if you like to do this kind of things\nbecause they will try to to get some of your data or keep you engaged in a conversation\nso that you can participate and try to get the more juicy price.\nSo that's something that you might want to consider or not, of course.\nAnd of course, if you go for this kind of things, just make sure you have enough room in your luggage\nbecause sometimes I find myself that I really pack my bag and that I don't have any space left\nfor extra stuff and it becomes a bit of a problem.\n\n\nLike do I need to throw away something or maybe buy another bag and then check it in?\nSo just make sure you account for that.\nSo last thing that I have in this space is that there are also tons of startups.\nSo companies that are either just starting or they have their first MVP\nor maybe they've been in the market but not for long\nand now they're trying to figure out what's next for them.\nAnd I find them to be the most interesting companies to talk to just because sometimes\nit's really the founders there and you can talk with them\nand they will really value talking to you, getting your experience, getting your feedback.\nAfter all, they're trying to build a product that is probably going to try to satisfy some of your needs.\nSo you are the best person to talk with them and they will really enjoy that\nand very easily they can give you access to the product to do a trial\nor even discounts or maybe give you free access to some of the new features that they are building.\nSo I really enjoy to do this kind of things and if there are startups that are doing something\nin the space that you are interested in, definitely don't miss the opportunity to go and talk with them.\nNow, what about social media stuff for content creation?\nWhat can people do to take advantage of that as well?\n\n\nEoin: Well, I guess you're definitely better at this stuff than me,\nbut I think there's a couple of things that we can suggest before the event.\nOf course, mention on social media that you're going to be there\nand you're happy to meet people because then you might get those meetings beforehand\nand say what you're interested in and that you'd be happy to talk to people\nwho are interested in the same kind of thing.\n\n\nAnd when people do reach out, book that meeting in advance and name a time and a place\nbecause like you said, at London, network was a disaster, mobile and Wi-Fi.\nSo it was like being cut off from the rest of the world during the event.\nIf you can get on the Wi-Fi, post about the things you're doing and like.\nAnd don't just report what you see, but try to add a value with your posts as well.\nLike your opinion, additional material, etc.\n\n\nIf you're on Twitter or LinkedIn, you can change your name to name at event\nand people will follow you and see that you're there.\nAnd if they're there as well, they might reach out to you.\nBut you can also show up then in searches\nwhen people are searching for info about the event.\nSo that will increase the number of people who will engage with you.\nAnd then after the event, you can do like a summary or a blog post.\nYou can also do videos during the event depending on how much content you want to get into.\nBut most people won't be at the event.\nSo you're one of the ones who are there at the event.\nThere's an audience out there of people who are curious to hear what's new\nand what they've missed out on.\nSo take that opportunity during and after the event as soon as possible to summarize and share.\nWhat other fun stuff can you do while you're there in the channel?\nIt's not all work and networking.\nHow can people, you know, get entertained?\n\n\nLuciano: Yeah, I actually found that this kind of events, AWS puts a lot of effort to create spaces\nwhere there is more of kind of recreational activities.\nBut even those activities, they always have a learning angle to it.\nSo this time in London, there were like Formula One simulators.\nThere was the AWS DeepRacer competition.\nThere were spaces where you could play like a deck building card game that was AWS themed.\n\n\nThere is the usual serverless espresso booth where you can just go and all digitally,\nyou can order your own coffee.\nAnd while that happens, you can learn about the step functions that are actually used in that time\nto manage your order and everyone else order and send you a message when your order is ready.\nSo I think those things might feel like a little bit silly or distracting.\nI think that they might be, first of all, just a nice way to just take a break and do something different.\n\n\nBut don't discount again that there is a lot of learning value there as well\nbecause they are all built around the theme of AWS and some of the services\nand they're trying a little bit more indirectly to showcase actual use cases of AWS.\nSo if you look at those things with that lens, I think you can still learn a lot about,\nI don't know, how would they build maybe an ordering system.\nIt doesn't necessarily have to be for coffee,\nbut you could apply those learnings to a similar use case in another industry.\nAnd maybe you already have that use case.\nSo you could be able to compare what you did with what AWS is doing for that particular implementation.\nSometimes there might be a long queue, but again, if you are bored,\nyou can just walk out, step into the queue and use that opportunity also to engage with people\nand do a little bit of networking.\nSo definitely just check out these areas and don't think that if you go there,\nyou are just wasting your time.\nActually, there might be a lot of useful learnings that come out from those in a fun and engaging way.\n\n\nEoin: Have a look out on LinkedIn or Twitter beforehand for some of the vendors who might be arranging for drinks afterwards\nor food or other events because they don't tend to, they don't have to go on for very long,\nbut it is a nice way to kind of get some more networking\nand just meet people in a more relaxed environment afterwards.\nIt's not always about alcohol as well because sometimes some of these events can be a little bit off-putting\nif you're not like a serious drinker, you know,\nso a lot of the events like you could just go and just have a chat with people\nand have a coffee or drink a soft drink or whatever.\n\n\nJust be careful of people who will try and lead you astray.\nThat is one warning I'll have to give.\nBut yeah, especially like it at this time of year in the Northern Hemisphere,\nit's getting nice and sunny.\nSo the afterwards after work after conference events are a nice way to kick back\nand just chat to more people and talk about what you learned.\nBut you do need to register in advance for a lot of those things.\n\n\nSo watch out for the invitations. A lot of them are completely open to anyone attending the event.\nYou just have to get a ticket in advance because they need to plan numbers and everything.\nAnd that's it after that you're free to head home and recover because sometimes as well,\nyou even if you're just trying to take it easy and not go too intense at these events,\njust the fact that you're standing and walking for a full day\nand talking to a lot of people and doing a day of work.\nThat's different to your normal day sitting behind your desktop.\nIt just takes a lot of ideas. Just account for that\nand you're planning for the day afterwards and don't be too ambitious\nbecause you do have to do those follow-ups as well and reach out to everybody who you met at the event.\nSo a lot of time for that and it'll definitely be worthwhile into the future\nbecause you know, you probably won't get to one of these events every month.\nIt's more like one or two a year in general.\nSo make the most of it. Indeed.\n\n\nLuciano: Yeah, by the way, I think I waltzed something like 20 kilometers at the LWS Summit London,\nwhich is not necessarily a lot,\nbut it's definitely way more than my average when I just work sitting at my desk for most of the day.\n\n\nEoin: You're in training for Vegas this year, right?\nSo that's the start of it. Let's see if you can do 50 in Vegas.\n\n\nLuciano: It probably, yeah, from what I'm told, Vegas, it's really intense\nand you end up walking a lot between one place to the next one.\nSo we will see. I'll probably mention something about that online\nor maybe in another podcast episode.\nBut that brings us to the end of this episode.\nSo we would be really curious to know if you have other tips,\nif you enjoyed the ones we just gave away,\nand I am actually curious to know what are the events that you want to attend next\nbecause I'm curious to compare which ones interest you\nand maybe there are ones that we are going to attend as well.\nSo maybe it could be a great opportunity to meet in person.\nSo let us know in the comment section if you are on YouTube\nor reach out to us on Twitter or on LinkedIn\nand we'll be more than happy to have a chat with you.\nUntil then, we'll see you on the next one and thank you very much for following along.\nBye.\n"
    },
    {
      "title": "86. How do you integrate AWS with other clouds?",
      "url": "https://awsbites.com/86-how-do-you-integrate-aws-with-other-clouds/",
      "publish_date": "2023-06-23T00:00:00.000Z",
      "abstract": "Are you struggling with securely integrating workloads running on-premises, in Azure, or in any other cloud with a workload running in AWS?\nIn this exciting episode of the AWS Bites podcast, we dive into 6 different options for securely and efficiently integrating workloads between clouds. From providing a public API in AWS with an authorization method to using IAM roles anywhere to using OIDC federated identities, we explore the advantages and disadvantages of each option. We even cover the use of SSM hybrid activations and creating the interface on the Azure/Data Centre side and polling from AWS.\nDon't miss out on this informative discussion about the best practices for integrating workloads between clouds. Tune in now and let’s have some cloud fun together!\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nOur repository with an example on how to use IAM Roles Anywhere\naws_signing_helper CLI\nDetailed article on how to set up OIDC Federated Identities\nOur previous episode on OIDC providers\n\n",
      "transcript": "Eoin: We recently had to integrate a workload running on Azure with some components running on AWS.\nTo do that, we explored a lot of different options and evaluated all the trade-offs in terms of\nsimplicity and security. So in this episode, we wanted to share our learnings and discuss how to\nsecurely and efficiently integrate workloads running on-premises in Azure or in any cloud\nwith a workload running in AWS. We'll review several different options for integration and\ndiscuss their advantages and disadvantages. My name is Eoin and I'm joined by Luciano for another\nepisode of the AWS Bites podcast. AWS Bytes is sponsored, as always, by fourTheorem, an advanced\nAWS consulting partner that works together with you on architecture, migration, and cost\noptimization. Find out more at fourtheorem.com. That link is in the show notes. Luciano, what are\nsome of the use cases and examples that might require us to think about authorization and\ncommunication between Azure, some other cloud, and AWS?\n\n\nLuciano: As much as we like AWS, we don't always run everything on AWS for different reasons. So sometimes you have this situation where you're\nrunning some kind of workload somewhere, can be in another cloud, can be on-premise, can be\nin your own home because maybe you have some devices to connect to the internet,\nand you might want to integrate that particular system with something running in your AWS account.\n\n\nSo the question that we want to address today is how can you establish a secure integration between\nthose two systems? And just to give you some example use cases, for instance, you might have\nyour own like home NAS where you keep all your personal files and just because you want to be\nextra cautious that you're never going to lose anything, maybe you want to backup the same files\nin an S3 bucket and possibly even use Glacier to reduce cost. How do you connect a system running\nin your own home network with AWS securely so that it can send data to S3 and Glacier? That could be\none use case. Another use case could be maybe a little bit more business oriented. You might have\nin a big corporate network, maybe physically in an office, you might have some kind of network\nsecurity device that collects network metadata. And probably if you're collecting this data,\nyou want to analyze it. So one way to do that could be you could send this data to a Kinesis stream,\nand then later on you can dynamically analyze this data and record interesting network activities.\n\n\nOr maybe you can even implement intrusion detection type of algorithms once you have\nthat kind of data. So again, how do you connect a device running on a local network or in an office\nwith something that is running on AWS like a Kinesis stream? Another example could be actually\nyou might have another application running in another cloud. Maybe your company is using\nmultiple clouds. For instance, you might have like a billing engine that is running on Azure,\nand this billing engine is producing invoices, but then the rest of the system is running on\nAWS. So for instance, you might have an SQS queue where you receive messages coming from Azure.\nThen you can take messages from this SQS queue and process them and maybe send emails to your\ncustomers with the attached invoice using SES. So again, this is kind of an integration where part\nof the system is running on Azure, part of the system is running on AWS. So how do you let,\nfor instance, Azure send data to a queue, which is an SQS queue, so running in AWS in a secure\nand simple way? So the question for today is what are the mechanisms? Should we use IAM?\nIs that secure enough? What are some of the alternatives? So where do we start? What's\nthe first option?\n\n\nEoin: Yeah, I think IAM is generally secure enough as long as you've got the credentials in the right way. And just thinking through what you're saying there, I guess this is\nquite valid as well. All of this kind of use case, it's valid in a case where you're migrating to\nAWS and maybe you're going for a hybrid approach. So you're deciding not to put everything on AWS,\nor just as you're migrating, you end up in this intermediate hybrid state.\n\n\nSo maybe one of the easiest ways or most understood ways is to put a public API in the AWS\nside with an authorization method. And that could be something like a API key. And then you just\nshare the API key with the external side. You can use IAM authorization on that API, but you still\nhave to figure out how to get that IAM session. Or you can use OAuth or OIDC and get a token that way.\nAnd that allows you for secure integration, but it makes the integration side fairly easy\nand understood because most people will understand how to call an API. So it's a fairly common way of\ndoing it. So on the AWS side, you could use API gateway. API gateway allows you to provision\nAPI keys and share them with your clients. If you wanted to, you could go with more of a secure\napproach using an authorizer like an OIDC authorization. Then you need to configure the\nclients correctly to have some client ID in secret, for example, to be able to perform an\nOIDC flow and obtain credentials. Similarly, if you just wanted to go with IAM authorization,\nyou have to figure out some way of getting IAM credentials. So I suppose a lot of the other\noptions we're going to go through are going to cover getting IAM credentials anyway, whether\nyou use the API gateway approach or not. But the advantage of an OIDC approach, if you already have\nan identity provider, is that you can hook it into that method and get a JWT token that they can use\nand attach it to the request. So that's public API method. Let's talk a little bit more about\nIAM. And I suppose the first thing people would reach for when they think about IAM is an IAM\nuser, but we often talk about how this is discouraged. So what do you think? Is it a viable option?\n\n\nLuciano: I think it's definitely the simplest one.\nSo for very simple integrations, it's probably the one that I've seen the most, even historically. The idea is that you just go to your own AWS\nconsole, you create a user, you create credentials for this user, then somehow you propagate these\ncredentials to your client. And then your client is basically just doing using the SDK or the CLI\nwith these credentials. Sometimes you can make it a little bit better by just creating an external\nrole and you give to that particular user only permissions to assume the role. At least you have\nanother extra step where you can track exactly when the system is assuming the role. And\ntherefore we have a little bit more control and more visibility on the actions that are\nhappening there. But with that, you can also have proper logging and alerting, and you can try to\nset up automated key rotation to make it a little bit more secure. In reality, I've seen that when\nyou have to do all these things, people just say it works and you end up with long-lived credentials,\nwhich at some point might become a very serious security liability. So this is something that\nbrings us to explore other options just because this can be very convenient and very easy,\nbut also to make it secure requires so much more work. So the risk is that you just stop yourself\nat the first step, it works, and don't make it secure until eventually maybe you have an incident\nin production. So just be aware that this solution is always a bit tricky, looks simple, but it's\ndangerous. So let's figure out if there are other options that maybe take a little bit more time to\nbe set up, but then they will be more secure by default.\n\n\nEoin: If you wanted to use the IAM user approach, I would suggest you can use infrastructure as code. So in CloudFormation, you can\ncreate your IAM user and you can also create the access key either in CloudFormation or\nprogrammatically, and then store it in Secrets Manager and some other secret store or vault\nthat might allow you to share it with the external identity. But yeah, it starts to be a lot of work\nif you want to implement rotation and alerting yourself. So another option is IAM roles anywhere.\n\n\nAnd I think this is probably the purest, most suitable solution today for the problem in many\nways, because it's really designed for this purpose actually. I mean, the hint is in the name.\nI think the service has been around for about a year or so. And the idea is that it allows you\nto use your PKI, so your private key infrastructure to exchange a certificate\nand a signature for IAM credentials. So a lot of organizations already have the PKI infrastructure,\nso sometimes it's a really good fit. They're already issuing private certificates for other\nreasons internally. So sometimes it's a really easy jump to roles anywhere.\n\n\nOnce you have your private certificate authority, then you can set up a few resources in roles\nanywhere. So the process works like this. You have your certificate authority, you create a\ntrust anchor, which is basically creating the trust relationship between your certificate\nauthority and IAM. And then you create an IAM role that roles anywhere can assume.\nThat will give you the permissions you need. And then there's another thing called a profile,\nwhich essentially links roles anywhere to that role. Once you have those three things,\nyou can use a tool called the AWS Signing Helper. You can use it, execute it manually, or you can\nuse it to pick up credentials in your SDKs. And that will then basically share a signature\nand a public search with IAM roles anywhere, and it'll give you back the IAM credentials.\n\n\nNow, this might sound a little bit complicated or unfamiliar if you haven't come across these\nconcepts before. So we have put together a very simple demo for this. So there's a demo\nfor this. So there's a link on GitHub. There's a repository we created, and you can check it out.\nAnd it'll give you steps to create like a dummy private CA on your laptop with open SSL,\nand then a CloudFormation template to deploy everything else. And once you have that,\nyou can set it all up in about five minutes, and then you can get these credentials.\n\n\nIf you don't have a private CA already, it's setting one up and maintaining it and securing\nit is not for the faint-hearted, and I wouldn't generally recommend it. So the good news there\nis that you can use AWS certificate manager, private certificate authority, and that will\nmanage all of that for you. It's a managed service. There's always a bit of bad news,\nthough, and the bad news with that is that it costs $400 per month per certificate authority.\n\n\nSo be careful creating multiple certificate authorities for different development and\ntest environments. You do get one for free per month, but I was recently given a bit of bill\nshock when I was creating certificate authorities and only ever creating one at a time, but following\nbest practices, creating immutable stacks with infrastructure as code. And when I created and\ndeleted three times, I saw an $800 forecasted bill on Cost Explorer. Now, I think this was more of a\noverestimate in the forecasting of Cost Explorer, but I did have to go to AWS support and open a\ncase and check that I wasn't going to be billed for that. So just be careful of the cost there.\nBut if you do have to create one, I would say try and create one and share it amongst multiple\nenvironments. So roles anywhere. If you're going to leverage a lot of instances externally,\nover time, looking for IAM permissions, I think that's a good one. What else have we got?\nAnything else useful?\n\n\nLuciano: Another approach that we consider is OIDC federated identities, which again, if you have an OIDC provider like Azure AD, that can be easy to set up because\nyou are already using that system. So it's more creating the integration between that system and AWS.\nAnd the idea is that you create managed identities in Azure and link them to whichever\ncompute you are running in Azure, for instance, IBM or Azure functions. So you don't really have\nto generate explicitly some kind of credentials or secrets and keep them stored somewhere because\nthe managed identity will do all of that transparently for you.\n\n\nThen you also need to create this integration between the OIDC provider in AWS, so inside IAM, pointing it to your own\nAzure AD. And then as a client, you just need to perform the authentication flow, the OIDC\nauthentication flow. That will give you a token. And basically with that token, then you can use\nthe AWS SDK to do a similar role with web identity, pass the token, and at that point you have\nAWS credentials that are associated with that particular role that you are assuming,\nwhich will give you the permissions that are defined in that role.\n\n\nSo it seems a little bit convoluted, but the idea is that you already have Azure ID, so it's more about creating the trust\nrelationship. On the Azure side, it's made easier because you have the concept of managed identity,\nso you automatically or easily enough you get access to that token. Then you can use the SDK\nto exchange the token for basically assuming a role and getting the credentials for that role.\n\n\nAnd at that point, these are short-lived credentials, so you can have that peace of mind\nthat if the gate's leaked, the blast radius is very limited. And in this particular case,\nwe found a blog post that has very good instructions and examples, so we'll make sure\nto have the link in the show notes. And we also have a previous episode about how the\nintegration between OIDC providers and IAM works that goes into more detail about the protocols,\nthe different ideas, and why all of this approach is secure. So if you are really curious to go a\nlittle bit deeper into the details, we recommend checking out that episode, and again we'll have the link in the show notes.\nSo in general, the advantage of this approach is that you don't have\nto store any secret, which is great from a security perspective because every time you're\nstoring secrets, you need to make sure you have a process around it, you need to make sure you are\nauditing it, you need to make sure you have to do some kind of rotation. So in this case, you are\nkind of relieved from all these concerns. And it's especially good if you already use Azure ID or\nsome other form of OIDC provider because you don't have to set up all of that.\nIt's already an organization, it's just a matter of connecting it with AWS. So that's another option and I\nactually quite like this one, but is there anything else worth considering?\n\n\nEoin: One that might not occur to everyone, at least didn't occur to us until the very end, but then we\nreached into the back of our memories and found reference to SSM hybrid activations. So what are\nSSM hybrid activations, you might ask. We've talked about SSM a good bit recently because we've\nbeen talking about Session Manager in the context of Bastion hosts and ECS and EC2, and it's a nice\nway for accessing EC2 instances. But SSM also supports hybrid cloud setups as well through\nthis hybrid activations method. So the idea here is that by running the SSM agent on the external\nhost, you can SSM shell into them or just use some of the other SSM things like patching or run\ncommand. And this, the typical use case for this isn't really what we're talking about here, but\nit's if you've got a fleet of Windows machines and you need to run patches on them, then you can\nactivate these hybrid activations and then you can run your patching automatically from SSM in AWS\nand that will cover your AWS instances as well as your external instances. So that's pretty useful.\n\n\nBut it works in this case as well because you can just install the SSM agent on the Azure side or\nin your data center. Then you go into AWS and with any of the usual ways, you can create this\nactivation resource and the activation resource will be linked to an IAM role. It needs some\nspecific SSM related permissions and then whatever other permissions you need for your use case.\nOnce you create that activation, you get a code and an ID. So these are kind of your secrets in\nthis case. And when you start the SSM agent on the instance, you provide that code and ID,\nit will register the instance in SSM and all of a sudden it'll appear in your SSM console\nand you can shell into it if you turn on that option or you can just do run command.\nAnd if you have this role and you have SSM agent running on your Azure instance, then all of a\nsudden you can do a run command from the AWS side and that can trigger some logic on the Azure side,\nwhich can then call back to AWS with the permissions you've given it. And that would\nachieve our goal as well. Now the SSM hybrid activations, there's two tiers, standard tiers,\nfree for up to a thousand instances. Then the advanced tier starts to get a bit spicy in terms\nof cost. So I think it looks like $5 per instance per month. So if you wanted, I think you don't\nneed that for generally the kind of case we're talking about. So don't worry too much, but just\nbe aware, like if you want to actually SSM into your instances using start session, you need the\nadvanced mode and that's when it can get expensive if you've got a lot of instances. So what else\nhave we got?\n\n\nLuciano: I think this ties nicely with another idea, which is a little bit different from most of the other ones we explored today, because in all of them, basically the idea is that you have this\nexternal system, then you have AWS and you are always starting the communication from the\nexternal system to basically call some kind of action into AWS. So in a way it's kind of a\npush model, right? But it doesn't have to be a push model. If we look at it from the perspective of\nAWS, it could also be a pull model. Maybe we can initiate the communication from AWS itself.\n\n\nNothing is really stopping us to use that approach. So the idea could be that rather than\nimplementing an API in AWS, which is the option one we explored today, we could implement an API\nin the other side. Let's say it's Azure. On Azure we can expose an API and then from AWS we call that\nAPI to start to trigger some kind of integration. Again, you still need to figure out some kind of\nauthentication because if that Azure API is running on the public, is exposed on the public\ninternet, then potentially anyone could call it. So you might still want to make sure that it's only\nyour trusted AWS side that is actually calling that API and sending data that you can actually\ntrust. But that might not be the only way of solving this problem because maybe that connection\nis not really on the public internet. Maybe you have some kind of private network connection\nand maybe you can just trust that that network connection is good enough to give you guarantees\nthat it's fine. Maybe you are, I don't know, white listing IP addresses or maybe you have some other\nform of network security. In general, I think today we focused on more of a zero trust\napproach where every call is authenticated strongly with tokens and specific mechanisms where\nyou are never assuming that the network is secure. So I think we had a little bit of a bias towards\nthis kind of solutions, but I think there is an entire realm of more kind of network-based\nsecurity approaches that could be considered as well. I don't know if you have any idea in that\nsense, but I think that that's also worth calling out anyway.\n\n\nEoin: We've really been talking more about how to get IAM credentials where possible and trying to do it based on fine-grained authorization.\nBut there's a whole other set of options we didn't cover, I suppose, network-based solutions,\nlike a site-to-site VPN between your other cloud and AWS or data center. You could have a direct\nconnect link or something else in place already. So there are lots of network-based approaches\nthat you could try and do if you have some secure tunnel between the two environments.\n\n\nWe still think it's a good idea to have IAM or some authorization if possible. You could also\nthink about IoT as well. AWS IoT also has methods similar to roles anywhere and hybrid activations\nwhere you can use certificates to get credentials to talk to AWS. But they're really more geared for\nlots of different sensor devices or other IoT devices. I think in general, we've presented,\nI think, six options in total. The OADC identity provider and the roles anywhere approaches are\nkind of the preferred ones, I would suggest, just because of the fact that you're looking at\nlimiting the need to store and rotate secrets. The API one is a nice one as well, but you just have\nto make sure you have some authorization method in place. So there's six options in total you might\nneed to reach for some of the password or secret-based ones, just depending on your\ncontext and your restrictions. So let us know what you think. And if we've missed any other\noptions as well, if there's any other cool ideas for integrating Azure and AWS. If you like the\npodcast, please do leave a review wherever you listen to your podcasts because our audience is\ngrowing, but we can always reach more people and get lots more feedback and grow the community. So\nwe'd love to hear more from you. Thanks for listening and we'll see you in the next episode.\n"
    },
    {
      "title": "87. Interviewing for AWS Roles",
      "url": "https://awsbites.com/87-interviewing-for-aws-roles/",
      "publish_date": "2023-06-30T00:00:00.000Z",
      "abstract": "Are you interested in landing an AWS role? Or maybe are you looking to hire some cloud talent?!\nIn this episode of the AWS Bites podcast, we share our insights on the interview process we have adopted at fourTheorem.\nThis process is not just about testing AWS knowledge, but it's also about evaluating cultural fit, way of working skills and knowledge, and future plans.\nFrom the “Fiona chat” to the technical interview, we provide valuable tips for candidates, such as being honest about your knowledge and asking questions during the interview.\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\n",
      "transcript": "Luciano: AWS expertise is in high demand.\nWe both work for an AWS consulting partner,\nand we have seen hundreds of applications for roles\nin AWS development, architecture, operations,\nand data engineering.\nIf you're looking for a role like this,\nwhether it is your first of such role,\nor you have lots of experience, this episode is for you.\nWe are going to share our process, what we look for,\nhow we try to make it fair,\nand give some tips that we hope we're gonna help you\nand people like you to learn the role\nthat takes your AWS career to the next level.\nI am Luciano, and here with Eoin,\nthis is another AWS Bites podcast episode.\nAWS Bites is made possible by the support of fourTheorem,\nan AWS consulting partner that works with you\nto make AWS migration, architecture,\nand development a success.\nSee fourtheorem.com to find out more details about that.\nThe link is in the show notes.\nShould we just jump straight and describe the application\nand the interview process that we do at fourTheorem, Eoin?\n\n\nEoin: Yeah, we're happy to share this\nand find out what people think.\nWhen we're hiring at fourTheorem,\nespecially for AWS expertise,\nit is important to state that the process\nisn't just about testing AWS knowledge.\nIn general, it's a two-way process\nwhere the candidate and fourTheorem are asking questions\nand listening to see whether there's a mutual fit for both\nin terms of culture, way of working with them,\nway of working and the type of customers we work with,\nskills and knowledge, of course,\nbut also future plans in terms of the company\nand growth opportunity for the candidate as well.\n\n\nProcess in general, like we've evolved this\nover the course of the last five or six years,\nand when we get an application or a referral,\nwe review the candidate.\nSometimes we have a CV and a cover letter, sometimes not.\nSometimes you just get a LinkedIn profile\nor maybe you get a referral.\nIf it looks like a possible fit,\nwe'll just go for the first interview.\nAnd the first interview is a 30-minute introductory call\nwith Fiona, who's our amazing founder\nin charge of finance and people.\n\n\nAnd that's really about listening to the candidate\nand giving the candidate an opportunity\nto hear about fourTheorem and learn more about us.\nAnd if this indicates a good cultural fit\nand the general agreement, we'll go to the next step.\nAnd the next step is a one-hour\nkind of conversational interview\nwith two people usually from fourTheorem.\nThose people are generally in senior technical roles,\nnot exclusively.\n\n\nAnd this is where we cover a lot of broad topics,\nincluding the experience, your knowledge,\nexpectations for you, your future plans,\nwhat you want to do next, not just where you've come from.\nWe also talk about technical skills,\nbut then there's the aspects like how people work,\nwhat kind of process they work in traditionally,\nwhat they liked, what they didn't like,\nwhat they'd like to change and do differently,\nhow they communicate the team ethic\nand how they like to work with others\nand how they have in the past.\n\n\nAnd there's no right answers really here.\nYou know, you don't have to be completely worried\nabout your previous experience.\nIt's more how open are you to in general.\nIt's also a good opportunity for candidates to learn\nabout how we work at fourTheorem, the kind of projects we do.\nSo after that, if everybody's still happy,\nthe next step and the final kind of interview phase\nis another interview, but it's a technical interview.\n\n\nAnd this is also one hour.\nAnd the candidate has a choice here.\nSo there's three options.\nThey can do a collaborative coding exercise\nwhere they start a solution to a brief technical problem\nthat we present.\nActually, we present two problems and they get to pick one.\nThe other option is a take home coding exercise,\nwhich is the similar kind of thing,\nbut they just do it on their own time.\nAnd then they come back and present it and discuss it.\n\n\nAnd the third option is that they can present a project\nthat they have already written prior\nin like other previous work.\nIf it's something they can share,\nmaybe it's an open source project,\nmaybe it's something they just did for fun,\nwhatever you have.\nSo we try overall,\nlike we try not to take too much of people's time.\nIn our view, if you take more of people's time,\nyou should start paying them at that point.\nI know that some companies have take home exercises\nwhere they expect people to work for days or weeks.\nI just don't subscribe to that.\nSo let's go into some more detail\nand about how all this works.\nLuciano, in general, if you think about the roles we are for,\nwhat are the things we're looking for?\n\n\nLuciano: Sure, so I will say that in general,\nwe are looking for technical knowledge and skills,\nbut I think it's also fair to say that in this industry,\nwe tend to overemphasize that the importance\nof how much technical skills do we need to have\nto land a job in IT or technology in general.\nI think that level of skills really depends\non the type of role that we are hiring for.\nIf we are looking for very senior roles,\nwe have certain levels of expectations,\nbut I think there is a lot more\nthat we need to bring in the picture\nto really evaluate a candidate.\n\n\nIt's not just about technical skills, for sure.\nAnd just to give a little bit more context,\nbecause context is probably important here,\nI think we need to describe a bit how we work at fourTheorem,\nbecause of course that is important\nin the way we hire people as well.\nThe majority of our work is for clients.\nSo we basically work as architects, engineers,\nbringing our expertise with AWS in the context\nwhere a company is trying to migrate\nor build a specific system in the cloud.\n\n\nSo most of our expertise is on AWS,\nso we try to bring that on board in the project,\nbut we'll need to be flexible in the sense\nthat we will be working with a range\nof different technologies,\nbecause our customers might be very different\nin terms of all the different kinds of technologies\nthat they use, including software, programming languages,\nand even methodologies that they use\nto actually work together.\n\n\nSo I think in most cases, it's important to see\nwhen we try to evaluate candidates,\nhow that ability of being flexible and take ownership\nand have conversation with the customers,\nit is actually part of the personality\nof the person we are trying to interview.\nSo that's something that is important\nto be successful in our company.\nSo it's a skill and probably like a more characteristic\nalso of the specific character of the individual\nthat I think it's very important for us\nand sometimes can be as important,\nif not more, of technical skills.\n\n\nAnd I think that needs to be also mentioned together\nwith the fact that internally we do a lot of training,\nwe do certifications, we have acceleration programs,\nwe try to share as much knowledge as possible,\nwe try to pair individuals.\nSo we believe that even if you don't have\nthe most amazing technical skills,\nwe will give you the tools to get there.\nBut I think it's much harder to give you the tools\nto learn how to deal in this environment\nif you are not already kind of a cultural fit\nfor the particular kind of environment.\n\n\nAnd another interesting point is that going back\nto the topic of different levels of skills,\nwe will be hiring from junior to all the higher level,\nlike principal, senior, architect,\nwhatever you want to name them.\nSo that again, the skill expectation\nwill be proportional to the level of experience\nthat we are hiring for.\nSo I think in general, if I have to summarize a few points,\nwhat we're looking for are people that are curious\nto keep learning and building,\neven if they are very senior,\nI think we want people that still accept\nthat there is always more to learn,\nthere are always new methodologies, new technologies,\nand it's always a continuous journey of exploring\nwhat is the possible today and trying to do our best\nwith new tools that we get all the time.\n\n\nPeople that are collaborators,\nso people that don't just want to work in silos,\njust write code all day on their own\nand just come back with a fully completed solution.\nWe actually want people that can have a positive impact\nin a team, they can have conversations\nat the very beginning of a project,\nwork together on building the project,\nreviewing failures and reconsider the approach,\nand always be there to try to help each other\non what can we do to do things better,\nwhat can we do to get better as engineers,\nand how can we improve our work style,\nbut also the work that we do with our customers.\n\n\nAnd I think to do all of that,\nyou need to have a certain degree of compassion and empathy,\nbecause that's, I think, kind of a fundamental key\nthat enables a lot of the conversation\nand a lot of the collaboration activities\nthat we try to promote in our company.\nOn the technical side, we want people,\nas I said before, that are flexible,\nso you might have, of course,\nyour own preferred choice of technology and tool,\nbut you need to be flexible to work also with other things,\nand ultimately, you need to be aware\nthat there isn't really a silver bullet for things.\n\n\nAll technologies, they will have their own pros and cons,\nso you can use all of them probably\nto achieve the specific goals that you have.\nSo another thing that is important is very often\nwhen you join a project that is in progress,\nthere might have been a lot of decisions\nthat have been made before you actually started the project,\nso that if you just look at the current picture,\nthere are a lot of things that might look weird or wrong,\nand I'm sure that that happens all the time to everyone,\nbut I think you can have an attitude of just saying,\nthis is wrong, we should have done this differently,\nor a much better attitude,\nwhich is the one that we will prefer,\nis to start to ask questions like, why are we doing this?\n\n\nHave we considered that?\nIs there anything that we can experiment\nto see if it's a more viable solution?\nAnd I think that most of the time\nthat I've done that in my career,\nI have found that actually\nthere is a very interesting history\nthat led the project to be in that stage,\nand you can learn a lot from that history,\nand that can give you a lot of insights\non what you can be doing next.\nAnd if you just look at the current static picture,\nit's only telling you a very small portion of the story,\nand it's probably not enough for you to be successful.\n\n\nSo we want to really make sure\nthat people will have that kind of attitude,\nor at least that they can be open\nto approach projects in that way.\nAnd other couple of things,\nand I'm gonna speed up just not to bore you too much,\nis that we are just looking for people\nthat are not expected to know everything,\nbut are more aware that they don't know everything,\nbut they know that they can learn\nand what to do to unblock themselves,\nwhat to do when they need to ask for help.\n\n\nAnd if there is a project with lots of unknowns,\nthey can work with that and figure out\nhow to move it to a better place\nwhere all these unknowns are slowly being addressed,\nand we move to a position with more certainty.\nAnd the last point is that we really appreciate\nnot over-engineering things.\nSometimes it's very easy,\nespecially when you're starting a new Greenfield project\nto over-engineer everything\nand just try to do everything in your perfect ideal way,\nbut we still need to be pragmatic\nbecause at the end of the day,\nwe are working with businesses.\n\n\nBusinesses will have specific needs, goals, timelines.\nSo sometimes you need to find the right trade-off\nbetween what can we achieve in a short term\nthat gives value and then take an iterative approach\nto keep improving things,\nrather than just trying to make it perfect at the first go\nand maybe take months and months\nbefore you can deliver something that the business can use.\nSo one final point I have,\nI already said that that was the final point,\nbut there is one last one,\nis that you might see us doing a lot of public speaking,\npodcasts, and so on.\nThis is not necessarily the only kind of people we hire\nbecause we appreciate that it's not for everyone\nto be engaged in public speaking,\nand you can still be an amazing professional\neven without doing all of these activities.\nSo if you think that you need to be a public speaking person\nor a very public profile\njust to be able to work with a company like fourTheorem,\nwe definitely don't think that that's the case.\nSo just want to make sure that that's clear.\nThat's definitely not a prerequisite.\nWhat about the application?\nSo let's just say that we receive a CV\nor we receive an introduction.\nWhat do we do at that point?\n\n\nEoin: The CV, LinkedIn, referral, email, or all options,\nwe may just ask for a CV further down the line\njust as it helps with the interview process.\nWe don't use any kind of automated software\nfor interviewing people.\nFirst of all, we just review all the CVs\nand the applications and just discuss them\nbefore deciding whether to proceed or not.\nI suppose in the past, we had some kind of organic growth\nand we've been fortunate enough to hire people\njust from our network.\n\n\nAnd also just because people hear about us\nand like what we do, we get applications directly.\nWe've managed to get a decent, I suppose,\nproportion of high quality candidates.\nAnd we generally don't use recruiters\nand take unsolicited applications.\nWhen the profile is an internal referral,\nwe generally get a little introduction\nfrom the person who may have worked with them in the past,\nintroducing the profile and thinking,\nsaying whether they think they're a good fit for fourTheorem.\n\n\nIt is important to note that the person\nwho does the referral won't be involved\nthen in the interview process,\njust to keep it as unbiased as possible.\nIt's also important to notice\nthat you don't have to have a computer degree.\nYou don't have to have a degree.\nI've hired great people with computer degrees,\ncomputer science degrees,\nalso have seen applications from people who do\nand haven't had great application success.\n\n\nI've also worked with great people\nwho've got different degrees in different fields\nor none at all, and just have great experience\nand just a real passion.\nSo those things are not hard requirements at all.\nAnd you don't have to have an AWS certification either.\nI mean, these things may help\njust to show some basic level of knowledge.\nWe talked about that in the previous episode,\nwhich we can link in the show notes about certification.\nIt can be helpful in some situations,\nbut it's not the be all and end all.\nSo once we have the application,\nas long as you put a good bit of effort\ninto presenting the application well,\nwe'll go to the next step,\nwhich is the all important Fiona chat.\n\n\nLuciano: I like the title.\nWe should rename it internally like that.\nBut yeah, this conversation is the first starting point\nis more just to have a first impression on both sides\nof what the company looks like,\nwhat the opportunity looks like,\nand for us is to have an opportunity\nto get an impression on the candidate.\nGenerally, the topics that are being discussed is\nwhat can we offer as a company?\nSo what is the work-life balance that we try to promote?\n\n\nWe are a remote company, so we work remotely.\nWe don't even have an office.\nSo that's an element that we present at this stage\nand that some people might like it, some people not,\nbut it's something we need to make very clear very early on.\nThen we also talk about what can we offer\nin terms of bonuses scheme, holidays, and other benefits.\nAnd in general, I think it's an important conversation point\nto get a feeling for what the future will look like\non both sides for the company itself,\nlike what are our plans for the future,\nbut also if you are gonna be part of this company,\nhow can you expect to grow in the company, in your role,\nwhat can be basically the evolution of your profile\nif everything goes well in this particular engagement.\nSo this part of the interview isn't very technical.\nSo it's more about cultural fit.\nIt's more about understanding if there is a common ground.\nAnd then if we see that there is a receiver\nor expectation that is aligned,\nthen we can decide to move on.\nOtherwise, I think that that's probably the point\nwhere the interview stops.\nBut let's say that everything is great.\nWhat do we do next?\n\n\nEoin: The next one that this conversational interview\nwe mentioned at the start,\nit's I think at the one I really try to focus on the most,\nI think, and in it, we cover your experience.\nIf you have some experience,\nthis depends completely on the level you're at\nand what stage you're at in your development career.\nWhen you're talking through your experience,\nI would say don't just talk through the technologies,\nbut also the process, the business goals,\nthe business context,\nhaving awareness of that wider picture\nis really interesting for us\nbecause we're working in that way with our customers.\n\n\nSo we would ask questions about how you work in a team,\nwhat process you like and how you've addressed challenges.\nWe're also kind of interested in cases\nwhere you help others, collaborate with others,\nmaybe mentor others,\nand also where you're open to help.\nSo showing that you have some humility\nis very valuable here.\nAre you open to listening and learning from other people?\nEven people who are more junior,\nthis is a very good indicator\nin terms of the principles of empathy,\nopenness, humility, curiosity.\n\n\nEverybody has a voice in the company.\nSo if you think just because you've got 15 years experience\nthat you shouldn't listen to graduate engineers\nbecause they can't know any more than you,\nthen that's probably not a great fit.\nWe would, of course, have questions from the candidate then\nabout fourth year and how we work,\nwhat our projects are like, what our customers are like.\nAnd then if we're hiring for an AWS role, of course,\nwe'll ask you what you've done with AWS,\nwhat services you've used and how you have used them.\n\n\nIt's not about listing off 250 services.\nThere is no number to reach at this stage.\nIt's more a question of how you use them\nand depth certainly more important than breadth\nin this case.\nMaybe some interesting story about challenges\nyou faced with AWS is interesting.\nAnd then sometimes we ask general questions,\ntechnical questions.\nSo an example is like,\nhow would you build a landing zone page with a signup form\nfor some case that we would devise?\n\n\nAnd usually it's a very simple question\nor at least a simple challenge that we're presenting.\nSo we're not trying to trick anybody.\nIt's not a really intense kind of a question.\nIt's just understanding how would you go about\nthinking about a set of business challenges\nand turning it into an architecture or a piece of software.\nAnd one hint I would give here,\nand maybe it's a little bit of a spoiler,\nbut you don't always have to use AWS\nwhen you're describing solutions.\n\n\nAnd if you think you do,\nthen that might actually work a little bit against you.\nIt's a very positive sign for us to see people\nwho don't just reach for AWS every time,\nespecially when there are simpler,\nbetter solutions for the customer out there.\nSometimes the simplest better solution for the customer is\ndon't build anything.\nThere's an off the shelf solution, for example,\nor don't build anything.\n\n\nThere's no business value in that.\nIt's not worth the effort.\nYou know, those are equally valid answers\nfor there are customers.\nSo that's really good if you can show anything like that.\nSo then we'll also discuss,\nbecause there's time generally here,\nwhat life at Fora theorem is like,\nand begin to hopefully paint a picture in the candidate's\nmind about what it might be like if they decided to go ahead\nand get a job at Fora theorem.\nWe've had the half hour chat with Fiona,\nthis round about an hour on the conversational interview.\nWe've got one hour left of people's time,\nand that's the technical interview.\nI think this can be daunting for people, the idea of this.\nNobody likes to feel like they're in the spotlight\nbeing tested on their technical prowess.\nHopefully we've got some solution to that.\nWhat do you think Luciano?\nHave we solved that problem here?\n\n\nLuciano: I hope so, but I have to be fair because I am one of those people that enjoys doing the kind of art core coding\nchallenge and type of interview.\nAnd I've been practicing that a lot.\nSo I think for a long time,\nI had an expectation that that was the way to do interviews\nfor everyone, but I'm glad that in Fora theorem,\nwe all recognize that it might not be for everyone.\nDifferent people might not necessarily shine\nin this kind of setup.\n\n\nAnd that doesn't necessarily mean that they don't have the\ntechnical skills we're looking for.\nSo one of the things that we do is we try to offer\ndifferent options.\nWe try to offer not just a coding exercise on the spot,\nlet's do something together,\nlet's solve a specific short problem.\nIt could be also, if you have a project you have been\nworking on, you can present it and then based on that,\nwe can ask you questions and we can have a conversation\nabout that.\n\n\nAnd our alternative is that if you prefer to build\nsomething which needs to be short,\nyou can just do it as a tech home exercise,\nbut we wouldn't expect you to spend more than a few hours\nworking on it.\nAnd then we just use this extra hour to discuss your\nsolution.\nSo we have these options and based on the option,\nthe interview might be a little bit different,\nbut in general,\nI think it's worth remarking that the coding bit or the\ncoding assessment ability bit is still important,\nbut not the most important part,\nbecause what we care the most is how do we work with this\nparticular person?\n\n\nHow easy it is to basically understand what is their\napproach, what they're thinking about a specific problem.\nIf they understand the requirements,\nif they are specific questions to clarify some of the\nrequirements,\nit doesn't matter too much if they complete the problem and\nsolve it in the most perfect solution.\nIt's more about this entire process,\nthis entire conversation and this kind of collaboration\nexperience.\n\n\nAnd I think in general,\nour position is that you can get better at coding\nthroughout your career,\nbut to have the kind of communication skills that we are\nlooking for,\nit is a little bit more difficult to develop those if it's\nnot part of your character.\nSo we try to balance the interview in that way,\ntrying to put more emphasis on,\nare you the right fit for this team?\nIf you are,\neven if you don't have all the skills we need right now,\nyou can develop them easily.\nOtherwise it's probably going to be a much harder\nconversation.\nSo I think in general,\nER is just important to establish that relationship with\nthe candidate,\nmake sure that there is good communication,\nmake sure that there are enough technical skills.\nAnd I think that would be probably more than enough to,\nto pass the interview,\nbut I think there are some common mistakes that people will\ndo.\nSo can we give people suggestions to,\nto avoid some of these mistakes?\nYeah, let's try to do that.\n\n\nEoin: I was also just going to say as well Luciano,\nbecause we've emphasized communication quite a lot,\nand I don't want people to get the impression that we're\ntalking about looking for people who've got like perfect\nEnglish or super eloquent or have a great vocabulary.\nIt's not,\nit's more about being open to listening and asking when you\nneed help or being willing to communicate ideas.\nSo it's not,\nit's not about having perfect English and about being an\nextrovert for sure.\n\n\nWhere lots of us at fourTheorem are more kind of social\nintroverts.\nSo we completely understand if people aren't like super\noutgoing and bubbly,\nyou don't have to be a great talker.\nIt's, it's more about great communicator.\nAnd there's a big difference there.\nIt's more about, can you work with other people?\nAnd I think people generally know what we mean,\nhopefully when we say that.\nWhen we talk about common mistakes,\none that I think comes up quite frequently and is a real\nshame is when we see people just not listening or letting\nthe interview speak, interviewer speak properly.\n\n\nSometimes this can,\nwe all understand that people are sometimes nervous in this\ncase.\nSo we'll always try to take that into account.\nBut if people are just trying to dominate the conversation\nand talk over the interviewer and present their ideas and\nkill the conversation from the other side,\nthat's not a great indicator for collaboration and just\ndominating the conversation in general is not a good look.\n\n\nEspecially in the technical interview,\nI've seen lots of technically proficient people who don't\nend up getting an offer because they just want to take full\nownership of the solution and present it as their work.\nAnd we try to present it and make sure that people know that\nit's a collaborative coding interview where we kind of talk\ntogether and we see how we would work together.\nIt's not about showing what a rockstar you are,\nbecause that's really not what we're looking for at all.\n\n\nIt's actually a big turn off.\nWe're looking for people who can work with our clients,\nlisten to them, collaborate,\nwho show that humility will ask questions when they need it.\nAnd we say to people, look, it doesn't matter.\nIf you can't remember, if you're,\nwe let people do the coding interview.\nIf they're doing the live one or they need to take home one,\nor if they're presenting something, it can be any language,\nany technology, any platform, whatever tools they want to use.\n\n\nIt's completely up to you. First of all,\nI would say pick the language that you're most comfortable\nwith, not the one that you think will be most impressive\nbecause there isn't one that's most impressive.\nIt's generally just the one that you're more comfortable\nwith. But if you say, okay,\nI've done JavaScript for 15 years, so let's do JavaScript.\nAnd then all of a sudden in the interview,\nyou can't remember the name of the function to get the\nall the properties and values of an object.\n\n\nAnd you have to go to MDN and look it up or Google it.\nThat's completely okay.\nI would say just do that or just ask the interviewer,\ndo you remember?\nAnd just say, I don't, I've forgotten what this is.\nThis happens to us all, all the time.\nSo it's much better to be open and honest.\nThat brings us to the second common mistake,\nwhich is not admitting when you don't know.\nIt's too easy to tell when people are bluffing and the\nculture we have with our customers is one of radical\ntransparency.\n\n\nSo our customers trust us and value us more because we don't\ntry to bluff. We'll quickly admit when we don't know,\nand we'll openly admit our mistakes.\nAnd that means that when we say we do know something,\nwhat we say is valued, right?\nSo I think it's very important when you don't know it in\ninterview context to say so.\nA third one is not asking the interviewer questions.\nSo if you need clarification, you should ask.\n\n\nAnd it's just a good idea in general to show you understand\nand collaborate with others by engaging in a proper\nconversation. It shouldn't be a one-way quiz.\nThe last common mistake, you've probably heard it before,\nbut it's just like basic errors on CVs,\ncover letters and emails.\nNow we understand that people make typos.\nWe all make typos and mistakes.\nThese are all very common,\nbut a CV is a chance to show you have some pride in the\nquality of what you create.\n\n\nAnd I think it's quite easy to get it right these days.\nI'm kind of amazed sometimes that we get CVs in from a\nrecruiter and the recruiter hasn't helped the candidate to\nimprove the quality of the CV as well.\nI wonder what value is being added if they're just passing\non the CV exactly as it is.\nSo please just like get when you're writing a cover letter,\nif you get our names right, get our company name right,\nget the spell,\nthe names of the technologies you've worked with correctly.\nIt's just better.\nYou're giving people much too easy an opportunity to reject\nyour application.\nIf you've got basic mistakes at that stage,\nthat's the negative side of it over with.\nWhat happens then after the interview Luciano?\nI think this part is actually very easy.\n\n\nLuciano: We generally have a small group of people that have been\ninvolved in the different stages of the interview.\nSo what we do is we just have a chat among ourselves to just\nget a feeling of what do we want to do with the candidate?\nDo we feel comfortable going forward or not?\nAlso we have been collecting notes throughout all the\ndifferent phases.\nSo we already have a feeling for everyone else's opinion.\nSo we might just discuss the points where opinions might\ndiffer and decide, okay, well,\nlet's take a decision together.\nSo at this point it's more a matter of, it is a yes.\nAre we going to make an offer to the candidate or we stop\nthe interview there and we just give them a rejection.\nSo depending how it goes, it might be a yes and happy times.\nAnd there is the usual conversation in terms of negotiation\ntiming and if the candidate accepts the offer is just about\nthe logistics of getting started.\nBut if it is a no, what do we do then?\n\n\nEoin: Well, we're always available to give honest feedback to people. Some people want to hear it.\nSome people don't, you know, if they don't get the role,\nthen they're just happy to move on.\nAnd often actually it's not that the candidate isn't great.\nThey often are great,\nbut it's just not the right fit at that time.\nWe're always careful to just kind of think about what they\nwould do when they come in, who they would be working with.\nIf you've got the kind of team dynamics, right?\nSo sometimes it just isn't the right fit at that time.\nAnd maybe you're too experienced or not experienced enough\nat the right time.\nAnd sometimes also it's not a no from us,\nbut a no from a candidate because they don't see it at the\nright fit for the right time.\nAnd in both of those cases,\nsometimes you just resume the conversation down the line\nand the timing is better.\nAnd we have done that in the past and it has worked out\nvery well.\nSo I suppose in conclusion,\nwhat can people take from all of this?\n\n\nLuciano: Yeah, I think we focused a lot on fourTheorem\nand our specific project process,\nwhich is very tailored to our needs and our company culture.\nBut I think there are some things that you can take away,\neither if you're not interested in fourTheorem\nas a potential candidate,\nor maybe you are hiring people in another company.\nSo what can you learn from all of this?\nWhat do we think is important?\nAnd if you're hiring,\nI think it's important to always make sure that you give\ncandidates a very clear view of what's the interview\nprocess, what would be the different steps,\nwhat is going to be the timing,\nand you share all this information as early as possible,\njust to give them a chance to come prepared and try to be,\nto be the best version of themselves,\nI imagine, for the interview.\n\n\nIt's also important though,\nto give them different opportunities for expressing their\nskills. Again, talking about technical challenges,\nwe realize,\nand I personally realized that hardcore pair programming is\nnot for everyone.\nEven amazing developers fails them just because there is so\nmuch pressure and they require a specific environment and\nnot everyone would be comfortable in working in that\nenvironment.\n\n\nAnd this environment doesn't really reflect the reality of\nthe day-to-day job.\nSo there is also that to keep into account.\nSo just be aware of that.\nAnd if you feel the candidates have good technical skills,\nbut they might be afraid of that kind of approach,\ntry to think about alternatives,\ntry to figure out what are other ways to assess the\ntechnical skills of that particular candidate.\nAnd I think that kind of leads me to a more generic comment\nof just be aware of potential bias.\n\n\nI think this is more and more important these days.\nThere will be candidates with all sorts of different\nbackgrounds,\nand that doesn't mean that they cannot contribute to the\ncompany, that they cannot be good candidate,\nthey cannot contribute to the team.\nSo try to always assess whether you are projecting some bias\non candidates and try to be honest with yourself and figure\nout, am I taking this position because I have a bias,\nor maybe there is an actual problem with the person that is\nnot going to be a fit for this particular role or company.\n\n\nAnd I think I personally have to try to do that myself more\nbecause I think there is always some kind of bias that we\nkind of underestimate.\nSo that's probably a suggestion for myself as well.\nSo on the other side, if you're looking for a new role,\nso if you are a potential candidate trying to look for\ncompanies and maybe specifically in the cloud space,\nI think there are some generic suggestions that we can take\naway from today's chat.\n\n\nAnd for sure,\nyou need to try your best to prepare yourself for the\ninterview.\nDon't just walk in without even thinking about the\ninterview.\nI think the interview is still important.\nYou still need to do some preparation.\nYou still want to do a research on the company.\nIf it's a technical interview,\nmaybe there are topics that you want to review because\nmaybe you know,\nyou're going to be chatting about cloud architecture.\n\n\nSo do you have some use cases in mind that you can\nimmediately start to describe and explain when that kind of\nquestion comes up.\nSo don't, I would say don't underestimate interviews.\nDon't just think that if you have been in the industry for\nmany, many years, by default,\nyou're just going to shine in an interview.\nI think it's still important to do some amount of\npreparation.\nAnd when you walk into the interview,\nalways have a positive attitude, be curious, ask questions,\ntry to learn as much as possible about the company and the\npeople that work in that company.\n\n\nIf you are doing something technical and you feel like you\nare getting stuck, it's totally fine to ask for help,\nto ask for clarifying questions.\nDon't try to just come up with the best answer to every\nquestion.\nSometimes the questions are actually strategically created\nwith some gaps just to see what is your reaction?\nAre you going to be asking for clarification or just going\nto make a bunch of assumptions around with them?\n\n\nSo just be aware that sometimes it's actually the best\nstrategy to ask for a question and make sure you really\nunderstand what it is being asked.\nAnd finally,\nI would also suggest people to be proud of the work that\nthey've done so far.\nI'm sure that everyone has very interesting stories.\nEven if you are a graduate,\nprobably there are things that you have done through your\njourney to get where you are.\n\n\nAnd I think you can find ways to highlight where you're\ncoming from and how you can be valuable for the team or the\ncompany.\nSo just keep that in mind and use that to also give you an\nextra positive attitude.\nAnd I think with that, we are at the end of this episode.\nI'm sure that this episode was probably more opinionated\nthan any other one we've done before.\nSo I think it's really,\nI'm really more excited to hear about your opinion.\nIf you agree with our approach,\nif you do something radically different,\ndo let us know in the chat,\nbecause I think we can all learn and we can all get better\nby sharing the different experiences in this field.\nSo thank you very much.\nAnd we'll see you in the next episode.\nBye.\nBye.\n"
    },
    {
      "title": "88. What is VPC Lattice?",
      "url": "https://awsbites.com/88-what-is-vpc-lattice/",
      "publish_date": "2023-07-07T00:00:00.000Z",
      "abstract": "In this episode of AWS Bites, we discuss VPC Lattice, a new service in the Salad Bowl of AWS Networking. We cover all the concepts, applications, and exciting possibilities for VPC Lattice and share tips on how to use it effectively.\nWe talk about reducing friction between network admin and dev teams and how VPC Lattice can be a game changer for traditional and serverless workloads.\nGet ready for some greens and don't miss this informative episode of AWS Bites!\n\nAWS Bites is sponsored by fourTheorem, an AWS Consulting Partner offering training, cloud migration, and modern application architecture.\n\nIn this episode, we mentioned the following resources:\n\nOur demo repository of VPC Lattice\nSome useful slides to recap the main concepts about VPC Lattice\nServerless Networking with VPC Lattice (Serverless Office Hours)\nAmazon VPC Lattice workshop\n\n",
      "transcript": "Eoin: VPC Lettuce is a new service in the salad bowl of AWS networking\nthat makes it easy for developers and admins teams to set up networking between workloads.\nWe have been taste testing Lettuce and are ready to leaf you with all the knowledge we learned.\nSo, Romaine calm and hear about how it can rocket your networking setup to new levels.\nWe'll share plenty of tips so you don't hit any icebergs on your journey.\n\n\nLuciano: Yeah, okay. Oh, and sorry, I have to stop you there.\nThese puns are excellent, but it's called VPC Lattice, not Lettuce. I don't know if you are aware.\nLattice? Really?\n\n\nEoin: Okay. But I spent ages working on that intro. I'm not going to redo it.\n\n\nLuciano: I like it. Let's keep it.\n\n\nEoin: Okay. Lettuce, Lattice, whatever. Right. Well, whatever.\nWe're going to talk about all the concepts, applications for VPC Lattice, apparently, and how this is a game changer for traditional\nand serverless workloads. I'm Eoin. I'm here with Luciano and this is the AWS Bites podcast.\nAWS Bites is made possible by the support of fourTheorem, an AWS partner that works with you\nto make migration, architecture, and development a success. See fourtheorem.com to find out more.\n\n\nLuciano: So, let's start with this question. What is VPC Lattice?\n\n\nEoin: Well, now that we know what it's apparently really called, Lattice is a service that's really designed to make inbound and outbound east-west connectivity between services\nand applications possible with a zero trust approach to authorization. So, when we talk\nabout east-west connectivity, we're talking about horizontal connectivity between services\nand applications within a workload or a set of workloads and not necessarily like public-facing\nAPIs or say API gateways down to services in the backend, which would be north-south communication.\n\n\nAnd it's designed to work in single or multiple AWS accounts, and it's really focused around\nminimizing the amount of network configuration you do. So, we talked recently about whether\nVPCs were necessary for serverless developers. If you're somebody who's allergic to networking\nas a developer and would rather get away from VPC routing and security groups, then you\nhave to kind of liaise with development teams and always have this back and forth to get\nnetwork setup right. This is another area where Lattice would really help. And there's some\nreally nice things with it. Like, you'll never have any issues with overlapping IP ranges\nand CIDR blocks. So, that's something. And you get this nice kind of high level or fast\nlevel of security. So, you get this nice kind of high level of security. So, you get this\nnice kind of security that you can get to the point where you're kind of stuck in the\nmiddle of the network. And you get this nice kind of high level or fine-grained access\ncontrol support as well. If you're in the field of microservice communication with service\nmeshes and sidecars and all of that stuff, VPC Lattice also aims to take all of that\naway as well so you can focus on the actual workload and simplify that communication.\nIt also supports things like traffic control, load balancing, and path-based routing as\nwell.\n\n\nLuciano: That's pretty cool. I can definitely resonate with some of the problems you described there.\nAnd it's exciting to see that this service is effectively trying to solve most of these\nissues and giving us a new tool that we can just use to be more efficient, more effective.\nBut I'm wondering, this service needs to integrate with other services, of course. So, what does\nthat look like? Is it going to be available only for one service or it's already quite\nwidely available and we can use it for all sorts of integrations?\n\n\nEoin: It's widely available and what you do with it is really up to you. The use cases are\nkind of the communication between microservices, like we said. But also, if you just want to\nhave a mechanism to support private APIs within an organization, that's possible too. The\nnice thing there actually is that you could do custom domains in a much easier way than\nAPI Gateway. If you've got things like migration in your workloads and that's part of your\nplans and you want to modernize over time and maybe switch, have an API that's backed\nby an EC2 or a container, then switch it to containers or Lambda, it supports that as\nwell without having to go through lots of network reconfiguration. So, it integrates\nwith, if you've got existing VPCs, it will integrate with those. It integrates with EC2\ninstances. It'll integrate with anything with an IP address, actually. And it also works\nwith ECS and Kubernetes as well, including EKS. The special feature here, and I think\none we're particularly interested in, is that it also works well with Lambda. So, it can\ntrigger Lambda functions, even Lambda functions that aren't running in a VPC.\n\n\nLuciano: The way you're describing this makes me think about other services that have been in AWS\nfor a while, like, I don't know, VPC peering connection, Transit Gateway, PrivateLink,\nand even just doing your own routing tables or other stuff like that. So, how does it\ncompare? Like, why should this be better than these other solutions?\n\n\nEoin: All of those things you mentioned are pretty much the traditional way of doing this East-West\ncommunication. First, we had VPC peering, and then came Transit Gateway, which enabled the\nsame sort of routing across different VPCs and different accounts in a much more scalable\nway. The container world, you've got lots of approaches around service discovery and\nservice meshes. The whole idea there is that you end up with quite a lot of configuration,\nand you've got this kind of split responsibility between the admin teams and the development\nteams. So, I suppose if there's one message you take away from this session about Lattice,\nit's really that it's trying to address that friction between admin and dev teams,\nand allow the admin teams to focus on centralized access control monitoring and the devs just to\nlaunch the service to create private VPCs that they control and be able to provide and consume services then.\n\n\nLuciano: Nice. Okay. So, what are some of the main concepts when you start using it?\nLike, what does it look like? What do you need to... Which terms do you need to start structuring\nthe usage of this product?\n\n\nEoin: I think the concepts are pretty simple, really, and there's two main ones. The fundamental building block is a service, a VPC Lattice service. So, this is something\nthat's going to be backed by IP addresses, EC2, Lambda, containers. And this is the thing that's\nusually owned by the dev team. So, I mean, the ownership can be changed from organization to\norganization, but I think the basic idea here is that the dev team owns the service, and then they\ngovern its domain, all of the APIs within it, what services are backing it, all of that is controlled\nby that team. So, it makes it very agile from the team's perspective. And the service supports\ncustom domains. Then the service is kind of grouped within the next concept, which is the service\nnetwork. And the Service Network is usually the thing that's owned and controlled by the network\nadmins. And this is essentially a logical control plane that groups VPCs and the services from\ndifferent teams, and you can put IAM policies on it as well. And this is getting into the zero\ntrust approach, which we can dig into. So, the dev team can put an IAM policy on their service\nif they want to, and the admin team can put a network policy on the Service Network. So,\nit allows you to have control at two levels. And it's also possible with resource access manager,\nor RAM, to share both the service and the Service Network with other accounts. So, you can have the\napproach where the admin team creates a Service Network, shares it with dev teams, and then they\ncreate their services within it, or vice versa. Now, within a service, then it starts to look\na little bit like load balancer concepts. So, within a service, then you have a listener,\nwhich supports HTTPS and gRPC. So, there's no kind of TCP or UDP network possible. It's really just\nlike an application load balancer. And again, like an application load balancer, it has target groups,\nand then in the target groups, you can have your IP addresses, instances, Lambda functions,\ncontainers, load balancers as targets. And then you have your rules, just like load balancer rules,\nwhere you've got prioritized path-based routing and that sort of thing. All of those things we\nmentioned, like IP, EC2, ECS, they need a VPC anyway. But Lambda, you don't always configure\nwith a VPC. And actually, to provide a service that's backed by Lambda, you don't need to\nassociate it with a VPC at all, but it can still be triggered by Lattice. So, you only need a VPC\nwhen you're actually consuming a service through VPC Lattice.\n\n\nLuciano: That's pretty cool. So, I imagine that behind the scene, AWS is taking care of routing all this traffic correctly for\nyou, pretty much. So, tell me a bit more about how it works. You described some potential models\nto start to use this in a company, but maybe we can clarify more what are some of the potential\npatterns.\n\n\nEoin: So, if you're starting from scratch, you can imagine the admin team might first create the Service Network and specify an authorization policy on that. You can actually\nspecify security groups as well if you want to do network-based control as well as the zero trust\nIAM approach. Once you have that Service Network, they would share it with RAM,\nand they could share it with individual accounts or with the whole organization. And then dev\nteams will see it in their AWS console and can reference it in their SDK or infrastructure's\ncode templates. So, the dev team could then create a service and then make the association\nbetween the service and the Service Network. And that's what gets it to join this networking\nconversation. They can also specify their policy if they want. And then if you're consuming a\nservice, basically you have a VPC because you have to have a VPC to consume a Lattice service,\nand you just associate that VPC with the Service Network as well. So, once you've got the RAM share\nthrough the resource access manager with your account, you can make that VPC association.\nAnd then any of the consumers running in the VPC can invoke all of the services that are associated\nwith the Service Network, of course, provided that the authorization policy allows. And that\ncan be really fine-grained or coarse-grained, whatever you need. The service consumer will\nthen use a DNS to discover invoked service. So, they're using HTTPS with a domain name,\nand you've got two options there. You can use the Lattice-generated domain names that it creates\nfor you, and they're always there, and they're global. Or you can actually specify a custom DNS,\nand those names will resolve too within your private DNS. And you can use public DNS or private\nDNS for that, and then invoke the service. So, I guess the two points there to remember are that\nto consume a service, you need to be in a VPC associated with the Service Network. And to\nprovide a service, you don't necessarily need to be in a VPC, but your service does need to be\nassociated with the Service Network.\n\n\nLuciano: That makes sense. So, it seems that you have a lot of freedom in terms of defining the access control rules. And I don't know, is there anything there\nthat we want to deep dive on and just provide a bit more background?\n\n\nEoin: Yeah, we can give a couple of examples. So, if we're saying that Lattice is really for kind of private internal APIs within\none or a set of AWS accounts, you're imagining that the boundary is usually within an AWS\norganization. Now, consumers will have to be in a VPC that is associated with the network. So,\nit doesn't have to be within your organization, but you would have to share your Service Network\nwith another AWS account. It could be a third party one, and then they can communicate. You\ncould also use the policies to restrict who can access it. You can choose no authorization,\nand that's a valid approach. And then you could just say, okay, well, let's not share this service\nnetwork with anybody outside the organization. Okay. But you might want to say, okay, let's be\na little bit more careful about that and turn on IAM authorization on the Service Network and the\nservices and perform some stricter checks there. So, those policies are optional, but they can be\napplied at both the Service Network and the service level. So, a couple of examples then,\na Service Network policy might say, only allow principles from the AWS organization. So, you\ncould say allow star resources, but the condition is that the principal org ID is my AWS organization.\n\n\nAnd that would mean that if somebody accidentally shared a Service Network or a service with a\nthird-party account, that they wouldn't be able to invoke your service because they don't have\nthe organization in their principal. And then in the service policy itself, you could say,\nonly allow principles with a principal tag on their identity or restrict them to HTTP get\nrequests or even restrict them by IP address. So, you can get very fine-grained and specific.\n\n\nNow, I think at this stage, it should sound like it's fairly simple to set up because we don't have\nany routing tables. We don't have any VPC peering, no transit gateway. So, it's all fairly\nstraightforward. The main thing, I guess, from an IAM authorization point of view, and it might be\nobvious, but when you try it, you'll need to enable AWS version four signatures on the requests because\notherwise you won't be able to pass an IAM authorization. So, the first thing you do when\nyou invoke a URL against one of these services, if you've got auth turned on, is going to do an\nIAM check just like it would with any other AWS service. So, you need to have a signature with the\nservice, which is VPC Lattice services in the scope of your credentials. It doesn't support\npayload signing. So, you have to explicitly call out the header that says there's no payload\nsigning. If this sounds like a little bit complicated, don't worry because we do have a\nfull code example with a complete Lattice setup and also some client code for invoking services\nas well. So, it should be easy at that point to see how it works.\n\n\nLuciano: That's interesting. So, I imagine that one of the trade-offs is that from a development perspective, every time you are doing\nthe call, you need to add that extra bit of code, making sure that your requests are probably using\nsync before and adding the signature correctly, which I don't know. I've done that in different\nlanguages and it's always a bit of a hit and miss in some languages. It is easier than others\nbecause maybe you have some libraries that can make most of the stuff easier.\nIn other cases, you end up implementing some of it and it's very easy to get something wrong and\nthen you spend hours and hours troubleshooting it. So, maybe that's an interesting trade-off to keep\nin mind. You mentioned that this is something you can use basically in a very free way. You can\norganize your teams in different accounts. So, how would that look like? Is that something we\nrecommend to do? Is it more complicated or it's just seamless with Lattice?\n\n\nEoin: This is really, I guess, where it really shines actually in cross-account because of the lack of\nrouting and everything. Once you have your Service Network set up, the process of sharing it with\nRAM is quite easy. You can do it in a single line of SDK or a single resource with CloudFormation\nor with the console even and the Service Network then just automatically appears in all of the\naccounts that you've shared it with and then they can quickly create the association with\ntheir service. So, the order is the admin would create and share that Service Network with RAM\nwith specific accounts or users or roles or the whole organization and then the dev team just sees\nit associated with the service and they would associate the VPCs they consume with the service\nnetwork and that's it. All of the things can talk to each other at that point. So, it's really,\nreally seamless and I can imagine as well, once you've got this set up for the first time,\nyou can start scaling to like hundreds of services really quickly and each of them has a DNS name\nand the process of communication is really easy. You can have conventions around your policies and\nwhat needs to be in there and it just happens a lot quicker than the typical setup when you've\ngot like all these teams trying to coordinate, make sure you don't have the right, you don't\nhave overlapping IP ranges and all that kind of stuff.\n\n\nLuciano: How does the routing work other than, I mean, you described the DNS mechanism, but do you need to explicitly configure anything or it\njust happens out of the box?\n\n\nEoin: This is the other piece where Lattice is completely different to everything else because it's got a very special mechanism for routing. So, like we say, it doesn't\nhave any routing tables as such. So, when you associate with your VPC with a Service Network,\nthose VPCs will automatically then resolve DNS names for Lattice services to a link local IP\naddress. So, you might have come across link local IP addresses in other places. If you've ever used\nthe EC2 metadata service, it starts with a 169.254 IP and that means those, these are special\nIP ranges in the IP spec that are not routable. So, they're only valid on a local host.\n\n\nBut Lattice is essentially using these as kind of a door into the hyperplane infrastructure where\nthey do all of the network virtualization. You've got this VPC control plane that we've already\ndescribed where you create these logical constructs like services and Service Networks,\nbut in your VPC, there's now a special VPC Lattice data plane and these link local addresses\nare the door into that data plane. So, when you do a DNS lookup on a Lattice DNS name,\nyou'll end up with one of these link local IP addresses. These DNS names are global,\nso anyone in the world, if they know the DNS name, they can look up the address,\nbut it's completely meaningless outside of your VPC. The other beautiful thing about this is that\nLattice doesn't consume any of your IP ranges. You can imagine if you had a sidecar or a proxy,\nit needs to have an IP address and needs to do some proxying and routing. That doesn't happen\nhere. It just automatically goes into this Lattice data plane. Lattice works within and across\naccounts, but it's always within a single region. So, there's no multi-region possibility or cross\nregion propagation. Cross region routing is something that's really kind of hardcore networking\nanyway. So, it's not really in the domain of inter-application East-West communication.\n\n\nLuciano: So, how does that work instead with Lambda?\nBecause you mentioned before that it is possible to effectively send the traffic to Lambda, but we also know that Lambda is totally event-based,\ndoesn't have a concept of listening in a port, for example. So, how did they make that integration happen?\n\n\nEoin: VPC Lattice actually is a new trigger type for Lambda.\nSo, I think it's the fourth HTTP-based synchronous triggering mechanism for Lambda. So, you've already got Application Load\nBalancers, API Gateway, and you've got Function URLs. Actually, fifth, because I forgot about\nAppSync as well. So, now you've got this fifth one. So, it's a new event trigger for Lambda,\nand if you invoke a service, the VPC Lattice data plane is going to do that synchronous\ntriggering of the function for you. Now, the payload is similar to putting a server on a\nVPC Lattice, similar to, but actually different from API Gateway or Application Load Balancer.\nSo, it looks similar, but you'll have to parse it slightly differently. Just because it's VPC\nLattice doesn't mean that the Lambda functions have to be running in a VPC with subnets configured.\nYou only need Lambdas to have a VPC if they're going to consume other services from Lattice.\n\n\nLuciano: Okay, what about in terms of observability? Because if you use regular VPCs, you're probably\ngoing to have to use logs, but what are the options here?\n\n\nEoin: One of the things that network admins are going to be really happy about with VPC Lattice is the\nfact that you can create a log group in CloudWatch on the Service Network level, and you get logs\nfor all of the traffic through Lattice that you get everything. You get your HTTP request,\nand then you can see who is calling, what IP address are they coming from, what's their\nprincipal identity ARN, what Service Network are they coming from, what service are they coming\nfrom, what service is their target group. All of that information is in the logs. So it's really\nnice that you've got one central log with all this east west traffic in it. This is like a one line\nconfiguration. I'm sure a lot of network engineers have spent months configuring great observability\nfor this kind of communication in the past. So I think this is one area which will really sell it\nto a lot of people. You also then get CloudWatch metrics, but those aren't on the Service Network\nservices, and you get them per service and per target group.\n\n\nLuciano: So I am almost sold, but before opening my wallet, I want to know what's the cost. So let's talk about pricing.\n\n\nEoin: Before I do talk about pricing, actually, one of the things is that somebody asked us earlier on,\ndoes Lattice support VPC Flow Logs then? Because if you've got this CloudWatch log, is it possible\nto do Flow Logs? And after they asked, we went and tried it, and I was surprised to see that they\nactually do, because I thought you'd only see a flow log between two VPC IPs and not these link\nlocal IPs, but you can turn on Flow Logs for any VPC that's connected to a Service Network, and\nyou'll still get all of the flows with the link local addresses in there. Nice. So I've held off\non talking about pricing for long enough. Let's go for it. So the pricing dimensions are three\ndimensions. You got per service, and it varies a bit per region, but let's look at US East One.\n\n\nAt the moment, it's two and a half cents per hour per service. So that sounds fine if you've got a\nfew services. If you've got hundreds of services, which you can do with Lattice, you can see that\nwe're shutting up pretty quickly. Then you pay 2.5 cents per gigabyte as well. And there's another\ndimension, which is per request, and you pay 10 cents for a million requests. I think most people\nwould be focused on the first two, really. And if you compare it to Transit Gateway,\nTransit Gateway, you pay 10 cents per attachment per hour, and then two cents per gigabyte. So\nit's more expensive in one dimension, less expensive in the other. But comparing it to\nVPC Peering, I mean, VPC Peering is completely free. You could also compare it to just your\nexisting service mesh set up in your applications, which you might've spent a lot of engineering\neffort on. And for that reason, now I see AWS talking about using Lattice for microservice\ncommunication, but if you're one of the companies that's got hundreds, thousands of microservices,\nI can imagine that Lattice could be a bit expensive. Despite that pitch, I think it\nmight be a bit more palatable for service to service or application to application communication\nthan just microservices. I also think looking at how much it simplifies networking, and if you\ncompare that to the engineering cost and the opportunity cost of all the engineering effort\nand the interaction between teams, it might actually be a very valuable trade-off to look\nat Lattice if you can really go all in on it, especially, and get rid of a lot of that\nengineering effort.\n\n\nLuciano: Okay. That's really cool so far. You mentioned we have a demo application that is available in our repository that we will share in the show notes. Do you want to describe\na little bit what's the idea for that particular demo?\n\n\nEoin: This is what we use to explore and learn about Lattice and it's a multi-count setup. So it has everything we talked about. There's a\nnetworking setup for a networking account where you create the Service Network, share it with RAM.\nYou've got your centralized logs, and then we've got a kind of assumption that you've got a\nexisting registered domain, public one, and a public hosted zone in that networking account.\n\n\nAnd then we've got two other accounts, account A and account B, we call them, and that's where\nthe two different services run. And we've got Lattice services there. One of them is quite\nsimple. It's just got a Lambda function at the back. And the other one has kind of used the\nweighted traffic routing so that half of it goes to another Lambda function and half of it goes to\na Fargate service. And interestingly with Fargate, the way you integrate Lattice with it at the moment\nis by using a load balancer. So you don't route to individual tasks or containers.\n\n\nYou use a load balancer in front of it, which is, you know, it simplifies it in some ways, but the negatives\nwith that are that you still pay for the load balancer traffic as well as Lattice. It just\nseems like an extra resource you don't need given the fact that Lattice supports all this kind of\nstuff anyway. So maybe in the future, we'll see an improvement there, some additional ability,\nbecause with Kubernetes, there is a gateway controller that AWS have provided that automatically\ncreates Service Networks and services and creates IP addresses in the target groups in Lattice\nfor you. So it's a very different approach. Anyway, back to the demo application.\n\n\nThis other service is going to route traffic between Lambda and Fargate. And the Lambda function is\nactually going to go through Lattice and invoke the other service in the other account as well.\nSo it's kind of got the whole thing set up. And then we've got in that CDK application for the\ndemo, we've also got like an EC2 instance and a VPC in the networking account that you can manually\nhook up to the Service Network. And then there's a client with the signature set up all in there\nthat you can use to invoke one of the services. And you can see the traffic that comes back.\nYou can see if you keep invoking it, that it'll hit sometimes the ECS container,\nthen it'll hit the Lambda target. And the Lambda target will then talk to the other service and you\ncan see the traffic coming back and you can explore the logs and you can look at the flow\nlogs and all this kind of stuff. So check it out. We'll give the link to that in the show notes.\nAnybody who's got any interesting questions on that repo or improvements as well, we'd love to hear from you.\n\n\nLuciano: That's awesome. So I guess that's a great introduction to Lattice.\nI'm going to try to quickly summarize the use cases that you described. So communication between microservices,\nprivate APIs, east-west communication, app modernization, because you can easily switch\nover from the old endpoint to the new endpoint in a transparent way. And in general, it seems that\nthe main selling point is that it kind of reduces all that communication friction and ownership\nfriction between admin teams and development teams. So it kind of defines very clear integration\npoints between the two teams and then there is a lot more freedom to operate. So that can be a very\ncompelling reason to use this service. I think we also have a few resources that we collected while\ndoing all of this research. There is a Serverless Office Hours session, which was really interesting\nand we're going to have the link in the show notes. And then there are other few articles\nthat we are going to link, including the official AWS documentation. So check out the show notes for\nmore links. And with that, I think that's everything for this episode. We are really excited, as usual,\nto know your perspective. If you've used it, what do you think? Do you actually have a real use case\nin production? Tell us about that and that way we can compare opinions and learn something more\ntogether. So thank you very much and we'll see you in the next episode.\n"
    },
    {
      "title": "89. Should you simulate AWS locally?",
      "url": "https://awsbites.com/89-should-you-simulate-aws-locally/",
      "publish_date": "2023-07-14T00:00:00.000Z",
      "abstract": "Welcome to the epic tale of AWS Bites! In this chapter, we embark on a perilous journey through the challenges of developing distributed applications on AWS.\nWe encounter fierce foes in the form of deployment times and limited access to real AWS services during local development.\nBut fear not, for we have powerful tools at our disposal, including the legendary LocalStack and Serverless offline. And if that's not enough, we have tips and tricks for optimizing our development flow without local simulations, using well-structured code and unit tests. We even share CloudFormation tricks to speed up deployment times and reveal the secret of speeding up the development of IAM policies with Session Policies.\nSo grab your swords and join us on this epic adventure to overcome the challenges of local development on AWS!\n\nfourTheorem is the company that makes AWS Bites possible. If you are looking for a partner to accompany you on your cloud journey, check them out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources:\n\nLocalstack service coverage\nLocalstack v2.0 announcement\nCargo-Lambda\nAWS .NET Mock Lambda Test Tool\n[Step Functions Local](https://docs.aws.amazon.com/step-functions/latest/dg/sfn-local.html for local Step Function simulation\nArticle by Yan Cui on the topic of testing serverless applications\n\n",
      "transcript": "Luciano: Development on AWS is often slowed down by deployment time, particularly in development environments.\nOften people turn to local simulation of AWS. While this can solve some immediate challenges\nand speed up your feedback loop, it comes with its own problems. So today we'll try to answer\nthe age-old question, should you simulate AWS locally or go to the real cloud every single time?\nMy name is Luciano, I'm here with Eoin and this is AWS Bites podcast.\n\n\nAWS Bites is made possible by the support of fourTheorem, an AWS consulting partner that\nworks with you to make AWS migration, architecture and development a success.\nSee fourtheorem.com, this link is in the show notes and this way you can find out more about fourTheorem.\nSo, I remember that when we were working with containers or monolithic applications like\nNode.js servers or Spring servers or whatever PHP framework you would like, it was always the\ndefault to just run things locally, develop that way and when you were happy you had a way to\npublish things to an actual environment remotely. Now things seem to have become significantly\nharder because you don't really have that real option of running things locally, you can maybe\nsimulate certain things but since you are dealing with distributed applications it is becoming\nincreasingly harder to be able to simulate everything on your own machine. So what are\nthe options in AWS? Is it true that things are getting harder? What can we do?\n\n\nEoin: When Docker became popular I think it made local development a lot easier because the local runtime really matched the production runtime quite closely. You really only had to think\nabout whether you would use a remote database or a local one and then with things like Docker Compose\nand then other container orchestration, things like Kubernetes, you could run a set of microservices\nand related resources like ElastCache, RabbitMQ and your databases locally too.\n\n\nNow particularly with serverless applications, a lot of the application is realized with lots of distributed\nAWS services outside of the code running in containers and it might be in Lambda functions\nas well. We try to take advantage of managed services to save on production maintenance and\nprovide that scalability, robustness and security but the trade-off then is that local development\nhas become a lot more challenging. So if you want real SQS, DynamoDB, RDS, IAM, you need to deploy\nto AWS usually using Terraform or CloudFormation or something else like that. And sometimes\ndeployments are fast and it's not really a problem but often they can take many minutes and when\nyou're doing it multiple times per day this really adds up if you're making lots of changes especially\nearly on in development. So I think this is really the challenge that has led a lot of people to try\nand simulate things locally. What have you tried and what are the options there?\n\n\nLuciano: I tried a few things. The first one that comes to mind is LocalStack which has been around for a while and it's\nan open source project that tries to effectively simulate a bunch of different AWS services\nand allows you to run them locally using Docker containers and then you can connect all of them\nand the API tries to match exactly the API that you have in AWS. But of course it's not perfect.\nI don't think it's humanly possible to try to recreate AWS in its entirety and run it locally.\n\n\nSo the coverage is limited. It might work well for certain services and certain specific use cases,\nmaybe if you're using only certain specific APIs that are the most common, but if you're doing\nsomething a little bit more outside the common use cases probably you will find that the coverage is\nnot there yet. There was recently a new version that was released, version 2, which seems like\nthe project is trying to push forward and try to extend that coverage and try to make the whole\nuser experience even better. But I think it's still something that we need to figure out exactly\nto which extent is going to allow us to cover the full range of things that you might want to do in\nAWS. I think that the things that I found are generally very hard to simulate are IAM permissions,\npolicies, the whole thing where you kind of generally give up on simulating that locally.\n\n\nYou just say, that's something I'm going to test in the real environment. For now,\nwhen I run things locally, I'm going to assume everything is open and there is no permission\nissue. So there is always some kind of art trade-off there and you kind of think it's\nworking and then you have a permission problem in production and you need to test that part\nin, sorry, not necessarily production, but like the remote environment on AWS.\n\n\nAnd then you'll need to test that until you actually get it right. So maybe that's an area\nwhere there might be a little bit more tooling and support for local development.\nAnother similar one is CloudFormation. So I haven't found a way, not that I even tried that much, to test your\nCloudFormation locally. Generally, CloudFormation is supposed to run against AWS. So that's\ngenerally the way I test it. I just write code, try to deploy, see what happens and iterate that way,\nwhich can have a very slow feedback loop. So maybe that's an area where there could be some tool as\nwell, maybe in the LocalStack space that can help us to do some more testing locally. Another thing\nthat is worth mentioning about LocalStack is that this used to be a free and open source\nproject. It still is an open source project, but now there is a company behind it and there is a\ncommercial offering as well. So that might have its own pros and cons. Maybe there is more commitment\nin trying to improve the quality of the open source version, but of course there is also going\nto be a push for more commercial offerings. So let's see what happens.\n\n\nOther ones that I tried are serverless local simulation. So these are especially important when you are working with\nthings like Lambdas and you maybe use some frameworks to be able to write and deploy\nthose Lambdas easily. Oftentimes this framework will give you some kind of capability to do\nsome level of local testing. Worth mentioning Serverless offline, which is a plugin for\nserverless framework and allows you to simulate API Gateway and Lambda locally. So effectively\nit's going to run for your web server and you can call it, you can send call request, postman,\nuse the browser, whatever you like to actually send request directly to your Lambdas running\nlocally. So it's a very nice way to be able to test your APIs locally as you are developing them\nusing serverless technologies. Very similar if you use SAM. SAM has an option called SAM local\nand that does pretty much the same thing. And I have been recently playing with Lambdas in Rust\nand there is another tool called Cargo Lambda, which is kind of the default tool to bootstrap\nyour Lambdas and that comes also with some commands that allow you to simulate your Lambdas\nlocally and integrates very well with SAM. So if you do your project in SAM and you are writing\nyour Lambdas in Rust, you can use Cargo Lambda for all the building and running part.\n\n\nBut then you still use the commands that you are well used to use when you use SAM. So you can still run SAM\nlocal and behind the scenes is going to use Cargo Lambda to kind of compile your Lambda and run it\ncorrectly. There are other ones in .NET. For instance, there is AWS.NET Mock Lambda test tool.\nI haven't used this one, but we will have a link in the show notes in case you want to look at that one.\n\n\nAnd finally, also worth mentioning that for other services like DynamoDB and Step Functions,\nthere are some packages that you can download directly from AWS. These are generally either\nDocker containers or JAR executables and they give you some kind of local simulation of that\nparticular service. For instance, for DynamoDB, you don't get a replicated distributed version\nof DynamoDB, but it is pretty close to the API that you get when you use DynamoDB remotely.\n\n\nSo I think you can rely on that for most of the things that you will do with DynamoDB.\nAnd very similar, there is the Step Function local tool, which allows you to run some local\nsimulation of a Step Function. And it's also interesting because it gives you ways to mock.\nBecause with Step Function, we can effectively call other services in AWS, it gives you a way\nto mock those steps. So you can still execute your Step Function simulation, even though you might be\nrelying on other services that are outside the scope of Step Function. So I guess my comment\nthere is that in general, these simulations are good as a starting point, but you only get so far\nbecause you always end up eating some kind of limitation, missing API or difference even in\nthe way that the API works locally with the way it works remotely. And again, there is still the\nproblem of permissions that you don't generally, you don't get any check on your roles or policies.\nSo you kind of assume that everything is correct until you actually run it in AWS.\nSo what do we recommend? Is there any approach that you find works best than others?\nI used to spend a lot of time on trying to get LocalStack to work.\n\n\nEoin: I Remember the early days, trying to make a pull request on LocalStack so I could get event bridge support for some\nintegration tests I was doing. And that was just because I was determined that it was going to\nwork for me and I ended up spending more time on trying to work around the limitations. So\nthis is what you're faced with. And similarly, you mentioned Step Functions local, and recently I tried\nto use that, but I think there was an issue with support for the new map type because they changed\nhow you can declare maps when they added support for distributed maps, but that's not there in Step\nFunctions local yet. So these tools will always have a challenge keeping up with the latest in\nthe cloud. So I am more now of the opinion that you should put less effort into local simulation\nand more into trying to optimize your development flow in other ways. So one thing you can do\nwithout local simulations is just try and focus more on unit tests. It's a good practice anyway,\nbut if you focus on separating your code well, then you can just unit test the logic and you\ndon't have to deploy every time. So this is just a recommendation really around code structure,\nfollowing clean code principles, separating the AWS specific stuff from the actual logic,\nand following some sort of pattern. Like there's plenty, especially in the serverless community,\nof talks and articles around hexagonal architecture, also known as ports and adapters,\nwhich is essentially a fairly simple mechanism to ensure that you separate out the inbound\nconnections and the outbound connections and integrations into your database and other services.\n\n\nWhen you are unit testing, for example, in Python, you have moto and moto is a Python library that's\nactually the internal logic that is used by LocalStack. So it's like the library that makes local\nstack possible. You can also use moto to mock AWS services in your unit tests. Now I have sometimes\nused LocalStack and still do from time to time in unit tests and integration tests if the needs are\nsimple and it can be valuable in that case. So what I would also say then is that kind of look\nat where you're spending your time and how you can just optimize for that. So if your issue is\nCloudFormation deployment speed, disabling rollbacks can help because sometimes, especially\nin development, a lot of the reason why it's slow is that you're making mistakes and misconfigurations\nand then when you deploy, the deployment time is reasonably fast, but the rollback time is the\nthing that gets you. So by disabling rollbacks, you can make that process a bit faster. And then\nif you've got lots of resources in your stack, then you don't have to roll back everything just\nbecause you've got one configuration value missing in some small resource.\n\n\nSimilarly, you could just try and incrementally build out these stacks and deploy them and kind of resolve the problems one\nby one. I also think I'm kind of hopeful that CloudFormation will become significantly faster\nin the future. I notice sometimes how it's just a little, some of the services implementations of\nthe resource providers end up doing things like trying to create a resource. It fails because\nthere's a misconfiguration, but you can see in cloud trail that CloudFormation is retrying\nthis thing with a back off and then it just, it can really slow things down. So those are fairly\nsimple things, low hanging proof that I think the AWS can address. I do see that AWS are trying to\nmake things faster in general. So we talked before about SAM acceleration, SAM sync command.\n\n\nThis is essentially a backdoor to updating things like Lambda functions, API gateway,\nand Step Functions without using CloudFormation. So it's really just a quick and dirty hack really,\nbut it speeds up development flow. So you don't have to deploy real resources using CloudFormation\njust because you want to update the code or configuration for a function.\nAnd there's a similar thing in CDK called CDK hot swap, which provides support for Lambda Step Functions, S3\nassets, CodeBuild and AppSync resolvers too. So I'd say look for optimizations wherever you can.\n\n\nYou can spend a lot of time trying to get the right IAM policy and deploying lots and lots of\ntimes to get it right. I think we've all experienced that frustration that can be really slow. So one\nway to optimize this is to use session policies locally or within a shell on AWS, like in cloud\nshell or on an EC2 instance. So session policies are like ephemeral policies that you don't\ncreate in AWS. You just specify them when you do an assume role and you don't need to create a\npolicy every time. So if you craft your policy locally, just in a JSON file, then you can do\nan assume role with it, test if it works. It's much quicker to test and make changes. And then\nonce you have the policy right, you can codify it in your infrastructure. You can also, if you're\nmaking lots of deployments, you can also just take the ClickOps approach and tweak the resources\nmanually in the AWS management console. And then once you're happy, codify those changes for proper\ndeployment and test that with automated integration tests. So I'd say there's a combination of\napproaches there you can take, deploy your resources by all means that you need, but then\nrun the code against those resources locally, just talking to the AWS services. Then test your\nLambda functions. You can put whatever tests you need in place, whether it's unit test or an ad hoc\nmanual test or some sort of automated integration test.\n\n\nLuciano: One final comment that I have is that there are some tools that we discussed before, like for instance, Application Composer or Step\nFunction designer, which are tools that sometimes they can help you to get unstuck.\nSo if you are getting some configuration wrong and it's painful to go through this cycle of it is wrong, I'm going\nto deploy again, fails again, fix something else, deploy again, fails again until you actually get\nit right. I think these tools can give you kind of a shortcut into figuring out what's the right\nconfiguration. You can easily sketch out with the visual designer something that looks like your use\ncase, understand exactly what is the right configuration, then transpose that configuration\nto your actual code. This is another, maybe it's a bit of a hack, but it's another strategy that\nsometimes I use to kind of unblock myself when I get stuck into the cycle of failing and retrying,\nfailing and retrying until you actually get what you want to do right. So that's another tip for\nyou that you might want to explore. So in summary, I think for today, what we want to share with you\nis that things like LocalStack are really, really good projects, but they are not to be trusted 100%\nas a perfect copy of AWS. Of course, you need to understand that there will be approximations there,\nthere will be missing features. So use them, but with the benefit of the doubt.\n\n\nSo if they help you to speed up your local development, take that value, but be aware there might be\ninstances where that local simulation is not going to be enough and you need to figure out\nanother strategy. And definitely remember to structure your code in such a way that you can\ntest most of it without having to rely on AWS. So you're definitely going to have some business\nlogic, try to make sure that that business logic is as decoupled as possible from the logic\nintegrating with AWS services. So you can test your own business logic without having to rely\non AWS. And for all the times where you need to rely on AWS, you can also mock parts of your unit\ntest. So there are libraries like Modo, if you do Python, that can help with that. If you use\nJavaScript, the new SDK also comes with an additional library that helps you to do mocking.\nSo there are lots of ways there where just by doing unit tests, you can be so much better\npositioned to trust your code locally before you ship it to production. Now, we probably missed\nmany ideas. I'm sure that people listening have lots of other ideas. Everyone is experimenting\nall the time with this topic. So if you have something you would like to share with us, we're\ndefinitely going to be happy to learn from you. So make sure to use the comments below or reach out\nto us on Twitter to share your feelings and your ideas about this topic. And as always, if you\nreally enjoyed this episode, remember to thumbs up, subscribe, or if you're listening to the\npodcast only version, give us a review. Thank you very much, and we'll see you in the next episode.\n"
    },
    {
      "title": "90. 9 Ideas that would Skyrocket AWS to New Heights",
      "url": "https://awsbites.com/90-9-ideas-that-would-skyrocket-aws-to-new-heights/",
      "publish_date": "2023-07-21T00:00:00.000Z",
      "abstract": "AWS is great, but it can surely be better, much better! Today we want to mention 9 things that could help AWS to reach new heights: hard billing limits, better UX and documentation, and much more! We also have a special message to AWS and all the AWS professionals who have helped throughout the years.\nJoin us in this intergalactic journey to improve the leading cloud provider!\n\nfourTheorem is the company that makes AWS Bites possible. If you are looking for a partner to accompany you on your cloud journey, check them out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources:\n\nOur previous episode on how to simulate AWS Locally\nOur previous episode on bastion hosts\n\n",
      "transcript": "Eoin: AWS is the leading cloud provider today,\nbut there is competition and with AWS's popularity,\nthere are plenty of haters and also lots of valid criticism as well.\nThere's a lot about AWS that we love, but there's always room for improvement,\nso today we are going to reveal our top nine improvements\nthat can catapult AWS to the next level.\nI'm Eoin, I'm here with Luciano and this is AWS Bites.\nWhat is your list of improvements?\n\n\nLuciano, I'm dying to hear what your list is,\nbut I want to get mine off my chest first.\nIf I could sit down with Adam or Werner today and present my wishlist,\nwhat would I say?\nI think the number one thing is probably hard billing limits.\nAnd what I'm talking there about is preventing people from major bill shock\nby allowing us to limit API requests once you hit a budget.\nSo let's say you set your budget,\nthen every time you make an API request or have something running,\nAWS is clocking up the bill in real time.\n\n\nAnd then once you hit that budget, you suddenly get throttled, rate limited,\njust as if you hit a quota.\nThere are other APIs that do this.\nI think a lot of people have just experimented recently with the open API,\nand you get an API key, you get a budget,\nand once you reach that budget, it's capped.\nSuddenly your API requests are denied.\nI think something like that would be nice with AWS.\nOf course, they can still allow you to play with the free tier\nuntil you hit those limits too.\n\n\nSo that's number one, and I think that would work well for AWS in the long run.\nIt's probably a lot of engineering to implement a feature like that.\nIt might result in a bit of lost revenue,\nbut I think ultimately it would bring more people to AWS\nand increase the trust relationship between AWS users\nand the company.\nNumber two is much faster deployment times.\nAnd this is something that I think can really make developers' lives easier.\n\n\nAnd we're talking about APIs that have control plane actions,\ncreating instances, creating databases.\nEvery speed improvement in those things matters.\nCreating load balancers and registering targets and load balancers,\nall of that stuff takes developer time.\nDeveloper time is a really valuable resource.\nCloudFormation speed as well.\nI've talked about this quite a few times,\nand I'm hopeful that it will improve over time.\n\n\nBut the sooner deployment time gets closer to zero,\nthe better off we'll be.\nI've heard a lot of people asking for more consistent coverage\nfor CloudFormation and X-ray as well.\nSo I think there should be like a checkbox of things\nthat teams need at AWS before they can launch a new service.\nOne is complete and consistent CloudFormation support,\nand the other one is X-ray support.\nSo we've seen... I think things have gotten a lot better.\n\n\nCloudFormation support used to drag a lot behind new feature releases.\nI think largely most services launch with CloudFormation support these days,\nbut there are still some gaps.\nAnd there's some weird ones, like you haven't been able to tag\na VPC endpoint in CloudFormation for many years,\nand it's still an open issue. I don't know why.\nThe fourth one on my list, so my second last one,\nand it was hard just to limit it to five,\nis easier multi-account management.\n\n\nSo we do a lot of kind of setting up AWS landing zones\nand best practices and all of the account structure,\norganizational units, service control policies,\nand then like templates for each account\nand things like governance and security.\nAnd then you have to manage multiple accounts,\nand you have our AWS organizations, which is an umbrella for all of that,\nbut it's not really easy to see everything in one place.\n\n\nAnd when you need to share things between accounts,\nit has to be done very explicitly.\nOn the other hand, Azure has a much different approach\nwhere you can structure lots of different teams and resource groups\nand everything within different subscriptions in an Azure account.\nSo you don't have this kind of scatter of individual isolated accounts.\nIndividual accounts are great for isolating security boundaries\nand quota boundaries, but it becomes difficult to manage them.\n\n\nMy favorite tool for actually managing multiple accounts\nand deploying things to multiple accounts is OrgFormation,\nwhich is an open source project,\na bit like CloudFormation and based on CloudFormation,\nbut it allows you to deploy resources and deploy accounts\nand then deploy things to multiple accounts.\nIt's a really nice, simple declarative language.\nAWS, on the other hand, has Control Tower,\nand Control Tower is more based through the console,\nand there's a lot of magic going on in the background.\n\n\nSo Control Tower works pretty well, but there are a lot of moving pieces.\nSo I'd like to see AWS simplify this whole multi-account management thing\nand maybe provide some sort of CloudFormation-based Control Tower\nso that we can manage our organization in code.\nAnd the last one, which kind of relates back to the first one,\nis just price optimizations.\nSo I'm talking about price decreases everywhere.\n\n\nI think these days people are looking at AWS cloud cost quite a lot.\nI think the fear of pricing is slowing people down in AWS cloud migration,\nbut there are some things that can just be optimized in general,\nlike having a single VPC endpoint for all AWS services\nso that you don't have to create one for each service in each account.\nAnd then there's single NAT gateway.\nSo if you have to have a NAT gateway,\nif they're not going to make it cheaper, which would be great,\nit would be nice if you could just have a single NAT gateway easily\nand share it easily with your accounts\nwithout having to do lots of complex IP routing.\nSo I think two out of the five have been focused on cost,\nand that's an important thing.\nSo I hope the lovely people at AWS\nare listening and furiously scribbling notes.\nNow, it's time for your additions, Luciano, and I'm very curious.\n\n\nLuciano: Yeah, I really like your five, but I only have four,\nand I hope they will be interesting as well.\nThe first one is automatic multi-region.\nSo with many services today, you can just flick a switch\nand say, make this multi-AZ,\nand that will make sure, for instance, RDS,\nthat you have multiple instances running on different availability zone\nand failover and everything is already nicely configured out of the box.\n\n\nI think kind of trying to transpose that idea to multi-region\ndoesn't feel like too much of a stretch.\nI'm sure that there is complexity there\nbecause the network characteristics will be much different\nfrom what you have between AZs with what you have between regions.\nBut it feels from a product perspective like something\nthat would be really nice to have, especially when you start to think about\nDR strategies for your own deployments.\n\n\nIt is always very, very difficult to configure things correctly,\nand with many services, you end up building everything yourself,\nwhile it could be a feature that AWS could just expose out of the box.\nThe second one I have is something that annoys people a lot.\nEvery time there is an announcement for a new service,\nespecially when this new service is labeled as serverless\nand then it doesn't scale to zero.\n\n\nSo if you're labeling something as serverless, make sure it scales to zero\nand that you are going to pay zero when you are not using that particular service\nbecause I think that's the expected definition,\nor at least it's part of the expected definition of serverless for most people.\nSo don't just label things as servers just for marketing reason.\nMake sure that they are really truly serverless and they also scale to zero.\n\n\nSo that would make every announcement,\nI don't know, better or less disappointing\nbecause there is always an element of,\noh, a new serverless service is not really serverless,\nand then you forget about all the nice things about that new project or service\njust because you focus on the fact that it's not really truly serverless as it was advertised.\nSo I guess that's, I don't know, more of a marketing thing than anything else,\nbut it would be nice to see that because you will trust new announcement a lot more.\n\n\nAnother one that I have is better UX,\nespecially when interacting with service like ElastiCache or RDS,\nwhich are a little bit more traditional services,\nso they still run behind the scenes in virtual machines that AWS is managing for you.\nAnd of course, these virtual machines are running in their own secure network environment.\nThey, of course, they're not going to be easily accessible from the outside,\nexcept that you will need sometimes to interact with those databases from the outside,\nmaybe because you want to query the data, maybe because you want to troubleshoot something.\n\n\nAnd this is something we've been talking about quite a few times.\nThere are several episodes that you can check, for instance, when we talk about Bastion hosts.\nAnd those are all escape patches that you need to put in place just to be able to access those databases.\nIt would be really, really nice if AWS will give you even just a web UI in the console\nthat would allow you to easily query your data in ElastiCache or RDS,\nor even if that's not easily possible because, of course, building a UI is not an easy feat.\n\n\nIt takes development time. It can be expensive for AWS.\nBut even just giving VPC support to CloudShell, I think, would help a lot\nbecause at that point you can just run a shell in that VPC\nand then use any CLI client to interact with your databases.\nI think maybe not ideal because it's still a good amount of complexity for a developer to take in,\nbut it would make things significantly simpler.\nAnd the last point I have is better DX and documentation in general.\n\n\nProbably similar to my previous point,\nbut it's more about the fact that every time I need to explore something new in AWS,\nand there is always something new because AWS is so broad,\nfor me, even though I have some experience with AWS, I've been using it for several years,\nI easily get lost.\nLike, I try to figure out, can I use this particular service?\nMaybe let's make an example in the IoT space.\n\n\nI'm trying to investigate a potential architecture.\nOkay, there are many, many services that AWS is giving us in the IoT space.\nSo let's try to figure out which one can be more useful to my particular problem.\nAnd you end up reading all the marketing pages first,\nand they're very high level and they promise you lots of features,\nbut without really giving you insights on the details.\nThen what do you do next?\n\n\nIt's like, okay, I can just check a workshop and that there are tons of workshops, which is great,\nbut then doing all the workshops takes a lot of time\nand you need to make sure it's worth your time to invest in doing the workshops.\nSo is that really the real service?\nSo you end up looking at blog posts, or you end up looking at the official docs\nand very easily you end up spending days just trying to make sense\nof what are the capabilities of that particular service.\nSo I don't know really if there is a solution to this,\nbut I think having a more cohesive way to find information about services\nand drill down at the right level would be something very welcome.\nI think as much as to people that are new to AWS,\nbut also to people that have a lot of experience with AWS\nand they are just trying to expand their knowledge into a new service\nor a new set of features that they haven't used yet.\nSo this is my top four.\nWhat do you think? You have anything else you want to add?\n\n\nEoin: No, I completely agree with those additions.\nI think we've got nine there, which is probably a lot to ask.\nMaybe it's a good time to thank all the great people at AWS\nwho are working hard to make it as good as it is.\nI hope that all of these suggestions are taken in the spirit of continuous improvement.\nI think if we even got one third of this list,\nAWS would really start to become next level for everybody listening and watching.\nWhat did we miss?\nLet us know your big ideas for making AWS better for all of us.\nAnd if you want to hear more from us about AWS,\nsubscribe to our YouTube channel or add the podcast to your player\nso you get notified whenever we release new episodes.\nThanks for listening and we'll see you next time.\n"
    },
    {
      "title": "91. Our Journeys into Software and AWS",
      "url": "https://awsbites.com/91-our-journeys-into-software-and-aws/",
      "publish_date": "2023-07-28T00:00:00.000Z",
      "abstract": "In this episode, we take a journey through time and technology and learn the origin stories of Eoin and Luciano. In this captivating discussion, they share their paths into the world of software development and their eventual immersion into the realm of Amazon Web Services (AWS).\nEoin's story begins in the early days of home computing, where he tinkered with a Spectrum ZX, coding programs and saving them on audio cassettes. He walks us through his computer science studies, securing his first jobs, and his initial encounters with AWS, where skepticism eventually turned into fascination.\nMeanwhile, Luciano shares his first magical encounter with a computer, igniting his passion for programming, and his subsequent journey into computer science and early work experiences. Moving to Ireland opened doors to work on pioneering cloud projects, leading him to develop tools like Middy and eventually join fourTheorem.\nThe episode delves into their experiences with serverless architecture, solo startups, and how they would approach learning differently if given the chance to do it all again.\nJoin us for a nostalgic, inspiring, and educational episode as we explore the captivating stories that shaped our expertise in the world of AWS.\n\nfourTheorem is the company that makes AWS Bites possible. If you are looking for a partner to accompany you on your cloud journey, check them out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources:\n\nGorillas videogame (GORILLAS.BAS)\nMiddy, Lambda framework for Node.js\n\n",
      "transcript": "Luciano: We talk with a lot of engineers as part of our job.\nAnd one of the conversations that always pops up is,\nhow did we get into software and how did we get into AWS?\nSo this is gonna be a very personal episode\nwhere we share our stories about\nhow did we end up in the place where we are today?\nAnd hopefully we'll find that interesting.\nAnd today maybe we'll share something\nthat could be useful for your career as well.\nMy name is Luciano and today I'm joined by Eoin\nfor another episode of AWS Bites podcast.\nAWS Bites is made possible by the support of fourTheorem,\nan AWS consulting partner that works with you\nto make AWS migration, architecture\nand development a success.\nSee fourtheorem.com to find out more.\nOkay Eoin, why don't you go first and tell us the story\nof how did you get into software\nand how did you get into AWS?\n\n\nEoin: Okay, well, how I got into software,\nI suppose I was lucky in that kind of generation\nand back in that time when kind of cheap home computers\nwere becoming possible really on a wide scale.\nSo the first computer we ever got was a Spectrum ZX,\nSinclair Spectrum ZX.\nAnd this was a very popular in the eighties,\nkind of I think from the early eighties,\nI think we got one when I was around seven.\nAnd the nice thing about this was\nit was a tiny little computer which had like rubber buttons\nand you put it into the TV\nand it gave you the ability to play games\nand also write basic code.\n\n\nSo that was the first language I ever encountered.\nAnd the way you did it back then\nwas you would go to the news agent\nand pick up a magazine for that computer\nand in it it would have reviews of games\nand lots of tips and tricks,\nbut also code that you could type in into your computer\nand you could run games.\nSo you got, I suppose I got some exposure then\nto what is ultimately imperative programming,\nprocedural programming, not really until later,\nbut it was just like simple set of instructions\nand coaches mainly.\n\n\nAnd I didn't know most of that time\nwhat most of the code meant to be honest,\nbut it was certainly interesting enough for me\nto keep going with it.\nAnd we would then save the code you'd written onto a cassette\nso it was actually an audio cassette\nand the Spectrum connected with audio cables.\nSo it would convert the code to an analog signal\nand you could actually hear your code\nbeing saved to the tape.\n\n\nAnd then you could play it back.\nLike you could actually play this back in your stereo\nif you wanted, if you really wanted to hurt your ears.\nAnd it wasn't until a good few years later,\nlike I had a Commodore after that\nand much later when I was a teenager, we got a PC.\nI think it was like 386, you know?\nSo these first computers,\nI think the Spectrum had 128K of RAM\nand then going into computers,\nthings started to increase pretty quickly then\nas they had the exponential growth\nand Moore's law took off\nand processing power and everything increased.\n\n\nSo I was always into computers\nand it was pretty obvious that I wanted to study it.\nIt was really the one thing.\nSo I was lucky enough to be able to study computer science\nfor an undergraduate degree in Ireland.\nAt that time, the government had just announced\nthat they'd pay fees for college students.\nSo it was possible for everybody.\nIf you had good enough results, you know, to get in.\nComputer science wasn't the top of everybody's list.\n\n\nSo the entry requirements are pretty easy as well.\nBut my computer science degree was kind of interesting\nbecause it was very broad,\nlike hardware, software, algorithms,\nbut also some of the kind of business side as well.\nSo it was kind of interesting,\nalthough it's very abstract\nuntil you actually start working in it.\nSo I was able to get some summer jobs in college,\nbuilding websites and that kind of thing.\n\n\nAnd then once I finished college,\nI started working in like telecoms, Java applications\nand early distributed applications like Corva systems\nand that kind of thing.\nEvery job I worked in for a couple of years,\nbut nice thing about software industry\nis that you can move around different industries.\nYour software skills are always transferable,\nbut you get to pick up on new domains.\nSo I was able to work in investment banking\nand in kind of service ticketing systems\nand also worked in digital TV for a long time,\nwhich was really interesting building kind of like\nNetflix type applications.\n\n\nAnd that kind of all,\nmost of my career has still been kind of in building\nand deploying services onto servers, right?\nOn premises, in a data center,\nrunning on bare bones instances,\nthen things move quickly into virtual machines.\nAnd then I suppose it was,\nit wasn't like really early adopter to AWS.\nI do have an account from when AWS first launched\nand I have like a 20 year old API access key.\nThat's inactive, but it's still there just for like,\nit's like a museum piece now.\n\n\nWhen AWS first started to emerge,\nI don't tend to jump into new technologies very quickly.\nI like to see how other people get on with them first\nand experience some of the pain before I adopt them.\nBut it became clear pretty quickly\nthat AWS is going to be big.\nI remember we were talking in a recent episode\nabout AWS summit in London.\nI think I was at one of the first ones in London\nwhen it was basically just in one room in a hotel.\n\n\nAnd there was a couple of hundred people there.\nAnd that was, I suppose,\nwhen I really saw how big AWS was going to be.\nAnd then in my subsequent jobs,\nI started to do more and more.\nAnd with like a lot of people, it was EC2, S3, SQS,\na lot of stuff in the console at first,\nmaybe a bit of CloudFront, that kind of thing,\ngradually kind of increased my use of AWS over time\nand then using containers and then ECS.\n\n\nAnd I suppose it's really only at fourth theorem\nthat things have really accelerated\nin terms of AWS adoption.\nAnd I think that's all down to, I had a startup,\nmy own startup, which I was running solo before fourth theorem\nand I was then building a lot of product myself\nand kind of seeing how business works a lot more\nand trying to build things quickly.\nAnd that's when I started to use things\nlike serverless and Lambda.\nSo that's like 2016, really.\nSo it wasn't one of the first adopters of serverless,\nbut it was early enough, I suppose.\nAnd I saw what a difference it could make\nif you really started to use these managed services\nand how quickly you could do stuff.\nAnd that was kind of the philosophy we started\nwith in fourth theorem as well,\nand always trying to make things easier for everybody.\nAnd I think that's more or less in summary,\nmy journey to AWS.\nHave I missed anything?\n\n\nLuciano: That's pretty good.\nLots of stuff I didn't know about you.\nSo I'm glad you shared this.\nYeah, I don't know if it's too secret,\nbut happy to share it anyway.\nSo I started with my first computer\nwhen I was, I think about 12 years old.\nAnd I was always fascinated by computers,\nbut just even when I was even younger than that,\nbecause I had people in my family working with computers\nand I always thought they were a bit of a magic thing.\n\n\nAnd I knew eventually I would want to work with one as well.\nAnd at some point, I think when I was 12,\nI got lucky that my father was,\nhe owned like a kind of a warehouse business.\nAnd that business basically went bad.\nThey were dismissing everything.\nAnd as a result of that, I got a very old i386,\nI think it was, which just landed on my desk.\nAnd that was the computer that they used\nto kind of track the inventory of this place.\n\n\nAnd the only thing that was installed in that computer\nwas MS-DOS.\nSo I kind of didn't know anything.\nI just turned it on and I found this problem.\nIt's like, what do I do now?\nAnd I don't even remember how eventually\nI got into the kind of basic commands\nand figured out, OK, what are directories?\nYou can create files.\nAnd then I discovered that there were only two applications\nrunning in that machine, what installed in that machine.\n\n\nOne was a chess game, which was actually quite nice.\nSo I spent a lot of time playing chess in that machine.\nAnd the other one was another software\ncalled Banner, which was basically\nsome kind of word art.\nI would describe it that way.\nLike you can design effectively, like, I don't know,\ntext in a bunch of different shapes and print it.\nThat was it.\nI didn't even have a printer, so it was just fun to design it.\n\n\nBut then I couldn't print it.\nAnd for a long time, these were the only two things\nthat I was doing with that machine,\nuntil at one point in a school textbook,\nI found out that there was an addendum at the end saying,\noh, there is this thing called programming languages\nthat you can use to teach the computer to do things, things\nthat you want to automate somehow.\nAnd then it was explaining something about basic.\n\n\nAnd I actually went back to the prompt and typed QBasic.\nAnd I realized, OK, I also have this thing,\nso I can do this thing.\nAnd I was basically copy pasting manually examples from the book\ninto my own QBasic interpreter and running them.\nBut it was as simple as drawing like a Christmas tree\nor playing some tunes with the 8-bit tunes,\nor maybe just printing text and doing\nkind of form-like application where you would ask,\nI don't know, give me the input for this,\ngive me the input for that, and then do some calculation\nas a result. So for a long time, this\nwas the only type of programming that I did.\nAnd I didn't even understand for loops.\nSo everything was basically like extremely procedural step\nby step.\nBut it was super fun nonetheless.\n\n\nEoin: Can I interrupt you?\nDid you play Gorillas?\nDo you remember that?\nI think QBasic also came with Gorillaz.\nIt was like GORILLAS.BAS.\n\n\nLuciano: I don't know that one.\n\n\nEoin: OK, well, maybe it would have sucked up all your time\nand you never would have got into programming\nif you discovered it.\nI think it came with another one as well called Nibbles.\nIt was like a snake kind of game.\nYeah, I digress.\nI'm just reminiscing.\n\n\nLuciano: OK, I didn't know this one.\nSo now I want to search for a QBasic interpreter\nand play with these ones.\nSo yeah, long story short, after that, I\nknew I would be spending more and more time learning more\nabout programming.\nAnd during high school, I was lucky enough\nto get an actual pendulum with Windows 95\nand a proper desktop environment.\nAnd I remember FrontPage and Dreamweaver\nand getting into web programming and doing all this kind\nof stuff, learning HTML, CSS, JavaScript.\n\n\nAnd basically throughout high school,\nI ended up building websites for friends and family,\nsometimes actually getting paid for it, which was nice.\nBut you can imagine how little you\nwould get paid for that kind of stuff,\neven though it would take maybe your entire summer\nto build a website.\nBut it was fun, and I did learn a lot.\nAnd then I was also lucky enough to be\nable to go to the university to study computer science,\ndid a bachelor degree first, and then a master degree a bit\nlater.\n\n\nAnd also through that time, I did some work experience,\nactually working for a couple of companies,\ndoing kind of my own startup.\nAnd eventually, after all of that, I moved to Ireland,\nand I did more jobs still in the space of full stack web\ndevelopment.\nAnd funny enough, I ended up working\nin projects where the idea was more and more\nto go to the cloud, while in the company\nthere was no expertise to do that.\n\n\nAnd it was a great position to be in,\nbecause that meant that I was part directly, even though I\nwas hired to build the application as a full stack\ndeveloper.\nI needed to figure out, OK, how do we bring this to the cloud?\nWhat do we need to learn to be able to be\nsuccessful in the cloud?\nAnd of course, I was not doing that alone,\nbut I was part of a team where everyone was learning together.\nAnd that was my first time getting exposed to AWS.\nFunny enough, initially, it was lots of serverless stuff,\nbecause this was around, I think, 2016.\nSo I think we were among the first companies trying\nto build an entire product with serverless, which\nwas a lot of rough edges, like lots of limitations with Lambda.\nIt wasn't definitely at the level it is today.\nBut nonetheless, it was really interesting.\nAnd I remember we even did a few talks with AWS itself,\npresenting that particular project.\nSo it was quite successful, even though the company eventually\nshut down.\nSo none of this is still running in production today.\n\n\nEoin: Was it this early exposure to serverless that led to you building Middy for the first time?\nWas it because you were involved in the early days?\n\n\nLuciano: That's a very good point, yes.\nSo basically, we did all the usual mistakes\nthat I think most people will do when\nthey start using serverless.\nIt feels like you are doing microservices++,\nand you end up treating every Lambda as its own repository,\nproject, and code base totally isolated from everything else.\nEven though logically, maybe you have two Lambdas that\nare part of the same service, we were splitting everything\nto the most granular level.\n\n\nAnd then we realized, OK, the amount of code duplication\nis insane.\nSo we built this kind of internal framework\nto try to reduce that code duplication by applying\nthe middleware pattern.\nAnd then when the company was shut down,\nwe took that and open-sourced as Middy.\nSo that's actually a very good point.\nMy first exposure to AWS very early on\nhighlighted the need for that particular tool.\nAnd I was lucky enough to be working\nin a team that was willing to invest in creating that tool.\n\n\nAnd then they gave me freedom to keep working\non that and open-source it.\nSo after that, I was working in a couple of other companies\nwhere I was still part of cloud-heavy projects.\nBut at the same time, again, teams\nthat were a bit on the forefront in the company trying\nto figure out how do we bring something to the cloud when we\ndon't really have a lot of knowledge in the company\nabout deploying stuff in the cloud.\n\n\nSo again, I needed to roll up my sleeves and figure it out,\nOK, how do we do this and that?\nAnd in the other company, it wasn't as much serverless.\nIt was actually running big cluster of EC2 machines\nwith Elasticsearch.\nSo that is where I learned a lot the value\nof doing infrastructure as code, learning very well\nall the networking stuff, figuring out\nhow to implement auto-scaling, and lots of more hardcore\nskills that are very useful in the cloud.\n\n\nAnd I think those are, in many cases,\nthe reasons why you see a lot of value going into the cloud.\nBecause of course, you can run your own servers anywhere.\nBut if you don't use a cloud provider like AWS,\nyou don't always get a simple way to automatically scale\nand do big deployments of big clusters\nthat can scale elastically.\nAnd I think after that, there was another experience that\nwas somewhat similar, more in the space of e-commerce,\nso trying to optimize running e-commerce services online,\nstill using AWS.\nAnd then after that, I joined Fortier.\nAnd for me, Fortier was as well a big accelerator into,\nlet's try to become, as professional career experts\nin AWS, so let's try to get certified.\nLet's try to learn more about AWS architecture,\ndifferent applications of building stuff on AWS,\nfor different markets, different industries.\nSo that's a journey that has been going on for me\nfor two years and a half.\nAnd I think I've been learning a lot more about AWS\nand feel that I'm still probably 10% of, maybe less,\nof what there is to know about AWS.\nBut I'm feeling now I have a much more complete overview\non what's possible with AWS.\n\n\nEoin: We both have, I suppose, a kind of a traditional form\nof education in computer science.\nAnd that led to a career in it.\nI'm interested in a couple of things.\nOne is, if you had been learning software\nfor the first time in this age,\nwhen all the information is available\nand you've got LLMs to help you,\nand you've got massive amounts of educational resources,\nhow different would it be?\nAnd the other question is,\nis this representative of what you need to do\nto get into the cloud?\nI mean, I would say not.\nIn our last episode about interviewing for jobs in AWS,\nwe talked about like a computer science degree\ndefinitely not being a hard requirement.\nWe are interested in people\nfrom lots of different backgrounds.\nHow do you think it's different, I guess,\nfor people getting into the cloud these days?\n\n\nLuciano: Yeah, I guess for me, it's also interesting\nthat I had my education in Italy\nwhere there is a very common complaint,\nfor better or worse,\nthat education is very theoretical\nand there is very little practice,\nwhich is something that I didn't appreciate at the time.\nI think I'm starting to appreciate a lot more now\nthat I'm feeling the practical gaps\nand realizing how much effort was put into the theory\nthat I have a lot of ground already covered\nand then I can easily build the experience part.\n\n\nBut at the same time,\nI wish that when I came out of the university,\nthat I was more ready to go on the job.\nAnd I think if I was,\nwas only because I did a lot of stuff on the side on my own,\ntrying to work even just with friends and family\nbuilding things for them.\nI managed to fill that gap\na little bit into the practical knowledge.\nSo I don't know if that's representative for kind of career\nand studies everywhere else.\n\n\nI know that for instance,\ntalking with people in Ireland,\nin Ireland there is a good amount of practice.\nSo maybe it will be very different\ndepending where you get your degree,\nwhat kind of knowledge you come out with.\nSo I don't know how to answer the first question\nif it will be very different.\nI think it's already very different\ndepending where you study\nand exactly the curriculum that you follow\nand the professors that you have.\n\n\nAnd I don't know, the type of exams,\nif they tend to be more or less practical.\nSo I don't know, I think today it is true\nthat there is a lot more material\nand sometimes it's nice that you can learn things\nvery, very quickly.\nLike, I don't know, I remember every topic\nI did at the university,\nyou had like these massive books\nthat were like 500 pages each\nand you were supposed to study all of them\nand do the exam in like a month\nbecause if you needed to do 10 of those exams in a year,\nthat's the time you have.\nAnd basically you were always trying to optimize,\nlike how do I summarize this entire book?\nWhat are some notes that I can use?\nCan I use some slides from the professor?\nSo you were always finding shortcuts anyway.\nI think today's just gonna be easier\nto find those shortcuts\nand try to optimize for effectively the time that you have.\nLike you cannot possibly read 500,\nread, understand and memorize 500 pages\nof technical content in a month\nfor every single subject that you have to do\nat the universe.\n\n\nEoin: If you were 12 years old, again, I often ask this question to myself\nand we had access to everything that internet is today\nand the devices that we have today.\nWould you use the opportunity at that age\nto absorb a lot of knowledge\nand to really accelerate your learning\naround programming and technology\nor would you blow it all on TikTok and online gaming?\n\n\nLuciano: Yeah, I don't know.\nI guess in my personal case,\nI was having fun trying to literally,\nthat feeling of I can teach something to the computer\nand I can tweak it and I can try different things.\nSo it was kind of a game for me.\nSo I think that gamification part\nwas very fundamental for me\nto pave that path into computer science.\nBut I guess if you are missing that fun element\nthen you are only forcing yourself to do it\nbecause you think you're gonna have a career in the future.\nI don't know.\nI don't think it's gonna work out for people at that age.\nI think you need to be fun.\nSo maybe there are things today\nthat are more gamified for people\nthat might want to start to lay down that path at that age\nbut it still needs to be mostly a game\nbecause otherwise, I mean, you're still going to school,\nyou're still studying a lot of things,\nyou cannot possibly study something\njust because you're gonna need it\nat least in 10 years time if not more.\n\n\nEoin: The number of kind of younger people of that age, like kids getting into code for the fun of it\nmust be lower these days\nbecause back when we were writing basic programs,\nthe stuff you were doing was as compelling\nas the applications that were available\nbecause the applications were available\nwere not that sophisticated.\nWhereas now the applications that are available\nto everybody on laptops and smartphones\nare way more sophisticated than anything you can do\nwhen you're just beginning to write programming.\nSo it could become frustrating more early\nbut I think it just means\nthat people will take a different path into it.\nIt won't all be about the code first.\nThe availability of knowledge and information\nmakes people much savvier and much more rounded early on.\nSo maybe the coding part is something that will come later\nor become less relevant.\n\n\nLuciano: Yeah, I think that the meat of all the digital natives\nare automatically skilled with computers\nis definitely proving not to be true.\nSo this is definitely an interesting question\nthat we should ask ourselves.\nI'm finding that a lot of the younger generations\nyou give for granted that they understand our computer work\nbut that's definitely not a given.\nIf they don't actually study for it,\nthey have no idea it's just another magic box.\nSo maybe that's another interesting element\nof the education system where there is more that can be done\nat least to try to give some of the basic knowledge\non how to use a computer,\nunderstanding how it works and what's possible with it\neven though you might not want to pursue\na technology or computer science type of career.\n\n\nEoin: Yeah, I think at least it's good to see that now instead of like schools are adding programming skills\nor IT skills into secondary school curricula\nand now into also primary school.\nSo from a very young age,\nI think this is gonna take time to develop\nbut it does give hope that we'll have new generations\nof developers coming in with just a different path to it\nbut just a new perspective\nthat will ultimately improve things for everyone.\n\n\nLuciano: Okay, so I think that was a very good conversation\non I don't know, at least discover a bit more\nabout ourselves.\nI hope that was also interesting for people listening.\nI would be really good as to know more\nabout the people listening.\nHow did you start if you were happy with your path,\nif you think you did some early mistake\nor maybe something you would have done differently\ngiven the chance that you could go back\nand try something different.\nSo please share those kinds of stories\nbecause I think everyone has a different path\nand lots of people starting this path now,\nthey can take inspiration from what people have been doing\nto get to the point where they are today.\nSo use the comments below if you are on YouTube\nor reach out to us on Twitter\nand share your personal experience\nif you want to keep it private.\nWe still love to have a private chat\nso reach out to our direct message and we'll talk there.\nThank you very much and we'll see you in the next episode.\n"
    },
    {
      "title": "92. Decomposing the Monolith Lambda",
      "url": "https://awsbites.com/92-decomposing-the-monolith-lambda/",
      "publish_date": "2023-08-04T00:00:00.000Z",
      "abstract": "In this episode of AWS Bites, we take you on a captivating migration journey. Together, we'll explore how we transformed fullstackbulletin.com's automation process, leaving behind the complexities of a monolithic AWS Lambda and embracing the efficiency of Step Functions.\nJoin us as we dive into the challenges of automating a weekly newsletter, trying to strike the perfect balance between automation and manual curation. We'll discover the risks of relying on external services and how we navigated these obstacles during our migration.\nTogether, we'll uncover the step-by-step process of breaking down the monolithic Lambda architecture and orchestrating a more manageable approach with Step Functions. We will also briefly touch on alternative social platforms like Mastodon and other Twitter alternatives during our migration adventure.\nLearn with us about different migration strategies and the crucial role of observability for smooth operations. Finally, we will share some valuable lessons that you can apply to your production workloads.\n\nfourTheorem is the company that makes AWS Bites possible. If you are looking for a partner to accompany you on your cloud journey, check them out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources:\n\nFullstack Bulletin\nFullstack Bulletin on GitHub\nFullstack Bulletin remake recordings (YouTube playlist)\nSLIC Watch for automated observability\nEoin on Mastodon\nLuciano on Mastodon\n\n",
      "transcript": "Luciano: Lambdas are a great abstraction to write event-driven logic\nand to split this logic into small, composable,\nand maintainable units.\nBut as developers, we are not only sleeping bad\nat abusing technology, or at least I am.\nIn today's episode, I want to tell you the story\nof how I ended up creating a terrible monolith Lambda\nand how five years later, doomed by the shame and guilt\nof having done that, I am now decomposing that Lambda\nusing Step Functions.\n\n\nMy name is Luciano, and I'm joined by Eoin\nfor another episode of AWS Bites podcast.\nAWS Bites is made possible by the support of fourTheorem,\nan AWS consulting partner that works with you\nto make AWS migration, architecture,\nand development a success.\nSee fourtheorem.com to find out more.\nThe link is in the show notes.\nLet me start by telling you a little bit\nwhat's the context of this episode.\nI'm talking about a side project that I've been working on\nfor a few years called Full Stack Bulletin,\nand it's a free weekly newsletter that you can subscribe\nif you go at fullstackbulletin.com.\n\n\nAnd the idea is that if you are a full stack web developer,\nthen you might want to subscribe because you will receive\nsome interesting information every week,\nand you can try to keep up with this field\nthat evolves very quickly.\nAnd it's always very hard to find interesting information\nand try to stay up to date with the latest news.\nI guess that's the context, right?\nAnd let's try to figure out a little bit more\nabout the implementation.\n\n\nThis is a serverless project.\nAs I said, something I built a long time ago.\nAnd the way I built it is a Lambda project\neffectively triggered on a schedule every week.\nAnd the idea is that I want to automate as much as possible\nbecause it's something that I want to keep doing every week,\nand I might be busy.\nI cannot spend too much time on this.\nIt's a free activity after all.\nI think automation helps a lot.\n\n\nAnd the idea of this Lambda is that it's going to do\nas much work as possible to try to pre-combine a draft version\nof the next newsletter.\nAnd then I can edit it manually and improve things.\nSo what this Lambda is doing, the newsletter\nis basically made up of three parts.\nThere is a book suggestion.\nThere is an inspirational quote.\nAnd there are seven links that might\ncome from articles, videos, or GitHub\nrepositories, or other projects that might be relevant.\n\n\nHow do we fetch all this information?\nFrom different sources, but the most complicated one\nis fetching the links.\nAnd fetching the links is a bit of an involved process.\nSo the idea is that I'm going to be finding\ninteresting information throughout the week,\nand I will be tweeting it on Twitter,\nsharing these links on specific profiles.\nThere is a full-stack bulletin profile on Twitter.\nNow, to differentiate these tweets\nfrom other regular tweets, I generally use Hootsuite.\n\n\nAnd Hootsuite is a service that allows\nyou to schedule the tweets.\nAnd these tweets are also spread out during the week using\nHootsuite, because you can just put them on a queue,\nand they will be automatically spread out.\nSo this automation needs to connect to Twitter,\ndiscriminate between the interesting tweets coming\nfrom Hootsuite and the regular ones.\nThen it's going to take all the ones that contain links.\n\n\nIt's going to filter out all the invalid links or the ones\nthat have been used already in previous newsletters.\nThen it's going to try to rank them.\nThere is a super secret algorithm,\nwhich is just counting the number of shares\nof that particular URL.\nAnd then it takes the top seven.\nAnd for every link that is selected,\nwe need to get some metadata.\nSo there is a process that scrapes every link,\nextra the title, description, and it\ntries to get a relevant image.\n\n\nSometimes the image is not available, not every article\nhas an image.\nSo there is a fallback step that uses a placeholder image that\nis related to technology, something very generic,\nbut that still fits nicely with the overall theme.\nThen these images are uploaded to Cloudinary,\nwhich takes care of doing a CDN, downscaling the picture,\nand all that kind of stuff.\nSo all this stuff is then taken together and used\nwith Mailchimp.\nSo there is another step that takes all this information,\nsends it to Mailchimp.\nThere is already a template in Mailchimp.\nA draft campaign is created.\nAnd then I receive a preview by email.\n\n\nEoin: Wow, there's quite a lot of steps involved there.\nAnd I'm interested in knowing, I've seen Fullstack Bulletin,\nand it's really useful and looks really slick.\nIt looks like there's a lot of time put into it.\nSo how much of that is manual?\nI mean, I can imagine that just taking the top seven tweets\nbased on the number of shares is probably good in the most case.\nBut sometimes you end up with things\nthat are not so relevant, or maybe they've\ngone viral for the wrong reason.\nSo how do you manage that?\nIs it just a manual process where\nyou have to curate all of this?\nIs it fully automated?\n\n\nLuciano: So I think my goal has always been that I want to keep the curation.\nSo I don't want to get random stuff.\nI still want every single piece of content\nthat comes out to be something that I somehow selected before.\nSo every week, everything I read that I think is relevant,\nI will be sharing it.\nAnd I think that's part of the curation process.\nThen ideally, I wouldn't want to spend more than 15 minutes\nevery weekend, because I generally\ndo the process where I receive the email, the draft,\nevery Friday evening.\n\n\nAnd then I have a couple of days just to review and refine.\nAnd then the next Monday afternoon, the new issue\nwill go out.\nSo in my mind, I would like to spend no more than 15 minutes.\nIn reality, I will be spending a bit more,\nbecause generally, the draft is not perfect.\nI will need to change images that are not relevant.\nSome articles might not be very relevant,\nso I might want to swap them out with other ones.\n\n\nOr sometimes there are a few articles\nthat are very similar.\nSo I don't want just one topic to be a recurring thing\nin the newsletter for the week.\nSo that's also another reason to try to find different articles.\nAnd then for every single article,\nI try to change the description a little bit,\nbecause the style is always very different between one\ndescription and the other.\nAnd also, I would like to add a little bit of personal touch.\n\n\nWhy did I think that that particular link was interesting?\nWas it something that I played with during that week?\nOr is it some technology that I want to experiment more with?\nOr is it maybe something new that I think is, I don't know,\nrevolutionizing some kind of field,\nand therefore is going to be something to watch out for?\nSo I try to add the kind of bit of,\nthis is why I think this link is something that should\nbe worth your attention.\nAnd that ends up taking a lot of time sometimes.\nSome weeks is really 15 minutes, but most of the time,\nit's probably one hour a week that I just spent\nrefining the newsletter.\n\n\nEoin: That sounds like a really nice process, but I suppose\none of the things about the process that I'm kind of\nthinking might be problematic is that you're dependent on\nthird party systems.\nSo we're talking about Hootsuite and Twitter and Cloudinary.\nAnd I guess when you have external systems in the mix,\nyou have to think about fault tolerance and also what\nhappens when they change their contract and their API.\nSo what has the implementation been like?\nAnd what have the problems been with it?\nBecause I know you've been live streaming stuff on this\nrecently.\nMaybe you can give people a bit of background.\n\n\nLuciano: Yeah, I think that this Lambda is something that I wrote,\nI don't know, five years ago, if not more.\nAnd I didn't really have to change it much since then.\nSo even if today I'm not super happy with that\nimplementation, it hasn't really been a problem so far.\nLately, it has become a problem because lots of things\nstarted to break.\nAnd as you said, you depend on external services and things\nmight change with external services.\n\n\nFunny enough, lots of them change at the same time,\nmore or less.\nSo I was left with a lot of problems to try to solve\nin a very short amount of time.\nOne problem was Twitter, not only made it very difficult\nto use their APIs, very expensive as well.\nThere are some free APIs that you can use,\nbut I think it was very hard for me, first of all,\nto understand how do you even get API keys.\nI wasn't able to get API keys for my own personal account.\n\n\nI was able to get them eventually for the full stack\nbulletin account.\nBut then with those free API keys,\nsupposedly you can only read user information\nbut not even the tweets.\nSo it wasn't really useful at the end to do anything, right?\nSo I had to change strategy there.\nAnd we'll talk about that in a second.\nThen also, OotSuite decided to revoke any free plan.\nI was able to use the free plan for a while\nbecause I was tweeting something around 20 maybe\ntweets per week.\n\n\nSo it was not a lot.\nIt was definitely in the realm of the free plan.\nBut now there is no free plan at all.\nAnd I think that the cheapest one starts at $50 or $60,\nwhich is definitely not worth it for something\nI'm doing basically for free.\nAnd then there was another service called Place Image,\nwhich is one of those thumbnail generator services\nthat you would use.\nI don't know if you are sketching out a landing page\nand you want to have some place on the image, which\nwas really nice because just by composing\na URL with specific parameters, you\ncould get a random image, for instance, technology, right,\nthrough different categories.\n\n\nThat service was a free service and now it's shut down.\nSo I needed to figure out an alternative for that one.\nAnd with all these things starting to break,\ninitially it was a bit tricky to understand why is\nthe newsletter failing this week?\nWhy is this Lambda not sending me the preview?\nAnd I would log in into AWS, look at the errors\nin the console, and it would be just a random JavaScript error\nsomewhere.\n\n\nI'm very apt to tell why that was happening.\nAnd for a few reasons.\nAdmittedly, this Lambda was in a very bad state code-wise.\nIt was written five years ago.\nAsync-await wasn't even available in Node.js.\nSo I was using Babel and Webpack to transpile everything\nand be able to still use Async-await to make the code\na little bit easier to write.\nBut that made it so that when you have a stack trace,\neven though I was trying to use one of those source map\nlibraries that were available five years ago,\nit doesn't always resolve correctly.\nSo sometimes you get stack traces\nthat they don't make sense at all.\nThey will just point you to a random place, which\nis not really the place where the error is happening.\nSo you just need to rely on the error message\nand try to simulate things locally and try to figure out,\nOK, where is this really breaking?\nBecause if you just look at the stack trace,\nit's not really telling you the truth sometimes.\nSo definitely I had to get my ends dirty\nand try to fix all these things.\nAnd at that point, I realized, well,\nwhy did I put everything in one Lambda\nwhen there are clearly a bunch of different separated logical\nsteps?\nSo maybe a Step Function there could have been much better.\n\n\nEoin: I hope you don't mind.\nBut I'm looking at an old version of the Lambda function\nhandler from 2020.\nI just picked one at random just to see what you're\ngetting at here.\nAnd it looks like it's pretty well structured.\nI mean, it actually reminds me of the kind of orchestration\nlogic you might have in a web service endpoint\nwhere you've got lots of modules.\nYou've broken down each step in the process\ninto different node modules.\nAnd then you're calling them one after another\nin using async await.\nAnd it's very easy to actually understand.\nAnd this is the nice thing about it\nis that you can kind of see it top to bottom.\nAnd the process is pretty clear, at least\nfrom a kind of code understandability point of view.\nSo what is the motivation then to move to Step Functions?\nAnd what is your thinking there?\n\n\nLuciano: Let me try to describe first what\nI am thinking in terms of what is\ngoing to be the final state of the Step Function.\nAnd big disclaimer, this is still in progress.\nI have done some steps, but it's not fully completed.\nSo what I have in mind for the final design\nis that basically this will be a Step Function\nwith a bunch of different steps.\nThe first one is going to be like a parallel step where\nwe are fetching information from three different data sources,\none for the book, one for the quote,\nand then we have all the links.\n\n\nSo that could be three different branches.\nAnd each and every one would fetch their own information\nin parallel.\nNow, the book and the quote will be relatively easy,\nbut fetching the links is a little bit involved\nbecause inside that branch, technically, there\ncould be multiple steps, maybe one step all in sequence, one\nafter the other.\nMaybe the first step fetches the link from Twitter,\nwhich now has been replaced with Mastodon,\njust because I cannot use Twitter anymore.\n\n\nSo fetch all the base links.\nAnd then there might be another step\nthat tries to filter out the ones that are broken or not\nrelevant or used before, and another step where maybe you\ndo the top seven.\nAnd then once you have the top seven,\nyou could do a map step to try to scrape all of them\nin parallel and get the metadata for each and every one of them.\nAt that point, outside the big parallel step,\nwe could have an extra step that just takes\nall of this information from the state\nand use it to create the MailChimp preview\nand send it by email.\n\n\nSo that would be the structure.\nAnd I think with this idea, the point is basically,\nif something fails, it's going to be very easy for me to figure\nout exactly which step is failing.\nSo that's already a big advantage,\nbecause you visually see the representation of the step\nfunction.\nYou see the execution.\nYou can see all the green nodes, and you\ncan see where the nodes are read.\nAnd then you can click and start to zoom in exactly\non that particular Lambda.\n\n\nSo that reduces the scope for the error.\nThe other thing is that if something fails in a transit\nway, for instance, I don't know, maybe\na service as a networking glitch.\nYou are not able to complete a request.\nBut if you retry, that time will go well.\nSo withStep Function, it's actually very easy\nto define this retry logic without having\nto write custom code for it.\nSo that's another reason.\nAnd in general, I think the fact that you\ncan structure all the parallel steps so easily\nshould make everything faster, because you are not\ncreating that sequential logic.\nOr you are not making your code more complicated\nto try to do things in parallel at the level of your code.\nBut you let the Step Function do all the parallel\nization and dealing with concurrent stuff\nwherever possible.\n\n\nEoin: That makes a lot of sense to me.\nJust since you're talking about Mastodon,\nI mean, I'm on Mastodon too, but I\ndon't think I've reached the same level engagement as we\nhad previously on Twitter before it went the way it is.\nWhere it just seems, I don't know,\nevery time I open Twitter, it's worse\nin lots of different ways.\nBut let's maybe share our handles\nfor Mastodon and the full stack one as well in the show notes\nfor all the listeners who are living in the Fediverse,\nand we can try and grow the engagement that way.\nI'm also wondering, are you planning on,\nare you thinking about Blue Sky, Threads, LinkedIn,\nor other platforms as well for full stack bulletin?\nBecause it seems like things are going to spread out and move\nbeyond just one platform for this kind of interaction.\n\n\nLuciano: Yeah, absolutely.\nI did think about using pretty much all of them.\nI think I ended up with Mastodon just because the APIs are\nso easy to use.\nAnd there is a very good Node.js client that pretty much was\nlike everything I needed to do in one function call.\nAlso, you don't have a complicated authentication\nprocess.\nYou create an app, and then you can get an application token.\nSo you don't need to do like an OAuth 2 kind of process\njust to be able to get your own tokens.\nSo it was really easy to set up for this particular use case,\nand I was able to do that transition\nin a couple of hours.\nSo I was impressed how easy it was to use that API.\nAnd I think going with LinkedIn or Blue Sky or others\nwould have been more involved.\nSo I just went for the one that was giving me\nthe easiest path to migration.\nEven though the platform itself, I'm\nnot getting a lot of engagement yet.\nSo maybe that's something that will grow, but I don't know.\nI have some doubts that it's going\nto get at the levels of Twitter, to be honest.\n\n\nEoin: The fact that you have it now or you're moving towards a Step Function means\nthat it will be easier to modularize it and orchestrate\nmultiple platforms in the future.\nSo what's the plan to migrate?\nAre you looking at a big migration effort\nhere to get to the end state?\n\n\nLuciano: Yeah, I think the key here is that this is a side project, and I'm not going to be investing more than a couple\nof hours a week on it.\nAnd the refactoring is probably about one hour a week\nwhen I'm doing my live streams.\nSo I need to be strategic about that.\nI cannot do a big bang type of migration where I'd be spending\nhundreds of hours, and maybe two years later, I\nwill swap the thing entirely.\nAlso because it was broken at the time.\n\n\nSo I needed to figure out how to do a migration that\nwill fix the problem while progressing\nin that direction of the migration.\nSo the idea was that every time I do a change,\nI need to figure out what is the minimum valuable, I guess,\nchange in the direction that I want to go so that I don't\nbreak things, and I can ship it to production\nand make sure that I got some extra value.\nI went a little step forward in that direction\nwhile still keeping something that works.\n\n\nSo the idea there was let's try to extract.\nThe first step was, OK, let's just take the monolith Lambda\nand wrap it in a Step Function.\nThis Step Function doesn't do anything special.\nIt's just one state.\nBut at least now we are in the realm of Step Function.\nThat was my first change.\nThen after that change was in place,\nI was still able to deploy and run it.\nIt was still failing.\nMeanwhile, I had to fix in that Lambda some of the problems.\n\n\nLike for instance, I swapped Twitter for Mastodon.\nI removed Hootsuite.\nI changed place image with Unsplash,\nwhich also has nice APIs and a free tier that you can use.\nAnd at that point, I had everything\nworking in a Step Function.\nStill very monolithic, but was something\nthat I could use to produce the next newsletter.\nFrom there, it's very easy to start to extract states.\nFor instance, the first state that I extracted,\nI didn't even create a parallel step yet.\n\n\nI just created two sequential states\nwhere I think I started first either the quote or the book.\nI'm not sure.\nBut basically, there were two steps.\nOne extracted the quote and adding it\nto the global state of the Step Function.\nAnd then everything else was pretty much\nthe same monolithic Lambda, except that rather than taking\nthe book itself, it was just reading it from the state.\nAt that point, the next step was, OK,\nlet's start to create a parallel step at the beginning, where\nI can do multiple things together.\nSo not just fetch the book, but fetch the book and the quote,\nand then use them and put them in the global state\nand make the current monolithic Lambda a little bit slimmer,\nbecause it can just read things from the state,\nrather than doing more stuff.\nAnd the next steps would be to start to branch out also\nthe link fetching step and processing step\ninto another branch into the parallel step\nand remove all of that code from the monolithic Lambda.\nSo basically, the monolithic Lambda\nis still leaving us the last step of the Step Function,\nbut it's becoming slimmer and slimmer\nas I extract out steps that will go into their own dedicated\nLambda functions.\n\n\nEoin: Given that you're talking about changing a single Lambda\nfunction into Step Function with multiple Lambda functions being\norchestrated, do you have to think more\nthen about observability and how to monitor\nwhen things go wrong?\n\n\nLuciano: Yeah, so I think that just by virtue of being a Step Function\nis already a little bit more observable,\nbecause if something goes wrong, you\ncan just open that execution.\nIt's very easy to see where things are failing\nand what is the error.\nIf I look at the code, at the time,\nI used this debug module from Node.js,\nwhich allows you to create effectively logs,\nand then you can select a log level,\nand you can make it more or less verbose,\ndepending on what you want to see.\n\n\nSo that's something that is already in place.\nIt's not perfect.\nThere might be some refinement work there\nto make it nicer, maybe using structured logs,\njust to make it easier to send it to CloudWatch\nand then use CloudWatch logs to query the logs.\nBut I think overall, it's a good starting point.\nWe can improve it.\nOther things could be create custom alarms.\nLike right now, there is no alarm.\nI think I just get worried if it's Friday evening\nand I didn't get the preview email,\nand I have to go in and check out, OK, why did it fail?\nIt would be nice to just have an alarm\nif the Step Function fails that sends me an email anyway,\nsaying, you're not getting it because it failed\nand it failed for this reason.\nSo definitely, there is some room for improvement there.\nAnd one tool that we have been talking about\nthat we built at fourTheorem is SLIC Watch.\nSo it will be very easy because I'm\nusing SAM to integrate SLIC Watch\nand get some of that done automatically\nfor me at the stack level.\n\n\nEoin: That's true.\nIf you've got a SNS topic, you can just\nget alerts for any failing Step Functions or Lambda functions,\nAPIs, anything else.\nThis sounds like it's really going in a nice direction,\nand it's quite a professional approach you're taking,\ngiven that it's kind of a side project.\nBut since you want to run it once a week,\nit's a fairly controlled environment,\nand you're in control of it.\nSo does it really all matter that much\nto take this migration approach?\nIs it just an interesting exercise for you?\nOr is there some higher level lesson\nthat we can extrapolate from this for more serious production\nworkloads, where there might be users relying\non the feature 24-7?\n\n\nLuciano: Indeed, you're right.\nI mean, I could definitely afford to let this fail,\nand I have a weekend to fix it.\nAnd even if it doesn't, I don't fix it,\nthe worst that happens is that I'm not\ngoing to be publishing a newsletter for a week, which\nis not the end of the world.\nSo I'm probably over-engineering this thing a little bit\nas an interesting exercise.\nBut I think that there are lessons there\nthat we can extrapolate for more serious production\ntype of workflows.\n\n\nAnd one lesson I think is that I've rarely\nseen in my career Big Bang migration succeed.\nI think you end up spending way too much time, budget,\nenergies into Big Bang migrations.\nAnd very often what happens is that when\nyou are very close to completion,\nsomebody is just going to cancel the project,\nbecause it has been years in the making,\nand nobody has seen value so far.\nSo yeah, that's always the way they go.\n\n\nSo try not to make another Big Bang migration.\nI think in itself is a good exercise.\nAnd I think I can use that practice a little bit more just\nto force myself to think into, OK, even if it seems longer,\nbecause you are doing small incremental steps,\nand sometimes you have to work around things a little bit just\nto make it incremental, it's still functional.\nBut you get to see value every single time you do a change.\n\n\nI think it's something that I need to push myself to think\ninto those terms a little bit more.\nSo in that sense, the exercise is good.\nAnd in general, it's a lesson for every migration.\nTry to think that way whenever possible,\neven if it seems you're doing more work,\nbut you are doing work incrementally,\nand every single change gives you value immediately.\nThen the other thing is that moving to Step Function\nhas a bunch of advantages that we described.\n\n\nSo I think by doing that, I will be having something\nthat it is more observable by default, as we said.\nAnd it is something that long term I can maintain more easily.\nAnd another idea there is that if I\nmanage to split everything out into the wrong functions,\nthen if something changes, for instance, as you said,\nI want maybe to swap master for something else,\nor maybe I want to start to include multiple sources,\nit should be easier to do it with a Step Function where\nyou can just create more steps or change very specific steps\nrather than thinking, OK, there is a massive monolithic code\nbase.\n\n\nWhere do I do the changes?\nHow do I change the tests?\nHow do I test everything again?\nWhile if you can do all these things in isolation\nand then just compose them, it should be a little bit easier.\nAnd another idea there which could\nbe relevant in production environment\nis let's just imagine that at some point\nyou realize that you have a bottleneck,\nand you have a Lambda that takes forever because you're doing\nmaybe something very confusing, dense,\nyou could decide to swap the Lambda for something else,\nor you could rewrite it in another language.\nI think that that composability gives you\na lot of opportunities for change\nwhere optimizations opportunities arise.\nSo maybe you want to rewrite something you've asked for fun\nor for performance, you can rewrite one Lambda at a time,\nfor instance, or maybe just very specific Lambdas.\n\n\nEoin: I like the way you're taking the one bite at a time approach\nto eating this elephant.\nAnd I think it's going to be fascinating to watch\nthe rest of the live streams and see where this ends up.\n\n\nLuciano: Yeah, and on that note, I want to mention that we will have some links in the show notes\nbecause all the code is open source,\nhas been open source since day one.\nSo if you're curious, you can see all the evolution\nby just looking at the history.\nAnd we'll have the link of the repository on the show notes.\nI'm doing the live streams on Twitch,\nso you can check out my Twitch profile.\nIf you're interested, generally it's\nevery Monday afternoon in Irish time zone.\n\n\nAnd there is also a playlist on YouTube\nwith the previous recording.\nSo if you want to check out the previous episodes,\njust see the incremental changes, you can do that there.\nSo on that note, I think we've reached\nthe end of this episode.\nI hope that this migration story is going\nto be interesting for you.\nIf you have done something similar,\nbe curious to know what did you do.\nMaybe you took a different path using different services\nor maybe a different architecture.\nSo definitely share your experience with us\nin the comments or reach out to us online on Twitter\nor LinkedIn or master as well.\nAll right, thank you very much.\nWe'll see you in the next episode.\n"
    },
    {
      "title": "93. CDK Patterns - The Good, The Bad and The Ugly",
      "url": "https://awsbites.com/93-cdk-patterns-the-good-the-bad-and-the-ugly/",
      "publish_date": "2023-08-11T00:00:00.000Z",
      "abstract": "In today's episode, we're diving into the fascinating world of CDK Patterns - those ingenious building blocks that can transform your cloud journey. We uncover what CDK Patterns are, where to find them, and why you'll want to use them!\nWith CDK's object-oriented abstraction, L2 and L3 Constructs bring a whole new level of convenience. We'll explore where to find these powerful patterns, from the ones baked right into CDK to the inspiring examples showcased by community websites such as cdkpatterns.com.\nWhy bother with CDK Patterns and L3 Constructs? Well, imagine encapsulating best practices, avoiding tedious configuration repetition, and ensuring a consistent approach across your services. That's just the tip of the iceberg!\nOf course, we'll be candid about the challenges you might encounter, like versioning and resource oversight. Fear not! We'll share practical tips to address these hurdles, including automated testing and vigilant monitoring using CDK diff functionality.\nAnd wait, there's more! We'll reveal some exciting alternatives to CDK Patterns, giving you a broader perspective on reusable modules for your cloud adventures.\n\nfourTheorem is the company that makes AWS Bites possible. If you are looking for a partner to accompany you on your cloud journey, check them out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources:\n\nAWS Solution Constructs\nCDK Patterns\nAWS Cloudformation Templates\nTerraform AWS Modules\n\n",
      "transcript": "Eoin: Today, we are going to be diving into the world of CDK patterns,\nwhat they are, where to find them and why you might want to use them.\nJoin us as we discuss the benefits and challenges of using these powerful,\nreusable modules and explore some alternatives available\nif the thought of generating infrastructure with dynamic,\nreusable code gives you nightmares.\nI am Eoin, here again with Luciano for another episode of the AWS Bites podcast.\nfourTheorem is the company that makes AWS Bites possible.\nIf you're looking for a partner to accompany you on your cloud journey,\ncheck them out at fourtheorem.com.\nLuciano, can you give me a quick recap on CDK, just for people who don't remember\nor don't know what it's all about?\n\n\nLuciano: Yes, so CDK stands for Cloud Development Kit, and it's basically an object-oriented abstraction for CloudFormation.\nSo the idea is that rather than using YAML or JSON to write your infrastructure\nas code, you can actually use real code like JavaScript, TypeScript, Python,\nC-sharp, Java or Go, I believe are the ones supported,\nto actually define the infrastructure that you want to be provisioned\nin your AWS environment.\n\n\nIt's actually not really limited to CloudFormation,\nbecause if you look at the bigger picture in the realm of CDK,\nyou also have this project called CDK Terraform.\nSo you can also generate infrastructure that then is deployed with Terraform.\nAnd I think there is also a project that allows you to provision\nKubernetes configuration using CDK as well.\nToday, we only want to focus on the CloudFormation one,\nbecause this is the one that we have been using the most\nand the one we know the best.\n\n\nSo, yeah, infrastructure as code, as we said, is generally declarative.\nAnd that brings certain challenges, because it's always very tricky to do\nthings like loops or condition logics, or if you want to add extra code,\nhooks, maybe do something before or after.\nYou always need to figure out your own\norchestration, your own bash scripts to wrap around things or generate code\ndynamically using Jinja templates.\n\n\nI've seen all sorts of variations of that,\njust because there are limitations into the way you typically write\ninfrastructure as code in a declarative way with languages like YAML or JSON.\nSo, CDK tries to fill that gap and try to give you a nicer experience.\nAnd the idea is that you write code\nthat effectively, by instantiating a bunch of classes, you are defining the things\nthat you want to appear in your infrastructure and how they are configured\nand how they are integrated together,\nbecause you can easily reference properties from one another.\n\n\nAnd then at some point, when you're happy with it,\nyou can run a step that is called synthesize.\nAnd what synthesize does is basically taking all of that code definition,\nwhatever is your language of choice,\nsomehow evaluating it and converting it into a proper cloud formation stack\nthat can be used to be deployed, still using cloud formation behind the scenes.\nSo CDK gives you all the fundamental building blocks\nand it generally maps one to one to what you get with cloud formation.\n\n\nBut then you also have other things that are like other abstractions on top of it.\nAnd the basic abstraction is constructs,\nwhich is like representing all the entities that you can define in cloud formation.\nBut also you can start to use constructs to define your own custom things.\nAnd then you also have assets, which are not really a cloud formation thing,\nbut a nice extension that CDK gives you to be able to deploy code,\nfor instance, in Lambda functions or container images as part\nof your own infrastructure definition.\nSo how do we start to make sense of all these different concepts?\nFor instance, can we start by describing better what constructs are and how they are organized?\n\n\nEoin: Yeah, the construct is the main thing you need to be concerned with with CDK.\nAnd they're essentially classes that are\ngoing to generate one or more cloud formation resources.\nAnd you have three different levels.\nThere's actually a fourth level,\nbut the level one, two, three are the main ones you would encounter in the wild.\nAnd level one constructs are just simple\nrepresentations of the cloud formation resource exactly as it's defined\nin the cloud formation doc that just generated.\n\n\nIt's just the same as writing cloud formation, except it's represented by a class.\nSo you get type safety and code completion.\nAnd often for new services, this is all you get.\nYou just get the `Cfn` resources because all L1 constructs or level one constructs always begin with `Cfn`.\nThen you have where it really starts to add value is with the level two or L2 constructs.\nAnd they provide more convenient helper functions and types to reduce the amount\nof code that you have to write and allow you to connect resources together more easily.\n\n\nSo if we take an example, an L1 construct for an S3 bucket would be the `CfnBucket`\nclass, and it would require you to pass a string for the encryption method.\nBut the L2 construct for bucket has typed\nvalues for unencrypted KMS, S3 managed, etc.\nSo the L2 construct then also has helper functions like grant put that will\ngenerate the right resource policy statement to allow principal to put an object on that bucket.\n\n\nAnd this is one of the big benefits of level two constructs for many,\nsince it can reduce the human error encountered in creating IAM policies.\nAnd we know all about that.\nThen if we're moving into the realm of CDK patterns,\nthen we're looking at generally level three or L3 constructs.\nAnd these are really higher order constructs that combine multiple level one\nand level two constructs together to achieve a specific use case.\n\n\nFor example, you could create a construct to create a cluster of EC2 instances,\nsecurity groups, VPC, network routing, logging backups all in one class.\nYou can kind of compare it to React components or you have simple\ncomponents and then you have higher order components.\nAnd that's exactly what L3 constructs are trying to do.\nSo today we're talking about CDK patterns,\nand these are often created by providing L3 constructs.\nAnd there are tons of CDK patterns out there.\nAnd you can also create your own quite easily.\nSo Luciano, where can people start to find\nthe CDK patterns and level three constructs?\nThe first thing that comes to mind is that CDK itself has a concept of patterns built in.\n\n\nLuciano: And there are a couple of interesting sub libraries that are already available\nonce you install CDK. And one is called AWS ECS patterns and another one is called\nAWS Route 53 patterns. The ECS one, I think, is fairly powerful\nbecause also ECS is notoriously complex to configure yourself.\nThere are so many resources, so many configuration options that having patterns\nis really needed there because otherwise you might be always reinventing the wheel\nand always bumping into the same old mistakes.\n\n\nSo what you get out of the box with AWS ECS patterns is if you want to do\na web application running on Fargate, backed by a load balancer,\nall of that stuff is made very easy if you use this specific pattern.\nSimilarly, you can switch the application load balancer for a network load balancer as well.\nOr another use case that is covered very nicely is when you want to use Fargate,\nfor instance, to process jobs coming from an SQS queue.\n\n\nAnd you can do all of that with a container running on Fargate that scales.\nAnd it's very easy to configure all the different resources this way\nwhen you use this particular pattern.\nAnd the interesting thing, again, is because it's a pattern,\nthey will give you a higher level abstraction.\nSo you just in your code, you just instantiate one class or very few classes.\nAnd then behind the scenes, it doesn't really map one to one,\nlike one class with one resource,\nbut actually ends up creating all the necessary resources for you.\n\n\nSo you get load balancer, you also get health checks.\nIf it needs to create queues, it's going to create the queue,\nit's going to create auto scaling rules for you.\nAnd of course, everything that is customizable,\nyou just will have a higher level interface to specify how to customize the different things.\nAnd another interesting detail is that you can even let it create VPCs\nor you can use a VPC that you already have in your account, for instance.\n\n\nSo you can also reference other existing resources in some cases.\nSo the cool thing is that it's something that is going to save you a lot of time,\nit's going to save you a lot of headache because it's easier to end up\nwith the result you want without doing mistakes.\nBut at the same time, it's hiding a little bit what's being generated.\nYou need to be really diligent into looking into the generated resources.\nThey are not so transparent anymore from the code that you are writing.\nSo sometimes there might be things that you didn't account for.\nMaybe it's creating a NET Gateway that you didn't need,\nbut now you're suddenly paying for it.\nSo the general advice is there is to don't trust CDK blindly.\nAlways spend time looking at what's being generated, review the stacks,\nreview all the resources in the stack and make sure you understand\nwhy all the resources are there and if you really need them.\n\n\nEoin: AWS also have this open source extensions set for CDK\ncalled solution constructs.\nSo this is a different type of CDK pattern really, because rather than\nproviding these kind of reusable higher order constructs for complex\nconfigurations like Fargate with load balancers and all the other\nintegrations, this is essentially around 50 different simple patterns for\nconnecting commonly used resources together and normally like two resources.\nSo an example would be connecting a CloudFront distribution to an API gateway.\nSo they're not as rich as ECS patterns, but more just examples of connecting\ntwo services together with the right permissions.\n\n\nLuciano: Yeah, actually that reminds me of another similar project, which is more open source and community driven called cdkpatterns.com.\nAnd you might have heard of this one because it was also mentioned by\nVernon Vogels at one of the recent Dreambands.\nSo that it's kind of a similar idea.\nIt's still giving you examples of solutions that you might want to deploy using CDK.\nSo it's code that you can easily take and bring into your own CDK\nand do all the necessary changes.\n\n\nAnd just to give you some examples, there might be things like you can do an API\nwhere the backend is Lambda and that backend is using Polly and Translate,\nmaybe to do interesting things with audio and text.\nOr you have other examples where you take a CSV and import it into DynamoDB\nand from there you create a processing pipeline that does other interesting things.\nAnd these are typically not L3 constructs.\nAgain, more of examples that you can take and change as needed.\nSo not meant to be highly reusable, highly configurable, but more\nthese are use cases that we commonly see.\nJust take them and adapt them to your actual needs.\nBut I think the main question that we still have is what is really the value there?\nWhy we would want to use L3 constructs or higher level constructs in general?\n\n\nEoin: But the reason for using CDK patterns and level three constructs particularly are they're like reusable modules that can be shared\nwithin a community or an organization, especially if you're all in on CDK.\nWe've seen companies do this where they go all in on CDK, use it for everything\nand then do lots of sharing and collaboration and have central teams\nmanaging these reusable components.\nAnd there's lots of good reasons for doing that.\n\n\nFirstly, it allows you to encapsulate proven best practices.\nIt also allows you to build in well-architected framework principles.\nAnd you mentioned duplication and reusability.\nIt helps you to be dry by avoiding duplicating the same configuration\nfor groups of resources everywhere you go.\nAnd then you can get just consistency and usage of services across the organization.\nSo when people go from one team to another and one project to another,\nthey've got consistency and they can understand how things work.\nSo it just stops you from reinventing the wheel and provides you with hopefully\nsomething that allows your teams to go faster because they're getting\nthe encapsulated best practices for your organization out of the box.\nSo that's the positive, but it's not without its trade-offs.\nSo what are some of the challenges?\n\n\nLuciano: Indeed, there are challenges as with every technology is always a trade-off between some nice things, some less nicer things, and you need to find the balance\nand figure out when it's worth it or not.\nSo one of the challenges is versioning and keeping teams up to date\nwhen improvements are made.\nAnd this is both changes in CDK itself.\nWe have seen, for instance, a fairly big change between version one and version two.\n\n\nSo there might have been some disruptions for people having to upgrade\nfrom one version to the next one.\nBut also changes that you do in your own CDK code, right?\nHow do you keep that in sync with other teams?\nIf you change some of the best practices, how can you track down the places where\nyou are not using that best practice yet?\nAnd all this kind of concern wouldn't exist even if you use other tools.\n\n\nSo it's not necessarily a problem with CDK itself, but it's still something\nthat you need to think about CDK is not magically solving that problem for you.\nAnd in general, I would say that if everyone is using patterns, again,\nthere is a risk that you don't think anymore in terms of AWS resources being\ncreated, you just think about use cases and you kind of start to lose track\nof the bigger picture there.\n\n\nLike at the end of the day, you want to know which resources you are creating\nbecause they will impact you in terms of cost, quota, security.\nSo if you stop looking at those, you might end up with lots of problems\nthat you didn't expect and be surprised when you have a security issue or when\nyou start to reach quota, or maybe you have a massive build shock and you are\nnot really realizing why, maybe you just deployed a simple API project and it's\ncosting you way more than it should.\n\n\nSo all these things might be a problem.\nAnd again, the suggestion there is always try to put an eye on what's happening\nbehind the scenes and always try to think in terms of AWS resources at the end of\nthe day, not necessarily.\nYou still should focus on the abstraction layers, but without forgetting that those\nabstraction layers will create resources that ultimately is what you should be\ncaring about.\n\n\nAnd another thing is that CDK is not necessarily deterministic because of\nvarious reasons.\nAgain, changes in CDK itself, the way you write your stack might not be\ndeterministic on its own.\nFor instance, I don't know if you do a mat random in your code, right?\nThat value will change every single time you synthesize your stack.\nAnd if you're using that to synthesize different resources or maybe to change the\nname of a property, maybe the name of a resource, you end up with a stack that is\nalways different.\n\n\nSo when you try to deploy that, you will always have changes, even though you might\nnot want those changes because logically you are not changing anything relevant.\nSo these are just issues that people will bump into initially when they start to use\nCDK because they think about, I can write all the code I want.\nIt's just code.\nBut I think it's still important to understand that there is a very specific mental\nmodel.\n\n\nThere are phases.\nUltimately, you are generating cloud formation.\nYou are still deploying a cloud formation stack.\nSo you really need to understand some of the inner workings of CDK to avoid some\ncommon mistakes.\nI was personally burned a few times by trying to do conditional logic with values\nthat sometimes are not immediately available when the CDK code is evaluated.\nCDK has this concept of tokens, which are values that will be available only at\ndeployment time.\n\n\nSo if you try to do if statements, maybe checking if those kind of values are true or\nfalse and then based on that generate some resources or others, that code is just not\ngoing to work for you.\nIt's just going to always be true or always be false.\nAnd that conditional logic is not going to work the way you expect.\nAnd similarly, you can have other problems if you use the same approach for loops.\nMaybe you don't go through the loop at all, or maybe you just do one iteration or maybe\nyou do endless iterations.\nSo just be aware that you really need to understand what is the mental model, how the\nexecution flow of CDK works, because you cannot really write all the code you want.\nIt's not going to magically do everything you want to do in code.\nThe code you write still needs to fit nicely with the model that CDK was built for.\n\n\nEoin: It is possible to address those challenges, but it's also good to be aware of what it\ntakes to mitigate the risk there.\nSo lots of automated testing and continuous delivery is one thing that will definitely\nhelp.\nIf you have an organization and are going all in on CDK, having dedicated people to\nmaintain these constructs as well, rather than trying to scramble to maintain them in\na distributed fashion across multiple teams who are focused on other goals.\n\n\nGood semantic versioning enforcement, of course, is always important for reusable\nmodules.\nGreat documentation will really help because it helps to make everybody self-sufficient.\nObservability as well, so that when things go wrong, you can detect early and have maybe\nlike canary checks in your deployment pipeline as well.\nSo that even if things build and deploy successfully, you can check what happens.\n\n\nCost management is another thing, because if you're using patterns and aren't really\nlooking at what's being generated, then one of the risks is that you can incur cost\nunder the hood.\nSo if you've got good observability on the cost side of things, that will help with\nthat risk.\nAnd then a really simple one is just keeping a close eye on change sits.\nAnother simple one is just keeping a close eye on change sets.\nSo using CDK diff and CloudFormation change sets and inspecting generated output and\nwhat has changed from one to the other so that you can detect if you're upgrading to a\nnew version of a construct that there are changes that you may not have expected.\nSo this is the CDK world of reusable modules.\nBut do we have to use CDK if we want to get this level of reusability or are there\nalternatives for people who just don't want to go into CDK?\n\n\nLuciano: Yeah, there are definitely alternatives and I know lots of people that don't like this idea of writing code to define stacks.\nThey prefer something more declarative.\nThey still prefer something that looks more like YAML.\nAnd I can understand the way of thinking, of course, there are good reasons for that.\nSo what can you do in that case?\nIf you are more on that side, you want to stick with writing infrastructures code in a\ndeclarative way, not using programming languages.\n\n\nSo if you use CloudFormation, there are a few options.\nFor instance, you can create your own CloudFormation library.\nFor instance, you can create your own CloudFormation macros to try to do more stuff or\neven try to do things that CloudFormation cannot do today.\nMaybe integrate with other providers outside AWS.\nYou can definitely use macros for that.\nYou can also use CloudFormation templates.\nThere is actually a really good repository that we will have in the show notes that has\nlots of examples.\n\n\nSo with CloudFormation templates, you basically build stacks that are highly\nparameterized.\nAnd then by just passing the specific parameters, you can adjust that particular stack\nor solution to your needs.\nAnd there is also service catalog, which is somewhat similar to the idea of\nCloudFormation templates.\nInstead, if you are a user of Terraform, Terraform comes with a built-in concept of\nmodules.\nSo there is already an idea in Terraform itself to have reusable units that are\nconfigurable and you can compose them together.\nAnd there is a really good repository called Terraform AWS modules, which has a huge\ncollection of solutions and high-level models.\nSomehow they remind me of L3 constructs in CDK, but applied to Terraform.\nSo we will have that link as well in the show notes.\nAnd it's definitely a must if you're doing AWS using Terraform.\n\n\nEoin: We have plenty of options there.\nWell, I think that's it for today's episode of AWS Bites.\nWe hope we gave you a valuable take on CDK patterns and the power you get, but also\nthe responsibility you need to take if you want to make them work well.\nAs always, we want to thank you for learning and sharing AWS ideas with us.\nPlease leave us a review and share the podcast with your colleagues and friends.\nWe really appreciate your support and look forward to bringing you more cloud goodness\nin the next episode.\nCatch you then.\n"
    },
    {
      "title": "94. Get the Most out of CloudTrail with Athena",
      "url": "https://awsbites.com/94-get-the-most-out-of-cloudtrail-with-athena/",
      "publish_date": "2023-08-18T00:00:00.000Z",
      "abstract": "Ever wondered how to gain deep insights into the myriad of activities within your AWS organization accounts? In this episode of AWS Bites, we dive into the world of AWS CloudTrail and Athena, showing you how to seamlessly query and analyze CloudTrail logs for valuable information, troubleshooting, security, and compliance.\n\nfourTheorem is the company that makes AWS Bites possible. If you are looking for a partner to accompany you on your cloud journey, check them out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources:\n\nA gist with the code for a working example of a Glue Catalog Data Table for CloudTrail logs that can be used with Athena\nOur previous episode on CloudWatch Logs Insights\n\n",
      "transcript": "Luciano: Keeping track of what's going on in AWS organization accounts can be very tricky.\nYou might have potentially hundreds or thousands of changes happening every day.\nThere is a tool called CloudTrail, which makes it possible to log all of this activity.\nBut how do you best get insights into it?\nToday, we're going to drill down into how you can use CloudTrail and also Athena together\nto ask questions about what's going on, troubleshoot issues,\nand answer questions for security and compliance.\nWe will also touch on AWS Glue, and we will go deep diving into Athena,\ninfrastructure as code, and AWS organization trails.\nMy name is Luciano, and I'm joined by Eoin for another episode of AWS Bites.\nfourTheorem is the company that makes AWS Bites possible.\nIf you're looking for a partner to accompany you on your cloud journey,\ncheck them out at fourtheorem.com. The link is in the show notes.\nI'd like to start today by giving some definitions.\nSo what is CloudTrail and what is Athena?\n\n\nEoin: I think it's a fundamental service in AWS, actually, and useful for developers and administrators alike,\nbecause it lets you keep track of what's going on for audit and compliance purposes,\nbut also, like you say, for troubleshooting.\nAnd as a developer, it's a pretty useful tool to have in your toolkit.\nSo it captures mainly management events,\nlike those relating to create and update and delete actions on AWS resources.\nEvery account gets a trail for free,\nbut then you can also pay if you want multiple trails,\nor if you also want to capture some data events as well.\nSo it stores logs in JSON.\nYou can look at them in the CloudTrail console themselves.\nSo you just store it, look at them in CloudTrail for more recent events,\nbut then you can send them off to CloudWatch Logs.\n\n\nLuciano: I remember we spoke in a previous episode at length about CloudWatch Logs and how you can do ninja stuff with the syntax that it gives you\nto do all sorts of kind of queries and aggregations and filtering.\nSo that seems like a reasonable thing that you would want to do, I guess.\nBut given that we have all these tools in CloudTrail,\nwhy should we consider S3 and Athena instead?\nActually, let's start by defining what even is Athena.\n\n\nEoin: Athena is a distributed query engine used to query data in object stores like S3, and it's based on the open source Presto and Trino projects.\nTrino is a fork of Presto.\nAnd using Athena, you're basically running SQL queries on JSON, CSV, Parquet, or ORC data.\nAnd when you execute queries in Athena, it's basically making a query plan,\nand in a distributed way, scanning parts of that data concurrently in chunks\nand doing additional steps to filter and aggregate that data.\n\n\nCloudTrail logs are stored in JSON format.\nSo they show you things like the account and the region and the date,\nthe identity, who's making the request, what's the user agent and the IP address.\nIt gives you the event name, and you can see the request parameters\nand some of the elements that come back in the response as well.\nYou have the ability, like you say, to query CloudWatch Logs and Logs Insights,\nbut the storage for CloudWatch Logs is going to be more expensive.\nSo if you want to retain data for a longer period of time,\ngenerally what people do is store the data in S3 and query it with Athena,\nand maybe just use CloudWatch Logs insights for more recent activity like the last week or two.\n\n\nLuciano: Okay, that makes sense.\nSo I suppose the main difference is that with Athena, you can query directly into S3 as long as you're storing structured files,\nwhich is something that CloudTrail allows you to do anyway by giving JSON support.\nThat sounds pretty interesting.\nHow do we get started?\n\n\nEoin: Well, there is an easy way, not necessarily the best in the long run, but from the AWS console, if you go into CloudTrail,\nyou have a button there that if you've got a trail set up already with S3 bucket logs,\nit'll give you a button that says create Athena table.\nAnd when you click on that button, it immediately shows you\nCTAS statement is what they call it in SQL, create table as select.\nSo it's basically creating a table by selecting data from this S3 bucket.\nAnd in this big SQL DDL statement, it's basically saying which fields in the CloudTrail JSON\nmap to table columns in this virtual table that it's basically going to create for you.\n\n\nLuciano: Right.\nThat's also a bit different from what we are used to do with CloudWatch Logs insights, because with CloudWatch Logs inside, you just create a table from the S3 bucket.\nYou just query like you don't have to worry about creating a table.\nBut I suppose that's something that will come up when we start to talk about more details\non how Athena works.\nSo right now I have another question.\nThis seems simple enough for a single account.\nYou just go to CloudTrail, click a button, and then you're ready.\nSelect the table definition and then you're ready to query.\nBut what if you have multiple accounts?\nBecause we often advise companies to create their own landing zone,\nstructure their deployments across multiple accounts.\nThat seems kind of a more production-related solution.\nMm-hmm.\nWhat do we do in that case?\n\n\nEoin: That's all solved basically in how you set up the trail itself.\nSo you don't really have to do anything additional in Athena.\nYou can either set up accounts to log to a central bucket.\nSo rather than every trail logging to a separate bucket,\nyou can have a shared bucket with the right permissions and they all log into a different\nprefix.\nBut there's actually an easier way still, which is to set up an organization-wide trail.\nAnd that way you do it from your management account or from a delegated administrator account.\nAnd you say, create a trail for my whole organization,\nand it will capture all the events from all accounts and put them into one bucket for\nyou.\nThat's the way we typically do it at Fourth Hirm.\nAnd it's easy then to set up Athena to query from that one single bucket.\nAnd then you've got Insights and you can do queries across all accounts.\nLike find out, okay, who did a terminate instance on EC2 in any account today?\n\n\nLuciano: Yeah, I suppose accounts at that point is just another field that you can query from.\n\n\nEoin: Yep.\n\n\nLuciano: At the beginning, we also mentioned Glue. So how does Glue come into the picture with\nthis setup?\n\n\nEoin: So when you create a table in Athena using this CTAS statement we talked about, this\nis actually creating a Glue data catalog table under the hood.\nIf you haven't looked at Glue or Glue data catalog, Glue has a number of different features,\nbut we're just going to talk about the data catalog in this context.\nAnd it's basically a meta store for your tables.\nSo you can use those meta store tables outside of Athena, but every time you're using Athena,\nyou have to have one of these.\n\n\nIt comes from the Apache Hive ecosystem where you had this ability back from the big data\necosystem to define virtual tables for data that was stored on a file system or an object\nstore.\nSo when you do this, create table as, and you're mapping the columns to the fields in\nyour data, this is basically just creating a data catalog table in Glue.\nSo once you do this, create table as select statement, you can go over to Glue and you\ncan see the table appearing in your Glue console as well.\nSo this is just a schema representation of your table.\nThere's no data copied into Glue or anything.\nIt's just really a schema definition and it allows you to map SQL concepts to an underlying\ndata store in S3.\nNow, instead of actually using that create table approach, you can just go and create\nthe table include data catalog directly.\nThis means we can create it visually in the console or we can use CloudFormation or Terraform,\net cetera.\n\n\nLuciano: Right.\nThat makes a lot of sense.\nI imagine that it's also something that allows for a lot of optimizations because you are\ngoing to be writing SQL statements and the system underneath needs to understand which\nfiles can actually have the data that you're looking for and to scan the files in a smart\nway rather than always reading everything, which might be very expensive and time consuming.\nSo I suppose that's why we go through this extra step of setting up the stable definition\nand recording all of that into a catalog.\nWe do use that setup a lot for data analytics for theorem.\nAnd I know that you can get good performances if you store the data in a very specific way.\nSo what kind of suggestions can we give to people to try to get the best performances\nwith this kind of setup?\n\n\nEoin: Performance you get with CloudTrail queries with Athena really is going to vary hugely depending on the amount of data you're scanning.\nSo it could be a few seconds or it could be 15 minutes.\nIt depends on the query and how you optimize the Glue catalog table.\nSometimes Athena will have to scan all of your data depending on what the query is.\nAnd that's clearly not optimal.\nSo there's a couple of things you can do to make it faster.\n\n\nOne is by setting up partitioning.\nThis is a typical optimization when you're using Glue data catalogs in Athena.\nIt allows you to have different partitions for specific fields.\nFor example, you could partition the logs by day.\nAnd if you have a day in your where clause, then Athena only has to scan that limited\nset of data because it's like an index basically.\nCreating partitions, there's actually a few different ways of doing this.\n\n\nYou can use the Glue API to create a partition every time you have a new one, like every\nday.\nOr there's an alter table add partition command you can do to do it through the Athena SQL\ninterface.\nAnd there's also actually another one which is MSCK repair table.\nAnd that will tell Athena to go off and scan and find partitions automatically.\nAnd that's the same as doing a Glue crawler.\nSo in the Glue data catalog world, there's also the concept of a crawler, which is like\nan automated process that scans your S3 objects and finds partitions.\n\n\nSo you can use this to create the table in the first place, actually.\nIt can try and derive the schema for you based on the columns in your data.\nBut it can also identify partitions and find new partitions once you've created that table.\nFor CloudTrail, there's another feature in Athena which you can use called partition\nprojection.\nAnd this is quite nice.\nAnd this is the one I usually set people up with the first time.\n\n\nBecause you don't have to add partitions as data arrives.\nInstead, when you create the table, you just specify ranges of values that are possible\nfor certain fields.\nSo you mentioned that you might want to query on account ID, right?\nAnd this is just another field.\nBut if you know all of your account IDs, you can tell Athena in advance, this is my range\nof account IDs.\nAnd then it doesn't have to go to Glue data catalog to find out what the partitions are.\n\n\nIt can basically project those values and use them to build its query in an optimal\nway.\nAnd it can also do the same thing for date values or for the region field, right?\nBecause we're all generally working with a fixed set of supported regions.\nPartition projection would be my first recommendation.\nIt makes things easier.\nAnd once you set it up, it just works.\nAnother thing you can do with optimization is use the limit clause.\nThat's always a good one to reduce the volume of data returned.\nOne last optimization is actually if you want it to be really performant.\nJSON is the slowest data format for Athena by far.\nIt's significantly slower than all the other options, even CSV.\nWhat you can do is just build a pipeline to convert that data into Parquet, which would\nbe the most optimal format.\nAnd you can use that using Lambda or EMR or Glue, or you can even use it.\nYou can use a Glue crawler to do that as well.\nSo that's something if you really want fast query performance, you could set that up.\n\n\nLuciano: That's really cool.\nOkay.\nSo far, we are talking about setting things up in the console and creating resources pretty\nmuch by clicking around.\nSo click ups and then doing SQL queries.\nIs there any recommendation in terms of let's do this in a more production ready way and\nmaybe use infrastructure as code so that we end up with something that is easily reproducible\nbetween accounts or customers?\n\n\nEoin: You can, and I would prefer to do it that way.\nI think Athena is nice because it allows analysts to be able to create things on the fly, ephemeral\nresources and tables without having to worry about infrastructure as code.\nAnd that's a really valid workflow.\nBut if you want something like this, Cloud Trail queries for your whole organization,\nit makes sense to put it into infrastructure as code.\nThe process of creating tables in CloudFormation or Terraform is a little bit complex and it's\nnot something that's very well documented.\nMost documentation ends up pointing to the create table method, which isn't really like\nproper declarative infrastructure as code.\nSo in CloudFormation, you need to specify the table columns and serialization parameters\nusing your YAML or JSON.\nAnd it's a little bit strange, this syntax.\nSo we've actually figured this out for Cloud Trail, so you don't have to.\nThere is code in a gist and we'll link the gist in the show notes and you can give this\na try and let us know how you get on.\n\n\nLuciano: It's always fun when you're defining resources and there is some kind of special syntax that you haven't seen before, which is unique for that particular type of resource.\nBut I guess that's the reality with the cloud.\nSo many resources that sometimes there are these kind of exceptions and we just need\nto figure out how to deal with them.\nBut in general, I think Athena looks really nice and really powerful for the way you are\ndescribing it.\nIn some projects we saw that you can use it as a data source for QuickSight and at that\npoint you can create nice dashboards.\nSo is that something that you would use together with Cloud Trail to create some kind of visualization\nthat, I don't know, will give you a very quick overview, like a single pane of glass that\nyou can go to and have a feeling for what's going on in your set of accounts that you're\nmonitoring?\n\n\nEoin: Exactly.\nThis is a very powerful integration type and stops you having to jump into the Athena console the whole time and you can just, if you've got data that you just want to keep\nan eye on all the time on the activity in the account, this is a pretty nice integration\nto set up.\n\n\nLuciano: That covers a very nice overview on how you can achieve a good level of observability and auditing over or across your accounts for your organization.\nIf you set up this method, then you can have kind of a centralized way of queering and\nunderstanding what's going on and even building dashboards that you can just look and see\nif there are things that you should be worried about.\nSo I think for today, that's all we have to share.\nBut if you have any other tip, maybe have used different setups, maybe use different\ntools, maybe you have different ways of provisioning all this infrastructure, definitely share\nit with us.\nWe are always looking for alternatives and for learning.\nDefinitely looking forward to hearing more from you.\nAnd until then, we'll see you in the next episode.\n"
    },
    {
      "title": "95. Mounting S3 as a Filesystem",
      "url": "https://awsbites.com/95-mounting-s3-as-a-filesystem/",
      "publish_date": "2023-09-15T00:00:00.000Z",
      "abstract": "Saddle up for a cloud adventure like no other in this episode of AWS Bites, where Eoin and Luciano explore the untamed world of AWS S3 Mountpoint.\nJust like a trusty steed on the digital prairie, Mountpoint gallops into action to solve complex use cases, making it a valuable asset for managing massive data, achieving high throughput, and effortlessly fetching information from the AWS S3 wilderness. Dive deep into the inner workings of Mountpoint, a Rust-powered Linux-exclusive application that harnesses the Linux FUSE subsystem to provide optimal S3 performance.\nWhile exploring alternatives like s3fs-fuse and goofys, discover the benefits of sticking to native AWS tools for certain scenarios.\nUncover Mountpoint's performance prowess, thanks to its integration with AWS Common Runtime libraries, and learn when to hop on this cloud cowboy or opt for a more native approach.\nWrapping up, don't forget to check out AWS Storage's blog post for an even deeper dive into Mountpoint's capabilities. Whether you're a seasoned cloud wrangler or a newcomer to the digital rodeo, this video will equip you with the knowledge to navigate the AWS S3 Mountpoint frontier confidently.\n\nfourTheorem is the company that makes AWS Bites possible. If you are looking for a partner to accompany you on your cloud journey, check them out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources:\n\nAWS Mountpoint repository\nRunning Mountpoint in a container\nS3-fs-fuse\nGoofys\nAWS CRT\nCloudonaut podcast episode talking about how difficult it is to get 5 TB objects quickly\nCloudonaut custom high-performance Node.js S3 Client\nAutomated Reasoning at AWS\nDetailed blog post from AWS Storage\n\n",
      "transcript": "Eoin: Everyone loves the simplicity of S3 for storing and retrieving data.\nBut when you start pushing the boundaries and want really large objects, high throughput,\nand faster access, it can start to become a bit of a minefield. AWS recently released Mountpoint\nfor S3, a new client that promises to make fast access to S3 as simple as any file system.\nToday, we're going to take a look at Mountpoint for S3. And by the end, you should know where\nyou might use it and when you should give this a hard pass. I'm Eoin. I'm joined by Luciano\nfor another episode of AWS Bites. fourTheorem is the company that makes AWS Bites possible.\nIf you're looking for a partner to accompany you on your cloud journey,\ncheck them out at fourtheorem.com. Luciano, why would you need something like Mountpoint for S3?\nWhat do you think? What are the use cases that it might solve?\n\n\nLuciano: Yeah, that's a good question.\nSo we were reading through the announcement, and there are some use cases that are detailed there. And the first one is big data application,\nspecifically when big data application like data lakes, they don't directly support S3. So you can\neffectively use Mountpoint to mount S3 as a file system, like a FUSE file system, and then just\ngive it to the application you're using. But this is a bit of an interesting use case because for\nthe kind of big data application that we have been using, like Dremio, Snowflake, and others,\nlike all of them have already S3 integration, so it wasn't really convincing. But it seems that\nthere are other advantages. For instance, it's very optimized for performance. So if you're\ndealing with large objects, or if you need to have very high read throughput, or if you need to read\nwithout downloading an object entirely, you just need a subset of the data. In all those use cases,\nI think Mountpoint can give you ideal performance. So maybe this is already good enough to justify\nusing Mountpoint. We were also trying to figure out some potentially additional use cases where\nMountpoint can be useful. And we were thinking, okay, what if you have created a script, maybe\nyou were doing something quick and dirty locally, and now you need to do it using data that is\navailable in S3. You're probably going to be fast enough just using Mountpoint rather than changing\nall your code to actually use an SDK or the CLI. So that could be another use case.\nAnd a common use case is like you have a Unix pipeline, you read from S3, you do some kind of manipulation,\nyou save to S3. And if you were doing that on a local file system, you can immediately support\nS3 that way. And similarly, we have seen people doing a lot of work analyzing CSV files or Parquet\nfiles using notebooks or logarithms, so all sorts of kind of analytics. And often enough, people\nare just working off of logarithm files, and then they need to use real data in S3. And they have\nall the code written for using generic file system operations. They don't want to change their code\nto use maybe Porto3 or maybe some other kind of direct integration with S3. So in that case,\nyou have another valid use case for Mountpoint. And finally, this is always our... something we\nlike to remark that if you need to explore what you have in an S3 bucket and you are not very\nfamiliar with AWS CLI, because maybe you haven't used AWS that much, you can just mount the S3\nMountpoint and then you can explore the files using familiar bash commands like ls, for instance.\nSo that could be another use case, and it might be much more convenient than just browsing through\nthe AWS web console, especially when you have lots and lots of files in the bucket. So should we talk\na little bit more about how it is really implemented and some of the modulated characteristics of\nthis implementation?\n\n\nEoin: Yeah, this is where it gets actually kind of interesting in looking at how they're implementing this new client. It's written in Rust, like a lot of the new performance critical\nthings that they're doing at Amazon, they seem to be favoring Rust. And now it's only supported for\nLinux at the moment. But the idea of using Rust is to reduce latency, the binary size, good for\nserverless applications, thinking about cold starts, reducing resource consumption. And it\nprovides then a file system operation that is intended to deliver optimal S3 performance. So\nthe idea is that you get a simple interface, but you don't compromise on speed because they're\nproviding this level of abstraction. And it uses the Linux FUSE subsystem. So that's the\nsubsystem that you might've used before if you're a Linux user for providing user space file systems.\n\n\nNow, one of the things, I was a little bit confused because there are alternatives that\nalready do this kind of thing in FUSE. I was wondering what this provides us slightly different.\nAnd it seems from reading through the documentation and the code base, that the whole\nphilosophy here is to intentionally not implement operations that would result in suboptimal\nperformance. So to remove those foot guns where you might try and do a simple operation on the\nfile system that might result in thousands of operations with S3 under the hood that might take\ndays and might end up costing you a lot. So I think that is a little bit reassuring to see that\nin practice. I will have to see how it plays out. And it's also built on top of the native CRT.\n\n\nSo CRT is something you might come across very rarely, but the CRT is common runtime. It's a\nset of libraries that Amazon provide. And we can maybe talk a little bit about that further on.\nSo when doesn't it work? Given this implementation and design, when does it not work? Well, we've\nalready mentioned it doesn't work on anything that is in Linux because it uses FUSE. So it's not\nsupported on OSX. When I was playing around with it, I had to use Docker on Mac. And it doesn't\nwork in Fargate because it needs special permissions. And that's explicitly called out\nin the documentation. Fargate doesn't provide the special permissions needed for the FUSE device.\n\n\nSo if you wanted to use S3 with Fargate today, you're left with using the object paradigm,\nor you're just doing get object and put object yourself. Or you can use something like EFS with\ndata sync to sync up data from S3. And then when it comes to the specific operations,\nyou wouldn't use it when you need to do edits on an existing object. So you can't\nchange the middle of an object. You can only do sequential writes when you're writing an object\nfor the first time. You can't do symlinks because those aren't supported in S3. You can't do\ndirectory renames. And in general, you wouldn't use it for something like web serving either.\nI mean, you can do it, but performance is not going to be the best because you\ngenerally want caching there. So maybe before we go into the CRT and some of those things,\nLuciano, do you want to talk about some of the alternatives that are out there from Mountpoint\nor other kind of use cases in this realm?\n\n\nLuciano: One that I've been using in the past is s3fs-fuse that I think you already mentioned before. It has been around for a long time. Seems\npretty reliable, but again, they try to make it as possible. So sometimes you might find this\nkind of footcance where you try to do a simple operation and it results in something that's not\nvery optimal in S3. So it might be a little bit dangerous. And while we were researching for this\nepisode, we found out that there is an alternative called GOOFYS, which is written in Go. And in\nterms of design principle, it's somewhat similar to Mountpoint, meaning that they don't try to\nimplement everything in a POSIX compliant way, but they try to keep it as performant as possible.\n\n\nAnd in general, I would say the real alternative is don't try to do this stuff if you can. Try to\nstick with the object storage paradigm and use the CLI or the SDK and do the specific operations\nthat the actual service is providing you. Don't try to simulate with different abstraction,\nthe same things, because all these abstractions are a bit leaky and they don't always map\none-to-one and you might end up in this kind of weird situation where either it doesn't work or\nit's too expensive or it's too slow. And yeah, so the alternative is try not to do that whenever you\ncan. So speaking about performance, what can we say? Because that seems to be one of the main,\non one side, one of the main concerns because it might be a little bit obscure,\nbut on the other end, it's a bit of a promise that by using this kind of tool,\nyou get the best performance that you can possibly get.\n\n\nEoin: Yeah, we mentioned that it's fairly simple just to read and write from S3 at the beginning, but when you start pushing the boundaries with large objects and high throughput,\nthat's when it gets a little bit trickier. And S3 will give you performance tips in the\ndocumentation, like saying you should use byte range requests in parallel in order to get your\nobject faster rather than reading from start to finish. There's lots of other tricks like using\nmulti-part uploads to upload and even using multiple IP addresses. So if you're just using\nDNS with S3, you might get back one IP address that's used for your request. But if you're on a\nhigh bandwidth EC2 instance, you might want to maximize the number of flows you can do because\nthere's a cap on the bandwidth you can use for an individual flow. So you might want to use multiple\nIP addresses. So this is how it starts to become a little bit of a minefield. And this was really\nwell illustrated in the Cloudanaut podcast when Michael and Andreas Fittig went through this whole\npain in order to try and download five terabyte objects, the maximum object size, really quickly.\n\n\nAnd I'd really recommend checking out that episode. And they were using Node.js and ended\nup creating their own custom Node.js client because the AWS SDK wasn't giving them the\nbandwidth they needed to read these large objects quickly. So mount point is a little bit different\nbecause it's not just using like the JavaScript SDK. It's using the CRT libraries we mentioned.\nAnd CRT is like written in C native high performance libraries from AWS for lots of\ndifferent things, including they've built like a high performance HTTP client and MQTT client.\n\n\nAnd they've also built an S3 client. And it's designed for low overhead, high throughput,\nautomatically uses byte range requests, parallelization, multi-part uploads.\nAnd I think ultimately the goal with this CRT is to provide a common code base that all of the\nSDKs can use so they don't have to implement all of this optimization in every language separately.\nNow, right now, CRT can integrate fairly easily with the Java SDK. And it's possible with the\nPython Boto3 one as well. But it seems to be very vague how to do it with other languages,\neven though they provide kind of bindings for all languages. One of the interesting claims here is\nthat the team says that they prove algorithmic correctness using this fancy automated reasoning\nthat they're really into at AWS. So there's a link to that in the show notes. Now, going back\nto mount point, mount point is built on top of CRT. So performance should be pretty optimal. But\nas of yet, we don't see any published benchmarks. I don't see any benchmarks, even from the S3FS\nFUSE teams showing what the difference is. So it would be really interesting. Setting up benchmarks\nand running them on S3, there's a lot of effort to put into it. So we haven't had a chance to do\nthat yet. But if anyone out there feels like it, I'd be really interested to see what it would be\nlike. Are you optimistic Luciano, or do you see any potential problems with mount point?\nYeah, I think on one side, it's fair to say that it's a relatively new project.\n\n\nLuciano: So it will improve over time for sure. And it will get better, I imagine. So\nalthough there are some potential problems that we have observed in this experimentation that we\ndid in the last few days, and one interesting thing is that we were wondering, because this\nis an abstraction, how it's going to impact cost for me. Like what kind of S3 requests are actually\nhappening behind the scenes, right? So initially, we didn't really found a way to see that.\n\n\nEventually, we figured out that there is a CLI flag that you can enable to get advanced logs,\nlike you get more verbose logs. And these logs will give you a fair number of details about\nthe S3 operations that are happening. For instance, if you do a put, or if you do a get,\nand you get details like how many parts are being used, for instance, when you do a put.\nAnd that could be very useful to understand exactly what kind of operations are happening,\nhow fast they are, and can give you an indication of cost. The only annoying\ngotcha there is that you don't see the parts being used in S3. So if you just look at the logs,\nit's a little bit out of context. If you try to correlate the different operations with what you\nwere trying to do, you need to stick together your, I guess, common line history with the logs\nthat you see there to make sense of everything. But this is probably just something that's missing.\nIt could be easily added by the team. Or maybe if somebody is willing to do a PR, that's probably\nan easy feature to try to add to the project, which after all is an open source project.\nAnd the other problem, and we have been saying that over and over during this episode,\nbut I think it's worth reiterating that, is that we are using a POSIX model, which is not really\nPOSIX. So lots of footcans there. It could be dangerous. It's probably wrong in the first place.\nSo if you use it, use it with moderation and be aware of exactly the kind of trade-offs\nyou are buying into. Because if you try to use it as a general file system, you are going to have\nproblems for sure. What do you think is that? Should we say that the final verdict is to use\nit or not to use it?\n\n\nEoin: Generally not, I would say right now! Then again, if people have found it interesting and want to try it out for their own use cases, they'll probably already have a good\nfeeling from what we've said so far. I think it's better to stick with the object paradigm when\nyou're talking about an object store rather than trying to shoehorn it into a file system model.\nBut look, you could use it for a period of time during a migration while you work on the changes\nin order to use an object storage paradigm. I think you gave a good example of that back in the\nepisode where we talked about migrating like a CMS or for a legal firm to AWS and using something\nlike S3FS views at the time. It's better, I think, to try and use more native S3 integrations.\nI'm curious to hear if there's cases where you really need something like this. But look,\nif you need to use it, you can use it as a last resource, understand the risks and put your logging\nand metrics in place. If you wanted to use it for web serving, ultimately, you're really better off\nusing a CTN in front of S3. So I think in general, the jury is still out. If there are very compelling\nuse cases that we haven't spotted, let us know because we're really curious. And if you've done\nany benchmarking, please share them with everybody because I think the whole area of S3 performance,\nwhen it gets into really optimizing it, it can take a lot of time. But if you've got any data\non that, I'd love to see it because we can all benefit from it. So thanks very much for listening.\nPlease like and subscribe and share with your friends and we'll see you in the next episode.\n"
    },
    {
      "title": "96. AWS Governance and Landing Zone with Control Tower, Org Formation, and Terraform",
      "url": "https://awsbites.com/96-aws-governance-and-landing-zone-with-control-tower-org-formation-and-terraform/",
      "publish_date": "2023-09-22T00:00:00.000Z",
      "abstract": "In this episode of AWS Bites, Luciano and Eoin dive deep into the world of AWS governance, landing zones, and automation tools. AWS emphasizes the importance of good governance for customers of all sizes, whether you're starting from scratch or have been using AWS for years. But with so many tools available, which one should you choose?\nJoin us as we explore the best practices for setting up your AWS accounts correctly and discover tools that can automate the process, including AWS Control Tower and open-source alternatives like OrgFormation and Terraform.\nWhether you're new to AWS or a seasoned user, there's something valuable for everyone in this episode.\n\nfourTheorem is the company that makes AWS Bites possible. If you are looking for a partner to accompany you on your cloud journey, check them out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources:\n\nAWS Control Tower\nOrg Formation\nTerraform\nOrg Formation official examples\nOrg Formation resource providers\nGreat Terraform + Terraform Cloud demo repository by Conor Maher\nLanding Zone Accelerator with CDK\nAWS Control Tower Account Factory for Terraform (AFT)\nManaging AWS Organizations using Org Formation (AWS blog article)\nHow to get started with Org Formation (another interesting blog post)\n\n",
      "transcript": "Luciano: According to AWS, good governance is a must-have from the get-go for customers of all sizes.\nThis area is not trivial, however, whether you are starting from a clean slate or you have been\nwinging it with AWS for many years. Luckily, there are plenty of tools to automate this and\nguide you to a good setup from the start. But which one should you choose? If you stick until\nthe end of this episode, you'll know some of the best practices to set up your AWS accounts\ncorrectly and which tools can help you to automate the work, including AWS Control Tower\nand open source alternatives like OrgFormation or Terraform or OpenTF, should I say. I don't know.\nMy name is Luciano and I'm joined by Eoin and this is AWS Bites.\nfourTheorem is the company that makes AWS Bites possible. If you're looking for a partner to\naccompany you on your cloud journey, check them out at fourtheorem.com.\n\n\nEoin: Luciano, you mentioned that this is a must-have, so what must we have? What are our objectives?\nAnd we mentioned terms like landing zones and governance. What does this all mean?\n\n\nLuciano: Yeah, Landing Zone is a term that you probably hear a lot when you start to look into governance of AWS accounts. And it's something that if you look at the Well-Architected Framework is described as\nbasically a multi-account AWS environment that is scalable and secure. And it's something we\nmight have mentioned before in previous episodes, so check out our previous ones. But basically,\nif you want to summarize that, the idea is that you most likely want to have a multi-account\nsetup for different reasons. And also multi-account setup is going to ensure you that you can have\nisolation between workloads. For instance, if you have different teams, they're not going to step\nonto each other's toes. For instance, by consuming each other quotas, you can keep the different\nworkloads separated in different accounts. From a security perspective, you can also use ideas like\nsecurity boundaries, which basically they can reduce the impact for security incidents. And\nthe other idea is that you probably want to automate the process of provisioning new accounts,\nso just to make sure that you have all these best practices baked into the process. You don't have\nto repeat the setup over and over again, and especially you don't want to do it manually.\n\n\nAnd finally, a landing zone can also include the concept of auditing and compliance rules.\nSo how can you make sure that every time you create a new account that you have good\nobservability and you know exactly what's going on, you can collect the logs, and if you have\nshared resources, you can have ways to accommodate for all of that. So basically, when we set up\na landing zone, we are looking for a solution that allows us to define all the accounts in\nthe organization structure. You can imagine that as a tree where you have units and then\nunder certain units you can have multiple accounts. We want to have that very clear,\nbut then we want to have a way to define all of that in a formal way, basically.\n\n\nAnd we might want to set up service control policies or SCPs, which are permissions that\ncan be applied to multiple accounts within an organization unit. And sometimes they're also\nreferred to as preventive guardlays because they will stop certain actions from happening from the\nvery beginning. And just to give you an example, you might create an SCP that says this particular\nset of accounts cannot spin up a large EC2 instance. And that's something that at the\naccount level is not going to be possible at all. It's like a policy that stops that action\nfrom happening. You can also use things like AWS Config Rule for compliance. These are more\ndetective guardrails because they don't necessarily stop the action, but they will inform you if\ncertain things that are not compliant are actually happening in a given account. As just as an\nexample, I was looking into the set of managed rules that you get with AWS Config. And one of\nthem is if you have an API gateway, you might want to have X-Ray enabled in all of them. So you might\nenable that rule for you and it's going to notify you if somebody creates an API gateway that doesn't\nsupport, doesn't have X-Ray enabled. Another one is CloudTrail for log auditing.\n\n\nAnd this is actually really, really important. Imagine if you know that some credentials may\nhave been leaked and you want to investigate if they were used and for what. If you have\nCloudTrail configured from the very beginning, you know that you have coverage across all your\nown accounts and you can have that peace of mind that if you ever need to check something,\nyou can just go there and access that information. Another important thing is Delegated Administrator Accounts.\n\n\nSo basically, if you need to run important services in the management account,\nyou basically don't want to use the management account for pretty much almost anything. You want\nto use other accounts. And the reason is because the management account is somewhat special\nbecause certain security features like service control policies don't really apply to them.\nSo basically, if you want to make sure proper security is enforced, try to avoid to use the\nmanagement account. So you need to think how do we delegate certain things to other administrator\naccounts. And just to quickly mention other stuff that might be important, building alerts and budget,\nenabling things like GuardDuty and using Security Hub, SSO, networking, creating, for instance,\nVPCs and connecting them, or even deployment of common resources. Maybe by standard, you want to\nhave an EventBridge or an S3 bucket in every account. You might want to provision them\nimmediately as soon as the account is created. And again, just to reiterate, this idea is not\nsomething that you need to do only if you are creating a landing zone from scratch, like if\nfor a new set of accounts, let's say, it's something you should think about even if you\nalready have a large usage of AWS in your organization, but you never created all this\nkind of governance structure. And now is the time to put all of that into place. So it definitely\nseems like a lot of work, and I think it is actually. But we need to find powerful allies here.\nAnd in general, things like infrastructure as code can make your life so much easier when\ndealing with this stuff. What kind of tools can we use and do they all support infrastructure as code?\nWhat do you think, Eoin?\n\n\nEoin: There are a lot of tools now. The set is growing and the capability of each is growing. And it's kind of catching up with what people actually want because it has been a\nlittle bit difficult in the past. We're going to talk about three main ones, right? That's clues\nin the title. The AWS solution is Control Tower, and we'll dive into that. Then you've got terraform,\nwhich a lot of people will gravitate towards anyway. And another really, really good one is\norg formation, which is an open source one. And you have some other options. You could look at\nCDK. You could just look at using CloudFormation directly with stack sets and custom scripting or\norchestration to apply it to multiple accounts. You can't provision accounts with CloudFormation.\n\n\nYou can set up individual accounts with CloudFormation, but there are some gaps you need\nto address yourself, and then you have to orchestrate it all. So we're going to focus on\nControl Tower, org formation, and terraform. And we might as well start with Control Tower,\nsince this is the one that AWS is pushing and talking a lot about. It has been, I guess,\nbuilt on over the years and is becoming more and more capable. I think people are beginning to take\nit more and more seriously as a contender here. The main thing I would say about Control Tower\nand that differentiates it from all the others is that it's all driven from the console. So while\nyou can have some customizations in source control, it's not really infrastructure as code get ups\nfrom the beginning. The setup and the administration of it is all through the AWS Management\nConsole. Now, this is great for some people. I know a lot of people who think this is fine,\nbut it's not for everyone, particularly if you want to have all that control and visibility\nthat goes with infrastructure as code. I think one of the good things about the Control Tower\napproach is that it has an opinionated set of recommendations and best practices out of the box.\n\n\nSo it can help you to kind of jumpstart setup without worrying about all the different decisions\nyou have to make. AWS are basically just giving you the recommendation. With other solutions,\nyou tend to have to codify all of these practices yourself, although you can always take the\napproach, which I have seen plenty of people do, which is just to copy and replicate the control\ntower best practices in one of the other tools. Cost wise, there's no additional cost for control\ntower itself because it is essentially something that orchestrates a lot of other AWS services.\n\n\nAnd it's those services under the hood that you'll pay for - things like AWS Config and CloudTrail and\nall the rest. When you go into the console and you set up Control Tower, it'll set up an\norganizational unit structure for you with security OU, which is the one it has to create\nfrom the start. It mandates that one. You can also create a sandbox OU for other accounts\nat the start, and then it'll create your log archive account and your audit account.\n\n\nIt will also create things like the Identity Center, SSO users, if you want that, and\ncreate service control policies from the start. So it'll do things like ensuring that you can only\nuse the regions you want to use and other preventative and detective controls.\nSo it's basically using things like AWS Config for the detective controls, AWS\nservice control policies for the preventative controls, and then it gives you a nice UI,\na dashboard on top of all that, and it presents us all in pretty much one pane of glass.\n\n\nNow you can set it up with new accounts that don't even have an organization set up yet,\nor if you've already got an organization set up, you can kickstart Control Tower in there too.\nIt'll basically just leave everything you have and set up new resources in parallel.\nSo it doesn't migrate any existing stuff you have over to Control Tower or anything like that.\nIt sounds pretty simple. It takes about half an hour to get it set up in an\naccount. But of course we know that there's a lot of services out there where they're quick to set\nup, but can be painful to run in the long run. With Control Tower, it can be frustrating because\nit does try to hide a lot of stuff from you. And when things go wrong, it can give you very vague\nerrors. Like I've seen errors that just say fail to set up landing zone, please try again.\n\n\nAnd you don't really know what to do with that. And I was actually just trying with this today\nas preparation for the episode in one of my own accounts, I went through the Control Tower\nprocess and it failed because it said there was some error saying, oh, we can't deploy the stack\nbecause the bucket policy for the CloudTrail bucket has an invalid bucket policy. And I was\nthinking, well, I didn't create the policy, Control Tower did, yet it failed. So it can have a few\nrough edges. And I sometimes think that Control Tower is best if you've got somebody from AWS,\nlike your technical account manager, your solutions architect, or a good support\nor a good support agreement in place and they can guide you through the process.\n\n\nNow, there are some pretty cool features like the Account Factory. So we mentioned the ability to\ncreate new accounts in an automated way. Control Tower has this Account Factory, which allows users\nto come along and self provision accounts. And then you can deploy resources into them,\nlike specific service catalog products. So you can ensure that people can self serve when they need\nnew accounts, but you can also have some guardrails around that. That's pretty nice. Another thing they\nhave is customizations. So Control Tower customizations are a means for you as an\nadministrator of an organization to say when people create new accounts, resources can be\nautomatically provisioned in them. So the way you work with Control Tower customizations is actually\na whole CloudFormation template you deploy once you've got your landings on set up. And that puts\na whole load of resources into your AWS account, like event bridge rules and step functions,\na code commit repository, code pipeline, S3 buckets, CloudFormation, the whole lot.\n\n\nAnd it's basically this big machine that will listen for things like Control Tower accounts\nbeing created. And then it responds to that event, kicks off Lambda functions, code pipeline,\npulls templates from your code commit repository and deploys them to all the different accounts.\nSo it works pretty well and it achieves the goal of having the ability to customize what happens\nwhen an account is created. I have to say though, that the whole implementation scares me a lot.\n\n\nIt's one of those features that it seems like a Rube Goldberg machine where you've got all these\nAWS services and one thing happens, it kicks off another, kicks off another event. And there's,\nif you can look at the step function implementation, it's like got an amazing number\nof states. And I just kind of worry a little bit about what will happen when this goes wrong and I\nhave to troubleshoot it because it's a bit of a leaky abstraction, if you know what I mean.\nSo, but Control Tower is getting a lot of popularity and I think a lot of even bigger\ncompanies now enterprises are starting to adopt it. So it's definitely worth checking out,\nespecially if you're of the kind of letting AWS worry about it approach and don't want that\nreally tight control and customizability yourself. What other options should we talk about next?\n\n\nLuciano: Let's talk about OrgFormation. It is definitely quite different from Control Tower.\nThe main thing is that is a totally open source project. So it's a community effort. And the good\nnews about that is that if anything feels like magic, you just go and get up, check out the code\nand try to figure out exactly what's happening when the magic behavior is presented. It focuses\non simple extensions. So it's basically trying to enhance the capabilities of CloudFormation\nand then makes it a little bit simpler and more intuitive in a way, because if you know\nCloudFormation, you can understand what's missing and what org formation is giving you on top of\nCloudFormation. And because it supports CloudFormation, the idea is that they keep extending\non the idea of using YAML files. So it's all infrastructure as code. You have a special CLI\nthat you need to install from the repository. And at that point, you can use the CLI to give you\nkind of an initial structure where it can create all the, you can import all the existing AWS\naccount that you might already have. And it generates all the YAML files that contain the\ndefinition. And that could be a starting point for you. For instance, it's going to also figure out\nif you have SCPs, it's going to import all of them into this definition. And again, this is a great\nthing because at that point you can put all of this stuff in a repository and then you can manage\nchanges into your account structure using source control. When you want to deploy changes,\nit's basically running CloudFormation, but it needs to orchestrate the execution of different\nCloudFormation stacks across multiple regions and accounts. And it can do that in parallel as well.\n\n\nIt is interesting there because you might be wondering, okay, that's a fairly complicated\nbit to execute correctly. And if there is an issue or in order to understand what's actually\nchanging or not, it needs to keep the state somewhere. And if you have been using Terraform,\nthat concept should be very clear to you. How do you manage the state of changes?\nAnd it turns out that org formation uses the same idea, so it can manage its own state and it stores\nit into an S3 bucket. So you can actually check it out how it is stored and how it keeps track of all\nthe changes. You can use it to manage the organization and accounts, but also you can deploy\nstacks to multiple accounts. So in that way, it's similar to Control Tower because if you want to\nprovision a new account and deploy a set of resources, you can easily do that as well.\n\n\nAnd the way you do that is by just defining CloudFormation code. So if you are familiar with cloud\nformation, that shouldn't be anything surprising. You should be able to use it and learn how to do\nthat very, very quickly. There have been some interesting developments. For instance, you can\nuse CDK and Terraform as well, if that's something that you would prefer to use. So this is actually\na nice feature because it gives you still all that kind of orchestration, but then you can pick the\ntool of your choice to write the infrastructure as code for the things you want to provision in\nevery account. And another very cool feature, which is probably a little bit outside the scope,\nbut still very closely related to the project, is that there are plenty of custom CloudFormation\nresources that you can use to basically fill the gaps where CloudFormation is lacking a little\nbit. For instance, SSO assignment groups, service quotas, and much more. There is a repository, we\nwill have the link in the show notes that you can check out to see all the additional resources that\nthe project is providing to you to make this whole experience even more powerful. This is\npowerful. There is one big limitation. I have to say I really like it, but we have to be fair\nand mention the limitation as well. It is not simple to do a diff, for instance, when you\nhave done a number of changes in your infrastructure as code definition and before you deploy,\nyou want some kind of reassurance that the changes you are trying to apply are actually the ones that\nyou want to see, like the effect is actually something you might want to see before applying\nit. That feature is missing, so if you're doing something serious, it's not as simple as doing\na Terraform diff or a CDK diff if you have used these other tools. That feature is simply lacking,\nso you just run it and hope that everything goes well. Maybe something that can be fixed at some\npoint in the future, but right now it's a pretty important lacking feature that is worth mentioning.\nThere are some very good examples which you can use as a starting point to create your own\nlanding zone structure, and we will have a link to the specific part of the\nrepository that has the examples in the show notes. That's pretty much it. Should we talk\nabout Terraform?\n\n\nEoin: Terraform is one thing that's going to give you a good diff, which is probably a marked difference to OrgFormation. Although, I suppose, with OrgFormation, we have had the\nability here at fourTheorem to work around it by just implementing our own diff on top of it,\nbut it would really be nice to have proper implementation. Now, if you're not excited by\nOrgFormation because of these disadvantages, they both have limitations when it comes to\neither feature set or the level of control and visibility you get, but Terraform is a lot more\nmature than either of those solutions, I'd say. Now, it's not necessarily designed for multi-account\ndeployment, especially across a large number of accounts, but it still has a lot of distinct\nadvantages. You could provision AWS organizations and accounts just as Terraform resources,\nand there's a lot of great community modules that make this whole setup easy as well.\n\n\nYou can also create your own modules for resources that you want to be deployed across multiple\naccounts to make it easy for teams to get onboarded quickly. I think the whole idea of\nusing Terraform for this has improved with Terraform Cloud as well, which is just a really\nnice managed solution to manage your projects and your workspaces and integrates very well into\nGitHub and AWS. Previously, it was a little bit difficult when you had to manage your state\nyourself with S3 or DynamoDB. Terraform Cloud makes that a much more taken care of, robust\nsolution. You can also provision non-AWS resources, another major advantage. So if you wanted to think\nabout deploying Azure resources or GitHub repositories or even Terraform Cloud workspaces\nthemselves, you can do that with Terraform modules and do it in the same projects as your AWS resources.\n\n\nWhen you do this, you get very nice deployment controls, so you can have really nice\nGitOps workflows, and you can see when you have pull requests with changes to your infrastructure\nand your organization, you'll get a very nice Terraform plan. It'll integrate well into your\npull requests. You get a really good visibility of it in Terraform Cloud, and you can put in place\nmanual approvals. So I think if you're a mature organization already familiar with Terraform,\nthis is going to really appeal to you. I think the only real disadvantage with Terraform is that\nit's not really that easy, I don't know the good way at least, to have a dynamic number of Terraform\nproviders. And when you're deploying to multiple AWS accounts, which is essentially what we're\ntalking about here, you have to pretty much declare each Terraform provider for each account.\n\n\nSo it doesn't really have a seamless kind of Account Factory way, or for every account,\ndeploy the same stack concept like you do with control data and information. The more idiomatic\napproach in Terraform is basically to copy paste the boilerplate at the start that says, okay,\nhere's my entry point, my main for a new account. And then within that, you just use modules to\ncompose everything else that goes underneath it. So you don't have to have a massive copy paste\neverywhere, but you do have to have a kind of copy paste the entry point. And once you do that,\nthen you can integrate it seamlessly into your deployment workflow and get your diffs. You can\nget Terraform plan, which is like a really nice feature every time you have a pull request. And\nthen when you merge it, you can have your approval workflow. I'm not an expert in this area at all,\nbut luckily we have a colleague, Conor Maher, who has done a huge amount of work in this space. And\nhe's also provided a nice demo repository showcasing a really nice mature landings on\nset up with Terraform cloud, AWS and GitHub, and it's well documented as well. So we'll have that\nlink to Conor's Terraform demo in the show notes.\n\n\nLuciano: Yeah, let's close this episode with some honorable mentions. Another one is CDK. And there is a specific example that we will have\nagain in the show notes that shows how to do landing zones using CDK. It's not necessarily\nthe best solution for OptiMs or anyone will prefer a more declarative approach because CDK is more\nlike use a programming language and you instantiate classes that represent resources and then combine\nthem together. So it might be a little bit different from what you're used to if you do\ndeclarative stuff, but it's still very powerful and it's still a very good dynamic way of doing\nthe provisioning of all the different accounts in your landing zone structure. And you can make it\nvery modular by using this idea of constructs that are somewhat similar to Terraform modules,\nbut again more in the concept of a programming languages where you can import a library and that\ngives you a class that you can just instantiate and represents like an entire stack where you can\napply certain customizations very easily. So CDK is an option and you might want to consider it as\nwell, especially if you have CDK experience. The other one is AWS Control Tower Account Factory for\nTerraform, also called AFT for short, and it's a way of using Terraform to customize accounts\ninstead of CloudFormation. And once you have Control Tower landing zone already set up,\nyou can enable that and then use Terraform if that's something that's more familiar to you.\n\n\nWe didn't try it yet, so we just heard people using it and being relatively happy with it, so\ncheck it out. It might be worth experimenting, especially if you like kind of this mix of\nfeatures that come from Control Tower, but you also like the Terraform more as a kind of language\nfor writing resource definitions. Now to wrap things up, I'm gonna try to do a quick summary.\nI think what we mentioned today is that the best option is really... it depends on your context. I\nknow it's a bit of a cliche answer, but it really depends on the level of expertise in your company,\nthe kind of tools that you might have used already, and if you prefer a specific approach in terms of\nmaybe you prefer to go through the UI, then Control Tower might be a little bit more friendly to you.\n\n\nYou prefer to do infrastructure as code, so maybe Terraform or OrgFormation are a little bit more\nideal in that sense, so definitely try to weight all the pros and cons of the different approaches\nand pick the one that might be most suitable for your organization. I think if Control Tower had\nsome way of supporting infrastructure as code, it would come out much stronger, at least in our view\nin this comparison, but it's still a good tool if you don't really care too much about infrastructure\nas code. We will have more links in the show notes. We found some more additional deep dives\nand additional material that you might want to check out to understand even more about this topic.\nAnd again, there are probably many solutions out there. Let us know what works for you,\nif you liked a specific tool, why, or if you didn't like some of them, what is the issue that is\nmissing. I think if we can have an healthy conversation about these tools, especially on\nthe open source ones, chances are that we can get the features that we are looking for, and we can\neven contribute to make these features happen. Now, one last thing that I have to say is that\nwe are approaching 100 videos and almost 2,000 subscribers on YouTube, so please help us reach\nout to that milestone by subscribing, and if you can please share this podcast with your colleagues\nand friends, we would greatly appreciate that. So thank you very much, and we'll see you in the next\nepisode.\n"
    },
    {
      "title": "97. Configuration for AWS Applications (Env vars, SSM, Secrets Manager, AppConfig)",
      "url": "https://awsbites.com/97-configuration-for-aws-applications-env-vars-ssm-secrets-manager-appconfig/",
      "publish_date": "2023-09-29T00:00:00.000Z",
      "abstract": "Rev up your AWS know-how in this high-octane episode of AWS Bites Podcast, where we take you under the hood to fine-tune your AWS applications configuration!\nKicking things off, we rev our engines and stress the vital role of slick configuration management in the world of cloud-based applications, leaving those old-school methods in the dust. Buckle up as we steer you through the twists and turns, starting with the straightforward horsepower of environment variables, giving you the lowdown on what fuels them and when to put the brakes on.  We then shift gears to introduce AWS Systems Manager Parameter Store as a simple, yet effective solution that can provide you with all the torque you need. Secrets Manager rolls in next, guarding your valuable secrets with KMS encryption and IAM. The track leads to AppConfig, where they fine-tune your configuration game, ensuring smooth deployments and no pit stops for errors.\nFor the daredevils out there, we open the toolbox and show you how to custom-build your own configuration engine, putting you in the driver's seat. Finally, we rev up the engine one last time and hit the gas with our recommendations, offering you a turbocharged approach to AWS configuration, tailored to your application's needs.\nSo, tighten those bolts and get ready for a ride that'll leave your AWS configuration skills purring like a finely-tuned machine! 🚗💨🔧\n\nfourTheorem is the company that makes AWS Bites possible. If you are looking for a partner to accompany you on your cloud journey, check them out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources:\n\nThe Twelve-factor app\nAWS System Manager Parameter Store\nAWS Secrets Manager\nAWS AppConfig\nAWS Lambda Extension for SSM\nssm-cache-python module\nEoin's article &quot;3 Ways to read SSM Parameters&quot;\nMiddy middleware for SSM Parameters\nBeabetterdev: Deep dive into SSM Parameters\nMiddy middleware for Secrets Manager\nBeabetterdev: Deep dive into Secrets Manager\nMiddy middleware for AppConfig\nBeabetterdev: Deep dive into App Config with a demo\nMiddy middleware for pre-loading config from S3\nMiddy middleware for pre-loading config from DynamoDB\n\n",
      "transcript": "Eoin: One of the things that's very common for web applications running in the cloud\nis that you will need to handle configuration.\nYou're probably running your application in different environments,\ndev, staging, production, etc.\nAnd most likely you'll need to provide simple things\nlike database connection details,\nvarious secrets for things like API keys, session storage,\nor simply referencing different S3 buckets or DynamoDB tables.\n\n\nMost likely these values will be different for every environment.\nIn this episode, we'll discuss which AWS services\nyou can leverage to store configuration for your web apps.\nWe will discuss simple strategies such as just using environment variables\nor services such as SSM, Secrets Managerr, App Config,\nand even how you can even roll your own configuration storage.\nWe'll discuss the pros and cons of every approach.\nAnd if you stick till the end of the episode,\nwe'll also give you our recommendation on what's the best strategy\nfor different kinds of applications.\nMy name is Eoin.\nI'm joined by Luciano for another episode of the AWS Bites podcast.\nfourTheorem is the company that makes AWS Bites possible.\nIf you're looking for a partner to accompany you on your cloud journey,\ncheck them out at fourtherem.com.\nNow Luciano, before we start, as usual,\nwe should probably begin by clarifying the use case a little bit more.\n\n\nLuciano: Almost every application needs some degree of configuration.\nAs we mentioned in the intro, what is really configuration?\nIt's generally something environment specific that your software needs\nas an input to be able to perform whatever task it needs to perform.\nAnd just to give you some examples that can be different kinds of configuration,\nmaybe your application needs to call a specific third party API.\n\n\nSo you need to have an API key for that that is injected somehow at runtime.\nIt can be database credentials if you need your application to connect to a database,\nor maybe you need your application to do some kind of client side\nTLS under shake, so you need to have client TLS certificates.\nSo you need to have a way to also provide those as parameters.\nOr in AWS, it's very common that you build, I don't know,\na Lambda or a container running on Fargate,\nand they often need to use other services like S3 or DynamoDB.\n\n\nSo you might create everything together in a stack,\nand then you need to have a way to tell the application,\nOK, which DynamoDB table do you need to use or which S3 bucket do you need to use,\nand somehow be able to provide that reference.\nBut it can be also something more like application configuration level,\nlike what kind of logging level do you want?\nYou might want to provide that as a parameter\nbecause maybe in a development environment you want to be very, very verbose.\n\n\nBut in production, you don't need to be as verbose\nbecause otherwise you might collect too many logs that you don't really need all the time.\nAnd other more functional parameters could be, I don't know,\ntimeouts when doing HTTP requests or different kinds of connection.\nOr if you really buy into this mental model,\nyou can start to do things like feature flags to enable or disable specific features\nor maintain allow list or deny list to expose certain capabilities\nonly to specific users or customers that maybe have different tiers.\n\n\nSo really, there is no limit to imagination.\nYou can use different kinds of parameters for all sorts of different things.\nSo traditionally, configuration was stored mostly in code.\nSo you would have one configuration file that will contain all this information,\nmaybe multiple configuration files, one for a different environment.\nAnd this is a simple and effective practice, but it comes with a problem.\n\n\nAnd the problem is that you are effectively maintaining all your configuration as code.\nAnd therefore, every time you need to change even one single configuration value,\nthat means you need to go through a code change\nand through the full lifecycle of deploying that code change.\nAnd this is still not necessarily too bad,\nbut it becomes really bad when you need to store secrets\nbecause maintaining secrets in plain text in your Git or whatever other source control tool you use\nis not always easy to do securely.\nMost likely you are going to end up disclosing stuff that should be sensitive\nand should be managed more properly.\nSo definitely, there needs to be a better way to manage configuration.\nAnd today, this is what we want to talk about.\nSo what would be the first option Eoin?\n\n\nEoin: Well, there's an old document at this stage called the 12-Factor App,\nwhich is very popular, I think, still.\nAnd it's all about best practices for designing running applications.\nOne of the things in there is that they say\nyou should store your configuration as environment variables.\nSo maybe we can talk about that one first.\nSo what are environment variables?\nProbably have used them.\nBut when you start a process on any system, Windows, Linux, any Unix system,\nyou're provided with access to a set of key value pairs\nthat are in the environment of the running process.\n\n\nSo you might have seen AWS credentials, for example, like AWS_REGION,\nAWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, standard Unix ones,\nlike PATH, USER, PWD for the current directory, hostname, etc.\nAnd then different runtimes have their own ones as well.\nLike in Java, you'll have CLASS_PATH and JAVA_HOME.\nIn Python, you'll have PYTHONPATH.\nIn AWS, you can use environment variables with Lambda,\nwith Fargate, EC2, any process really.\n\n\nAWS generally provide mechanisms for you to configure the environment variables\nwhen you deploy runtime.\nNow, it's convenient with infrastructure as code as well\nwhen you're creating resources in the same stack,\nyou can define them in your infrastructure as code,\nso you can reference them when you need.\nSo it's quite typical that you'd have an environment variable\nto point to an S3 bucket,\nso the process will know which bucket to write to and read from,\nor an SQS queue or a DynamoDB table.\n\n\nThis allows you to use auto-generated names,\nnames that are generated by your infrastructure as code,\nand your infrastructure as code tools will then track dependencies\nand make sure to create all the necessary resources before the compute one,\nso that you have the right environment variables.\nSo what are the pros and cons then?\nWell, they're very simple to use and often very effective.\nThey're free, right?\n\n\nThey're a built-in concept for most operating systems,\nso you don't need to buy into a particular service and pay for it.\nOn the other hand, they're not great for secrets, right?\nSo environment variables are clear text.\nYou can obviously put an encrypted version in your environment variables,\nbut then you need to have a key somewhere.\nYou can generally see these values from the web console\nor querying resources from the CLI.\n\n\nIt's also risky in that environment variables may be logged to a log file,\nor anyone with access to the host can inspect the process\nand find out what the environment variables are.\nSo in general, it's not a good practice to use environment variables for secrets.\nThey can also only be strings, which can be a bit tricky\nif your configuration is complex with some sort of structure.\nDifferent runtimes will provide their own interminable environment variables as well,\nso there might be a risk of collision if you're not careful with naming.\nEven though this 12-factor app, which I mentioned at the start of this bit,\nis recommending environment variables,\nI find that to be a little bit dated and also not very effective for secrets.\nAnd I think we've moved on a little bit and we've got a lot more options now.\nSo in AWS, we've got a few different options for storing configuration.\nAll with our pros and cons, so let's get started on those.\nWhat's the first one?\n\n\nLuciano: The first one that comes to mind is Systems Manager Parameter Store,\nor SSM parameters for short.\nAnd it's a managed service that it's a little bit like a key value pair storage,\nwhere basically you can store as many parameters as you want\nand it gives you a very simple mental model.\nYou can store one parameter at a time, you decide the key, you decide the value,\nand that's pretty much it.\nIt's up to you to organize the keys in a manageable way,\nmaybe by application, maybe by environment.\n\n\nMaybe you find some kind of convention where you say,\nOK, I'm going to try to stick to a tree of different things\nwhere I always start with slash an environment, slash an application name,\nslash maybe database, slash maybe different parameters\nthat are relevant to your database.\nSo you can build a structure that way, but it's totally up to you\nto define that convention and actually implement it correctly and consistently.\n\n\nIt can give you values in different formats,\nso you can store strings, of course, but you can also store strings lists.\nSo if you have an array, that could be a more ideal way of doing that.\nAnd if you do that, there are certain conventions that you can use\nwhen you do infrastructure as code with CloudFormations or with SAM\nso that you can easily iterate through all the values in an array list.\nAnd you can also store secure strings, which are encrypted values,\nwhich give you some degree of extra security and control\nin case you are storing sensitive information,\nbecause that value is not going to be visible in clear text\nunless you have access to the key that allows you to decrypt that clear text.\n\n\nAnd also, that gives you a bunch of tools and automation\nthat can make integration more seamless,\nso you don't really need to manually encrypt and decrypt that information.\nOne of the downsides is that there is no validation built in.\nSo again, this is something else that is up to you to make sure\nthat every time you insert the values for the first time or change them over time,\nyou are actually doing that, respecting whatever is the correct format\nfor that particular key value pair.\n\n\nOn the good side, you also get an audit trail of changes.\nSo every time you change something, you can see that the value is changing\nand you can keep track of values being changed,\nwhich is something that can be very important,\nespecially again when dealing with security sensitive information,\nlike maybe you are changing an API key,\nit's important to know that that API key is changing.\nIn terms of service, it comes in two tiers.\n\n\nThe first one is called standard tier, and we also have advanced tier,\nand you just get slightly different features and different pricing.\nThe standard tier is free and allows you to store a maximum of 4 kilobytes in size.\nSo I think that's generally more than enough,\nbut if you really need to store more information in a key,\nyou need to use the advanced tier,\nwhich can go as high as 8 kilobytes per key value pair.\n\n\nThe advanced tier is also more interesting because you can use policies,\nso you can add additional rules.\nFor instance, you can say this particular parameter is going to expire\nafter a certain amount of days or months, whatever,\nbut it comes with an extra cost\nbecause when you switch to the advanced tier,\nyou have to pay $0.00 per parameter,\nso $0.05 per parameter.\nAnd there is also an interesting caveat that is that you can,\nif you switch to the advanced tier,\nyou can easily upgrade basically from the standard to the advanced,\nbut you cannot go back.\n\n\nSo of course, when you decide to buy into the advanced tier,\nyou need to consider it's not as easy to go back again.\nAnd if you think about that, it makes sense\nbecause you might start to store an 8 kilobyte value.\nSo how would you transition back at that point?\nYou will lose some information.\nSo AWS doesn't really give you a way to do that\nas a, I guess, a preventative mechanism\nto avoid you losing data in your parameters.\n\n\nNow, how do you use it?\nIs it, it is actually really easy.\nLike you can do an API called get parameter\nwhere you provide the key for the specific value\nyou want to read and you get back the value.\nAnd of course, you can do that from the CLI,\nyou can do that from the different SDKs,\nor you can even see the values from the web console.\nYou need to have the right permissions.\nThis is actually a really good thing.\n\n\nThat you can define fine grained permissions with IAM\nto effectively say which principles can have access\nto which keys.\nAnd if you have created that structure,\nthere's like a tree using prefixes,\nyou can use the asterisk just to say, okay,\nI'm going to give you access to this specific sub-tree\nof configuration.\nSo maybe just by environment and application\nand not every single parameter in your account.\n\n\nThere is also, if you're using it with Lambda,\nit is a bit of extra code that you need to write\nat bootstrap time.\nSo when the Lambda is doing the first call start,\nyou probably want to fetch some of this parameter\nand you need also to have some piece of logic,\nmaybe to try to refresh them every once in a while.\nSo it might be a bit complex to do it in a Lambda\nbecause the Lambda is generally more focused\non the business logic.\n\n\nYou don't want to pollute it too much\nwith all this extra management code\njust to fetch configuration.\nSo one idea there is if you don't want to do all of that\nyourself, you can use an AWS provided Lambda extension,\nwhich once you install it,\nis going to do all of this work in the background\nand in your Lambda you already have immediate access\nto the value coming from SSN parameters.\nIf you do Node.js and if you're using Middy\nas a middleware system for your Lambda,\nthere is actually a middleware that you can just enable\nand it does exactly the same thing as the extension.\n\n\nI am of course biased because being involved\nin the Middy project, I tend to prefer this option,\nbut I think it's a little bit easier\nbecause you don't need to install an extra extension,\nit's just dependencies in your Lambda.\nSo if you already have dependencies,\nyou can easily just do an MPM install\nand everything is then packaged together\nwithout needing additional extensions.\nIf you use Python, there is something similar,\nI think by Alex Casalboni, which is called SSM cache\nand it's pretty much a library that again you install\nand it can do all of this stuff seamlessly for you.\n\n\nSo with very minimal configuration,\nit takes care of all the life cycle of fetching\nthe parameters and making them available\nto your Lambda handler.\nAnd there is also something similar in Lambda power tools.\nI think there is definitely support for TypeScript.\nI imagine there is also for Python,\nbut worth double checking that.\nAnd then if you're using tools for infrastructure as code,\nsuch as SAM or serverless,\nthere are often very interesting pieces of syntax\nthat can facilitate fetching SSM parameters\nat different stages.\n\n\nOf course you can reference SSM parameters at deploy time,\nbut the more interesting thing is that sometimes\nyou can also prefetch this parameter\nbefore the deployment time.\nSo you can effectively interpolate the actual values\ninto your templates,\nwhich sometimes allows you to do more dynamic things\nlike, I don't know, conditionals,\nwhere you could say if the value of this SSM parameter\nis something that maybe you change the template slightly\nto deploy something rather than referencing something\nthat exists already in another stack.\n\n\nSo it can be actually a very powerful model.\nAnd actually I believe Eoin that you have an article on that\nso we'll make sure to link that in the show notes\nfor people that want to deep dive into this approach.\nSo in summary, let's try to say what are the pros and cons\nof this approach.\nI think it's generally a good approach\nbecause it's relatively simple and cheap\nand you also get a quite good throughput.\n\n\nSo if you have lots of services, lots of applications,\nreading parameters all the time,\nyou should have still significant throughput\nto be able to support all of that.\nBut of course there are some problems.\nIt's not great for very structured use cases\nbecause you need to come up with your own structure\nand make sure to be consistent.\nYou don't get any validation.\nSo you are always at risk that somebody is gonna\nmistype something and then the application breaks\nbecause you cannot really parse that information at runtime.\n\n\nIt doesn't deal too well with sensitive data.\nYou can definitely do encryption\nusing the secrets value approach,\nbut it's not very structured again.\nSo for sensitive data like API keys,\nyou also don't get a concept of rotation built in.\nSo it's up to you to create some kind of automation,\nmaybe a Lambda that runs on a schedule,\njust to make sure that you remember to rotate a key\nthat might expire after a while.\nAnd speaking again of throughput limits,\nyou have 40 reads per second,\nunless you buy into the higher throughput mode,\nwhich is 10,000, I think, reads per second.\nBut there is an extra cost for that.\nSo I say that it's good because you generally get throughputs\nbut if that throughput is not enough,\nit comes with extra costs.\nSo you have options there,\nbut you need to account for all the features\nthat you need to build yourself\nand all the extra costs that you get\nwhen you need the more advanced features.\nSo should we talk about the other approach?\n\n\nEoin: Yeah, I think we've got a few other approaches.\nAnd the last one is going to be less familiar to people,\nbut Secrets Managerr, which I think we'll talk about next,\nis probably more familiar.\nAnd this is a specific managed distributed service\ndedicated to storing secrets, right?\nSo this is about passwords, API tokens,\nthings you want to really protect.\nAgain, you can create key value pairs,\nbut unlike parameter store, you've got more options.\n\n\nYou can have structured JSON.\nSo if you've got hierarchical document oriented values,\nthat's possible too.\nThe difference between the secret value in parameter store\nand a secret in Secrets Managerr\nis that Secrets Managerr would use KMS to do the encryption\nrather than that being all hidden from you.\nSo you need to understand how KMS works a little bit\nfor the key management and also provide access to the key\nas well as to Secrets Managerr for principals\nwho are trying to retrieve and update secrets.\n\n\nSo to read a secret, you need to use the API,\nGetSecretValue with the IAM permission for that.\nAnd you can be very granular then as you would want to be\non which gets access to a secret.\nYou can keep data versioned also for auditing,\nwhich is important.\nYou can monitor access to secrets thanks to Cloud Trail,\nwhich is very important for governance and compliance.\nAnd then the really outstanding feature really\nfor Secrets Managerr, I think is the ability\nto automate secret rotation.\n\n\nSo it can rotate secrets automatically on a schedule\nfor certain types of credentials like access to Redshift,\nRDS or DocumentDB.\nAnd if you want to customize the nature of that rotation,\nyou can use Lambda as well.\nSo it's more of a complete managed service for secrets.\nAnd one of the advantages also when it comes to things\nlike databases is that it will integrate into RDS,\nDocumentDB and lots of other AWS services\nso that you don't have to go through the dance\nof retrieving a secret, making sure it's stored secretly\nand even in or sensitively in memory\nand then passing it onto another service.\n\n\nAWS will glue those things together for you.\nAn example of that is you're using CloudFormation\nto create an RDS cluster.\nYou can set the master password to be a secret\nthat's also created in that template.\nYou can configure the rotation for it.\nYou never even have to see that password.\nIt's all just wired together automatically.\nSo that's pretty nice.\nOn the cons, I guess, for Secrets Managerr,\nit can be more expensive,\nespecially compared to the parameter store free tier.\n\n\nIf you heard a few people kind of suggesting skeptically\nthat AWS are, you know, realize the parameter store\nwas a little bit too cheap, especially with the free tier.\nThat's why they invented Secrets Managerr.\nBut Secrets Managerr allows you, I suppose, more throughput.\nYeah, I think you get 30 days free per secret\nand then it gets into a 5 cents for 10,000 API calls.\nSo with all these things,\nyou really have to think about your throughput, right?\n\n\nParameter store, you've got those throughput limits.\nYou need to make sure you're caching.\nYou can't be reading too aggressively.\nI've seen lots of teams run into limits\nwith parameter store.\nWith Secrets Managerr, it might be just a question of cost.\nSo you need to think about, okay,\nhow many processes do I have running?\nHow often are they reading these values?\nAnd what's that gonna cost me?\nAnd will I stay within the throughput quotas?\nSo Secrets Managerr has that throughput cost,\nbut it also has a 50 cent per secret per month cost as well.\nSo think about that.\nAnd maybe think about some of the alternatives.\nSo where are we when it comes to alternatives?\nI mentioned one that's less familiar for people\nand I'm definitely interested to hear about App Config.\nCan you walk us through that one Luciano?\n\n\nLuciano: Yes, App Config is really interesting because I think it tries to give you\na more structured experience,\ntrying to fill all the missing gaps\nof the other approaches we mentioned before.\nSo let's try to describe everything in order.\nSo what it is really is an order managed service from AWS\nand it allows you to store configuration.\nBut this time, rather than thinking\nin terms of key value pairs,\nit's more you are storing an entire configuration object\nthat makes sense in a specific context.\n\n\nAnd this configuration object is of course replicated\nacross different ADs, so it's always highly available.\nSo you don't really have to worry about the storage piece.\nIt's more, it's there and AWS make sure\nthat it's always available for you when you need it\nto reference in your application or your infrastructure\nas code.\nAnd one of the new features compared to the other ones\nis that it uses a concept called validators\nthat is actually something you can configure\nvery, very granularly.\n\n\nAnd you can define exactly what are the rules\nthat basically say that the values you are inserting\nin this configuration object are actually conformed\nwith what your application is going to look for.\nSo basically that is gonna save you\nfrom somebody making a typo\nbecause maybe they forgot a quote or a semicolon\nor a curly brace, whatever.\nAnd that is something you will see\nwhen you try to change the value.\n\n\nSo when you try to deploy the value itself,\nnot when your application starts\nand then your application is going to crash.\nSo basically this measure allows you\nto prevent accidental crashes of your application\nby seeing the issues when you try to change\nthe configuration rather than when you deploy\nthe new configuration and the application crashes,\nwhich I think is really, really cool\nbecause it can prevent also downtimes,\naccidental downtimes just due to human error.\n\n\nAnd in that light of trying to make deployment safer,\nthere is an entire mechanism that allows you\nto roll out deployments of configuration changes\nin different ways.\nWe'll talk a little bit more about that.\nBut again, the idea is try not just to manage configuration\nin a more structured way, but also to make sure\nthat every time you change that configuration,\ndeploys are actually managed more carefully\nand you try to spot as soon as possible\nif that configuration is gonna break your application\nand take preventive measures or roll back\nas soon as possible.\n\n\nAgain, the service keeps an audit trail\nof all the configuration changes.\nSo this is not necessarily new,\nbut of course you also have that feature.\nSo let's try to talk more about what is the experience\nof using it.\nAnd I think that will describe a little bit more\nall the different concepts and how this tool\nis a little bit more feature complete than the other ones.\nSo when you start,\nyou need to define an application configuration.\n\n\nAnd this is already the first big change\nbecause right now we have been talking\nabout key value pairs.\nSo not necessarily tied to one environment\nor one application.\nAppConf immediately makes you think about,\nno, this configuration is not something very generic.\nIt's not one parameter that exists on its own.\nYou need to think about an application\nand we are defining the entire configuration\nfor that application.\n\n\nSo you start by defining this concept of a container\nthat represents your application.\nYou can integrate that basically\ninto once you have basically the application stored\ninto App Config, of course you need to do something\nat the application level to make sure\nthat you can fetch that information.\nAnd this is interesting because it's again a pull mode.\nSo it's your application that needs to know exactly\nwhen to fetch that information.\n\n\nAnd it needs to do that by calling explicit\nthe get latest configuration command again.\nSo you can do from the CLI,\nso you can do it from the SDK\nor with a bunch of other helpers\nthat we will describe later.\nOne of these helpers is an extension for AWS Lambda.\nIf you use Lambda,\nso very similar to the one we described for SSM\nthat can fetch the configuration automatically for you\nand can try to refetch it after a while\nto make sure it's always kept in sync\nwith the latest configuration.\n\n\nIf you use Middy, again, there is a middleware for it.\nSo very similar to the SSM parameters one,\ndoes auto fetching, caching and refreshing for you.\nAnd another thing that you can do\nis fetching and refreshing for you.\nAnd I think from a configuration perspective,\nthere are some interesting concepts\nthat are worth expanding on a little bit more.\nSo when you define an application configuration,\nyou also need to define environments.\n\n\nSo again, the approach is very methodical and structured.\nYou don't have to invent anything.\nYou just need to follow the process.\nSo an environment is something like depth,\nstaging, production, beta, preview,\nwhatever you want to call it,\nthat makes sense for the different stages\nof your application life cycle.\nSo you can pick different configuration profiles.\nYou can pick between freeform and Fisher-Flack,\nand they give you a very different experience\non how to define your entire configuration.\n\n\nSo Fisher-Flack is probably a little bit simpler,\nbut it's probably more specialized for the cases\nwhere you are actually really thinking about enabling\nor disabling specific features\nfor specific classes of users.\nWhile freeform is a lot more,\nyou have a big structured configuration file,\nI'm going to give you all the tools\nthat you need to manage that configuration file.\nAnd it's not really a file,\nit's just something you are storing in AWS\nand you load it when you need that information.\n\n\nSo when you use that freeform configuration profile,\nyou have a choice of how are you going to define\nthe object structure, and you can define plain text,\nJSON and YAML as the three available option.\nI think JSON is of course the most common.\nAnd if you use JSON, you can even use JSON schemas\nto create your own validators,\nwhich when you do APIs,\nthey're probably used to JSON schema.\nSo it can be very convenient way of defining\nall the validation rules for a piece of JSON.\n\n\nBut if you use something else,\nlike if you want to use plain text and use your own format,\nbecause I don't know, maybe you like TOML, let's say,\nwhich is not really supported out of the box,\nthen you can even create a Lambda\nthat can do the custom validation for you.\nSo it's really an extensible model\nwhere if you really have bespoke use cases,\nlet's say maybe you are migrating an application\nthat I don't know, from Java using INE files\nfor configuration,\nyou can still use this approach.\n\n\nYou just need to do a little bit of extra work\nif you want to have validators,\nmaking sure that everything is configured correctly.\nWhen it comes to deploying a change,\nas we mentioned before,\nwe have different deployment strategies.\nAnd just to give you an example,\nyou can go for like an immediate rollout where you say,\nevery fetch that happens after I click okay,\nneeds to get the latest version of the application,\nof the application configuration.\n\n\nThis is of course the simplest rollout model,\nwhere it's like, I'm sure everything is gonna work fine,\nand no worry, just push it to everyone.\nBut if you want to be a little bit more careful,\nyou can use different strategies for software layouts.\nAnd just to give you an idea,\nfor instance, you can say, okay,\nI want to linearly increase the number of clients\nthat see the latest configuration.\nFor instance, you might start with 10%,\nthen after a minute, an additional 10%\nis gonna get the new configuration\nuntil you reach the 100%.\n\n\nOr you can even define that by time.\nSo you want to say, okay,\nI want to gradually rollout everything\nin the next three minutes.\nAnd of course you can monitor the software rollout,\nand if something goes wrong,\nyou can basically define in App Config,\nyou can tell it to watch for a specific CloudWatch alarm.\nIf that CloudWatch alarm fires\nwhile you are doing a software rollout,\nthen it's gonna assume that something went wrong,\nand it's gonna roll back to the previous configuration.\n\n\nSo this is actually a very powerful mechanism\nthat allows you to safely rollout\nwith the least damage possible,\nbecause you have validation upfront,\nyou can still break things,\nbecause even if your configuration is syntactically valid,\nmaybe the content is not correct,\nmaybe you have the wrong API keys,\nso your application, when it starts,\nis gonna start to fail,\nbecause it cannot call a third party service.\n\n\nSo you can roll back as soon as possible\nwhile you maybe have impacted\nonly a small fraction of your users.\nAnd I think this is the most powerful feature,\nand if you would have to replicate that yourself,\nit is really a lot of work,\nand it's hard to get it right.\nSo this is definitely, I think,\nthe power feature that you get by using App Config.\nOne last note that I have\nis that it can be integrated with Secret Manager,\nso if you are worried about storing secrets,\nthere is a nice integration there\nwhere you don't really have to manage secrets yourself\nin plain text or encrypt them yourself,\nbecause you can rely on Secrets Managerr\ndoing all of that for you.\n\n\nAnd another interesting feature,\nwhich I was not really sure when it could be useful,\nbut if you want to store the actual content of the object\nin things like S3 or SSM parameter,\nor even SSM documents, you can even do that.\nSo the backend doesn't have to be App Config itself,\nbut you can even rely on using other services\nas the backend.\nFinally, let's talk about pricing.\nIt seems very appealing just from the outside.\n\n\nI haven't used it really at scale,\nso I don't know if there is any hidden surprise with pricing,\nbut basically it's a usual pay-as-you-use model,\nonly unfortunately there is no free tier,\nbut then the price seems relatively low,\nso I don't think it's really a problem.\nSo you basically pay for configuration updates,\nwhich is a very low charge.\nLike you will need to do 10,000 changes in a document\nto get charged $8,\nand I don't really see most applications\ndoing 10,000 changes even in like 10 years, probably.\n\n\nSo yeah, I think it's a very reasonable charge.\nSo most of the time that cost should be neglectable,\nunless you really have huge applications\nthat are changing all the time,\nbecause maybe they are extremely highly dynamic,\nintegrating, I don't know,\nconfiguration for multiple sources.\nAnd then of course you pay per API call,\nso every time you fetch the configuration, there is a cost.\nIt is relatively low, but again,\nworth doing some maths there,\nmaking sure that if you have thousands and thousands\nof services trying to read the same configuration,\nyou have multiple environments,\nso that kind of multiplies even more.\nOf course, that low cost can easily compound\nand get to a point where it's not sustainable anymore.\nSo always take our recommendation\nwhen it comes to prices with a pinch of salt,\nbecause every use case is very different,\nand you always need to do your own maths\nto make sure that service and that pricing model\nmakes sense for your use case.\nOkay, Shu will talk now about the crazy idea\nthat you don't like all the other services,\nand you are just feeling confident\nthat you can build your own service,\nmanaging all this configuration.\n\n\nEoin: I think the main reason that anybody would be motivated to roll your own probably based on everything we've said\nis if the pricing or throughput constraints\nof any of the services we've mentioned\ndon't really fit their access patterns.\nSeems like it would be simple to implement,\nbut not necessarily so.\nYou could do it, I've seen it\nand built systems like this in the past,\nand you can use services like S3 or DynamoDB,\nor ElastiCache or Redis or Memcache for this kind of thing.\n\n\nAll depends on what kind of performance you need ultimately.\nThen it's up to you to define all the necessary conventions\nto manage the data consistently per app and per environment\nand ensure that the consistency is in place\naccording to what you need as you replicate\nacross multiple availability zones.\nYou might need to add validation.\nYou'd need to think about how to manage sensitive data\nin a secure way and maybe provide rotation support,\nand then defining an API to make it easier\nto fetch specific versions or a subset of the configuration.\n\n\nThen you need to think about controlling access\nto the configuration layer,\nyour change log, keeping a history of changes for auditing.\nAnd if you do, probably in a regulated environment,\nyou need to then think about getting compliance for that,\nwhich AWS have already taken care of.\nSo for simple cases, it might work.\nYou can imagine if you look at the SSM Parameter Store API\nand you don't like the cost,\nyou could say, well, I can implement this with DynamoDB.\n\n\nI can easily do starts with secondary key match\nto retrieve the values,\nand I can pretty easily build an API\nthat's compatible with the SSM Parameter Store API,\nbut you have to think about all those other pieces\nand keeping it up to date, et cetera.\nEven though you might end up getting very good performance,\nthroughput and cost with your custom built solution,\nyou end up with another chunk of code\nthat you probably don't really want to maintain\nonce the novelty of building such a system has died down.\n\n\nFor simple cases, if you have config files\nand you just want to store them in S3 or a DynamoDB table,\nthere are solutions that help you there.\nEven we've mentioned Middy a few times\nand Middy offers a collection of middlewares\nthat make it easy to prefetch and cache configuration\nthat's stored in places like that.\nYou can even do that with power tools as well.\nIt's one for the people\nthat like to build everything themselves, I guess,\nbut best avoided otherwise.\nAnd I think that's our final one\nin this collection of options for storing configuration.\nLuciano, do you want to give people\nwhat we promised at the start,\nour recommendations for what approach to take?\n\n\nLuciano: Yeah, I'll try to give a more, it's definitely opinionated,\nbut hopefully sensible enough in terms of\nhow do you approach even choosing\nbetween all these different options.\nI think starting simple is always a good recommendation.\nIf you're building something small,\nmaybe doing infrastructure as code,\nyou don't really need to reference anything sensitive,\nbut maybe you just want to reference, I don't know,\nDynamoDB table names, S3 packets names,\nthat you are building the same stack,\ngoing with environment variables\nis going to be super simple,\nno problems in terms of security.\n\n\nEverything's populated by infrastructure as code,\nso even the risk of doing mistakes is very low.\nSo why not doing that?\nIt's something that you see a lot in every tutorial\nwhen you see, I don't know,\nhow to get started with Lambda and API Gateway and DynamoDB,\nyou will see something like that.\nSo also a very common approach\nand worth using it for the simplest cases.\nYou can switch to SSM and Secrets Managerr\nas soon as you start to have more advanced use cases,\nwhere maybe you need to manage a bit more configuration\nand you want to define your own structure,\nor maybe when you need to start managing secrets\nand you need to manage all the life cycle\nof those secrets correctly,\nmake sure that they are stored correctly, rotated correctly,\nand you have control on who gets to access those secrets.\n\n\nBut of course, when you get to work\non more complex applications\nand you want to really have a fine-tuned life cycle\nfor all the configuration,\nI think App Config is really the service\nthat you want to use.\nIt is relatively new,\nso no surprise that tries to fill all the gaps\nof the previous services,\nbut it seems to give you really an entire life cycle,\nso you can have, I guess, a better experience\nand you don't really need to build anything\nto fill the gaps yourself.\n\n\nNow, finally, we would, of course,\ndiscourage the custom build solution,\nunless you really have good reasons,\nand good reasons, as we said,\nit might be cost or performance,\nbecause you might have really bespoke use cases\nwhere going with the other services\nwould be either too expensive,\nor maybe you need a throughput\nthat is not really friendly enough.\nEither you cannot do it because there are service quotas\nor because it gets too expensive again, right?\n\n\nSo considering cost and throughput together,\nyou might want to build something yourself\nthat might be, I guess, reasonable at that point.\nOr maybe another use case for this\nis when you are doing a migration\nand you already have a very bespoke mechanism\nto manage all the configuration for your application,\nand you don't really want to start to change all of that\nas a first step in your migration,\nmaybe that's a place where it might make sense\nto do something a little bit custom\nas you continue through your migration.\nBut I suppose you still eventually want to migrate\nto something more structured like App Config,\njust so that you can clean up all this custom code\nand keep your application a little bit more focused\non what is the business value\nthat it's providing for your company.\nAnd I think that's our generic recommendation.\nLet us know if you agree with it\nand which kind of services have you used already,\nand if you have, I guess, a similar experience\nand perspective on all of those.\n\n\nEoin: Well, to wrap up, I'm going to point people\nto a few resources that we've found,\nand Be a Better Dev has some great videos with deep dives\nand demos on all of the options here.\nThose links will be in the show notes.\nEverything we mentioned with Middy and power tools,\neverything else, all the other articles we mentioned\nare also in the show notes.\nSo once again, thank you very much for joining us\nand listening, we really appreciate it,\nand we'll see you in the next episode of AWS Bites.\n"
    },
    {
      "title": "98. Is AWS Going to Kill Pinpoint?",
      "url": "https://awsbites.com/98-is-aws-going-to-kill-pinpoint/",
      "publish_date": "2023-10-05T00:00:00.000Z",
      "abstract": "Today, we will explore the enigmatic world of Amazon Pinpoint. Pinpoint boasts a wide range of capabilities that can prove advantageous for various marketing endeavors.\nIn this intriguing episode, we will shed light on Pinpoint's core features and use cases. Additionally, we will compare it to several other marketing products, including Google Analytics, Marketo, Mailchimp, and more.\nHowever, the most startling revelation pertains to the recent changes in Pinpoint's limitations, which have left users deeply concerned. The astonishing reduction from 7,000 events per second to a mere 15 has prompted us to ponder: is AWS attempting to relegate Pinpoint to oblivion?\nJoin us in the eerie tranquility of a snow-covered hotel as we unravel the mysteries surrounding Pinpoint's destiny. Could it be that AWS aims to discontinue Pinpoint altogether? Alternatively, are they endeavoring to revitalize it, akin to a vengeful spirit seeking redemption? Or could there be a nefarious pricing strategy at play?\nAs the ethereal specter of Pinpoint looms large, we implore AWS to provide clarity regarding its intentions. If you are a Pinpoint user or contemplating entering this mysterious realm, tune in and share your thoughts in the comments. Will Pinpoint endure, or is it destined to become a fading memory in the annals of technology?\n\nfourTheorem is the company that makes AWS Bites possible. If you are looking for a partner to accompany you on your cloud journey, check them out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources:\n\nAmazon Pinpoint\nAmazon Pinpoint service quota\nAWS support\nReddit thread discussing the quota change\n\n",
      "transcript": "Luciano: If you're running a software as a service application\nor an enterprise application for web and mobile users,\nyou will likely need to select services for analytics,\ncustomer communication and campaigns.\nThere are lots of commercial options,\nopen source options as well,\nbut in the world of AWS,\nthere is one service that tries to rule them all,\nand this is Amazon Pinpoint.\nPinpoint promises to make it easy for developers\nto integrate these capabilities\nand also to meet the needs of the market.\n\n\nHowever, we have just heard some surprising\nand a little bit worrying news\nthat causes to question the future of Pinpoint.\nToday, we're going to talk about Pinpoint,\nits capabilities and the recent changes\nthat makes us a little bit worried and makes us ask,\nis AWS going to kill Pinpoint?\nMy name is Luciano and I'm joined by Eoin\nfor another episode of AWS Bites podcast.\nfourTheorem is the company that makes AWS Bites possible.\nIf you're looking for a partner to accompany you\non your cloud journey,\ncheck them out at fourtheorem.com.\nSo, Eoin, should we start by maybe giving an introduction\non what Pinpoint is?\nI recognize it's not probably the most famous AWS service\nand maybe some people haven't heard about it.\n\n\nEoin: Let's try to describe it, what it is, what it does, and why it can be useful.\nYeah, when it came out,\nand since I've heard a lot of people excited about it\nbecause of the range of things it can do,\nit hasn't seemed to live up to the hype in some ways\nof people talking about it, articles, blog posts,\nthe kind of thing where you wonder,\nis it just used by a couple of large enterprise customers\nwho just don't talk about it,\nor is it something that hasn't seen the uptake\nthat we expected?\n\n\nIt's designed to solve a lot of different things\nin the marketing space.\nSo if you imagine what you might want from a SaaS application\nis to track user engagement and key metrics,\nthe first thing Pinpoint will tackle is basic analytics.\nIf you imagine things like Google Analytics,\nMixpanel, Hotjar, Fathom,\nanalytics within Pinpoint is an alternative.\nAs a developer, you would typically just use\nthe Amplify SDK, integrate that into your client.\n\n\nSo that would be your web client, mobile applications.\nIt can also integrate with like your Cognito user pool,\nor you can import your user data\nand your segments from somewhere else.\nAnd then you can immediately get automatic metrics,\nlike you'll get metrics on sign in\nand engagement with the user interface,\nbut you can also get metrics relating to specific things\nyou're trying to track by adding in custom metrics\nin your client application with the Amplify SDK.\n\n\nAnd that will send them directly to a Pinpoint API\nand stream those click events up\nor whatever those engagements are you're tracking.\nAnd then the next thing is kind of\ncommunication and engagement.\nIf you want to send either transactional messages\nor promotional messages to specific categories of users,\nPinpoint is also geared towards that.\nSo examples of transactional messages\nwould be sending one-time passwords or reminders to users.\n\n\nAnd then you've got promotional messages\ndesigned to convert, upsell, reduce turn,\nmaybe things like abandoned cart follow-ups.\nSo you've got channels like email, SMS, phone,\npush notifications to mobile devices,\nand you can also add your own custom channels as well.\nSo if you want to support some custom messaging service\nor WhatsApp integration,\nyou can do custom integrations as well.\nThe difference, I suppose, between this\nand using some of the lower level services\nlike SES and SNS for engagement\nis that Pinpoint will give you tracking\nof engagement as well.\n\n\nSo it'll give you the number of opens on emails\nand messages and engagement with push notifications too.\nIt's one of the few AWS services\nwhere developers wouldn't interact with it\nas much as your marketing team would.\nSo you'll actually find your marketing team\nin the console engaging with it,\nlooking at metrics and creating campaigns and stuff.\nOnce you have your analytic\nand you've got your users on the system,\nyou can create segments of users,\nand that can be based on a lot of different filter criteria.\n\n\nThen you can create campaigns that will use the channels\nlike email or SMS or whatever to reach out to those users.\nYou'll create your campaign with a channel,\nyou select an email template or a message,\nthen you pick a schedule.\nAnd Pinpoint has that capability for you to say,\nokay, I want to send this out\nover the course of the weekdays,\nbut I don't want to interrupt my users during a quiet period.\n\n\nIt'll even adjust that to the user's local time zone.\nSo you can imagine if you were building\nall this stuff with SES yourself,\nthere's actually quite a lot of complexity.\nIf you think about other products in the market\nfor messaging and engagement,\nyou're looking at Twilio and MailChimp\nfor this campaign stuff segment is a good example.\nYou also have Salesforce marketing\nand Adobe marketing products that do similar things.\n\n\nIf you have an application that has a use case\nconsisting of multiple steps,\nyou can track a user's progress through that journey\nand then take actions to try and encourage them\nto fulfill the journey to its completion.\nExamples of user journeys are tracking signups\nto ensure that users log in\nand then actually use the application once they signed up\nor bringing new users through an onboarding process\nand also maximizing the number of sales and conversions,\nfor example, following an initial free trial.\n\n\nIt's almost like step functions for marketers in some ways\nbecause you design the flow,\nbut then you track the user's engagement\nand can see how successful it is at each point and as a whole.\nAnd you can also do sort of multivariate splits,\nrandom splits of users and everything,\ntake different paths in that flow for different users.\nSo there's a whole visual designer\nin a canvas there for that.\nJust finally, reporting is a big thing.\n\n\nPinpoint analytics doesn't allow you access\nto the raw data by default.\nYou're essentially looking at aggregate data\nby demographics, et cetera.\nBut you can also stream the events that you capture\nthrough to Kinesis or Kinesis Data Firehose.\nAnd then once you have it in there,\nyou can take it into Redshift or S3\nand then use things like Athena and QuickSight\nto visualize and report on it.\nHopefully now that we understand kind of what it does\nand what problem it solves,\nwe can kind of understand why it might be a bit of a concern.\nThe messaging we've seen around recently,\nthis kind of relates to quotas and limits.\nSo Luciano, do you want to take us through\nthe limits around Pinpoint and the bombshell?\nLet's start by discussing the limits first.\n\n\nLuciano: So the bigger one to start with is that Pinpoint\nis not available in every region.\nFor instance, it's not available in South America.\nSo you'll need to go and choose one of the bigger regions.\nAnd there might be some compromises with that,\nespecially if you are far away from those main regions.\nThe other one is that you have a maximum of 90 days retention.\nAnd you can kind of work around that.\nYou already mentioned that you can stream data to Kinesis\nusing Firehose and send data to S3.\n\n\nSo you can basically use this process\nif you want to retain data for longer.\nBut Pinpoint itself is going to hold the data\nfor no longer than 90 days.\nThe breaking news that we wanted to mention is that a week ago,\nAWS sent an email to customers using Pinpoint\nand basically changed a little bit the limits\nthat the quota that you have in Pinpoint\nin a way that we think it's actually quite important\nand might have interesting side effect.\n\n\nAnd there are different changes,\nbut the most interesting one is that\nyou could originally have 7,000 events per second per account.\nBut from October 22nd of this year,\nthe limit changes from 7,000 to just 15.\nSo this is like 99.8% reduction of throughput.\nAnd also, AWS says that this is an hard quota,\nso it cannot be increased.\nSo it's literally going from 7,000 to 15,\nwhich is like going from 100 to zero almost.\n\n\nSo it's, of course, obvious to start to ask very hard questions\nlike what can you really do with 15 events per second?\nIs that going to be enough?\nEspecially if before you had 7,000,\nwas it like massively more than you needed before\nor now it's just too low?\nWhat's going on here?\nAnd we can speculate a little bit\nand try to figure out from the AWS perspective\nwhat is the motivation for this reduction.\n\n\nWe have a few ideas.\nThe first one is that the more, I guess, negative one\nis that AWS is just using this strategy to try to kill Pinpoint.\nAnd they don't want to explicitly sunset it,\nbut they want to reduce usage drastically to the point\nthat nobody's using it\nand then maybe eventually they can get rid of it,\nwhich is very concerning because we are familiar\nwith other cloud providers killing products all the time\nand not really caring about what happens to the users.\n\n\nBut to be fair, we always had a very good experience with AWS\nand there are very famous cases like SimpleDB\nthat technically doesn't exist anymore.\nIt's very hard to find material online.\nIt's very hard to see anything in the AWS console.\nBut we know that there are old users\nthat still have access to that and they can still use it.\nAnd AWS is supporting that,\nwhich we always seen that as a commitment\nto keeping the user, giving them a good experience,\nmaking sure that the cloud provider\nactually removes concern from the teams.\n\n\nAnd so in that way, it's kind of a partner more than a liability.\nSo if that changes, I think AWS is kind of putting themselves\nin a little bit of a dangerous position\nfrom a marketing perspective, from a security perspective.\nAnd as consultants, we often hear this kind of concerns\nfrom customers and we can always reassure them by saying,\nlook, we have been in this business for a while\nand we've never seen the same things\nthat we have seen in other cloud providers.\n\n\nSo we are relying on AWS and we trust them.\nSo if that changes, I think it's going to be\nan interesting conversation.\nThe other point that is worth mentioning\nis that it's a product, as Eoin, you mentioned,\nthat is not really focused for technical users,\nwhich is a little bit different from what AWS mostly does\nbecause AWS always provides API infrastructure as code\nand automation built in pretty much every service.\n\n\nSo every service is very geared towards developers.\nThis particular service is more marketing focused.\nSo I think there might be a little bit of attrition there\nin the way that AWS builds services, builds UIs\nto give marketers a very good experience.\nSo just by the fact that users have to go into the AWS console,\nif they are not technical enough, if they don't know AWS,\nit might be a very distracting experience.\n\n\nIt might not be very well optimized\nfor the kind of workflows that they need to do.\nSo maybe that's something that AWS is recognizing\nand maybe wants to do something about that,\nmaybe wants to create an alternative service,\nor maybe wants to start, wants to focus more\non kind of the core functionality,\nlike messaging like SES, SNS,\nand just rely on partners like Salesforce, Market,\nor Segment, Wilio to do kind of the user-facing part.\nAgain, it's worth reminding that AWS has a very good reputation\nfor not killing services.\nSo maybe we are kind of exaggerating the problem,\nbut it's worth asking these other questions.\nThere is another option there.\nLike it's not necessarily AWS trying to kill the project.\nMaybe there is a pricing conversation to be had there.\nAnd if you look at the pricing of Pinpoint,\nfrom the outside, it actually looks quite cheap.\n\n\nEoin: You've got 100 million events for free in the free tier before you pay anything.\nSo after that, it's a tiny fraction per event.\nSo then it might start to count.\nBut I guess for a lot of startup applications,\nthey won't get even near 100,\nparticularly while the user volume is small.\nYeah, absolutely.\n\n\nLuciano: So the idea that is maybe AWS is recognized\nthat they are not being profitable with the service\nwith the current pricing model.\nSo maybe they are trying to reduce the usage.\nMaybe they will change the pricing a little bit,\nor maybe they will have private conversation\nwith the heavy users and try to figure out\nhow to make the product better for them,\nbut at the same time,\nfigure out a price in the works both ways.\nSo these are just speculation again.\nWe don't really have official news from AWS at this point.\nSo you can, of course, come up with your own opinions,\nbut I guess it's fair to ask what happens next.\nLike if you are a user of Pinpoint,\nwhat can you do now?\n\n\nEoin: If I was using Pinpoint at scale and really relying on it, it's definitely in a question to be had\nwith your account manager at AWS\nand your solutions architect to figure out\nwhat is the strategy.\nI've looked in some of the accounts\nthat we are working with,\nand I can still see that we've got the quota\nof 7,000 events per second.\nIt's not clear whether the new limits\nwill only apply to new accounts or to existing accounts\nor to existing Pinpoint users.\n\n\nSo there's definitely clarity required.\nAnd I think that's the main challenge here, actually.\nIt's not just a limit decrease in its own right.\nIt's kind of the limited communication around it\nand the clarity from AWS.\nIt would be good to understand\nwhat the strategy is for Pinpoint,\nwhether you should stick or twist.\nIf you're planning on adopting Pinpoint,\nI know customers who are either using it\nor are planning on using it,\nand this is obviously something\nthat's going to scare people quite a lot.\n\n\nPinpoint did have this advantage\nof having all this capability within AWS,\nso you didn't need to integrate lots of third-party solutions.\nAnd it would be nice to have that option of using it,\nbut using it at scale, because 15 events per second,\nI mean, if you can imagine one user\nclicking around a website or a mobile application,\nit's easy enough to create hundreds of events\nin a few seconds for one user.\n\n\nSo if you've got thousands of users,\nyou need to have that capability\nto have thousands of events per second, I believe,\nand to be able to increase that as your business grows.\nSo there's lots of alternatives out there\nthat people are going to immediately go to\nif they need something today,\nGoogle Analytics, Segment, Mixpanel, Adobe,\nall of those for email communication\nwithin AWS, you can always just use SES and SNS\nfor the SMS and push notifications.\n\n\nYou'll just have to do a little bit more work to set it up.\nAnd if you want to track and analyze user events\nor just track events in your application,\nyou could always do the kind of vanilla raw approach,\nwhich is to capture them and send them directly\nto Firehose or Kinesis from the applications,\nwhich will give you a similar kind of data set.\nAgain, it's just not as seamless\nas it would have been with Pinpoint.\n\n\nSo let's hold out for some information.\nIt's nearing the 22nd of October at the time of recording.\nSo we'll kind of keep a keen eye on any communication\naround this and any other chatter.\nThere's very little, there is one Reddit thread\nwhere this is being discussed by people using Pinpoint\nand there's a lot of concern there.\nMaybe we can link that in the show notes too,\nbut there's very little chatter about it.\nI couldn't find anything on Twitter\nor any other social media outlet.\nSo we'll kind of keep a close eye on it\nand let you know as well\nif we hear anything official.\n\n\nLuciano: Yeah, I think that brings to the end of this episode, but before we wrap up,\nwe'd be very curious to hear your opinion\nin general on Pinpoint.\nHave you been using it?\nWhat was your experience?\nAre you planning maybe to use it in the future\nand why compared to all the other alternatives?\nI'm curious to know if you have other speculations\nin why this is happening\nor what will be the future of this service.\nSo definitely reach out to us and let us know\nif you're watching on YouTube, leave a comment,\notherwise reach out on social.\nAs always, if you find value in this episode,\ngive us a shout, give us a thumbs up,\nreviews, share it and help us to grow the channel.\nWe recently reached 2,000 subscribers on YouTube.\nSo that kind of makes us very happy.\nSo thank you everyone for following us so far\nand for supporting us.\nWe'll see you in the next episode.\n"
    },
    {
      "title": "99. The fears of adopting AWS (and how to fight them)",
      "url": "https://awsbites.com/99-the-fears-of-adopting-aws-and-how-to-fight-them/",
      "publish_date": "2023-10-13T00:00:00.000Z",
      "abstract": "In this thrilling episode of AWS Bites Podcast, we delve into the murky world of cloud computing and discuss the most haunting fears that deter businesses from adopting Amazon Web Services (AWS). In this gritty discussion reminiscent of a noir novel, they reveal the sinister concerns of cost, complexity, security, and vendor lock-in that keep organizations in the dark.\nIf you're in the cloud consulting business or facing internal resistance to moving your projects to AWS, this episode is your secret weapon. We shed light on how to reassure your clients and your boss that AWS can bring value. We also provide valuable tips on how to prepare your organization for a successful migration, as these transitions often require significant changes within the company itself.\nIn this episode, you'll discover: How to tackle the fear of cost and gain control over your spending; Strategies to navigate the labyrinth of AWS complexity and maximize productivity; Techniques to secure your AWS environment and shield against potential breaches;  The trade-offs of vendor lock-in and how to mitigate risks; Whether AWS is the right path for your business and when to embrace it.\n\nfourTheorem is the company that makes AWS Bites possible. If you are looking for a partner to accompany you on your cloud journey, check them out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources:\n\nOur previous episode on AWS Governance and Landing Zone with Control Tower, Org Formation, and Terraform\n\n",
      "transcript": "Eoin: As cloud consultants, we often talk with clients\nthat are considering moving their projects to AWS,\nbut they have significant fears holding them back.\nToday, we want to walk you through the most common concerns\nwe hear from customers when it comes to AWS.\nAnd we want to disclose some of the answers\nthat we have to reassure our clients.\nWe'll discuss hot topics such as cost, complexity,\nsecurity, vendor lock-in, and knowledge gaps.\n\n\nIf you are in the cloud consulting business,\nchances are you are having similar conversations with your clients,\nor maybe you're trying to convince your boss\nto move certain projects to AWS\nand deal with some internal resistance.\nThis episode might give you some ideas\non how to reassure your clients or your boss\nthat AWS can provide value.\nMore importantly, we will also share some tips\non how to prepare the organization for successful migration,\nbecause often these migrations require significant changes\nto the organization itself.\nMy name is Eoin, and together with Luciano,\nwe are here for another episode of the AWS Bites podcast.\nfourTheorem\nfourTheorem is the company that makes AWS Bites possible.\nIf you are looking for a partner to accompany you\non your cloud journey, check them out at fourtheorem.com.\n\n\nLuciano: As usual, Eoin, I think it's a good idea to start by laying down some potential scenarios\nthat we can reference just to understand,\nlike, what are we talking about? What is the context here?\nAnd it might be something on the line\nthat you have an application running on-premise\nand you want to migrate it to the cloud\nbecause maybe you need more scalability,\nbecause maybe you need new capabilities,\nor maybe you have some application that is running\non some old-school web hosting provider\nor a virtual private server somewhere.\n\n\nI'm sure that if you have done any PHP in the past,\nyou probably have been in that situation,\nand at some point you realize,\nthis is not going to be enough if I want to grow a business.\nSo, again, some reasons why you might want to migrate to the cloud,\nscalability is probably the first one, but it's not the only one,\nbecause another interesting aspect that the cloud gives you\nis speed of innovation.\nFor instance, you might want to leverage cloud-first services\nlike Lambda or DynamoDB\nor maybe all the machine learning type of services\nthat you can just use with an API.\nSo that kind of a capability is something that you gain\nwhen you start to switch to the cloud,\nand therefore it might be a good motivation\nfor your organization to consider that switch.\nSo with that being said as a context setting,\nwhat is the main concern that we hear from customers?\n\n\nEoin: The first one I think is always cost.\nAnd the common concern there is,\nis AWS going to be more expensive than my current setup\nand then cost predictability?\nBecause previously, while you might have needed\na large capital expenditure investment at the start of a project\nto purchase hardware or some fixed price annual cost,\nwith on-demand compute, you have the issue of predictability.\nSo you have a fixed contract and you want to know,\nhow are you going to pay,\nhow much are you going to pay every year with this new setup in AWS?\n\n\nAnd then the other thing is hidden cost.\nHow much is it going to cost me to train staff?\nWill I need to buy additional support?\nWill I need third-party services as well to be successful?\nSo this is, I think, the most common concern\nand probably the most challenging one as well.\nAnd it's impossible to say in absolute terms\nwhether moving to AWS will result in a cheaper\nor more expensive deployment,\nat least if you're just looking at raw IT cost.\n\n\nNow, there's a mindset change required here as well.\nEverything in the cloud is dynamic and consumption-based.\nYou're buying into a more flexible environment,\nbut you also have the trade-off of that flexible pricing\nfor better or worse.\nSo let's try and give some tips for fighting that fear.\nWith this flexibility, resources can be configured up and down.\nSo you pay for exactly what you need and not a penny more.\n\n\nAnd you can also back out of decisions\nand look at ways to reduce cost in a very reactive way\nas soon as you detect issues with your cost.\nFor certain resources, like auto-scaling groups,\nyou can set auto-scaling upper bounds to avoid unbounded scalability.\nYou can also set billing alarms and notifications\nso that you can be alerted\nif your predicted cost is going above your expectations.\nAnd we set that up and use it all the time, and it's very useful.\n\n\nAnd we very rarely have any kind of significant surprise there.\nOver time, once your usage patterns stabilize,\nit will get easier for you to predict cost.\nYou will need to be able to spot opportunities to invest, I suppose,\nin re-engineering activities\nto cut costs if needed.\nAnd then the other thing is, of course,\nthink in terms of total cost of ownership.\nAnd this is easier said than done, I think.\n\n\nBut when you use managed services, the idea of this\nis that you need less people to look after the infrastructure.\nSo even if your infrastructure bill ends up being higher,\nyou might still be able to save a lot of money somewhere else\nand even free up people to be able to work on new features and new products\nthat you wouldn't have been able to do if you were just focused\non building a lot of infrastructure\nthat AWS are now taking care of.\nSo with this mindset change, you might need to think\nabout restructuring the organization a little bit.\nYou might need more cloud engineers than Sys administrators,\nbut cloud engineers are generally more aligned with your business goals.\nThe idea is that you should be reshaping your teams and your skill sets\nto be focused more on business-aligned feature delivery\nthan just on keeping infrastructure running.\nSo Luciano, I think that's as much as we can say about cost.\nWhat else have we got when it comes to some of the top fears\nwith AWS adoption?\n\n\nLuciano: Yeah, the next one is one of my favorite ones, which is AWS is too complex.\nAnd sometimes it gets a bit funny.\nLike people will say, I'm not smart enough to use AWS,\nor I will need a PhD before I'm able to do anything with AWS,\nwhich is, of course, an exaggeration,\nbut it's funny to hear that concern in those terms.\nAnd more reasonable ways of phrasing that is like,\nhow can I be productive with such a complex environment?\n\n\nHow long is it going to take for me before I'm up and running?\nOr what if I don't really understand the model\nand eventually I'm going to end up getting something wrong\nand that mistake is going to affect the business negatively?\nAnd all of these are very reasonable concerns,\nand it's only fair to admit that AWS is somewhat complex,\nbut that complexity is what gives AWS all the powers\nand all the nice things that you can do with it.\n\n\nSo I think that the way of addressing this concern\nis trying to think what is a system that allows me\nto isolate all of the complexity and all the many things\nthat I see in front of me when I think about AWS,\nand only focus on the few things\nthat are really important for the organization.\nAnd basically you might ask yourself,\nokay, how do I structure my understanding of AWS\nso that I can simplify things down?\n\n\nAnd one way of doing that is you might think of AWS\nas an ecosystem of many, many services.\nThere are, I think, more than 200 services at this stage,\nbut in a way you can think about them as different companies,\ndifferent service providers,\nand you don't need to buy into all of them.\nOf course, some of them are more foundational.\nWe can call them this way and you cannot really avoid them.\nSo these are probably the ones that you should focus on first.\n\n\nLike make sure you really understand them\nbecause you will find those pretty much\nwith everything you do in AWS.\nSo I think it's an important first step,\nbut once you have those covered,\nand I'm talking, for instance, about services like IAM,\nlike for permissions,\nonce you have those covered,\nyou can pretty much roll with them\nand start to learn more specific services\nthat you need to build specific projects in AWS.\n\n\nThe other thing that you might think about is that these services,\nyou can group them in different areas,\nlike different types of services.\nLike you might have services for compute,\nyou might have services for machine learning,\nand you probably don't need to think about all of them.\nI think only a subset of all the services\nwould be applicable to your business.\nSo that reduces a lot the complexity.\n\n\nLike you don't need to be worried about\nall the machine learning services\nif you're not planning to use this particular set of features.\nOf course, if new use cases will emerge over time,\nyou can always learn new stuff as you go.\nSo I think that the three steps are\ntry to think about all the AWS services as foundational ones,\nspend some time and put some effort into learning those,\nthen identify the ones that are really important for you\nand start to learn them.\n\n\nIgnore all the other ones pretty much,\nunless at some point you realize you have a use case for a new service,\nand then you can start to investigate and learn it over time.\nAnother point is that AWS is quite consistent\nin terms of experience that you get as a developer or as a cloud engineer.\nSo when you start to be familiar with the concepts that AWS gives you\nwith some of the APIs, I think it gets easier and easier over time\nbecause you will spot some of the same patterns\nacross different services and different features.\n\n\nSo just to reiterate some points in terms of the strategy,\nfoundational services first,\nthen you might want to look at best practices\nand how to bootstrap various accounts.\nlanding zone, for instance, we have a dedicated episode on that.\nWe will provide a link in the show notes.\nYou might want to hire a cloud consulting partner like fourTheorem\nbecause that might accelerate the onboarding process\nand reduce the risk that you might be doing something wrong\nin the very early stages.\n\n\nSo you are basically set up and ready to go very early on,\nand there are experts helping you to make sure that all the path\nis laid down for you rather than you having to figure it out\nby trial and error and maybe by doing pretty dangerous mistakes\nthat might affect your decision making\nand thinking that the cloud was a good idea in the first place.\nAnd in the company, I think you need to promote a new mindset\nof this is a learning journey,\nso everyone needs to start thinking,\nokay, we need to learn together, we need to have study material,\nmaybe we need to look into certification,\nwhatever we learn, we need to share it\nand make it available in the company as a team.\n\n\nAnd that's something that you can encourage through pairing,\ntraining, code reviews and so on,\nespecially if you already have a core team of people\nthat starts to be more skilled AWS professionals,\nthose can be the ones that can help the rest of the organization\nto basically get better at using AWS and using it correctly.\nIn terms of services adoption, it might be a good idea\nif you are building a new product\nand you don't really have technical constraints\nto look into more serverless services\nbecause with those ones, you generally have more managed solutions,\nwhich basically means that AWS will take care of a lot of concerns\nthat you don't really have to be worried about\nand therefore you can focus a lot more on the business value,\nlike implementing the business flows\nthat actually are enabling your business to scale and grow.\n\n\nAnd then the usual apply, iterate, keep growing.\nAnother thing that comes up very often in this kind of conversation\nis building a cloud center of excellence,\nso trying to build a core team of people,\na group of people in your organization that are the cloud experts\nand they are the go-to to basically try to address any issue or concern\nthat you might have with the cloud.\nAnd also that might go around and review other people's code\nand make sure everyone will get to the point of expertise\nwhere they can be very proficient with AWS.\n\n\nI think the goal is that eventually you want to become a cloud-first company\nwhen you start to adopt the cloud.\nSo you have to think about the cloud as a new capability\nthat also requires a little bit of change in your organization,\nbut it will become a very important business tool.\nSo it will enable a lot of things that you are not able to do now,\nbut you will be able to do later\nand therefore it might give you new business opportunities.\nSo it's a bit of an investment. You have to see it as an investment.\nYou have to understand how much you need to invest\nand make a plan for that.\nBut eventually you are going to get a return from that investment\nif you do all of this stuff correctly.\nAnother one that always comes up is vendor lock-in then.\n\n\nEoin: So we hear from people, I don't want to use AWS.\nI don't trust them. What happens if something goes wrong?\nIf the pricing changes, it'll be really hard to migrate away.\nOr people say, we can use AWS,\nbut we're just going to stick to virtual machines,\nmaybe some Kubernetes so that we can easily move away if needed.\nNow, it's true that if you're using a lot of AWS services and APIs,\nyou're buying into a very specific technology ecosystem.\n\n\nAnd as such, it might be expensive to migrate to something else.\nBut I don't think there's a way of adopting the cloud\nwhere you don't have some cost when it comes to migration.\nAnd if you're going to try and do it in a vendor neutral approach,\nor you're just going to say, okay,\nlet's use kind of lowest common denominator infrastructure\nlike virtual machines or Kubernetes,\nthen you're going to put the cost into doing more engineering upfront\nor into doing something in a cloud agnostic way.\n\n\nI think no matter what option you take,\nyou're going to lock into something.\nAnd if you think about it from the point of view\nof the AWS shared responsibility model, the idea is you pay them,\nthey'll take care of a certain number of layers of the stack\nat the bottom, like the core infrastructure,\nthe security of the cloud. And then you just look at the services above that.\nYou understand your business logic, your deployments,\nyour security of your workload.\n\n\nAnd if you're not taking advantage of those services,\nyou're essentially taking that responsibility back\nand you're giving yourself more of a burden.\nSo there's a trade off there,\njust like with any technology decision, you can get fast time to market,\nthe ability to build highly scalable systems,\nthe ability to swap in and out components of your architecture,\nto evolve your architecture really quickly,\nand other benefits in exchange for just buying in\nand going all in on AWS with their specific tools and APIs.\n\n\nWith abstract approaches where you're saying,\nokay, let's build our own abstraction to hide AWS underneath\nand make it more portable,\nyou're just going to spend more time engineering,\nbuilding these abstractions and distract yourself\nfrom just building business requirements.\nSo you might be missing out then on the more innovative capabilities\nthat might be convenient for your business.\nSo in terms of fighting this fear,\njust accept that any technology choice comes with some degree\nof vendor lock-in, cloud is no different.\nAnd if there are areas of your business that you need to protect,\nyou can always use well-known patterns,\nlike hexagonal architecture, for example,\nwhich is quite commonly used in serverless deployment\nto reduce the coupling between your core business logic\nand the physical resources you might be working with,\nlike databases, API Gateway, AppSync, or whatever it is.\nI think we've covered probably the top three ones\nhave we got other concerns and fears from people?\n\n\nLuciano: Yeah, I think we have at least a bonus one that we want to cover today, and that one is security.\nAnd the question we often get or more of a comment sometimes\nis the cloud is too complex and there are too many security risks.\nAnd I'm worried that, of course, that's going to compromise the business\nbecause ultimately that's what you want to do.\nYou want to protect your business.\nSo you're trying to figure it out with all these trade-offs\nthat I have to take, is it more of a risk or an opportunity?\n\n\nAnd security is always scary.\nSecurity is always scary because there is a lot of unknowns\nand you are more afraid because you don't really have a...\nYou don't always have a way to fight all these unknown unknowns\nand therefore the worry comes up.\nSo I think in the cloud, it's important to understand\nthat it can be a little bit scarier than the alternatives, in a sense,\nbecause if you have, for instance, if you are deploying something on,\nI don't know, an FTP, right, the old style providers\nwhere you just upload files on an FTP and you have a website running,\nwhat you are exposed to is very limited.\n\n\nMaybe they can compromise your website.\nThey can alter the homepage.\nThey can try to steal some user data, all nasty things,\nbut that's limited to basically what you have in that particular account.\nWith AWS, it gets scarier because if you think,\nwhat happens if my account is compromised,\nyou may start to think, okay, anyone can start to spin up resources\nand those resources are tied to my wallet\nand I will have to pay even for that attack.\n\n\nSo it's not just a damage in terms of reputation,\nbut it can be a pretty significant damage in terms of cost as well.\nAnd it might affect the company really negatively.\nSo it's a very reasonable fear, I guess, to have\nwhen it comes to the cloud to be worried about security.\nSo with that being said, what can we do?\nHow can we think about this in a more positive way?\nAnd I think whether you run an application in the cloud,\nin your own data center, in your own virtual private server\nor Austin provider, there is still a security concern anyway.\n\n\nSo the way you build applications, it doesn't change.\nYou still need to implement all sorts of application security best practices\nto make sure that your application is not vulnerable.\nSo that is not really, that doesn't change, I guess,\nin the cloud necessarily in a negative way.\nAnd actually, there are some new interesting things\nthat you might start to consider that are more in favor of AWS\nbecause AWS actually gives you a lot of tools\nto make your security posture actually a little bit better\ncompared to when you run your own infrastructure\nor when you run using other kinds of providers.\n\n\nAnd one thing is that if you use serverless applications,\nwhich is very common when you go to the cloud\nand you're building something new, it's kind of a default.\nI think serverless applications are a little bit unique in terms of security\nbecause you can be extremely, extremely granular.\nLike every single Lambda function, for example, can have a policy.\nSo if you're building, for instance, a crude API,\nyou might have different Lambdas to manage different operations\nand therefore you can say this Lambda can only read data.\n\n\nThis Lambda can only delete data.\nAnd you can also be very specific on the kind of data,\nmaybe the table, maybe the record,\nmaybe attach additional conditions to the policy.\nAnd that means that if that specific Lambda gets compromised,\nthe surface that is exposed is extremely limited.\nNow, if you think about a monolith,\nthat's very complicated to achieve at that granular level\nbecause generally if a monolith gets compromised,\nyou have the entire surface of the monolith to be compromised.\n\n\nSo this is already an advantage in terms of,\nbecause of the cloud, you adopt serverless,\nyou have a lot more control in how granular your security posture can be\nfor different parts of your application.\nThe other thing is that AWS itself is very aware about security\nand there is an entire suite of services that you can use\nthat are in this kind of security realm.\nAnd just to mention some of the capabilities\nthat you can get out of the box just by adopting certain services,\nyou have detection and response.\n\n\nFor instance, by using tools like Amazon Inspector, Security Hub,\nGuardDuty, so all these tools will help you to monitor your posture\nover time and spot if there is something wrong\nand help you to fix it straight away.\nThen there are more kind of application-based security tools.\nFor instance, if you're building a website,\nyou probably want to use Web Application Firewall or Shield,\nwhich are two services that you can just put in front of your application\nand they will make sure that they stop things like common attacks,\nlike SQL injections or known IP addresses\nthat are known to be bad, I guess, bad actors in the network.\n\n\nOr with Shield, you can do things like getting some kind of degree\nof protection against DDoS attacks.\nOther things that you can do, there are services that allows you\nto manage sensitive data in a kind of proper way.\nFor instance, you can think about Secret Manager, AppConfig.\nYou have lots of options when it comes to encrypting sensitive data.\nAnd if you think that you have to do all this stuff anyway\nin your own solution without using the cloud,\nit is a lot of work. And even if you can use open source tools,\nyou need to make sure you install them, configure them correctly,\nuse them correctly. So I think with the cloud,\nonce you start to have a security-first mindset,\nyou have just much more tools that can help you out\nto make sure you do things correctly.\n\n\nSo just to recap this point, I think security is still hard\nand something you should build more as a process\nin everything you do as a business.\nIt's not just like a switch that you say, I am secure, I'm not secure.\nIt's something you always need to think about\nwith everything you build and you always need to observe\nand you always need to monitor and try to improve.\nSo one thing you will do is try to establish a security baseline,\nfigure out ways to always validate your security posture.\n\n\nYou can use automation, you can use all the tools from AWS,\nand often reassess and adopt new best practices.\nOf course, again, the usual advice stands.\nIf you feel that you don't know exactly what you need to do,\nask out for help.\nThere are lots of companies that can help you out\nto make sure that your security posture is correct\nand that can help you to establish all these best practices\nso that at some point you can be self-sufficient and confident\nthat you are managing all the AWS security correctly.\nWhat else do we have?\nIs there any other comment that we want to make in terms of AWS spheres?\nYeah, I think that was a really good one on security.\n\n\nEoin: I think ultimately the capability to do fine-grained security in AWS\nis like nothing I've ever seen in any other option.\nSo I think it's good to allay those fears.\nBut maybe not so much a fear, but a less common question is,\ndo I really need to move to AWS in the first place,\nforgetting about the fear factor?\nAnd the answer isn't always yes, right?\nGoing to the cloud, it's a big decision.\nIt has substantial consequences for organization.\n\n\nIt's non-trivial.\nIt can have huge benefits, but it shouldn't be taken lightly.\nSo you can always understand the trade-offs\nand evaluate things carefully.\nNot all businesses need to be on AWS or on the cloud at all\nto be able to succeed.\nIf you've got something that's making money, running well,\ndoesn't require a lot of maintenance, speed of innovation,\nit doesn't necessarily need to be in the cloud.\n\n\nYou don't necessarily need the flexibility or the scalability.\nSo if you decide not to go with AWS, that's completely fine.\nWe're not trying to convince people to go to AWS\nregardless of the context.\nYou're not necessarily missing out as long as your decision\nisn't just motivated by fear or fear of the unknown.\nSo I guess maybe just as a recap, I think when you're moving to AWS,\nit is common to be worried about cost, complexity,\nvendor locking and security.\n\n\nThere are real concerns. There's ground for them.\nBut most often it's just a strong fear of change and fear of the unknown.\nSo it's important to recognize these fears\nand the risks that they highlight.\nBut if you're convinced that the migration might be worth it,\nit's important to approach it as a new step in your evolution.\nIt's not just a technical project.\nIt's quite a radical change in the organization.\n\n\nIt's something that needs to be supported long term.\nAnd I think we've both seen in our work\nthat when it is given proper organizational backing and support,\nthen the results can be really, really good.\nAs a long-term investment, it's also important to evaluate\nthe magnitude of the investment and the trade-offs attached to it.\nAsk yourself, is it really worth it?\nAfter all, if your business is running smoothly\nwith your current infrastructure\nand you don't see many problems in growing the business further,\nor you don't need to grow that business line further,\nyou might not need to go to AWS at all.\n\n\nBut if you do decide to go, just make sure you understand\nwhat are the unknowns, research your best practices,\nask for help where needed.\nI mean, the way we tend to work with our clients\nis just kind of sitting beside them, co-developing, architecting,\nbuilding with them, changing things, seeing what happens.\nAnd I think that gives people a lot more reassurance that, you know,\nwith AWS, while there are a million different permutations\nof building any architecture,\none of the daunting things can be, okay, which services do I choose?\nHow do I stick them together?\nSo having a little bit of support will help you there\nbecause you can get some guidance on the path forward\nand make decisions quickly.\nSo it's going to be a journey.\nJust prepare yourself and then start on your path.\nAnd don't forget to have some fun.\nYou'll be learning lots of new things\nand expose yourself to a lot of new opportunities.\nAnd with that, as usual, let us know\nif you have any different perspectives or experience.\nWe value your opinions and we can certainly learn\nsomething from all of you out there as well.\nSo do let us know.\nThat's all. Thank you for staying with us.\nAnd we'll see you in the next one.\n"
    },
    {
      "title": "100. Exploring Ampt, a new way to build cloud apps on AWS",
      "url": "https://awsbites.com/100-exploring-ampt-a-new-way-to-build-cloud-apps-on-aws/",
      "publish_date": "2023-10-20T00:00:00.000Z",
      "abstract": "Greetings, my fellow innovators, and welcome to this illuminating episode of AWS Bites! In this edition, we embark on a journey into the realms of Ampt, a groundbreaking solution that simplifies the intricate landscape of AWS application development, allowing you to direct your focus toward the very essence of your applications, unhindered by the burdens of infrastructure management.\nAs your guides through this remarkable odyssey, hosts Luciano and Eoin delve into the ingenious facets of Ampt. We unveil its &quot;code over infrastructure&quot; paradigm, which resonates with the principles of efficiency and simplicity. Furthermore, we explore Ampt's intelligent compute options, designed to adapt to the dynamic needs of your applications, and its streamlined deployment process, which paves the way for a more seamless journey into the world of cloud development.\nJoin us on this voyage as we unravel how Ampt simplifies the intricate art of crafting full-stack applications. Notably, it offers individual sandboxes for each developer, eliminating the cacophony of distractions caused by noisy neighbors. Together, we will also uncover the straightforward path to beginning your journey with Ampt, highlighting its exciting features that promise to reshape the landscape of cloud development.\nThis episode is a testament to innovation and the pursuit of progress. So, heed the call and stay at the forefront of AWS development by immersing yourself in this episode today!\n\nfourTheorem is the company that makes AWS Bites possible. If you are looking for a partner to accompany you on your cloud journey, check them out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources:\n\nAmpt\nAmpt blog (with many case studies)\n\n",
      "transcript": "Luciano: AWS is all about removing undifferentiated heavy lifting.\nAs it evolves, we got more services that are meant to take away complexity and maintenance.\nNow that we have become used to building with serverless and AWS,\nwe are beginning to take a step back and still notice that there is still plenty of complexity left.\nWhile we wait for AWS to evolve further and handle more of this for us,\nother companies are innovating and try and get there first.\n\n\nToday, we're going to take a peek at Ampt,\na recently launched solution that builds on AWS but aims to take the pain away\nand deliver the utopia of only ever focusing on the business value.\nMy name is Luciano and today I'm joined by Eoin and this is AWS Bites podcast.\nAnd if you're wondering why I'm wearing a shirt, this is to celebrate our episode number 100 💯 🎉!\nSo let's get to it.\nfourTheorem is the company that makes AWS Bites possible.\nIf you're looking for a partner to accompany on your crowd journey, check them out at fourtheorem.com.\nOkay, Eoin, you spend a little bit of time playing with Ampt\nand I think you have a fairly good idea at this point of what it is,\nwhat is the value proposition.\nSo maybe we can start by describing high level,\nwhat kind of problem does it try to solve and how it works.\n\n\nEoin: I think we've discussed in previous episodes some of the efforts in this space and Ampt was one that sounded really interesting and just recently launched out of beta.\nAnd I was quite curious to give it a try and I'm pretty impressed so far.\nWe'll go into all the details in a second.\nSo Ampt, you can find it at getampt.com.\nThe idea is that it's an approach to building applications and according to them,\nit lets developers rapidly build, deploy and scale JavaScript apps in the cloud\nwithout complicated configs or managing infrastructure.\n\n\nAnd it's the without part that's probably the most appealing, right?\nFor people who've been building serverless applications,\nyou can understand that sometimes it can get pretty complex,\nespecially when you have to manage configurations of resources,\nevents, functions, permissions.\nSo Ampt has been, I think, kind of spun out of the serverless framework\nand it's got some big names behind it.\nJeremy Daly is the CEO.\n\n\nThey've got a great team, it seems like,\nand Adrian Cockcroft is one of the advisors, him of Netflix and AWS fame.\nSo I think they've got a pretty good team.\nIt gives a bit of confidence, I guess.\nA few things to note then about the actual product.\nIt's JavaScript focused, so that includes TypeScript.\nAnd basically right now that means no JS 18.\nAnd it's really focused on building API backends and frontends\nthen with all of the modern frontend frameworks.\n\n\nSo it's about writing code, not infrastructure.\nSo what does that mean?\nWell, the promise of Ampt seems to be that you deploy your code.\nIt auto-optimizes and creates the infrastructure for you.\nI've seen this term, self-provisioning infrastructure.\nSo you don't have to worry about creating YAML\nand configuring loads of resources.\nYou're just really writing the code for the logic\nand the code that glues pieces of logic and your data together.\n\n\nAnd that's really it.\nSo it's really boiling everything down to the essential fundamentals\nand getting rid of all that mess\nthat we typically have to wrangle with on a daily basis.\nAnd the other interesting thing about it\nis that it provides isolation for every environment\nand that includes developer sound boxes.\nAnd that's just something that you get out of the box\nfrom day one when you sign up.\nAnd that's something that we've probably seen in AWS environments.\nIt can take you days, weeks or months or sometimes\nto figure out how to do those things correctly.\nSo that's already a pretty big win.\n\n\nLuciano: Absolutely. Sounds really interesting.\nBut let's try maybe to understand a little bit better.\nWhat kind of applications can you really build?\nYou mentioned APIs and frontends, but can you use any framework\nor are you only forced to use specific things\nthat Ampt gives you?\n\n\nEoin: Ampt gives you a JavaScript SDK and a CLI to get started with.\nAnd then a couple of frameworks that you can use\nif you're already using on the frontend,\nNext.js, Nuxt, Remix, Angular, Nest, React, SvelteKit and Vue.\nAnd also Eleventy if you're using Eleventy for static sites.\nOn the backend then, if you've got an API already\nthat uses one of the existing Node.js web frameworks\nlike Fastify, Restify, Koa or Express,\nthen you can automatically kind of plug those in to Ampt\nand get up and running.\n\n\nI guess the only major difference there is that\nyou're not doing a create server or a listen.\nYou're not actually opening a port and listening.\nYou basically just have one line of code\nthat's going to wire it into the Ampt ecosystem\nand that will connect all of Ampt's magic into the roots\nthat you write in your code.\nSo you might then wonder how does this work?\nLike if you don't have to create\nany infrastructure in advance.\n\n\nSo you only write one type of code.\nAnd this is really just your logic\nand setting up the SDKs.\nAll of the infrastructure is kind of generated from that\nor generated in advance.\nThe term infrastructure from code\nas opposed to infrastructure as code\nis being used to describe this.\nSo you don't need any CDK type code.\nYou don't need any YAML.\nIt doesn't really compare to things like CDK\nor Pulumi or even Wing Lang.\n\n\nThat other new project that kind of gives you a new language\nfor generating your infrastructure.\nYou don't really think about the infrastructure\nthat's being generated too much here with Ampt.\nSo if you take the example of a database,\nright now Ampt supports a key value data store.\nSo, and it's there and ready for you to use right away.\nSo in your code, you can start doing sets and gets\nand remove operations.\n\n\nYou don't have to create any tables in advance.\nAnd the same goes for objects, storage and events.\nEverything's just there out of the box.\nSo if you take an example of creating an API,\nyou just write the API implementation\nin the framework of choice.\nAnd with one line of code, just wire it in.\nAnd then Ampt will automatically handle the API infrastructure\nand routing.\nYou don't have to think about load balancers\nor API gateways, any of that stuff.\nYou're just writing routes like you would back\nin the old days in more, let's say monolithic applications,\nmaybe.\n\n\nLuciano: That's a good point that makes me think about the next question.\nLike where does the code run then, right?\nThere must be some kind of wrapping thing\nthat happens when you want to deploy to Ampt.\nAnd in order to take all of your code\nand package it in such a way that can be effectively\nexecuted in a scalable way on AWS infrastructure, right?\n\n\nEoin: I'd love to know more about exactly how some of this magic works under the hood.\nWhat we do know is that it has this concept of smart compute.\nAnd I think this is one of the most interesting and exciting\nparts of Ampt because it allows your code to,\nby default, run in Lambda.\nAnd I think from day one, that's where your code will run.\nBut if they detect that your traffic is consistently high,\nthey can move that into AppRunner.\n\n\nAnd you don't have to do anything.\nOr if your tasks start to run for longer,\nI think it's longer than five minutes at the moment,\nthey'll start running your code in Fargate.\nI think you have to be on a certain pricing\ntier for that to happen.\nBut the idea there is really nice, right?\nThat you don't have to think about monitoring\nyour function, optimizing memory, all of this stuff,\ntimeouts, scalability, quotas.\n\n\nThe idea that they can take your code\nand move it around behind the scenes,\nand you don't notice anything, but it's kind of cost\nand performance optimized for you,\nthat's something that's really exciting.\nAnd it's something I can imagine them doing a hell of a lot\nmore with when you think about maybe even automatically\noptimizing your data storage as well.\nYou asked the question, where does it run?\n\n\nAnd we're talking about Lambda and Fargate and AppRunner.\nBut that means it's running on AWS.\nSo all of Ampt's infrastructure is built on top of AWS.\nAnd every developer gets their own sandbox,\nwhich is really cool out of the box.\nWhen you deploy as a developer on the team,\nyou automatically have an isolated environment\nthat you can share with other team members.\nYou can kind of share a snapshot of it.\n\n\nBut it's automatically synced as well.\nSo as long as you're running the Ampt front end,\nit's automatically updating your infrastructure and code.\nAnd I think it's doing a lot of smart stuff\nin the background there, because the deployments\nat times I've seen are pretty fast.\nThe feedback time is really good.\nYou can also then deploy your own Ampt server.\nYou can also then deploy to any stage,\njust by running deploy space stage.\n\n\nAnd you can create different stages like QA, pre-production,\nand production.\nThis isolated environment, it's a big USP for Ampt,\nbecause they're focused on kind of eliminating\nany kind of resource contention problems, noisy neighbors.\nAnd from what I understand of the launch party announcement,\nit seems that each environment runs in its own AWS account\nunder the hood.\nSo you don't have these noisy neighbor problems\nwith quotas and rate limits and everything like that.\nAnd I'd love to know how that's done.\nBut assuming that they've handled all of that,\nit's really nice from a user perspective,\nbecause you don't have to worry about setting up\nthose accounts and managing environments.\nIt just happens automatically for you.\n\n\nLuciano: Yeah, I can definitely see lots of edge cases in trying to think how they might come and implement\nall of that.\nBut I mean, that doesn't mean that it's not possible.\nI'm sure that considering that all the smart people that\nwork at this project, they have figured out\na bunch of interesting solutions,\nwould be nice at some point to discover some of them.\nBut that's maybe a topic for another episode.\nOK, let's talk how do you get started.\nAssuming we are kind of making you excited as well\nand you want to know how to get started, what is the first step?\n\n\nEoin: I'd suggest doing what I did, which\nis just start with the very simple instructions\non the website.\nYou just npm install the CLI and then run the frontend\namped command.\nAnd from that, it'll ask you to pick from a number of starter\ntemplates, like an API backend with Express or Fastify,\nfor example, or a frontend application built on, say,\nAstro.\nAnd then it automatically gets generated for you and deployed.\n\n\nSo you immediately then get a link\nto a dashboard where they've got a really nice UI where you can\nmonitor the applications.\nAnd you can see the metrics and logs right away.\nSo I think the usability and the aesthetics around amped,\nI've been pretty happy with.\nI think they look much nicer, feel much like a better\ndeveloper experience than we're used to.\nAnd there's obviously plenty to be done there,\nI think, in terms of making the logs\naccessible in different ways.\nBut I like that it's pretty simple.\nAnd automatically with one command,\nyou're up and running with that dashboard.\nAnd then you get a generated link for your API endpoint\nor a static site, if that's what you've deployed.\nOK.\n\n\nLuciano: In terms of features, you mentioned already HTTP APIs.\nYou also mentioned key-value store.\nIs there anything else worth mentioning?\n\n\nEoin: On the API side, I guess it's notable\nthat you've got support for API keys, also web sockets.\nAnd even though I haven't tried it,\nI've seen that HTTP response streaming is supported.\nAnd I'm interested in how that works across the different\ncompute platforms.\nBut I guess that's another curiosity.\nWhen it comes to the data side, what\nI've seen from looking at it and observed\nis that it's basically a much nicer API for DynamoDB.\n\n\nThat's what it feels like.\nBecause with DynamoDB, the API is a little bit strange,\ntakes a bit of getting used to.\nThis is like a more developer-friendly API.\nSo you don't have to create any tables\nor worry about creating additional indexes.\nYou basically get set, remove, and add operations.\nSo you can do everything pretty much\nthat you could with DynamoDB through those operations.\nAnd then you have nice things like they automatically\ngenerate metadata, like created timestamps and modified\ntimestamps for your objects.\n\n\nAnd then you have namespaces to separate\nwhat would be your DynamoDB hash key and your range keys.\nSo it's just a little bit more easy to get used to.\nAnd you don't have to worry about the different types\nof operations.\nYou can just use wildcards, for example,\nfor searching starts with.\nWhen you need to have secondary keys,\nyou just provide attributes that are called labels.\nAnd these seem to automatically generate\nthese secondary keys for you under those.\n\n\nSo that's pretty nice.\nSo it's really just key value store.\nIf you need to use anything else,\nlike relational database, they're\nrecommending that you use partners\nlike PlanetScale, MongoDB, Memento,\nand you just use their SDK.\nThey also have a nice support for parameters,\nwhich can be automatically injected\ninto your application as environment variables.\nYou can have organization-wide parameters or application\nspecific parameters.\n\n\nAnd then you have object storage,\nwhich is just like S3, but also another simple S3 abstraction.\nYou don't have to worry about creating and managing buckets.\nAnd then you have events.\nAnd events and tasks are quite a nice feature as well.\nSo you can have your cron events,\nlike run events on a schedule every hour.\nBut you could also get events based on storage and data.\nSo if you have your data stored in the data store,\nyou can just say data.on.\n\n\nAnd then you can put a filter which\nsays if an object is written with a certain key,\nthen call my function.\nAnd this is where the infrastructure just\nstarts to disappear, because you're just basically writing\nwhat looks like a Node.js event emitter or handler\nor something.\nIt's just an on event, then call this function.\nAnd after handling all of the wiring and the event\nevery bruise or whatever other magic\nis happening under the hood.\nI have seen that they create a queue in this account.\nI have seen that just from snooping a little bit\ninto the environment variables in the code and stuff.\nSo I don't know if that's used for that\nor if that's part of a future feature.\n\n\nLuciano: That's pretty interesting.\nI am a bit curious to know how they deal with things like,\nI don't know, if you're passing a callback to this event\ninterface, how do they actually serialize the callback\nin a way that it is, I guess, responsive to events\nin a distributed way.\nIt's not just running in one Node.js process.\nBut yeah, I don't think we have that information.\nWe can just speculate.\nSo again, maybe in future episodes,\nwe might be able to figure out this magic\nand give you more details.\n\n\nEoin: Yeah.\nMaybe we can get talking to a member of the Ampt team\nat some point and find out all of the great details.\n\n\nLuciano: Absolutely.\nThat would be fun.\nBut meanwhile, should we talk about pricing?\nBecause is there going to be a huge pricing,\nis there still reasonable?\nBecause I guess that's where the trade-offs are.\nYou get a much nicer experience, simpler, get started,\ncomplete your project quickly.\nBut if it's going to be too expensive,\nis it going to be worth it?\n\n\nEoin: Right now, it looks not too bad.\nBut I guess it depends on your usage.\nAnd there's three tiers.\nNone of them are ridiculously expensive.\nBut the pricing is essentially per team member per month.\nSo you have a preview tier, and that will give you\nthree apps, 10 environments, and 500 invocations per hour.\nAnd then you have a $7 tier and a $29 tier.\nAnd it gets more capable.\nYou can have more team members, more apps, long-running tasks,\nall of that stuff.\nI don't know exactly how this works yet.\nLike, if you've got pricing that kind of dictates\nthe number of invocations per hour,\nbut what happens if you're doing like a massive number of DynamoDB\ncalls in that one invocation?\nAnd that causes a lot of spend on their side,\nhow that's managed.\nThey also say in the pricing details or in the FAQ,\nthey're working on implementing spending limits.\nAnd I think that would be a big differentiator.\nObviously, we've talked a lot about how that feature is\nmissing from AWS.\nIf they can pull that one off, that would be really cool.\n\n\nLuciano: Yeah, that actually makes sense.\nConsidering they provide you that kind of abstraction layer,\nthey can probably see what kind of operations\nthat it's trying to do.\nAnd then if they keep track of all of that\nand the cost it might have, I think\nthey might be able to implement some kind of smart blocker\nto limit the expenditure there.\nBut again, I'm just speculating just because, yeah, of course,\nmy mind is curious to figure out how would I build that kind\nof feature with these ideas.\nBut speaking of which, another thing that I have in mind\nis how easy it is to do CI/CD, which I think these days is\nsomething that everyone is doing, possibly\nthrough things like GitHub Actions.\nSo is that something mentioned in the documentation,\nsomething you tried yourself?\nI've tried a couple of different things.\n\n\nEoin: One is using their GitHub app, which\nis a bit like using Netlify or a lot of other services\nlike that, pretty seamless.\nYou just connect the GitHub app to Ampt,\nand then it can automatically deploy your environment\nto branches you set up.\nSo you can say, from this branch,\ndeploy to the staging environment,\nfrom another branch, deploy to the production environment.\nBut you also get feature branch deployments out of the box\nwith that, which is really cool.\n\n\nApart from that, they're basically\njust providing examples for you to write your own GitHub\nActions workflows.\nAnd then you're just running the Ampt to deploy command.\nSo it's pretty straightforward.\nIt seems like every application is just an isolated piece.\nThere's no kind of, I guess you would say,\nlike microservice approach, where you're deploying lots\nof things from one repo.\nI guess you could do monorepo deployments,\nbut with separate deployment pipelines for each single\nservice, if you'd like.\nSo it seems like if you've got an API backend and then\nanother Ampt to app for your front end,\nyou would just do two deployment pipelines or just deploy them\nseparately in one pipeline, if that makes sense.\n\n\nLuciano: It does make sense, even though I'm not sure\nhow they could manage the fact that you still\nhave one environment per user.\nI guess the question is, when you buy into CI/CD,\nis it going to be just one user, or you can still\nretain some control of which user space is going to be used?\n\n\nEoin: Oh, yeah.\nI think I have the answer to that one,\nbecause when you're deploying from your local environment,\nyou're just running the Ampt command,\nand that picks up your GitHub credentials or whatever way\nyou've logged in to Ampt, generates an environment name\nfor you from your name.\nBut then if you do deploy production,\nit's deploying to the production environment, which is shared\nby multiple developers.\nOK.\n\n\nLuciano: So you will use the CI/CD only to deploy\nto production, which is a shared account, effectively.\nThat seems like the pattern, yeah.\nYeah, actually makes a lot of sense.\nOK, you mentioned that you were able to see some\nof the underlying AWS stuff.\nWell, first of all, how did you do that?\nWas it really a feature, or did you\nfigure it out in some kind of an indirect way?\nAnd how much did you get to see?\n\n\nEoin: Yeah, it wasn't any advanced hacking or anything here.\nI mean, they make it clear that it's running\non AWS that's completely open.\nAnd then I just ran some code that\nprinted the environment variables of the process,\nso I could see that there was a table name and a bucket name,\nand a queue name in there.\nAnd then I tried a few AWS SDK actions.\nAnd I could see that they seem to have implemented\npretty good least privileged IAM policies, because I\nwasn't able to do much snooping.\nBut I could list the objects in the bucket,\nand I could do a scan on the DynamoDB table.\nSo it's not giving you the power to do anything\nthat you can't already do from the Ampt to SDK.\nBut like I said, I think they're going\nto allow you support to deploy to your own AWS\naccount in the future, and then you would have full visibility,\nI guess.\nAnd you'd also get the benefit of being\nable to monitor resources, connect into other AWS\napplications, and even achieve compliance,\nbecause it's not all abstracted away from you.\n\n\nLuciano: Yeah, that makes sense.\nI guess you also get the risk of you\nmight mess up in different ways, because then you\nmight have more control that they would want you to have,\nand either not change the table schema or stuff like that.\nSo let's maybe try to wrap up this episode by listing\nwhat do we believe are the trade-offs here,\nbecause of course, this is not going\nto be the ultimate silver bullet to develop in the cloud.\nI think it's just another way with a different set\nof trade-offs, so probably worth remarking what those are.\n\n\nEoin: The idea of Ampt, I don't know if it's\nfair to call it like a \"serverless Platform-as-a-service\"\noffering.\nI think if it's very clear what applications\nit's going to be geared for and optimizes for those,\nI think it looks really, really promising.\nOf course, if you compare it to the options in AWS,\nthere's a big trade-off, because AWS has hundreds of options\nand thousands of permutations for building applications\nlike this.\n\n\nIn Ampt, you have a very limited set of options,\nbut that can be a really good thing.\nLike in AWS, you have all the databases, all different types,\nand then you have services for data analytics and machine\nlearning and chat and video, sending email.\nWith Ampt, it's more like a pass where\nyou will write the code for your business logic\nand then integrate it into AWS services or other SaaS\nto achieve all of that.\n\n\nAnd it just depends on how much control you\nwant over that infrastructure.\nThere's other trade-offs there.\nLike it doesn't seem to have very fine-grained access\ncontrol right now.\nI don't know how that's going to work in the future.\nThe only way of protecting what you deploy\nis to have an API key right now.\nBut I'm sure that will change.\nThey do mention that they will be providing support\nfor private VPCs if you have network connectivity needs\nas well.\nI'm also interested if there is a potential trade-off here\nwith the ability to have this code moving from Lambda\nto Fargate to AppRunner.\nAre there some cases where you might write code\nthat may run well because it's running in one environment\nand then suddenly it starts running in another environment\nbut it won't execute correctly?\nI know that if you've got container code,\nsometimes it won't run in Lambda.\n\n\nLuciano: Yeah, I definitely had that question in mind as well.\nAnd I was wondering, on the timeout example\nthat you gave before, what happens?\nDo you get the timeout first and then they start to move\nyour compute to something else?\nSo you have a failure in the transition between one system\nto the other?\nOr there is some amazing smart system\nthat figures out before you get there and moves you in time\nand avoids any failure, which I don't\nknow if it's even possible.\n\n\nBut I think that would be really cool if it worked that way.\nSo definitely curious to find out more.\nAnd hopefully we will have the opportunity to do that.\nBut overall, I think it's fair to say\nthat if you're building web applications, API or full stack\napplication, Ampt seems like a very interesting contender,\ndefinitely innovating in this space.\nSo it can also make a lot of things easier,\nwhich is great to see.\n\n\nSo if you're building something that sticks\nto the fundamentals of event-driven applications\nor event-driven logic, API, compute data storage,\nprobably you can build a lot of complex things.\nSo you probably don't need all the potential of AWS\nand all the millions of permutation and configuration\noptions.\nSo in that sense, it might really\nbe a good platform to use for many different kinds\nof projects.\n\n\nSo we'll try to see if we will have in the future\nthe opportunity to try it in a more serious project, something\nmore big scale or production-ready,\njust to see what the experience will\nlook like when you try to do something a little bit more\nambitious.\nBut there are actually some customer case studies\nthat you can see on the Ampt website.\nSo if you're curious to see what are some real cases that\nexist today in the market, you can check out that page\nand get yourself more informed on the capabilities\nand the different solutions that people have built with this.\n\n\nNow, again, I think it's worth remarking\nthat it's really admirable to see somebody trying\nto innovate the cloud space, which it changes every day,\nbut we haven't seen this level of fundamentals\nwhich in a while.\nSo I think it's trying to push a different approach.\nAnd that's really something that is always really welcome,\nbecause I think from this kind of ideas\ncome the best innovations that we see in general in the tech\nindustry.\nSo let's keep an open eye for the future of Ampt\nor maybe other similar alternatives.\nIf there are other tools like this that you have been using\nand you are happy with, please share them with us.\nWe'd be looking forward to check your suggestions\nand maybe think about other episodes\nwhere we explore alternatives like Ampt.\nSo that's all for today, and we look forward\nto seeing you in the next episode.\n"
    },
    {
      "title": "101. Package and Distribute Lambda Functions for fun and profit",
      "url": "https://awsbites.com/101-package-and-distribute-lambda-functions-for-fun-and-profit/",
      "publish_date": "2023-10-27T00:00:00.000Z",
      "abstract": "Today we embark on a fascinating journey into the world of AWS Lambda functions and how to make them accessible to the public. In a recent use case, involving the creation of a public Lambda function for AWS users, we asked ourselves some interesting questions. How can you securely, cost-effectively, and conveniently publish AWS resources, especially Lambda functions, for others to use? And... can we possibly make some money out of this?\nJoin us as we explore various options and share our findings for making your AWS resources available to the world. We dive into the Serverless Application Repository (SAR), an AWS treasure trove for publishing resources. And SAR isn't the only way! We also discuss alternatives like CloudFormation templates, GitHub publishing, Terraform modules, and container images. We explore the pros and cons of these methods and debate the implications in terms of cost, security, and ease of use. Finally, we touch on the AWS Marketplace as a platform to monetize your AWS resources.\n\nAWS Bites is brought to you by fourTheorem, an Advanced AWS Partner. If you are moving to AWS or need a partner to help you go faster, check us out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources:\n\nOur previous episode about HTTP Lambdas (including an overview of Rest vs HTTP API Gateways)\nExample of publicly accessible Lambda code using CloudFormation templates on GitHub\nAWS SAR (Serverless Application Repository)\nThe Terraform Registry\nAWS Marketplace\n&quot;Selling an AMI and a CloudFormation template as an alternative to SaaS&quot; by Cloudonaut\n\n",
      "transcript": "Eoin: We recently had a use case for creating and publishing a public Lambda function\nso other AWS users could make use of it.\nThis gave us an interesting challenge.\nHow do you easily publish a function or indeed any other AWS resource\nin a way that is simple for users to adopt,\nbut also is secure, cost-friendly and maintainable?\nToday, we are going to go through all of the options\nand let you know what we recommend if this is something you want to do.\nMy name is Eoin, I'm joined by Luciano, and this is the AWS Bites podcast.\nAWS Bites is brought to you by fourTheorem, an advanced AWS partner.\nIf you are moving to AWS or need a partner to help you go faster,\ncheck us out at fourtheorem.com.\nLuciano, you raised this question recently.\nApart from your generosity, what was the rationale\nfor thinking about making a Lambda function public in the first place?\n\n\nLuciano: Let me try to describe the specific use case.\nI wanted to create an all-time high-level version of the Lambda function\nand I wanted to create a specific version of the Lambda function so I wanted to create an OIDC authenticator for API Gateway.\nAnd I think in order to really understand\nwhy that's something that could be interesting,\nwe need to remember something that we have been talking about previously\nin another episode and we'll have the link in the show notes\nthat API Gateway currently has effectively two competing implementations,\nREST and HTTP.\n\n\nAnd these two implementations have different feature sets.\nAnd the business use case I was working on\nwas basically requiring us to have a private API Gateway\nso something can be accessible only from a private VPC.\nAnd also because we are using OIDC tokens,\nthere needs to be some kind of authorizer\nthat could verify that the OIDC token received is valid,\nit's related to a specific user with the right permission\nbefore the request is forwarded to the backend.\n\n\nNow, the problem is that if you want to use OIDC authorizer\nwith the HTTP version of the API Gateway,\nthere is actually a very nice one already built in,\nso you just need to configure it, you don't have any extra cost,\nAWS will take care of everything for you.\nBut if you want to use a private API Gateway,\nthat's only available in the REST version of API Gateway.\nSo this kind of made us forced to choose the REST version,\nso we didn't have the option to use the built-in OIDC authorizer.\n\n\nSo at that point, the only option was we need to build our own custom authorizer,\nwhich thankfully is something you can do with Lambda,\nyou can create a Lambda that acts as an authorizer\nand give it to API Gateway,\nso API Gateway is going to call that Lambda to validate the token\nand then decide whether to forward the request or not,\ndepending on the result of that validation.\nNow, since we have the solution working,\nand this is apparently a gap that exists in API Gateway,\nif you're doing private API Gateways and you're forced to use REST,\nyou don't have an OIDC authorizer, we have the solution,\nand I think it could make sense to make it available to other people\nbecause I think it could be something relatively common in the market\nto have this particular use case.\n\n\nSo we were trying to think, okay, if we want to open source it,\nhow do we make it easy for the users to install it?\nIdeally, something that is like one-click install\nwith some minimal configuration,\nand the first thing that came to mind was SAR,\nServerless Application Repository,\nso probably our first option to consider.\nSAR is Serverless Application Repository,\nand it is basically something that allows you to create infrastructure as code,\nand you can use CloudFormation or some or something similar,\nand then you can publish it in this kind of publicly available repository.\n\n\nAnd you have to specify a special resource,\nwhich is AWS::ServerlessRepo::Application,\nwhich is the way that you can attach additional metadata to your project,\nthings like description and version,\nso when people are going to be browsing this catalog of different solutions,\nthey will see exactly what the specific solution is about.\nYou can also use parameters,\nso every time that you need something configurable,\nthat's one way to expose effectively the option to the users\nto provide their own configuration.\n\n\nFor example, in our use case,\nwe most likely need to make the token support different token providers.\nI don't know if you're using Azure ID,\nyou probably want to specify your own tenant,\nbut you might be using other OIDC providers.\nAnd maybe the user also wants to make sure\nthat the tokens are given for a specific audience,\nso they will need to provide that audience,\nor maybe they want to validate other token claims,\nso they need to have a way to specify all these different options,\nand parameters can allow us to make that flexible enough.\n\n\nYou can also use this approach to make private resources or private solutions,\nso it's something you might consider to use internally in your own company.\nIf you have certain things that you think might be useful for other teams\nor for other projects, you can just publish them as SAR applications,\nand then they will be available inside your AWS account.\nSo it's not something that you use only for public things,\nbut you can also consider it for private solutions\nthat you want to make reusable.\n\n\nNow, once you have published something on SAR,\nother people can install them using the CLI or using infrastructure as code.\nThe name is Serverless Application Repository,\nso you might think, okay, this is just for Lambdas, right?\nThis is just for serverless things.\nBut in reality, because you are effectively writing cloud formation,\nyou can use this approach to specify any cloud formation template,\nany resource, so you might also go beyond the scope of serverless application,\nif that's something that makes sense for you.\n\n\nSo the idea is more if you want to make bits and pieces\nof your infrastructure usable and configurable,\nthat's one way of doing it, regardless of whether it's serverless or not.\nI think I really like this approach because the user experience is pretty good.\nSo you basically can browse this catalog,\nyou can see all the different solutions.\nThere is some degree of documentation that describes every single solution,\nand then when you want to install it, it's pretty much almost like one-click approach.\n\n\nOr it's very seamless the way you do it.\nThere is one disadvantage, though.\nI have been using some of these publicly available solutions\nfrom other creators, and the problem is that,\nespecially, for instance, with things like Node.js,\nwhere the runtime evolves quite rapidly,\nthe owners of this solution don't always keep the runtime up to date.\nSo you might end up in a situation where you want to use a specific SARS solution,\nbut then the runtime is not available in AWS anymore.\nSo you are kind of forced either to try to open up your R\nand get the owner to update and republish,\nor just fork it and maintain it yourself.\nSo this might be one of the downsides that, of course,\nbecause somebody else is maintaining the solution,\nyou need to make sure that they're actually committing to keep it up to date\nand maintain it every time that there is a break and change like that.\nNow, of course, this is not the only way to share Lambda functions.\nWe have other options. So any ideas?\nOnce you have a CloudFormation template, there's actually a lot of options.\n\n\nEoin: Around how you can share it, you can just create it and publish it on GitHub\nor anywhere else on the web.\nThe main disadvantage really there is that you're giving your users a bit more work.\nJust putting it in a GitHub repo.\nVersioning support is something you will have to think about yourself.\nAnd another thing is that you will have to decide then\nhow to package the Lambda function code.\nSo if you publish it in a GitHub repository,\nyou can always just let the user package and deploy for themselves.\n\n\nFor example, you could provide a SAM template\nalong with the code assets.\nThis might be more work for the user because they'll need all the tools\nto deploy the function in whatever language you have chosen.\nOn the other hand, it does have the advantage that the code is easily visible\nand the user has the freedom to change things as needed, fork it\nand make their own versions of it.\nNow, when you're creating Lambda functions,\nyou have the options of specifying the code inline.\n\n\nYou can specify it as a base 64 encoded zip file\nor putting a zip file on S3.\nI think the zip file on S3 is probably the most common.\nThe inline options are easier to publish.\nThey limit because you don't have to worry about buckets,\nbut they limit how much code you can have\nbecause there's a maximum there of four megabytes for the zip.\nAnd then you have to think about how do you bundle dependencies\ninto that inline code.\n\n\nIf you do go for the bucket option,\nyou essentially have to make the bucket public\nif you want it to be shared and usable as is by the user.\nNow, you can restrict your bucket to read only on specific prefixes,\njust get object.\nAnd you can even use condition keys in the policy\nto get access to the AWS Lambda service itself.\nSo that's the only principle that will be able to read the code\nwhen it's deploying the Lambda function.\n\n\nAnd this is something that we have tried out\nand we have a GitHub repository with a code example\nshowing you exactly how to configure that bucket.\nIt is a public bucket, so it's not going to be for everyone's taste.\nIt's getting to a stage now where public buckets\nare becoming like socially unacceptable\nas regarded as being a bit insecure,\nbut it can be done for specific cases like this.\nSometimes if you want to provide code publicly via S3,\nyou need to have public access.\n\n\nSo there are ways of doing that\nwhere you give the least privilege possible.\nAnother simple way to let users deploy your CloudFormation template\nis to create a one-click URL.\nThat's been around for a while.\nYou might have seen websites and GitHub repos\nwith click to deploy in CloudFormation,\nand it just gives you the ability to have a button on your website\nthat would take users directly to the CloudFormation UI\nwith the template preloaded from S3.\n\n\nNow, if you don't want to make the template available on GitHub or S3,\nyou can publish it as a module on the CloudFormation registry.\nSo this also gives you options for public and private access\nlike the serverless application repository,\nand it will also allow you to do versioning.\nIt's basically a way to publish a set of resources in a template\nand let other users include that module in their template.\n\n\nThen when the user deploys their one,\nCloudFormation will automatically kind of inline\nall of your resources from the module.\nNow, CloudFormation registry is there for lots of different purposes.\nYou can publish your own providers there too.\nIt's not incredibly common to see it used,\nbut if you're doing something public, it's an option.\nIf you want it to be private,\nso just for your organization accounts,\nthen you might say,\nwell, why bother using the CloudFormation registry?\nBecause I think service catalog is probably a more common approach\nthan the CloudFormation registry in that case.\nSo you've got, I think, quite a few options\nwhen you just have a CloudFormation template and some code,\nbut of course, it's not restricted to CloudFormation Luciano.\nModules are something you can do also with Terraform or OpenTofu, I guess.\n\n\nLuciano: Yeah, exactly, and this is one of the killer features of Terraform.\nAnd if people have a preference for Terraform over CloudFormation,\nyou can basically make things reusable\nby defining infrastructure as called as Terraform modules.\nAnd the idea is that you can package together\na collection of resources in Terraform files,\nand then you expose an interface that can receive inputs for configuration\nand provide outputs to basically be able to connect\nwhat you generated from your module\nwith the rest of your own infrastructure that you're working on.\n\n\nAnd this is a fairly common approach.\nPeople using Terraform should be quite used to this,\nespecially since modules have been available for so long.\nSo in this particular use case, what you could do\nis basically define everything you need for this Lambda Authorizer\nto work as a module and then make this module available.\nSo now, once you have the module,\nwhat are the options to make it available?\nAnd I think the most common one is to just publish it on GitHub,\nbecause one of the many ways that you can install modules\ncoming from third parties\nis just by referencing the GitHub repository.\n\n\nSo if that's a public repo, it's very easy.\nSo most of the modules out there, public available,\nwill follow this approach.\nThere is also another option called Terraform Registry,\nwhich is a bit more centralized.\nIt's managed by Terraform itself.\nAnd as a Terraform user, you can publish your own modules\ninto the registry.\nThis is basically designed specifically for sharing Terraform modules\nand providers.\n\n\nSo you can also use that not just for modules,\nbut if you have more advanced use cases\nwhere you want to create custom providers,\nmaybe because you are interacting with resources outside AWS,\nyou're using other, I don't know, SaaS solutions,\nand you are creating your own Terraform binding code\nto be able to provision resources into other third-party SaaS.\nSo it's very powerful because it kind of gives you\nall the extensibility of Terraform in one place.\n\n\nOnce you have your modules in the Terraform registry,\nyou can easily include them in your configuration\nso anyone can just reference a module from the registry.\nAnother approach which is somewhat similar to CloudFormation\nis that you can just make things available in a URL.\nSo this is called direct download,\nand it's basically a tarball or a zip file\nthat can live in a CDN or your own website\nor basically anywhere else where you can access through an HTTP URL.\n\n\nAnd this way, you have another additional way\nto reference all the Terraform module code\nand include it into another project when needed.\nIt's, I don't know if I see a use case for this.\nLike, I think if I had this need, I would probably prefer GitHub\nrather than having to think how to host that package myself in a URL.\nBut if you have strong reasons not to use GitHub,\nthat can be probably another approach.\n\n\nThere is another idea that might come to mind\nbecause we are thinking about packaging this Lambda into a zip file,\nbut you probably know that this is not the only option to provide Lambda code\nbecause the other option is to use container images.\nAnd if we think about that,\nwe open up another bunch of possible options\non how to host the Lambda code\nbecause basically we can host it everywhere\nwhere you can host a container image.\n\n\nAnd the obvious options are you can use Docker Hub\nor you can use an ECR registry\nor you can even use GitHub registry\nbecause recently GitHub opened up support also for Docker images\nin their own registry.\nThis is actually not something we have tried,\nbut because you can do public images, that should work out of the box\nbecause it's still using the OCI standard\nin terms of allowing people to pull their containers.\n\n\nThere is one thing that I think is worth discussing\nbecause we were actually thinking about this option.\nWhen you think about hosting stuff on S3,\nif this is your own bucket,\nthen imagine that that thing becomes very successful,\nit goes viral, everyone is using it,\nyou have millions of downloads per week,\nand then suddenly you realize, wait, I'm paying for all of this.\nI'm paying for all the access to this bucket.\n\n\nSo this is maybe not desirable\nbecause you created something open source,\nyou are not monetizing from it, and suddenly you even have a cost.\nSo one thing that is commonly done in those cases,\nso when people are sharing resources over S3,\nbut they don't want to take a hit on the cost,\nis to use a feature called RequestedPays,\nwhich basically allows you to say,\nyou can only access this public resource\nif you decide that you are going to be paying for the access cost.\n\n\nThere is one problem with this in this particular case,\nis that you need to pass a header in the S3 request\nto say, I accept the cost of downloading this.\nSo it's almost like saying, I am aware that there is a cost,\nI'm not just downloading this for free,\nI'm going to be taking on the cost of the download.\nAnd this way, AWS basically allows this mechanism\nwhere you as a publisher don't have to take the cost\nand the user is aware that they are paying the cost for the download.\n\n\nThe problem is that because it requires an extra header,\nthis is not something that is built in\nwhen you reference the Lambda code from S3.\nYou cannot just say, use the x-amz-request-payer header.\nSo basically, this option is not available for you.\nAnd this is one reason why if you want to use SAR,\nin that case, all your code is going to be hosted\nin a bucket that is owned by AWS.\nSo you don't have to worry about cost,\nyou don't have to worry about the fact that people cannot specify\nthe request by your header as another alternative.\nSo basically, this reinforces the idea that SAR\nis probably the simplest approach for this particular use case.\nThat maybe makes me think that you mentioned SAR\nis like an app store for your resources.\nSo now the next question is,\nwhat if we actually want to make a business out of it?\nMaybe we want to make this thing something that people have to pay to use it,\nbecause maybe it's giving them so much value that...\nAnd we have so much maintenance burden\nthat it makes sense to make it as a, I don't know, paid service.\nDoes that make sense?\nAnd if it does make sense, what are the options\nin terms of hosting it as a paid solution?\n\n\nEoin: While SAR might be like a bit of an app store,\nyou can't monetize on it.\nBut there is an option for that.\nYou can publish CloudFormation templates to the AWS Marketplace\nand set a price there.\nSo you might be familiar with AWS Marketplace\nas a way to get third party products like AMIs, SaaS solutions.\nYou can buy through AWS Marketplace, like even Datadog and things.\nYou can buy through there so you can get it on your AWS bill.\n\n\nBut you can just provide CloudFormation templates as a product\nand then you can charge people a monthly or a once-off cost\nor lots of different billing options.\nSo if you think there is a market for your highly prized Lambda function,\nthis is a reasonable option.\nNow, I don't have any experience really of doing CloudFormation templates\nin the AWS Marketplace,\nbut I frequently listen to the Cloudonaut podcast.\n\n\nWe've mentioned it a few times where Michael and Andreas talk\nabout their experiences in publishing their products to the Marketplace.\nAnd we will have a link in the show notes\nto a really great blog post from them with an accompanying video.\nAnd it's all about how to provide CloudFormation\nas a product on the AWS Marketplace.\nAnd unlike the Apple App Store,\nthey don't completely fleece you on commission.\n\n\nI think it's much more reasonable on the Marketplace.\nI think that's probably a good place to wrap this one up.\nAnd I hope we've covered all the options for publishing Lambda functions\nand indeed other AWS resources for public consumption.\nWe generally recommend SAR, the Serverless Application Repository,\nas the first option since it handles the code distribution\nand a lot of the complexity.\nBut let us know if you can think of any other creative ways for this.\nAlso, watch out and see what Luciano does in the future.\nAnd if he manages to become a high-earning AWS Marketplace\nCloudFormation entrepreneur! 😜\nThank you for listening and until next time, goodbye. 👋\n"
    },
    {
      "title": "102. Getting Ampt with Jeremy Daly",
      "url": "https://awsbites.com/102-getting-ampt-with-jeremy-daly/",
      "publish_date": "2023-11-03T00:00:00.000Z",
      "abstract": "In this episode, we have the pleasure of speaking with Jeremy Daly, CEO of Ampt and a leader in the AWS and serverless community. We discuss Jeremy's journey into AWS and serverless, the prolific open source work and content he creates, the evolution of serverless over the years, common myths about serverless, and, finally, the story behind building Ampt to improve the developer experience. Jeremy provides his perspective on the state of serverless and predictions for the future and it also gives some fantastic pieces of advice for wannabe tech-entrepreneurs!\n\nAWS Bites is brought to you by fourTheorem, an Advanced AWS Partner. If you are moving to AWS or need a partner to help you go faster, check us out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources:\n\nServerless Microservice patterns for AWS (article by Jeremy)\nDatadog research: The state of Serverless\nBref: serverless runtime for PHP\nAmpt original announcement blog post\nOur previous episode on Ampt\nLong-running tasks and smart compute with Ampt\nAmpt official website\nJeremy's official website\nJeremy on X (ergo Twitter)\nJeremy's newsletter &quot;Off-by-none&quot;\n\n",
      "transcript": "Luciano: Today, we have a very special episode.\nFor over 100 episodes, it has been just the two of us discussing AWS topics.\nWe talk about other community contributors\nand mentioned their projects, articles, podcasts, and videos.\nSomeone we have mentioned who created a massive amount of this content\nis Jeremy Daly.\nWe are very excited to have Jeremy with us today\nfor the very first AWS Bites interview.\nJeremy is the CEO of Ampt.\n\n\nHe is also a fellow AWS Serverless Hero,\nspeaker, podcaster, and writer.\nHe is one of the first names that comes to mind\nwhen you think of leaders in the topics of AWS and Serverless.\nToday, we are going to talk about Ampt,\nhear Jeremy's view on the state of things in AWS and Serverless,\nand get his predictions for the future.\nMy name is Luciano, and I'm here with Eoin.\nAnd today, we are thrilled to be joined by Jeremy Daly\nfor AWS Bites podcast.\nAWS Bites is brought to you by fourTheorem, an advanced AWS partner.\nIf you're moving to AWS or need a partner to help you go faster,\ncheck us out at fourtheorem.com.\nOkay, Jeremy, you are very welcome to AWS Bites.\nWe are really excited to have you.\nI'm sure you need very little introduction\nand people may know you from your Twitter, Serverless Chats,\npodcast, blog, off-by-none newsletter,\nand whatever else you have been doing in the open-source space.\nSo we'd like to start by asking you,\nwhere did your journey into AWS and Serverless begin?\n\n\nJeremy: Well, first of all, thank you both for having me.\nI've been a huge fan of AWS Bites for quite some time.\nI share it all the time in my newsletter.\nReally appreciated that episode you did on Ampt.\nSo I am just as honored to be here as,\nand I again appreciate the fact that you're having me on.\nSo in terms of where I got started though,\nit was kind of funny.\nI had a web development company that I was running for,\nI don't know, 12 years, something like that.\n\n\nStarted it out of my dorm room in college\nand did that for quite some time and got much deeper into,\nwe weren't just doing web development,\nwe're doing back-end applications, things like that.\nBuilding pretty complex stuff,\nintegrating with UPS tracking systems and e-commerce\nand going through the whole PCI compliance thing.\nAnd so a lot of fun stuff,\nbut I was actually racking and stacking servers.\n\n\nWe had our own data center that we used,\na co-location facility.\nSo I was very, very old school back then,\nbut I kind of got sick of building forms\nfor somebody else's website.\nAnd I was just like, I want to do something different.\nLike I'm building things for other people.\nI want to kind of build something for myself.\nAnd my second daughter had just been born at that point\nand I had built this thing, I called it the Live Baby Blog.\n\n\nIt was like this interactive, almost like a chat.\nIt was before WebSocket,\nso we were using a thing called APE Server, whatever.\nBut essentially it was a way that I could send updates\nto all my friends and family about what was happening\nwith the birth of my daughter,\nlike while we were actually in the delivery room and stuff.\nAnd so people loved it.\nI had like 100 people that were on it\nand commenting and whatever.\n\n\nAnd I said, I wonder if there's a business here,\nlike just out of curiosity.\nSo I started building this thing.\nWe ended up calling it SproutShout,\nchanged the name to Lifeables eventually.\nBut I said, I'm going to get rid of this web development company\nand I'm going to go into the startup world\nand actually build a startup myself.\nSo I got together with a couple people that I knew.\nI ended up hiring a very, very amazing person to be my CEO\nand she did a wonderful job.\n\n\nBut essentially when we started building it,\nwe were like, well, do we want to host this ourselves\nor do we want to get into the world of AWS?\nSo this was 2009 and we started looking at AWS.\nAnd of course, this was before serverless existed.\nThis was EC2 instances.\nThis was, I think load balancers were like ELBs,\nweren't even a thing yet.\nLike, I mean, this was very, very early.\nNo RDS, none of that stuff, right?\n\n\nSo we were building like, this was true lift and shift type stuff,\nlike building exactly what we would be building\nif we were running these servers ourselves.\nSo we started doing that and I got into AWS\nand it was just amazing.\nLike actually one of the things I did was I moved all of the stuff\nfrom our co-location facility from the hosting\nthat we were doing for web development clients.\nI moved that over to AWS and went from spending like six,\nseven thousand dollars a month in electricity and bandwidth\nand server rental costs and all this kind of stuff\nto about $700 a month in AWS.\n\n\nAnd I'm like, you know, if I would have switched things over sooner to AWS,\nI might have actually been able to build a profitable business around that\nand kept it going for longer.\nBut so anyway, so long story short,\nI spent a lot of time building this startup and got into AWS\nand then had just been building everything on there,\nyou know, from that point forward.\nAnd so what happened is we ran the startup for a couple of years,\nhit up against the Facebook timeline launch,\nthe Instagram acquisition.\n\n\nSo, you know, perfect timing to be building sort of a site\nfor parents and sharing photos, right?\nAnd so we ended up selling off some of that tech to another company.\nI went to work for another company.\nThey were all in on AWS.\nThey were using DynamoDB.\nThat was my first experience with Dynamo and got into that\nand like just actually absolutely fell in love with that.\nAnd then what ended up happening is we had this massive outage\nwhen we were featured on the Good Morning America\nor the Today Show or something like that for the app that we were building.\n\n\nHad a massive massive outage all caused by a single point of failure\nin a relational database.\nAnd so I started looking around.\nI was the VP of product at the time and I'm like,\nI wonder if there's a better way to scale\nand like how we can make this work better.\nAnd I came across AWS Lambda.\nSo this was right in the beginning of 2015.\nStarted playing around with it, fell in love with it.\nAnd I said, this is the future.\n\n\nThis is how things are going to work.\nLike this idea of setting up servers and trying to parallelize them\nor you know, trying to scale them horizontally,\nlike makes no sense in terms of what you can do with Lambda.\nAnd again, this is before VPCs.\nThis was before API Gateway.\nSo I started playing around with a couple of these things\nand then the next thing you know, all these new services started coming out\nand as soon as you could do API requests or HTTP requests with it,\nI knew I'm like, this is it.\nThis is going to be the thing.\nAnd so I've literally dedicated the last,\nwhat has it been, eight, nine years or something like that of my career\nto promoting serverless and trying to get this to be the default paradigm\nthat people built with.\n\n\nEoin: And I think we're probably going to get back to, you know,\nwhat the promise of serverless was like back then versus, you know,\nwhat the reality of it is today, all these years later.\nLuciano also mentioned the content and all the open source work you create.\nI think you regarded it as pretty prolific in the content creation space\nand also you became an AWS serverless hero.\nHow did this all happen?\nWas this a concerted effort on your part to put all this time and energy into content?\nAnd I suppose then what has that given back to you?\nHow has it then influenced your career?\n\n\nJeremy: Yeah, yeah, no.\nSo I mean content creation, I am not quite as prolific as I once was.\nI used to write a lot and produce a lot of episodes of the podcast and so forth.\nI've been very, very busy with my startup for the last year plus,\nalmost two, well, yeah, a year and a half now, something like that.\nSo it's been, I have not been producing as much content as I would like to.\nBut to sort of go back to the beginning,\nso when I discovered Lambda and I started playing around with it,\nthere was just nothing out there.\n\n\nThere was no content.\nNobody was talking about it.\nServerless wasn't even really a word.\nI actually kind of came across the JAWS framework,\nwhich is now serverless, the serverless framework.\nI came across that very early, right?\nSo that was 2005, it was re-invent 2005, sorry, 2005, 2015.\nWell, my brain is not working.\n2015, when Austin presented, I think he presented the JAWS framework at re-invent.\n\n\nSo I started kind of trying to figure out how some of the stuff works,\nand it was all experimentation.\nAnd it was like, well, can you do this?\nAnd, you know, okay, well, when VPCs came out, connections to VPCs,\nand we could connect to ElastiCache, I was like,\nokay, now this is getting even more interesting.\nSo I started playing around with these different things,\nstarted creating stuff with it.\nWe built a whole bunch of things internally at that company I was talking about.\n\n\nAnd then I actually left that company to go work for another company\nand did everything as serverlessly as possible I could there.\nSo I was learning a lot and figuring out a bunch of stuff,\nand still the content wasn't really there.\nBen Kehoe was posting a lot of stuff, which was helpful.\nThe Burning Monk, Yen Trey, was posting a bunch of stuff back then.\nThis was like maybe early 2018.\nAnd so I started, I said, look, I've got all this stuff.\n\n\nI said I'm going to put some stuff out there.\nI had been blogging in the past, and I was like,\nI'll just stop putting some stuff out there.\nSo I started writing and just sharing some of the things that I was learning,\nsome things about security.\nI wrote a big post about security.\nThat actually connected me with Ori Segal over at PureSec.\nAnd so he and I started talking.\nWe became friends, right?\nAnd then next thing you know, it's like I'm getting ready to,\nI'm getting ready, or I get introduced to Tom McLaughlin.\n\n\nAnd I met him at an AWS Startup Day.\nAnd he's like, hey, we're thinking about doing a Serverless Days thing.\nAnd I'm like, what's that?\nSo anyways, we get together.\nI met Eric Peterson.\nI met all these other great people that were in the space.\nAnd so one of the things that I did was I published a post,\nI think it was in 2018.\nI think this is one of the first posts I wrote that got a lot of traction,\nwas my Serverless, AWS Serverless Microservice Patterns,\nor Serverless Microservice Patterns, or AWS, whatever it was.\n\n\nAnd I think I put 16, 17, 18 patterns, something like that,\nof things that I had seen other people doing, things that I had used.\nAnd I didn't put them out there like, hey, this is how you do it.\nI put them out there like, hey, this is how I'm doing it.\nThis is what I'm seeing people do.\nLike, is this right?\nAre you doing this?\nAre we, is there a better way to do it?\nAnd I actually think that started a really interesting patterns movement,\nlike people started really talking about patterns after that,\nwhich again, I don't take credit for,\nbut I think it was just the start of the conversation at least.\n\n\nAnd that's, I think more people started thinking about it\nin terms of those patterns.\nSo, I did that.\nI started my newsletter in September of 2018.\nSo again, that's been over five years now.\nAnd I went to Serverless Days, New York,\nand saw Kelsey Hightower was the keynote speaker.\nBen Kehoe was there.\nI think I met Taavi Rehemägi\nfrom Dashbird.\nLike, so I met like, again, just connected to people.\n\n\nAnd it was great.\nAnd then it just kind of took off from there.\nAnd then Heitor Lessa invited me onto his show.\nHe kind of let slip that apparently I was going to be a AWS Hero.\nBut, so anyways, so I was made an AWS Hero early 2019.\nAnd then since then, I kind of put my foot on the gas\nand I started the podcast.\nI spoke at, I was lucky enough to speak at Serverless Days, Milano,\nor Serverless Days Milan.\n\n\nI just spoke at Serverless Days Cape Town.\nI keynoted Serverless Days ANZ in Melbourne.\nLike I was, I've spoken in Belfast.\nSo I've been able to do all these crazy things\nand meet all these amazing people.\nSpoke at re:Invent last year, which was absolutely amazing.\nSo I think I forgot what your original question was,\nbut essentially I just, it had a massive impact on my career.\nLike this idea of sharing what I did and figuring these things out.\n\n\nAnd I think it, I think because I hit it a little bit early,\nlike when it was kind of coming up that I was one of a,\nyou know, I became a recognizable voice in the space.\nBut it's only because I learned so much from talking to other people\nand willing to put it out there.\nBut there's so many people writing about Serverless now,\nso many amazing Serverless, you know, frameworks,\nI didn't say framework, but you know, I mean like deployment frameworks\nor, you know, NPM projects and things like that,\nthat are just, you know, that are amazing.\nAnd so many, so much great work going in there.\nIt's like, I almost feel like, you know, continuing to write content\nand putting more content out there.\nI'm almost like, I don't know, there's so many new voices\nI kind of want to hear from them and, you know, and see where this goes.\n\n\nEoin: Well, I guess that's the value of the Off-by-None on newsletter\nbecause for me, it's, it kind of helps me to short circuit,\nhaving to troll through everything.\nBut I guess that makes the job more difficult for you\nas more and more people join the community\nand you've got more and more content out there.\nIs it becoming a bit of a effort for you to do all that?\n\n\nJeremy: Yeah, I mean to give you some perspective, I have a couple of systems that are all serverless, by the way,\nthat I wrote that scan some different things.\nI grabbed some stuff from Google automated searches.\nI have some, I have a whole bunch of RSS feeds that get aggregated,\nyou know, and a bunch of other ways that I collect content.\nSo every week I have about 400 to 500 pieces of content\nthat end up in this system.\n\n\nAnd I can filter out a fair amount of them.\nBut I usually, you know, I usually start somewhere around a hundred or so\nat the top level and try to get that down to like maybe like 50 if I can,\nwhich still seems like a lot.\nBut the, you know, the interesting thing is,\nand if anybody wants to know,\nbecause people have asked me this in the past,\nlike how do I make it into your newsletter?\nSo there are certain articles that I read that I open up\nand I can immediately dismiss them and say,\nthis just isn't something that's interesting.\n\n\nAnd usually it's because, you know, especially if it's something that is,\nif you're writing a tutorial and it shows all screenshots from the console,\nlike most likely I'm not going to include that\nunless it's something really, really interesting.\nSometimes new things that you can only do from the console,\nyou know, I would include something like that,\nlike bedrock and some of those new things.\nBut the other thing is, is that also like well formatted,\nlike if you just have giant chunks of code that it's like,\nI can't, I can't understand it.\n\n\nIt's not highlighted, whatever.\nLike that's sometimes frustrating.\nGated content I almost never share.\nSo if there's a, if you do Medium and you should,\nand I get it, I know, and a lot of people like to,\nyou know, sort of get that revenue.\nSome people take that revenue and donate it to other places.\nI think that's really noble and I appreciate people doing that.\nBut for me, my readers get very frustrated when they click on a link\nand they can't read it because not everybody wants to pay,\nyou know, to be a Medium member.\n\n\nBut yeah, so I mean, just some basic tips there.\nBut like something interesting, right?\nAnd so much has changed in the last year with ChatGPT.\nLike I think I've become a human ChatGPT detector now\nbecause I just read so much content and I'm like,\nyou know, that's definitely ChatGPT.\nAnd so yeah, I mean it is a challenge.\nBut for me, I look at it and I say, I know when I started,\nI think that I had the benefit of being, like I said,\none of few voices in the space.\n\n\nAnd that made my content more discoverable\nbecause somebody would search for it and they would find me.\nAnd again, I got great SEO and a boost from that.\nAnd I think that I read a lot of people's stuff,\npeople that are doing really, really good work\nand answering interesting questions too\nand challenging things, which is what I always like to see.\nAnd again, I see they get like two claps on Medium\nand I'm like, how does this article not have more?\nHow does it not have more reads?\nHow does it not have more interactions and comments?\nAnd so that's what I try to do with my newsletter.\nAnd try to feature the ones that I think are consistent\nor they're interesting.\nAnd they don't have to be right.\nYou don't have to be right.\nI don't always comment on whether or not\nI think it's a good thing that you're doing it this way\nor a bad thing, but I just like to get the information out there\nand let people think for themselves.\n\n\nLuciano: Yeah, I can definitely resonate because I also have, I guess, much smaller newsletter in the full stack space.\nAnd yeah, definitely there is lots of work\non curation. Automation can help a little bit,\nbut ultimately you need to read and check every single thing\nyou publish and make sure it is actually something good\nyou are giving to your audience.\nOtherwise, the whole thing doesn't make sense anymore.\n\n\nSo definitely resonate with that.\nAnother thing I want to connect to is you mentioned\nthat you started very early with serverless.\nI think I also started around 2015 and I definitely remember\nthat the feature set of Lambda, for instance,\nwas so much smaller.\nAnd at the same time, the adoption of Lambda\nwas so much smaller.\nAnd in the last few years, we have seen a growth\nboth in terms of features and possible integrations,\nbut also the way that people started to use Lambda,\nthe use cases.\nSo I guess the question that they want to ask is\nif you are seeing this perception that serverless\nis changing over the year, and if there are things\nthat maybe today we can consider as myths\nthat we need to debunk when we talk about serverless.\nAnd in general, when we talk about the benefits of serverless,\nwhat are those benefits?\nAre we overselling them or there is some kind of genuine value\nthat we need to communicate more to get a larger adoption?\nLots of questions, but hopefully the context makes sense.\n\n\nJeremy: Yeah, no, a lot of questions in there.\nBut I maybe start with the first part in terms of\nthe feature set of where it was versus the feature set\nof where it is today.\nSo I think that in some regards, serverless\nhas become extremely mature and to some degree boring.\nI think if we look at like the Datadog\nstate of serverless report and we see that 70 plus percent\nof companies are using some sort of serverless system,\nwhether that's Lambda or Fargate or AppRunner\nor something like that.\n\n\nI think that goes to show that you just can't get away\nfrom serverless almost, right?\nLike it's just there.\nIt's embedded in the cloud.\nYou know, if you're using Dynamo or you're using SQS now,\nI mean, you're using serverless in some degree.\nAnd so I think most companies, I mean,\nif you think about it, that company I talked about\nthat was using DynamoDB, I mean, technically we\nwere using serverless before serverless was a thing.\n\n\nSo I think it's really hard to define it now.\nYou know, again, mindset, ladder, whatever you want to call it,\nright?\nServerless first.\nI think the idea is just that it's the way to build cloud\napplications now.\nAnd the feature sets have grown to a point\nwhere it's become incredibly complex.\nI mean, I go back to the days where I'm like,\nyou know, I was installing, you know,\nNginx or Apache or something like that on a Linux box.\n\n\nIt was, you know, it was running as a virtual machine.\nAnd I'm like, yeah, that was complicated.\nBut I don't know if it's as complicated\nas figuring out tumbling windows in Lambda\nand making sure that we have the right,\nyou know, extensions installed or the right layers installed\nor I've got layers that interact with the extensions that then,\nyou know, give me these things.\nIt just gets very, very complicated when you think about\nhow much it can do.\n\n\nSo from a feature set standpoint,\nwe're nowhere near feature complete.\nIt can't do everything.\nI'm sure that you can make some other system do\nif you needed to, if you really wanted to run bare metal.\nBut I do think that it's gotten to a point\nwhere there's not much you can't do with it, right?\nSo if you're building an application today,\nand I know everybody says this, so this is probably,\nyou know, probably just redundant advice,\nbut start serverless first, right?\n\n\nIt makes no sense to say,\nI'm going to spin up an EC2 instance\nand set up auto scaling groups and this kind of stuff.\nThere's just so many ways to do it.\nIf you're a PHP developer, check out Bref.\nLike Matthew Napoli has done such an amazing job\nwith that service, you know what I mean?\nAnd I know that there's not really official PHP support\non Lambda, but there's Lara,\nwhat's the Laravel one?\nI'm trying to think what it's called,\nbut there's another one for Laravel\nthat is all serverless based.\n\n\nLike there's just so many things you can do now.\nLike just do it that way, start that way.\nAnd I think that, you know,\nthe common things we hear from a myths perspective\nis vendor lock-in, cold starts, right?\nHigh costs, some of these sort of things.\nServerless can get very, very expensive\nif you use it wrong, right?\nIf you don't set it up the right way,\nif you're doing what the Prime team did\nand trying to run step functions\nfor every single frame of millions and millions of videos,\nthen yeah, it's going to get stupidly expensive.\n\n\nAnd that's just a poor architectural choice,\nbut it probably wasn't when they did it the first time,\nwhen they set it up the way they do it to do samples.\nLike it probably made perfect sense\nand it probably took them a fraction of the time\nhad they built some other system to do it.\nSo I think that the cost aspect of it is,\nyou know, depending on what your workload is,\ndepending on what you're doing,\nthat's certainly something that it can get expensive,\nbut I mean, everything gets expensive\nif it's misconfigured or not being used efficiently.\n\n\nI think the other thing around vendor lock-in,\nexcuse me, too, is I don't know any system\nthat exists that you're not locked into a vendor.\nI mean, data is the biggest thing.\nI mean, even if you say, well, we're using Postgres,\nand we're running it in RDS,\nso we can move that wherever we want to.\nYeah, good luck.\nI want to see you transfer terabytes of data\nover to PlanetScale or over to some other provider.\n\n\nI mean, the data gravity there is huge.\nAnd that's one of the reasons why I hate ORMs,\nand no offense to anybody who's building ORMs\nor things like that,\nbut I've never seen an ORM that allows you\nto go from Postgres to MySQL to some other,\nyou know what I mean?\nIt just never, that doesn't happen, right?\nSo you're locked in no matter what you do.\nAnd the question is, is that who do you lock yourself into?\n\n\nI mean, even Next.js now is something\nwe've been talking about.\nThere's been a whole bunch of buzz about this.\nIt's not easy to run Next.js not on Vercel, right?\nSo either you're running it on Vercel\nor you're jumping through hoops\nin order to make it run somewhere else.\nAnd so you're locked into Vercel pretty heavily\nif you choose to run your Next.js app there\nand take advantage of the benefits.\nSo this is true of everything.\n\n\nBut the question is, is where do you lock yourself into\nand what are the trade-offs of choosing a particular thing?\nLike I would rather be locked into Lambda\nand serverless on AWS than I would be locked into\nrunning a Kubernetes cluster on GCP, for example, right?\nLike, I mean, so to me, it just makes sense.\nIt's faster, it's easier to do.\nAnd then the last one I mentioned, I think,\nwas the cold starts thing.\n\n\nThis is something that you really got to think about\nwhat your workloads are.\nIf you're running a webhook or an API or something like that\nand you're running that on a Lambda function,\nlike, yeah, you're gonna get cold starts\nif you don't have sort of high velocity\nor you don't have that ongoing stuff.\nAnd they're working on that too.\nThere's ways that it makes it better.\nIf you're using Node or using Rust\nor some of these other ones,\nlike it's very, very low cold starts anyways.\n\n\nBut this is just one of those things\nwhere you have to make a decision where it's like,\ndo you want that scalability of scaling down to zero\nor do you want the availability and the cold start,\nthe minimal cold starts?\nBecause if you do, then just deploy to AppRunner, right?\nAnd if you deploy to AppRunner,\nthen you pay a couple of dollars a month,\nmaybe it costs you $30, $40 a month to run that API.\n\n\nBut you can run all those.\nYou don't have the cold starts.\nYou get good performance.\nI mean, there's different ways to do it,\nbut it's about architectural trade-offs.\nAnd I think that's the last point that I'll make.\nAnd I'm sorry, I know I'm rambling a little bit.\nBut the big thing here is that Serverless\nhas introduced, I guess, thousands of trade-offs, right?\nLike there's so many different ways to think about\nhow to make a particular workload run,\nwhether you're using choreography through EventBridge\nor orchestration with step functions\nor a combination of those,\nor you're still running certain things within Lambda functions\nor you're trying to hand stuff off to Fargate\nor you're doing any of those things,\nor you're choosing SQS over Kinesis or those.\nLike there's just so many things that,\nso many decisions that you have to make that, again,\nthere's a very big difference to me between a developer,\na sort of ops slash cloud architect,\nand then somewhere in between where we're,\nI call them Serverless developers maybe,\nbut like there's different sort of knowledge sets\nthat you need to have to be on either end of those spectrums.\nAnd in order to find yourself in the middle,\nand this is one of the reasons why I think Serverless\nis still really hard to adopt for a lot of people,\nis there's a huge learning,\nthere's still a lot of learning to do\nand you have to bring knowledge from both sides\nin order to be really effective,\nI think, at building Serverless applications.\n\n\nEoin: Serverless was simple to begin with,\nyou know, when it was a little bit naive\nand maybe less capable,\nbut I guess as more and more features have been added\nand now everything is possible,\nis it, have we arrived at the case\nwhere just the cognitive load is just very intense for developers?\nHave AWS kind of missed an opportunity there\nto kind of continue to remove\nthat undifferentiated heavy lifting?\nCould there have been a different path?\nAnd is maybe, maybe this is leading into the Ampt story then.\nIs Ampt's mission to rectify that?\nMaybe you can go through your thoughts on that\nand how Ampt all began.\n\n\nJeremy: Yeah, so I mean, I think, you know,\nI think you know my answer to this question,\nso whether it's made it too difficult.\nI mean, I think AWS is,\nhas done an amazing job building these primitives.\nI think they've done a terrible job\ntrying to find a way to make the developer experience,\nyou know, smooth.\nAnd so I don't think you solve the problem\nwith just developer experience.\nI don't think that's the ultimate solution.\n\n\nBut what I will say about the developer experience,\nand you mentioned the cognitive load,\nwhich if you think of the sort of the triangle of DevEx, right,\nyou've got one thing which is fast feedback loops.\nLike as a developer,\nyou can't wait two, three minutes to figure out\nwhether the code change you just made works, right?\nLike that's just incredibly frustrating\nand it's just, it just, it fries your brain, right?\n\n\nThe next thing is the cognitive load piece\nand this was something interesting.\nAnd Stanley was just talking about this actually\nat Serverless Days Cape Town,\nwhich is, you know, the new number is four.\nThat's how many things you can hold in your head at one time.\nIt used to be seven.\nNow it's like, you know, between three and five.\nSo if you have to be thinking about more than four things\nwhile you're writing code or trying to build an application,\nyou can't.\n\n\nYou have to stop, you have to do task shift,\nyou have to go look something up like it is.\nIt's just really, really frustrating.\nAnd then the third piece of that is what we call flow, right?\nThis idea of, or the flow state,\nwhere you are in a point where you're just cranking, right?\nEvery time you have to stop\nand go look up something in documentation\nor any of those things that break your flow,\nit's really hard.\n\n\nAnd unless you can put those three things together\nand somebody can just write code or do whatever,\nwhatever task they're working on,\nthey have that feedback, the fast feedback loop.\nThey have limited amount of things that they have to keep,\nyou know, sort of at the top of their mind\nin order to make those things work.\nAnd they can get into a state where they can be uninterrupted.\nThat is where you get rid of things like developer burnout,\nwhere people are just, you know, happier.\n\n\nI think the developer burnout isn't talked about enough\nand it really should be because in today's day and age,\nit is very, very real.\nBut anyway, so to me, I look at the developer experience\nthat AWS put together.\nAnd I think that they missed the mark,\nbut they didn't miss the mark\nbecause they didn't necessarily care about it\nor they didn't try.\nI think they missed the mark because of the complexity\nof the underlying technology that they put together, right?\n\n\nI mean, things can only be so simple.\nYou can only abstract so many things.\nAnd so when we originally,\nand the predecessor to Ampt was serverless cloud.\nSo this is something we started actually working on\nat the end of 2020.\nSo it's been a while that we've been, you know,\nbeen playing around with this idea.\nAnd the goal with that actually was to make it easier\nto deploy the serverless framework for people.\n\n\nBut Doug Moscrop and I, we were working on this thing\nand we're kind of like, you know what, though?\nIt just seems crazy to us that you actually need to define\nthat you need a Lambda function or an API gateway\nor that you need to say, I need this route\nto point to this file\nwhen you're already defining that in your code.\nAnd we didn't know where this would go.\nWe didn't know if it was even possible.\n\n\nAnd so what we started doing is playing around with some ideas.\nWe built out an early version with API,\nlike with an API router and with a task\nor a schedule type thing.\nAnd the data component as well.\nThat was something early we were working on.\nAnd we just kind of found out, we're like, okay,\nright now, out of the box, this solves a lot of problems\nwithout you even thinking about needing to set up Lambda functions\nor configuring anything.\n\n\nAnd the developer experience for it was very, very simple.\nAnd one of the things we knew we wanted to do was we said,\nyou can't emulate this stuff locally.\nAnd I just saw a bunch of stuff, by the way,\nrecently on Twitter where people are talking about,\nwell, if I have a Mac M3, I can run all of this stuff locally\nand do all my tests locally and it's super fast and whatever.\nI think that might be fine.\n\n\nBut we looked at it back then and I still look at it now and say,\nI think you need to run everything against real cloud infrastructure\nto get the fidelity that, you know,\nto know that all these interconnected pieces are actually working.\nSo we wanted to do that.\nWe wanted to create these high fidelity sandboxes.\nSo in order to make that work,\nwe had to create a syncing technology that allowed us to save stuff,\nlike a watcher that would upload code and whatever.\n\n\nNow, we were doing this originally by zipping stuff\nand then deploying it to the Lambda function.\nIt would take like seven, eight seconds, something like that.\nIt was okay.\nBut seven, eight seconds is a long time when you just flip over from,\nyou know, you make a change and then you wait for it to deploy\nand then you flip over to, you know, Postman,\nyou run a thing and you're like, okay, that worked.\n\n\nIt's still too long and we found it was too long.\nSo we worked on that for a long time.\nWe actually get that down to about 400 milliseconds right now.\nSo it's pretty fast how quickly you can make code changes and do that.\nBut so for us, we looked at that and we were like,\nwe want to fix the developer experience, first of all.\nThat was the big piece of it.\nWe knew that that was part of it.\nBut then the big thing was we wanted to reduce cognitive load.\n\n\nWe felt like as serverless developers that cognitive load was killing us.\nLike it was always and the worst thing is and look AWS is,\nI don't want to diminish what they've done.\nThey have amazing people there.\nThey work really, really hard.\nThey do some really amazing things,\nbut their documentation, especially around CloudFormation,\nis just it's like you just have to keep diving deeper and deeper and deeper.\n\n\nIt's like, okay, you click on this and then it gives you the possible options\nand then oh, here are the settings for that.\nYou click on that and you go deeper and deeper and deeper.\nI mean, I remember times where it's like, what are even the same defaults?\nLike what just happens by default that would, you know,\nlike how much do I actually have to configure and change?\nAnd all of that load on my brain, again, maybe I'm just not a great developer,\nbut that honestly, it just gives me a headache and I get frustrated\nand I'm like, I can't, you know, I stare at these things and I'm like,\nwhy, why does this do what it needs to do?\n\n\nSo we wanted to take as much of that cognitive load off of people.\nSo again, nailing the developer experience,\ngiving people those fast feedback loops, reducing that cognitive load,\nthat we were hoping would then produce this,\nyou know, sort of flow state where people could just get into actually building the apps they were building.\nThis has evolved tremendously.\nWe added support for full stack stuff.\n\n\nWe saw most people were like writing stuff with, you know, Next and Express and,\nand, you know, React or, you know, Vue or whatever.\nSo we wanted to support all those things.\nAnd we added in support for all kinds of different, you know,\nthings like tasks, long-running tasks.\nWe built this thing called Smart Compute.\nI mean, I don't want to sell Ampt here,\nbut if you want to go check it out, go to getamp.com and the documentation.\n\n\nI mean, it does quite a bit of these things for you.\nBut I think back to the original premise is,\nwe built this and we started on this journey because we wanted to solve this,\nthis larger developer experience thing and just make it easier for developers,\nlike developers capital D, I don't know, lowercase D, whatever,\ndevelopers who were building applications, who are experts in writing,\nyou know, Express and interacting with databases and those people, right?\n\n\nThe ones who weren't experts in setting up CloudFormation templates\nor writing CDK and figuring out what the cloud architecture was.\nWe wanted to see if there was a way to go from cloud or from code to cloud\nwith as little friction as possible.\nAnd I think we've, you know, it's not perfect yet,\nbut I think we've gotten pretty close.\nSo, yeah, so that was really, that was the genesis of the idea,\nwhere, you know, where we kind of are now and I mean,\nthe other thing too is like CI-CD, AWS Cloud Account Management, right?\n\n\nLike that, I mean, honestly, we have customers using Ampt\nthat I think the biggest benefit they get is just from the account management.\nSo we automatically spin up and I was watching your episode the other day about Ampt,\nI think you asked this question, every single environment that we spin up\nis a separate AWS account, completely isolated, you know,\nyou've got that blast radius there, all the quotas are tied to that individually.\n\n\nSo, you know, we have one customer that does e-commerce sites\nand they deploy these e-commerce sites for their customers.\nAnd what they do is they use the Ampt'd stage,\nyou know, to create stage, create these stage environments,\nthey use that to create, you know, environments for each one of their customers.\nAnd then what's cool about it is they update, they have like sort of a,\nyou know, their code that they update that they can deploy to staging\nand check to make sure it works.\n\n\nAnd every one of these environments they have running for their customers,\nthey get different data from our parameter system,\nthey have different data obviously in their data tables.\nAnd then if they want to push out an update to them,\nthey can just update them individually and say,\nokay, we're going to move you to v1.2 or whatever it is\nand update each one of those different services or those different environments.\n\n\nAnd it works really, really well.\nWe've got another one that's using it, BlockSec is using it to do tenants.\nTheir tenancy is based off of individual AWS accounts, do some security and stuff.\nSo there's just so many things that are taken off your plate.\nAnd then again, the last thing that the CI/CD portion of it is to basically say,\nyou know, CI/CD in my opinion is dumb and it's broken\nand I don't see anyone who's ever done CI/CD well.\nSo we were like, let's just remove CI/CD from the equation.\nAnd so we take your code and it's all built in your environment.\nSo your environment actually builds your code and processes your code\nand reconfigures itself based off of what your code has specified.\nSo it eliminates that process, you get CI/CD out of the box.\nIt just takes so many headaches away from developers\nand just lets them write code, which is again,\nwas our ultimate, that was our original tagline was just write code.\nLike that was, you know, that's where we wanted to be\nand that's where we are right now.\n\n\nLuciano: You mentioned the smart compute feature and this is actually something that got me very curious\nbecause as I understand, you can basically,\nyou have different constraints when you run on Lambda,\nup runner or Fargate, but all this complexity is kind of abstracted\nfrom you as a developer, the system somehow is just going to figure out\nwhich one is the best environment for the kind of workload\nyou are trying to deploy.\nAnd that feels a little bit magic if you ask me.\nSo I'd like to ask you if you can disclose some of that magic.\nAnd I guess my question is, does it just work?\nLike is it able to transition from Lambda to up runner to Fargate\nor something like that automatically without like any interruption\nor maybe there are certain trade-offs that developers still need to be aware\nand somehow adapt to that specific model when they write their code.\n\n\nJeremy: Yeah, so we tried to make it so that the trade-offs\nwere handled by the system and not by the developer.\nI mean, you're still building on a distributed system, right?\nSo we try to make sure people know that, right?\nSo it's not like if you run the same code multiple times\nthat it's always going to have access to global variables\nthat you've set and things like that.\nSo you should assume that every single time your application runs\nit is stateless and would need to hydrate itself\nwith any information that was there.\n\n\nBut what we tried to do from a different approach is like,\nyou know, look containers are great, like Docker, you know,\nrevolutionized a lot of different things in terms of how people\nwere able to encapsulate code and slim down applications\nso that they only had what they needed\nand give people more control over sort of not the operating system\nbut certainly the runtime that was baked into those things.\n\n\nAnd we don't want to take that away from people, right?\nBut Lambda kind of did, at least initially.\nAnd it said just write code, just put some application code in there.\nWe'll manage the runtime, we'll manage the operating system underneath.\nSo what we said is let's start there.\nLet's start with the just give us the code thing.\nAnd when you just give us the code, because again, we build it, right?\n\n\nWe're not running a builder in your CI/CD process that packages your code.\nThat runs in your amped environment.\nBecause we can do that, we can actually deploy that code\nor take that code and turn it into or deploy it into a container\nand we containerize it for you.\nOr we can, you know, put it into Lambda directly and so forth.\nSo because we have that underlying code, the actual code you've written,\nwe can sort of massage it and change it and deploy it in multiple different ways.\n\n\nSo the trade-offs are, obviously with Lambda, is you can only run it for 15 minutes.\nThe problem is if you run Lambda for 15 minutes,\nit gets very, very expensive to run Lambda for 15 minutes.\nSo we put in some, you know, some basic heuristics that say,\nlook, if you're going to run something for more than a few minutes,\nwe'll just launch it into a Fargate container.\nAnd if it's a, and again, these are scheduled tasks, right?\n\n\nSo if you schedule a task, it's pretty easy for us to trigger Fargate\nbased off of a scheduled timer.\nYou know, we've got some timer things in there with Lambdas\nthat trigger some things and do some of that stuff.\nBut essentially, that's a pretty easy switch\nto just take something that would run in Lambda and run it in Fargate\nbecause we also, we do all the permission stuff for you as well, right?\n\n\nSo there is a lot of, I don't want to call it magic,\nit's just good practices or best practices behind the scenes\nin order to make these things work.\nWe're just kind of handling the deployment things.\nThe real magic, I think, comes with the app runner piece.\nSo we had to do some really interesting things for app runner.\nIt does have to be deployed as containers.\nWe do have to run a supervisor on there with multiple versions of node\nbecause if the node process gets blocked,\nthen it will start throwing different errors for you.\n\n\nAnd so we have to set sort of aggressive timeouts and things like that.\nAnd so we had to do some magic there.\nBut again, it's just something that you'd have to do yourself, honestly.\nI mean, that's the crazy thing.\nAnd so, but what we do for that is\nthere are thresholds at which it makes more sense\nto switch things to, you know, to a different service.\nAnd so like with app runner, for example,\njust as an, you know, if you are doing,\nI forget the number here,\nbut let's say you're doing about 50 million invocations a day.\n\n\nThat'll cost you to scale that and have some flexibility in app runner.\nThat might cost you $600 a month, something like that,\nsomewhere in that range.\nIf you do that on Lambda, it's going to cost you over $3,000 a month, right?\nSo there's a huge cost savings to switching to something like that.\nThe problem is, is that if you're building this\nand you're trying to use high-fidelity sandboxes\nor you want to preview it, you want staging accounts,\nobviously you pay more for the throughput\nbecause you need more resources running.\n\n\nBut do you want to run app runner in 30 developers AWS accounts\njust so that it's there so they can test against it or whatever?\nI mean, because that starts to add up, right?\nAnd so this is where, this is why we like the idea of the smart computers to say,\neven if it's running in Lambda functions in your preview environments\nand your developer sandboxes,\nbecause we can guarantee the fidelity between these different compute services,\nwe can switch that on to app runner when you're actually running it in production.\n\n\nAnd then the other thing we do too is we've eliminated API Gateway\nfor most of what we do and most of the stuff now runs through CloudFront.\nCloudFront is very good.\nYou can still use WAF if you need to do something like that.\nAnd honestly, API Gateway, most of the services in there, it's just overhead\nunless you're using it for quotas or some of those other things.\nSo we actually use Lambda Edge functions.\n\n\nWe do routing based off of different things.\nWe have some very cool stuff we do with static routes\nso that you never have to touch Lambda functions in order to load static routes.\nSo there's all these things that are just, they're complex.\nAnd I guess maybe that can lead me into the idea of patterns.\nSo obviously, you know, I love patterns.\nBig fan of these serverless patterns that run.\nWhat I found is most of these serverless patterns that are complex ones\nare very, very hard to manage, right?\n\n\nLike how do you tell the system, okay, now we're switching over,\nyou know, these routes are going to, you know, to app runner,\nthese routes are going to Lambda functions or to function URLs\nbecause we have to do some streaming with them now.\nAnd then these ones actually trigger based off of, you know,\nthese different SQS queues or whatever, right?\nSo it just gets really complicated and you got to write that all in CDK\nor CloudFormation or however you're doing it or Terraform.\n\n\nIt just gets really, really hard to do.\nAnd the cognitive load there is huge.\nSo maintaining, so it's sometimes easier to maintain simpler patterns\nbecause they're easier to grok and easier to put into these,\ninto these IEC documents.\nWhereas we look at and we're like, you know what,\nthe patterns themselves are actually much more complex\nand more complicated and really hard to manage.\nSo if we can manage these really complex patterns for you,\nthen not only are we doing exactly in a sense what you would be doing\nif you were writing these patterns yourself,\nbut we're doing them better because our system can manage\nand automate much more of it for you.\n\n\nAnd not only that, but we can learn from every single person using our system, right?\nSo we see how does this pattern actually work when it gets 60 million requests per day, right?\nHow does this pattern work when it only gets, you know, 10 requests per hour?\nLike what are the cold starts here?\nLike would it make more sense if we did this versus that?\nAnd we can try those things.\nWe can experiment, we can change memory settings.\n\n\nIt's all kinds of stuff that we can do to optimize those workloads.\nAnd you benefit from it as a user of this.\nAnd that's why we kind of joke a little bit and I'm not,\nit's kind of a joke, but not really a joke\nwhere some people have called us like an autonomous platform engineer.\nLike essentially, like what we do is we are a serverless expert, right?\nOr the platform's a serverless expert and you say,\nhey, here's my code, make it run as efficiently as possible in the cloud.\n\n\nAnd somebody goes and writes all that cloud formation and all the, you know, whatever for you\nand deploys it.\nLike that's essentially what our service does and then optimizes it over time,\nwhich I think is, which is really interesting.\nBut yeah, and you know what, the other thing I wanted to mention too,\nbecause I want to give credit to Yantray for this too.\nLike he pointed out a long time ago some of the cost changes for other services.\n\n\nSo it's not just compute, right?\nSo if you're running, I forget what his numbers were,\nbut it's like thousand requests per second with SQS that cost you like $1,800 a month, right?\nSo it gets really expensive to run SQS when you're doing that kind of throughput.\nBut if you switch to Kinesis,\nwell, then it's like $30 a month because you only need six shards or whatever it is, right?\nSo the question is, do I write my application to use Kinesis\nassuming that I'm going to have this type of throughput\nor do I write my application using SQS\nbecause it's going to cost me nothing in the beginning?\n\n\nAnd so smart compute is just one aspect of this switching piece for us.\nWe look at it and we say,\nthere's no reason why you should have to choose Kinesis over SQS\nor maybe EventBridge for certain things.\nLike why not just write your use case,\nexpress your use case and we'll run it as SQS or whatever,\nyou know, when it's in your developer environments\nand these or you're not getting a lot of traffic.\n\n\nBut as soon as you start getting traffic\nand there's a breaking point where it makes sense to switch to a different service,\nwe can automatically set up Kinesis,\nstart routing anything from that Kinesis to the same place that is feeding off your SQS queue,\nthen start changing the producer so it's sending it to Kinesis\nand then once the SQS queue is drained, go ahead and remove that SQS queue.\nWe can do that for you all in one without you even doing anything.\nIt just happens and it works as opposed to you having to,\nyou know, do six CloudFormation deployments in order to make that work.\n\n\nEoin: Yeah, I can certainly see the benefit of that and I think it's one of the challenges we have all the time is,\nyou know, you're aware of all of these different services,\nbut to really understand all of their characteristics\nand their delivery method and the latency and is it at least once or whatever\nand then also understand, okay,\nwhat are the cost trade-offs and mix all these into the one thing,\nit's pretty difficult.\n\n\nSo I can really see where Ampt is going to help with that.\nI guess as well, you know, you've got a very smart team,\nclearly very capable of delivering all this stuff.\nI guess one of the challenges then is you've got all these different challenges out there in the cloud,\ndifferent perspectives on what needs to be done,\nprobably a long list of feature requests and a roadmap.\nHow do you get a focus for Ampt?\nWhat's your kind of North Star?\nDo you have like a specific target market, the kind of application that you're trying to target\nor specific challenges you're trying to solve\nor is that something that kind of evolves as you see customers\nand understand their pain points?\n\n\nJeremy: Yeah, I mean, we're very big on listening to customers and what their needs are.\nI mean in terms of a sort of a North Star, you know,\nthe goal here is focusing on web applications.\nI mean, I think that's the big thing.\nIf you're building some sort of IoT system in the background or you're, you know,\nyou've got, I mean, we can technically support that.\nBut you know, if you're trying to build some massive machine learning thing,\nor like we're just not competing with those right now.\n\n\nI think what we see is that there are a lot of people that are doing interesting things out there.\nNow with AI, we just launched our Gen AI integration with Bedrock.\nIt's early, it's beta, but it's interesting where it's like,\nhow do you just give people the ability to build this stuff very, very quickly?\nAnd when I say people, I mean, I do mean developers.\nI mean that our focus is on developers.\nI think long term like we see a vision where we can say,\nyou know, amped is this thing that you can just buy off the shelf\nalmost as a platform engineering team or as a larger enterprise and say,\nhey, I want to give these developers the ability to do this stuff.\n\n\nThat's talk to compliant and PCI compliant and follows all these rules and you know,\nsecure and stuff like that and I don't have to worry about that.\nI just, you know, all of that work is done for us and we just kind of monitor it to make sure that,\nyou know, people are doing what they're supposed to be doing.\nBut like essentially that out of the box solution is the longer term vision.\nSo, you know, we are looking at it now as to say, you know,\nwe could build a million different things.\n\n\nWe could go a million different ways.\nYou know, obviously, you know, there's a lot of hype around AI\nand we felt that that was important to get something like that in.\nWe're working on a whole bunch of partnership stuff\nso that you can easily connect, you know, memento or Mongo or those things.\nYou can do that now.\nYeah, I mean it just we, you know, you just do it through our parameters,\nbut trying to make those a little bit more official\nand a little bit easier to do and also manage the, manage the authentication for you.\n\n\nSo that's something you don't have to think about.\nBut yeah, I mean in terms of the feature sets that we're trying to do is focus on the ones\nthat we think move the needle for customers.\nLike what are the biggest frustrations they have and so forth.\nAnd, you know, the abstraction layer that we've built has been,\nwe've tried to take the approach of like, we're not trying to necessarily mask AWS here.\n\n\nWe want you to know AWS exists and that you're using AWS.\nAnd I think actually that's one thing you might have mentioned in your episode 100\nwas that we're like a serverless pass or like a serverless pass.\nSo the funny thing is, is we've been trying really hard to tell people we're not a pass, right?\nLike we don't want to be, we don't want to host your application.\nWe want to manage it for you\nand give you all the tools you need to interact with AWS and manage it on your behalf.\n\n\nBut we don't, we don't want to ultimately host it.\nWe don't want to own your application.\nIt's, that's your stuff.\nLike we're just trying to make it easier for you to get your application into the cloud\nin a way that, you know, people who have, you know, giant platform engineering teams can do that.\nSo, you know, we're trying to focus on the use cases that our customers have.\nIt's mostly around full stack Node.js applications.\n\n\nYou know, we're not focused on enterprises right now at all.\nWe're trying to focus on, you know, the startups and the agencies that are trying to build things,\nyou know, for their customers.\nAnd I think really the big thing is, is, you know, we live in a time right now.\nAnd this is the time I thought serverless was going to bring.\nI thought this was the sort of revolution that serverless was going to bring.\n\n\nIt was going to democratize, you know, application development.\nAnd I think it has to some degree in that people who, you know, have a couple hundred bucks\ncan go ahead and build something that, that scales and has all kinds of amazing features,\nthings that took us months and months and months to engineer, you know,\neven in 2009, 2010, things like that.\nYou had massive engineering teams to build these things.\n\n\nNow you can have one or two people that can build something pretty amazing,\nyou know, in just a few days.\nBut it does require a fair amount of skills\nand a fair amount of knowledge in order to be able to do that.\nAnd I think we live now, especially with AI\nand some of these other things that are happening where it's like,\nI don't care if you're, you know, in your college dorm room like I was or, you know,\nyou're a multi-billion, you know, multinational billion-dollar corporation,\nyou know, the ability for somebody to express their idea\nand see if it changes something for the good, hopefully for the good.\n\n\nYou know, that's something that I think that we need to continue to lean into.\nAnd that's why I love, I mean, I love platforms like Vercel and, you know,\nFly.io and some of these other ones that make that experience very, very easy\nfor people to just kind of get started.\nI think the problem is that for companies that, you know,\nor for when it goes beyond that, when you get to that graduation problem,\nand then people start thinking like, I actually kind of want to be on AWS or GCP\nor one of these other ones, you know, and that's one of the things that we're trying to do\nwith Ampt as well as to say, look, we want you to be able to get started easy,\nyou know, easy, build anything you want to build, but then also not worry about graduation, right?\nLike if we catch the next Uber really, really early and it's just, you know,\nfive people in a garage, you know, I would like to think that the way we deploy their app to AWS,\nnot only will scale as the throughput scales, right, and the patterns will adapt and evolve,\nbut we'll be able to support them, you know, through the lifetime of that business\nbecause if they were to go and rebuild it themselves on AWS,\nthey'd just do exactly what we were doing, but probably not as good\nbecause of all the experience and the benefit we have of seeing all the other customers use the platform.\n\n\nEoin: Is it the plan then for, given that Ampt is not a pass per se,\nto let people kind of run Ampt in their own AWS organization,\nfull kind of visibility over the high-level abstraction,\nbut also the low-level bits?\n\n\nJeremy: Yeah, yeah. So actually that, so this is something that we are trying to figure out.\nAnd again, we just launched, right, just over a month ago, right?\nSo, you know, we've been experimenting with trying to make the platform do what people need it to do.\nHow we deliver the platform to people,\nthat's something we've been experimenting with and trying to figure it out.\nSo we do have some customers where we deploy directly to their AWS accounts.\n\n\nThat's exactly what we would love to do.\nWe have our control plane that allows us to do all the AWS account creation and setup and so forth.\nWe have a way that we can plug these things in, we call them providers,\nbut essentially we can plug in your AWS organization\nand we can spin up and tear down AWS accounts for you in your organization.\nOf course, orgs you have to apply for quotas\nand there's some other things like that that you've got to deal with.\n\n\nBut, so that's one of the things that we would absolutely love to do and have that as an option,\nespecially for larger customers that want to manage all that themselves.\nWe'll just spin up and tear down these accounts for you and do all the deployments.\nAnd everything, like I said, everything runs within each environment.\nSo even the build step actually runs in the environment.\nWe did this for isolation purposes as well,\nso that we weren't building them on some central system\nand then sending them off to your environment.\n\n\nSo there's a bunch of security that is built into that.\nFor other accounts though,\none of the things that we didn't want to do is burden people with having to set up AWS accounts.\nSo even if you, you know, so if you want to just deploy, you know, your application quickly\nand test it and whatever, we're more than happy to manage those accounts for you\nand own those accounts and then just sort of bill through and you pay for whatever your usage is.\n\n\nBut we would love to get to a point where, you know,\nI think a lot of people will fall into the category where it's like,\nwe want to own our production account.\nLike we want that to be in our AWS.\nAnd again, if you have like 10 apps in Ampt, you know,\nwe might be managing 40, 50, 60 AWS accounts for you right now.\nSo you might want to set up multiple production accounts.\nIt depends on how you want to do it.\n\n\nBut essentially we do want to pass this through and let somebody,\nwe want to be a deployment platform, right?\nIf anybody, you know, maybe to answer this question, who do we compete with?\nWe're not competing with Vercel.\nWe're not competing with, you know, Netlify or Fly.io.\nLike we think we compete with Pulumi and Terraform.\nLike we're a different way to get your code into the cloud, right?\nPulumi and Terraform and CDK, all those are ways in which you define infrastructure.\n\n\nAnd then of course you have to set up CI, CD and some of those other things.\nWe're trying to capture all of that and say we're an alternative to Pulumi\nand an alternative to Terraform that really reduces that cognitive load for you.\nSo that's ultimately where we want to get to is let you deploy to your own AWS accounts.\nWe also don't want to burden somebody with having to do that if they don't need to.\n\n\nSo we'll find the right way to balance that, I think.\nAnd then the whole point is that, you know, we have some internal connectors that we've developed\nthat we haven't sort of made available to our customers yet.\nBut the goal is to say if you want to spin up a bunch of AWS accounts with Ampt\nand you've got some Ampt applications running, but then maybe you have another account,\nyou know, that you're running that has some bespoke machine learning thing\nor whatever you're doing in there, and you want to be able to connect to the database\nor interact with that, you know, that we would just use OIDC or something\nto generate temporary credentials from all your environments as you interact with those, right?\nSo we don't think we're going to own 100% of your workloads necessarily.\nI mean, we can for certain companies.\nBut for ones that want to expand, like we really just want to be a partner in your AWS journey\nand make sure that you can do the things you need to do.\nAnd as much of that heavy lifting as we can get rid of for you or the undifferentiated heavy lifting,\nlike we want to fill that gap for you and, you know,\nand just get you where you need to be as fast as possible with the best developer experience possible.\n\n\nLuciano: Yeah, let's change a little bit topic before we close off.\nI think we are going close to the end.\nI guess there might be people listening to us that are thinking about,\nI don't know, using all this technology that we talk about every day with AWS,\nand serverless in general, and build startups, build companies, build ambitious projects.\nAnd it's something that you have done multiple times based on what you're telling us today.\nAnd I guess the generic question is like, do you have any piece of advice,\npiece of wisdom, encouragement that you want to give to listeners that might be falling into this category of people?\n\n\nJeremy: Yes, I would say move to the mountains and be a goat farmer and just get away from all of this stuff\nbecause it is complex.\nBut no, I mean, I think you have to, you know, look, technology is what everything is.\nI mean, every company you work for is a technology company or a software company now, right?\nI mean, pretty much everybody, you know, and so whether you're working in a startup that's building another startup to work,\nto solve some dev problem for people that are building,\nI mean, there's a million different things, but if you look around,\nyou'll see that every organization out there is trying to solve some sort of technological problem.\n\n\nThey're using the cloud or most of them are using the cloud to do it now.\nIt would very much so behoove you in your career to take it seriously and understand that, you know, the cloud,\nI think there's no better place to be building applications and in the cloud.\nOn-prem, things like that, I'm sure some of it will still exist,\nbut my advice is, you know, you've got to start, you've got to pick a couple of technologies and go deep on those.\n\n\nI think that a lot of people talk about the T-shaped engineer, you know,\nit's like you have a lot of sort of low-level knowledge or high-level knowledge, I guess, of a bunch of things,\nbut then you go really deep on one.\nSo, you know, I would say you don't need to know Rust and Python and JavaScript and Go,\nand you don't need to learn all these different languages, right?\nLike there's a bunch of popular languages out there.\n\n\nIf you're doing data stuff like focus on Python and stuff like that,\nif you're doing more front-end, you know, obviously JavaScript and JavaScript is still a great language.\nWe're in a JavaScript renaissance again with BUN and Dino and all of these front-end frameworks, right?\nSo everybody's, when people count out JavaScript, I'm like, what are you doing?\nLike JavaScript is going to be here in a hundred years.\n\n\nPeople are still going to be writing JavaScript for some reason.\nEven when, you know, I think AI is going to code in JavaScript just because it's like we can't get away from it.\nPeople love it for some reason. They love to hate it, but they also love it.\nSo, but in terms of AWS, you know, you've got to start looking at some of these different services\nand obviously whatever your role is at the company you're at,\nit would be very helpful to pick something that is, like DynamoDB is an interesting one,\nI think that is worth, you know, sort of focusing some time on because there's a lot of things you can do there.\n\n\nBut I mean, just, you know, just Lambda in general.\nI mean, honestly, Step Functions is one of those things where I think you could get a PhD in Step Functions now\nbecause it does so much, right?\nSo, you know, so my advice would be pick a couple of these things that make sense that all kind of work together\nand really focus on learning the ins and outs of those.\nAnd then get a high, you know, get a high level knowledge of all these other things.\n\n\nBe aware of what these other things do.\nYou don't need to be an expert in everything.\nYou can't be an expert in everything.\nBut I would say, you know, focus on a few things that really, you know,\na couple of interesting services and go deep on those.\nAnd again, if you want to get noticed, start writing about it.\nI know not everybody is sort of built for that, but write about it, talk about it,\nyou know, post about it on Twitter, you read an interesting article or X or whatever we're calling it now, right?\n\n\nLike, I mean, feel free to, you know, to share your thoughts.\nAnd the last thing I'll say about, again, sharing thoughts.\nSo many people ask me this, they're like, yeah, but I just read an article the other day\nthat literally said exactly what I was going to say about a particular topic.\nAnd I always say to them, I guarantee you, you were going to say it a little bit differently.\nAnd when you say something a little bit differently,\nand that's maybe why I talk so much, I try to explain things like eight different ways.\n\n\nIt's just because of the way my brain works.\nBut when you explain something just a little bit differently,\nthat can click with somebody in a way that the other article didn't, right?\nAnd so you maybe you present the way that you do your demo differently.\nMaybe your example is different and it connects better with somebody's, you know, current situation or whatever.\nSo I would say, you know, I'm happy to read an article about, you know, whatever it is,\nyou know, turning off a dev machines with a Lambda function every night, scheduling the shutdown.\n\n\nI've read a hundred of those at least.\nI'm willing to keep reading those to see if there's something else in there that sparks something.\nAnd again, content goes out of date very, very quick.\nWe talked about this idea of, you know, of how quickly these, or we talked about all these different features.\nContent goes out of date very, very quickly, right?\nSo if somebody wrote an article three months ago,\nit's very possible that you could write the same article today with new information that would change,\nyou know, that would change someone's perception of it or help somebody out in a different way.\nSo again, I know not everybody likes to write and share.\nIt can be scary to put yourself out there,\nbut I would say it is definitely a massive thing that can help with your career.\n\n\nEoin: Cool. And I guess the other piece of advice, going back to your first point, is don't use ChatGPT too much.\nBecause you're now a human GenAI detector and it won't get in Off-by-None.\n\n\nJeremy: Don't use it for content creation.\nDefinitely use it for code.\nI mean, I think that I've seen...\n\n\nEoin: Or for reviewing your content, right?\n\n\nJeremy: Yep. It's great for checking grammar and some of these other things.\n\n\nEoin: So I think the future of Ampt looks pretty promising.\nI'm really curious, like this idea of a smart compute and kind of almost self-healing\nand self-optimizing infrastructure that moves from one service to another based on cost\nand all these other things is really good.\nI'm also kind of curious, will somebody eventually invent some sort of data sink\nwhere I can just put data into it and it doesn't matter like what my schema is.\nAnd it just kind of figures out based on how I pull the data back out, how to store it.\nWhere do you think it's going to go in the next three or five years?\nI mean between Ampt but also AWS and other players.\nAre there any kind of crystal ball moments you have\nwhere you can see maybe where this is heading?\n\n\nJeremy: Yeah, I mean, I think that as much as I've tried to fight it in some way,\nI think AI is going to play a huge role here.\nAnd I know everybody talks about this, but I think it's going to be a little bit different.\nIt's going to be applied differently than I think a lot of people are thinking about it now.\nEverybody's using it as like code completion and some of those things.\nI think those are all great use cases.\n\n\nI think this idea of AI somehow figuring out, you know,\nwhat's the best way to deploy infrastructure or even to optimize things like data structures.\nI think that's going to be part of it.\nI think there's going to need to be heuristics and human review and some of that.\nBut yeah, I mean, I think that what we're seeing now is an explosion of competition to AWS\nin very, very small pieces.\nThe serverless database space race is what I call it is this idea of like,\nyou know, between Zeta and PlanetScale and the new one just launched the other day,\nthe Nile or Nile.dev, whatever it is,\nlike there's just more and more of these different services are coming out that are all disparate, right?\n\n\nAnd I think that it's a good thing that somebody's trying to solve something differently,\nbut at the same time, I see a lot of larger companies\nand a lot of enterprises are just focusing their efforts back on the things that AWS provides.\nSo I do think that there'll be a consolidation.\nI think that if anybody comes up with a really interesting innovation other than just,\nyou know, we can scale your database a little bit better.\n\n\nI think that, you know, there's plenty of room here.\nThere's plenty of space for people to experiment,\nbut I would like to see some consolidation back into a few of the major players,\nnot because I don't like competition or the diversity of it,\nbut I like the idea of the centralization of these sort of systems.\nAnd I think AWS is the platform that most people are going to be building on.\nAgain, I get it, GCP and Azure out there as well, Oracle Cloud.\n\n\nCloudflare is doing some pretty amazing things.\nBut I do think that again, Cloudflare is still a bit at that surface level almost\nwhere it's like some of the more deep applications that you would be building\nare just things that Cloudflare is not going to support, at least not right now.\nI mean, I hope, you know, that it expands in the future.\nBut so again, Crystal Ball, I don't, you know, all I can tell you is any prediction I have ever made,\nI don't think has ever come true, right?\n\n\nSo that's why I don't gamble.\nThat's why I don't bet on sports or things like that\nbecause I have no idea what the outcome is going to be.\nAnd I'm not sure I trust myself enough to do it.\nBut I will say where I hope things end up.\nAnd I really, really like this idea of self-provisioning runtimes.\nI think it is something that is needed.\nAnd I think it's something that, you know, is just a matter of time.\n\n\nI think it's inevitable.\nAnd the reason why I say that is because, I mean, how many people now are like,\nhey, you know, I don't like using Rust or one of these other languages\nbecause I really like to malloc my own memory, right?\nLike I really want to know how much memory this is being used there.\nOr like, don't run an automatic garbage collection for me.\nI'll tell you when I want my automatic, when I want garbage collection to be run.\n\n\nLike, there are just so many of these things that we've abstracted away.\nWe don't write ones and zeros anymore, right?\nWe're not doing machine code.\nWe're writing an abstraction.\nAnd every programming language right now is an abstraction.\nAnd I feel like you take something like the CDK\nand that feels like an abstraction on top of CloudFormation\nand, you know, and the idea of IAC.\nBut it still feels very much like you're choosing primitives.\n\n\nYou're still making a lot of decisions.\nYou're still, you know, you're, I still feel like it's machine code for the cloud.\nAnd so we always get this argument of people who are like,\nwell, I need more control.\nAnd it's like, well, the people who are yelling about control\ndon't ever seem to change any of their default settings, right?\nLike how many of your Lambda functions are still at, you know,\none gig or whatever, or yeah, one gig.\n\n\nYeah, is it a gig, a meg, whatever it is, right?\nYeah, so, you know, like it's set to 1000.\nYeah, 1024 megs, right?\nSo how many people just never change those settings?\nOr don't even know that, I'll use this example again,\ntumbling windows exist in Lambda, you know what I mean?\nLike they just don't know these things exist.\nThey don't make these changes.\nThey talk about control.\nAnd I think that we need to get to a point where we say,\nI think the cloud is smart enough to figure out how to route a,\nyou know, an HTTP request to some thing of compute\nthat connects to a database that can load that with some sort of guarantees involved.\nAnd that doesn't have to be configured manually in a configuration file\nfor us to figure that stuff out.\nSo I don't know where self provisioning runtimes are going to go,\nbut I do think that we are going to see a revolution in the near future\nwhere, you know, there's just going to be,\nthere's too many people building in the cloud.\nAnd if we let AI solve this, I'm very, very nervous about AI deploying stuff to the cloud on our behalf.\nSo I think there's got to be a better way.\nI think self provisioning runtimes are the answer to that.\n\n\nLuciano: That's really exciting.\nI definitely don't disagree with that prediction,\nbut yeah, I will probably talk again in three, five years time\nand see what is the status of things then.\nSo before we wrap things up,\nis there any link or place you want to share for people to follow you\nor to basically follow up on everything you just shared with us\nand maybe deep dive on some of the topics?\n\n\nJeremy: Yeah, actually, you know, I would love it if people check out Ampt\nand give it a try.\nIt's getampt.com (and that's A-M-P-T) dot com.\nYou can check out my blog.\nI don't write there as much as I wish I did,\nbut jeremydaly.com, D-A-L-Y dot com.\nAnd then I'm on X, jeremy_daly,\nand obviously offbynone.io.\nYou know, you can find all my stuff, all my links usually,\nat those different places.\nBut yeah, I mean, I love, again, I love hearing from new people.\n\n\nI love meeting new people and hearing new perspectives on stuff.\nAnd if you've got articles to share, please, you know, send them to me\nand I'm happy to take a look at them and share them in the newsletter\nif it makes sense to do that.\nAnd yeah, but really, really want to get...\nWe're changing, we know we're changing a paradigm here with Ampt\nand we know that it's, you know, it's going to be a slog\nto get people to understand why, you know,\nI don't think it's hard to get people to understand why it's different,\nbut like coming, you know, the objections around, you know,\ncontrol and some of these other things are certainly, you know,\nthe things that we get.\nBut we've got some customers that we think, you know,\nwell, they've told us that we're revolutionizing the way\nthat they're building applications in the cloud.\nWe're saving them a tremendous amount of time and so forth.\nSo we're excited about the possibilities of this.\nSo yeah, so the more feedback we get, the more we can make this,\nyou know, make sense for people to use will help this, you know,\nhelp this movement and hopefully make, like we said,\nsort of democratize the cloud for even more people.\n\n\nLuciano: Yeah, we'll make sure that all the links you share\nare going to be available in the show notes.\nSo for people watching this, listening to this, don't worry,\nyou'll get all the links there in the description.\nJeremy, it has been a real pleasure to have you on the show.\nSo thank you.\nThank you so much for joining us today.\nAnd thanks everyone for tuning in.\nWe look forward to reading all your comments.\nAnd so definitely check out the chat, check out the comment section\non YouTube and share all your opinions.\nWe are always reading all of that and it's always amazing to have\nconversation following up every episode and see what people\nactually think about and what resonates with them.\nSo thanks again everyone and we look forward to catching up with you\nin the next episodes.\nBye.\n\n\nJeremy: Thank you so much.\n"
    },
    {
      "title": "103. Building GenAI Features with Bedrock",
      "url": "https://awsbites.com/103-building-genai-features-with-bedrock/",
      "publish_date": "2023-11-10T00:00:00.000Z",
      "abstract": "In this episode, we discuss how we automated generating YouTube descriptions, chapters and tags for our podcast using Amazon's new GenAI tool: Bedrock.\nWe provide an overview of Bedrock's features and how we built an integration to summarize podcast transcripts and extract relevant metadata using the Anthropic Claude model. We share the prompt engineering required to instruct the AI, and details on our serverless architecture using Step Functions, Lambda, and EventBridge.\nWe also discussed Bedrock pricing models and how we built a real-time cost-monitoring dashboard. Overall, this automation saves us substantial manual effort while keeping costs low. We hope this episode inspires others to explore building their AI workflows with Bedrock.\n\nAWS Bites is brought to you by fourTheorem, an Advanced AWS Partner. If you are moving to AWS or need a partner to help you go faster, check us out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources:\n\nThe GitHub repository with the code for the AWS Bites website:\nEpisoder: the open source project we created to handle this Bedrock-based automation\nPodwhisperer: out other open source automation that creates accurate transcripts for our podcast\nAmazon Bedrock Workshop\nUsing generative AI on AWS for diverse content types (workshop)\nDeploying a Multi-Model and Multi-RAG Powered Chatbot Using AWS CDK on AWS\n\n",
      "transcript": "Eoin: The hype around generative AI is dying down now,\nbut we are beginning to see a growing ecosystem\nand lots of real-world use cases for the technology.\nWe've just built some features using Amazon Bedrock\nthat help us to produce this podcast\nand save some manual effort.\nSo today we're going to talk about\nhow we automate the process with Bedrock,\nand we'll give you an overview of Bedrock's features.\nWe'll also talk about how to use it\nand share some tips and code on cost monitoring.\nMy name is Eoin, I'm here with Luciano,\nand this is the latest episode of the AWS Bites podcast.\nThis episode is sponsored by fourTheorem.\nIf you're looking for somebody to accompany you\non your cloud journey and need an AWS advanced partner,\ncheck us out at fourtheorem.com.\nNow, before we get started and talk about Bedrock\nand how we use it, let's talk about the manual process\nor semi-manual process we did before Luciano.\nWhat was the drudge work we wanted to remove?\n\n\nLuciano: Yes, so we are talking about the work that is involved\nwhen you create, in this case, a podcast,\nbut I think it applies to any video\nthat you want to publish on YouTube.\nSo you create this video and when you upload it,\nyou need to provide a bunch of additional information,\neffectively metadata that makes your content\nmore discoverable and provides all the necessary\ncontext information to your viewers,\nbut also YouTube to make the content searchable.\n\n\nSo we are talking, of course, about descriptions, tags,\nbut also other things like the chapters that you can add.\nYou probably have seen these in our videos\nor in other YouTube channels where you can add\na specific type of text that says this section,\nstarting at this point, is about, I don't know, introduction.\nThis other section is about why do we need serverless?\nAnd what happens is that YouTube is gonna use\nthat text information in the description\nto split your video into chapters\nand you will have those chapters as an easy way\nof jumping through different parts of the video.\n\n\nSo all of this information is something\nwe take the time to create for every single video\nwe upload because I think it makes them more discoverable\nand it gives our viewer a better experience,\nbut of course there's a lot of extra work\nthat we have to do for every single episode.\nAnd the process that we have been doing so far\nbefore the feature that we are gonna be talking about today\nmore or less looks like this.\n\n\nSo we have already some automation,\nwhich is called PodWhisperer.\nWe talked about that before and we'll link\nto the previous episode talking about that.\nAnd PodWhisperer, in short, what it does is able\nto extract a transcript of everything we said\nduring a specific episode.\nAnd that information is something we can use\nin different ways.\nSo all of that stuff is done through a Step Function.\nAnd again, you can check out that video.\n\n\nWe'll give you some other details later.\nBut other than that, the other piece of information we have\nis our notes that we keep in Notion.\nSo we have already some kind of groundwork\nthat we can use as every time we want to create a title\nor a description or tags,\nwe can look at all of this information\nwithout having to watch the entire video again\nand decide, okay, what kind of title do we need?\nWhat kind of description and so on.\n\n\nI admit that sometimes we use ChatGPT just to summarize,\nfor instance, all our notes and give us maybe a draft\nof the description or some ideas for titles.\nBut then there is still a lot of manual work\nin trying to adjust that result\nuntil we are happy with the final outcome.\nAnd when it comes to creating YouTube chapters,\nit is a very manual process.\nWhat I used to do so far is I just watch the final version\nof the episode at 2X,\nand then I keep track of all the points where I see,\nokay, now we are changing topic,\nand this is probably worked a dedicated chapter.\n\n\nSo I take track of the timestamp\nand I create the format that YouTube expects.\nSo we do all of this stuff,\nand then when we upload the video on YouTube,\nwe need to copy paste all of that information correctly.\nAnd some of this work, I think it's nice to do it manually\nbecause it's good that you can add\na bit of a personal touch.\nAnd you might have seen that we are taking\na bit of creative license when it comes.\nI don't know, sometimes we take a style\nthat looks like Tesla, the scientist.\nSometimes we think about, I don't know,\nlet's give it a medieval touch\nbecause maybe we are using some medieval artwork.\nSo it is nice that you can take the kind of freedom\nand make it a little bit more original.\nBut at the same time,\nwe thought that using some generative AI\ncan help us to do all of that work faster\nand with less manual work.\n\n\nEoin: When Bedrock came out recently,\nthis kind of was the inspiration.\nAnd especially, I'm gonna link some tutorials\nin the show notes because one of the notable things\nabout Bedrock is that when Amazon announced it,\nthey created lots of really good tutorials\nand workshops and example repos,\nsome of which are like really, really impressive content.\nI don't think I've ever seen that for a new service before.\n\n\nBut let's first talk about Bedrock.\nSo Bedrock is Amazon's new service\nfor building generative AI applications.\nIt's quite a bit different to SageMaker\nin terms of experience where SageMaker\nis designed to remove some heavy lifting,\nbut you still have to understand about containers\nand models and the model libraries like PyTorch, et cetera.\nWith Bedrock, it's a lot more managed high level service.\n\n\nAnd the idea of Bedrock is that it gives you access\nto third party foundation models\nfrom lots of different companies through a single API.\nAnd you just use one API to get a response\nto an instruction or a prompt.\nAnd the models that are available on Bedrock right now\nare from Anthropic, the Claude large language model.\nSo that's like a good one with a focus on safety,\nsafety and security and non-toxic data\nand safe data from reliable sources is their focus there.\n\n\nAnd then you've got Cohere command model\nfor cases like customer support and business scenarios.\nYou have the AI21 Jurassic large language model,\nand then you have Amazon's own Titan models,\nwhich are like general purpose large language models.\nAnd those ones are really aiming to be the lower cost ones\nwhere you're just really trying to cost optimize.\nThey're not fully available yet.\nNot all of the models are available yet.\n\n\nAnd then if you're doing image generation,\nyou have the stable diffusion models available there as well.\nThere are also other models planned\nlike the Facebook/Meta Llama 2 model\nis also supposed to be coming soon\nand a lot more expected to arrive.\nSo if you want to use a model that isn't on Bedrock,\nbut is available elsewhere, like on Hugging Face,\nyou would need to really host the model somewhere else\nlike on SageMaker in the more traditional way.\n\n\nBut going back to the ones that are available on Bedrock,\nthen if you're just trying to start to build practical,\nlike chat features or text summarization,\nimage generation features, build it into your application,\nI would just say to people\nthat I think it's a lot easier than you would expect.\nAnd there's very little work you have to do.\nIt ultimately depends on your use case.\nAnd if you need to pull in additional data,\nbut generally for the kind of use case\nwe're describing here,\nit's really quite a simple addition.\n\n\nSo it allows you to quickly use these models\nand then add things like chat applications,\ntext summarization, knowledge search\nbased on additional data you might have in documents,\ndoing text to image creation\nor image to image creation as well.\nNow there's a very small bit of setup,\nwhich is that because you're using these third party models\nbefore you go and use them,\nyou have to go into the console\nand go into Bedrock model settings\nand explicitly enable each model\nand agree to their end user license agreement.\n\n\nSo I guess there's because of the nature\nof these applications,\nthe fact that they're non-deterministic\nwhen you're using models and there's impact there,\nyou just have to get the legal sign off on those pieces.\nOnce you've done that, it's basically serverless.\nSo you can start making requests without having to wait\nfor any infrastructure to be set up.\nNow, if you're comparing Bedrock to other things\nlike maybe OpenAI APIs, for example,\nthe idea here is that with Bedrock,\nthere's more of a focus on privacy and security.\n\n\nSo the big difference compared to other alternatives\nis that your data is encrypted.\nIt's not shared with other model providers.\nYou can keep your data and traffic within your VPC as well\nusing PrivateLink.\nSo you've got encryption and your data isn't gonna be used\nto train the models further\nand that's part of the agreement that you get.\nSo you can use those models all as is,\nbut there's also a whole other set of APIs there\nfor fine tuning them if you need to\nusing your own training data on S3\nand simplifying that as well.\n\n\nBut we're just talking really about using foundation models\nfor inference, just for getting a response back\nand without any special training or any kind of fine tuning.\nSo for our use case,\nwe decided to use the Anthropic Claude V2 model\nbecause it supports the largest input size by far.\nIt supports up to 100,000 tokens,\nwhich usually equates to around 75,000 words.\nAnd we want to be able to give it a full episode transcript\nand our episodes can be anything\nbased on a historical evidence between 2000 and 30,000 words.\nSo that might be 40,000 tokens.\nSo that's what we started with.\nLuciano, what's then the goal of the design of the system?\nWhat do we want it to do?\nWe've got the problem, we've got the model.\nWhat was our thinking from that point?\n\n\nLuciano: We already had a piece of automation.\nWe already mentioned PodWhisperer a couple of times\nand PodWhisperer is effectively a Step Function.\nSo it's a workflow that orchestrates different things.\nAnd eventually what it does, it creates a transcript for us.\nSo the idea was, okay, we have already the Step Function.\nWe have already a step that gives us something\nthat we can use as an input.\nSo what we need to do next is basically extending\nthat Step Function, taking that input\nand basically do more stuff with it.\n\n\nSo once we have the transcript, what we can do\nas the new steps that we introduce in this automation\nis basically we can create a prompt\nthat will instruct the large language model\nto generate few different things.\nOne is the episode summary, then the YouTube chapters\nwith precise timing and topic\nand a set of tags for the episode.\nI think here is worth mentioning also\nthat when we generate the transcript,\nit's not just the text, but we also keep time references\nto all the different bits and pieces.\n\n\nAnd this is how we are basically capable\nof doing chapters with precise timing,\nbecause we are giving the model\nnot just the text information,\nbut also the timing of every text occurrence.\nSo it can determine exactly which piece of text is said\nat which specific point in time.\nSo once we create this prompt that needs to kind of summarize\nall of this instruction in a way\nthat the model can really understand\nand give us the output we expect,\nwe need to pass the prompt.\n\n\nSo we need to actually make an API call to Bedrock.\nAnd we also need to give the full episode transcript,\nof course, because that's part of the context\nthat we need to give it.\nAnd after a while, when this request is processed,\nwe receive a response, we need to pass this response,\nand this becomes the next step in our integration.\nSo what we finally want to do is basically\nwe want to create a pull request\nto our website repository, which is something we manage\nwith a static set generator called Eleventy.\n\n\nAnd it's all managed open source\nin a public repository on GitHub.\nSo we will also have the link if you're curious\nto see exactly how we build our own website.\nSo we create this pull request,\nand this pull request contains all this information\nnicely laid out in the PR description.\nSo this is what we got from Bedrock,\nbut also contains the transcript\nthat we can incorporate in the website as well.\n\n\nAnd this is something we were doing before.\nSo the new bit here is that the pull request description\nwill contain all this additional information,\ndescription, chapters, and tags\nin a way that we can easily copy paste it into YouTube.\nAnd this way we're saving lots of time.\nOf course, we still take some manual time\nto review everything and decide whether we like it or not\nand add a little bit of personal touch,\nbut I think that's saving us already a lot of time.\nI think one of the interesting bits here,\nwhich at least when we started to work on this\nwasn't obvious at all to me,\nis the part where we defined basically\nthe prompt engineering.\nHow do we tell the model, what do we want,\nand in which format it should give it to us?\nSo do you want to talk a little bit more about that,\nor what?\n\n\nEoin: Yeah, the prompt syntax for every model is slightly different.\nFor example, for the cloud one we're using,\nyou need to specify like human colon,\nthen your instruction,\nand then a new line with assistant colon,\nand then finish it with another two new lines.\nThat's just the way that the model has been trained\nand expects input.\nBeyond that, it's kind of like trying to come up\nwith the right phrases and instructions\nand restrictions and examples\nso that it has the best chance of giving you\nthe kind of inference results you're looking for.\n\n\nAnd the way you can do that,\nyou can start off with a Bedrock playground\nin the AWS console,\nand you can type instructions there.\nThe API or SDK is really simple for models.\nYou're just doing an invoke model request.\nThat's what we're doing.\nAnd there's only a couple of parameters you need to pass in.\nYou can look at the documentation for the parameters\nthat you need to specify,\nand then it's just understanding how to format your prompt.\n\n\nSo for us, it's just a string with this human start,\nand then we're asking it the instruction.\nSo what we're saying is provide us with a episode summary,\nfirst person plural,\nand we're aiming for around 120 words.\nAnd then we say followed by 10 chapter summaries\nfor this following transcript JSON, right?\nSo we're gonna include the JSON in the instruction.\nAnd then we're also asking for the chapter summaries\nto be based off the timings in the transcript segments,\nand for those timestamps to be included\nexactly as they were, the same format from the segments.\n\n\nAnd we're also asking then for the tags,\nlike up to 20 relevant tags for the YouTube video.\nBut we're also doing this kind of,\nit's kind of a single shot inference\nwhere we're giving it an example as well\nof the output we want to receive.\nSo we're giving it a sample JSON\njust to show the structure that we're looking for.\nAnd when we run that, then about 20 or 30 seconds later,\nwe can get back our response.\n\n\nIt starts off with a bit of text and then the JSON.\nSo we just need to strip out the JSON and parse it.\nNow, you might wonder, this is a non-deterministic model.\nIt can generate all sorts.\nWill it always generate valid JSON?\nThat's something to be mindful of,\nbut in our testing, it has always generated\nexactly perfectly formatted JSON,\nand we haven't had any issue there\nbecause we're using that example in the prompt.\n\n\nSo then tying this all in briefly\ninto the total architecture,\nwhich you can see on the website, there's a diagram there.\nWe're using Step Function to orchestrate.\nIt's a very simple two-step Step Function.\nIt's triggered by S3 events in EventBridge.\nIt runs the summarization lambda function\nthat calls Bedrock with the prompt,\npassing in the full transcript\nand getting back the response, extracting the JSON.\n\n\nThen we pass that to the next step in the Step Function,\nwhich is our pull request lambda,\nwhich is the same one we had in the other project before.\nWe just refactored this into the new repo,\nand that creates the pull request based on that JSON\nand gives us that nice GitHub description.\nAnd that's it.\nSo I think it's pretty simple all in all,\nbut it's quite powerful.\nAnd the results, I think, so far look pretty impressive.\nAnd recently we did the interview with Jeremy Daley.\nWe got really a great amount of time to talk to Jeremy,\nbut the more time you have, the more effort you have to do\nif you're trying to create chapters.\nSo all of this automation really helps us\nbecause this podcast is only really possible\nbecause we've managed to find a format\nthat doesn't take too much of our time.\nWe do some preparation, we record the episodes,\nand then we try to keep the post-production\nprocess as lean as possible.\n\n\nLuciano: Of course, all of this stuff is not free.\nWe are using a service, we are using AWS.\nAWS is running servers and GPUs for us.\nSo of course there is a cost to it.\nSo what is the cost?\nAnd there are actually two different pricing models\nthat you can pick.\nOne is called provisioned and one is called on-demand.\nProvision, you basically pay per hour.\nAnd it's interesting enough\nthat it's not supported for all models.\n\n\nSo the idea is that you pay upfront,\ndecide on which terms you want to commit,\nand then it looks a little bit like a compute saving plan\nwhere probably AWS is allocating something dedicated to you\nand then you are paying on the hour\nfor that set of resources.\nAnd we actually didn't use this one.\nWe used the on-demand just because it looks more flexible\nand it's, I think, better for us while we are experimenting\nand we don't really expect to use it\nin large volumes anyway.\n\n\nAnd the on-demand is what you would expect\nas kind of a serverless offering\nwhere you pay per the amount,\nthe units that's being processed.\nAnd it varies a lot depending on the model you pick.\nFor instance, we pay 0.3 of one cent to two cents\nper model for text.\nAnd then the most expensive is the stable diffusion one,\nprobably because it's also the most expensive\nto run behind the scenes, and it's 7.2 cents per image.\n\n\nSo based on that, it might not be very obvious\nto understand, especially because we are not generating\nimages, but we are generating text,\nand text might vary a lot.\nYou might have very short text.\nYou might have very long text.\nAnd also it's not just the text that is generated,\nbut even the input that you provide.\nSo if you have longer episodes, you are providing more text.\nSo it's very difficult to do a prediction and say,\nwell, we're gonna be spending X per every episode.\nSo how did we reason about cost?\nWhat did we do to try to make our costs\na little bit more predictable?\n\n\nEoin: Because it's so difficult to understand this pricing model\nand it varies from model to model,\nand then the dimension is a bit strange as well.\nSo this pricing example you gave,\nyou mentioned 0.03 of one cent at the lower end,\nup to like one and a half, two cents.\nThat's for a thousand input tokens\nfor these different language models.\nSo what's a token?\nWell, it's generally roughly one word,\nbut the input, depending on if you're dynamically\ngenerating your input or if the output is extra long,\nyour price is gonna change.\n\n\nSo it's important to get more of a real-time handle on costs.\nSo what we did to solve that was we created a real-time view\nof our pricing using a CloudWatch dashboard\nthat is generated from a CDK application.\nSo this CDK application is in the same repo.\nYou can take a look at it and you can use it as an example\nto create your own Bedrock pricing dashboard too.\nAnd what we do is we hard code the pricing in there\nfor a cloud model, because unfortunately,\nright now it's not available via the Amazon pricing API.\n\n\nSo we just had to hard code it,\nbut then we can just generate widgets for a dashboard\nthat allow us to see a blink of an eye in real time,\nlike based on a minute of granularity update,\nwhat's the price for input?\nWhat's the price for output?\nWhat's the total cost over the last week,\nover the last hour, over the last day, whatever.\nAnd then we can see based on the number of invocations,\nwhat does it cost for the average episode to be summarized?\n\n\nAnd we have that dashboard, but we also have alarms.\nSo we can say, if this goes above $1 per hour\nfor three consecutive hours, then send us notification.\nSo we don't have to wait for our budgets to fire\nor for the end of day billing reports.\nWe got much more real time alerts.\nAnd so it's an interesting model that you could apply,\nbut I think it's particularly useful for this kind of stuff.\nBy the way, in case you're wondering,\nit costs around 13, 14 cents per episode\nfor us to do all this summarization.\n\n\nI think the cost is pretty good.\nWe've run tens, almost a hundred so far\nand haven't spent more than $5, I think,\nor something like that, just with all our testing\nand running the latest few episodes through this engine.\nIf anyone wants to get this up and running\nfor their Gen AI with Bedrock, just check out the repo\nand you feel free to use that code as an example.\nAnd I think that's it for this episode.\nPlease check out this episode or product.\nLet us know what you think.\nWe'd love to have others contribute to it,\nadd new features and give us some ideas as well.\nSo thanks very much for joining us.\nI hope you're really enjoying\nthose robot generated YouTube descriptions\nand we'll see you in the next episode.\n"
    },
    {
      "title": "104. Explaining Lambda Runtimes",
      "url": "https://awsbites.com/104-explaining-lambda-runtimes/",
      "publish_date": "2023-11-17T00:00:00.000Z",
      "abstract": "In this episode, we celebrate AWS Lambda's 9th birthday by taking a deep dive into Lambda runtimes. We discuss how Lambda works, compare official runtimes vs. custom runtimes, and explain when and why building a custom runtime might be worth the effort. We talk through how custom runtimes work, options for deploying them, and potential use cases where they could be beneficial over standard runtimes.\n\nAWS Bites is brought to you by fourTheorem, an Advanced AWS Partner. If you are moving to AWS or need a partner to help you go faster, check us out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources:\n\nThe original announcement of the AWS Lambda launch in 2014\nList of official Lambda runtimes\nLibrary implementing the Rust runtime\nLibrary implementing the Go runtime\nLibrary implementing the C++ runtime\nThird-party PHP runtime (Bref)\nThird-party Swift runtime\nOfficial documentation on how to build a custom runtime\nAn official tutorial on how to build a custom runtime\nList of all the environment variables available to a runtime\nLambda Response Streaming official docs\nOur previous episode on Lambda Response Streaming\nOn-demand Container Loading in AWS Lambda (paper)\nDeep dive on container support for AWS Lambda (Eoin's article)\nWhen is the Lambda Init Phase Free, and when is it Billed? (article by Luc van Donkersgoed)\n\n",
      "transcript": "Luciano: Happy 9th birthday AWS Lambda!\nYes, AWS Lambda was launched nine years ago this week.\nAnd to celebrate this birthday today, we're going to answer the question,\nwhat's inside a Lambda function?\nI don't mean your JavaScript or Python code, I mean everything around it.\nHow does Lambda work as a service?\nHow does it execute your code and integrate it with the rest of the AWS ecosystem?\nToday, we'll deep dive into the fascinating topic of Lambda runtimes.\n\n\nWe will discuss how Lambda works, what a runtime is,\nwe will compare official runtimes versus custom runtimes.\nAnd if you stick until the very end of this episode,\nwe will also share when and why putting the effort into learning a custom runtime\nand or building one might actually be worth your time.\nI am Luciano, I'm here with Eoin,\nand today we are here for another episode of AWS Bites podcast.\nAWS Bites is brought to you by fourTheorem,\nan advanced AWS partner.\nIf you're moving to AWS or need a partner to help you go faster,\ncheck us out at fourTheorem.com.\nLet's start by recapping what a FaaS serverless system is, how it works,\nand in general, how does that refer to AWS Lambda?\nWhat do you say?\nYep, Lambda is the FaaS or functions as a service, service within AWS.\n\n\nEoin: And it's an event-based system.\nYou write some code in the form of a function,\nthat function takes an event as its input and responds with a single response.\nIt supports multiple programming languages.\nSo as a client, you will send your function code\nand the event configuration to your cloud provider like AWS,\nand they will make sure to run your code when the event happens.\nAnd this is the magic.\nIt just figures out where to run your code\nand how to place that within their vast set of compute infrastructure.\n\n\nAnd it's also well integrated with lots and lots of other things.\nYour function can be used to connect or to extend different cloud services.\nA few examples of that, you can use a Lambda function with API Gateway\nto define the logic for your web requests.\nYou can use a Lambda function to process jobs from a queue like SQS\nand signal which ones have been processed correctly\nand which ones may have failed.\nAnd another example is you can use Lambda functions\nto define your custom GraphQL resolvers.\nBut there's lots and lots more beside that,\nwhich I think we've covered in lots of previous episodes.\nSo this magic, how does it work?\nWell, I think at the core of that is the concept of a runtime.\nSo what is a runtime and why do we need one?\n\n\nLuciano: So yeah, you said that the cloud provider needs to have some kind of infrastructure\nthat can use to execute the code when needed,\nso when a specific event happens.\nSo this infrastructure also needs to make sure\nthat all the information is passed correctly into the function.\nSo some kind of description of the event needs to be passed as an input.\nAnd then the function is going to do some magic,\nit's going to do some computation and eventually provide a response or an output.\n\n\nAnd the runtime also needs to collect that output and do something useful with it.\nFor instance, if there are integrations to be triggered,\nit needs to make sure it takes the output and use it to wire things together correctly.\nAnd of course, there might be errors\nbecause in the cloud there are always errors around the corner.\nSo if there are errors, the runtime needs to make sure it captures the error.\n\n\nIn some cases, there might be the possibility to retry the execution,\nso it needs to make sure the execution is retried.\nIf there are too many retries, eventually it needs to stop retrying\nand make sure the errors are communicated correctly to the user in the form of logs.\nSo the runtime has to do basically all this kind of coordination\naround the execution of a specific Lambda function.\nThere is also an extension system that exists inside Lambda,\nso the runtime is also responsible for integrating possible extensions.\n\n\nAnd this is something that you might have seen, for instance,\nif you use an external provider to collect telemetries on the Datadog,\nthey might be providing their own extension that you embed in the Lambda execution.\nAnd as your Lambda is running, they can collect all sorts of information\nand record it in the telemetry system so you can inspect it later.\nSpeaking of runtimes, there are generally two main categories.\n\n\nOne is built-in runtimes and another one is custom runtimes.\nWhen we talk about built-in runtimes, we generally talk about the common languages\nthat we have seen with Lambda, so Node.js, Python, Java, .NET, Ruby, Go.\nEven though Go has been recently deprecated,\nwe'll talk a little bit more about that in a second.\nAnd generally, you can expect that the most recent versions\nof these programming languages are supported.\n\n\nSo if you have a long-term supported version of a programming language,\nthat's generally going to be supported within that runtime.\nYou can also use a custom runtime, as I mentioned,\nand that's the idea that you can support virtually anything else\nthat you want to run in Lambda.\nThere are some cases that are actually well supported,\neven though they are still custom runtime by AWS,\nbecause AWS provides libraries for you to make it easy\nto build a custom runtime supporting specific languages.\n\n\nAnd this is generally the case for languages that compile to native binaries,\nfor instance, Rust, Go, and C++.\nAnd I was mentioning before that Go was deprecated as a built-in runtime,\nand this is because now you have a library\nthat allows you very easily to build a binary\nthat contains all your code and the runtime itself,\nand then you can ship it as a custom runtime.\nSo pretty much the same experience you would get with Rust or C++.\n\n\nAnd that's, of course, not it.\nLike, you can effectively build custom runtimes for anything you want.\nMaybe you want to build older, newer versions of Node.js or Python\nor languages that are not even supported by Lambda itself\nwith the built-in runtimes.\nVery common examples are Bref,\nwhich is basically an open source PHP runtime.\nAnother one exists for the Swift language,\nwhich is really well supported,\neven though it's not officially coming from AWS.\n\n\nSo you need to download it from an open source project\nand figure out exactly how to compile it and ship it.\nAnd then there might be other interesting use cases,\neven though maybe a little bit less mature at this point.\nFor instance, I've seen Lua runtimes, WebAssembly runtimes,\nElixir, PowerShell, Bash.\nAnd there are even more crazy examples,\neven esoteric one, I would call them, like the BrainFact language.\nA lot of people have spent their time building a COBOL or a Fortran runtime,\nmostly just for fun.\nSo let's maybe try to deep dive a little bit\non what a custom runtime actually is.\nHow does it work?\n\n\nEoin: Yeah, well, a custom runtime is really just a program that communicates between your handler, I guess,\nand the control plane that is passing events in from the Lambda service itself.\nWhen you're creating a runtime,\nyou essentially just create a program that needs to be called Bootstrap\nand is placed at the root of your Lambda package.\nSo this can be a Linux binary or a shell script.\nRemember that, I guess, the Lambda runtime environment,\nit's just a Linux environment\nand it's running on Amazon's Firecracker, lightweight virtual machines,\nwhich are really low overhead,\nhighly optimized container-like things\nthat run an isolated and secure sandbox for a Lambda function.\n\n\nSo your Bootstrap program needs to target the Amazon Linux distribution.\nSo I think recently they've been moving to Amazon Linux 2023,\nthe latest version, which has just been released.\nNow, what does this program do?\nWell, there are two phases within this runtime initialization.\nYou've got initialization and then processing.\nAnd in the initialization phase, it's going to retrieve some settings\nand it can read special environment variables.\n\n\nOne is the handler, which handler file should be executed.\nThen you've got the Lambda task root variable,\nwhich tells you where the code is stored.\nAnd then you've got this AWS Lambda runtime API environment variable.\nAnd this is the host and the port of the Lambda runtime API.\nAnd this is a really important part, which we'll talk about in a little bit.\nSo there's lots of other environment variables.\n\n\nThe full link to all of them will be in the show notes.\nOnce that's done, then it can load the handler file.\nAnd this is into your function initialization.\nSo there are language-specific operations here.\nSo it might require initializing your runtime environment,\nlike your JVM, for example.\nAnd then loading classes, loading jars, etc., or loading libraries.\nAnd then for compiled languages, so we're talking about Rust,\nGolang, C++, the code is generally preloaded as part of that runtime binary.\n\n\nYou also need to think about handling errors during this phase.\nSo if any error happens while loading the runtime,\nthe program needs to notify specific API and exit cleanly with an error code.\nWhen it moves then into the processing phase, it's essentially running a loop.\nIt's like an event loop, so it fetches an event at a time from the runtime API.\nIt passes that to the handler function with the event payload.\n\n\nThen it will collect the handler's response and forward it back to AWS.\nThere are also other secondary things that it needs to think about,\nlike propagating tracing information, creating the context object,\nhandling errors, and cleaning up resources.\nNow, we talked about this runtime API.\nSo this is how you communicate with the AWS Lambda service.\nAnd the AWS Lambda service is responsible for receiving the events from its API,\nlike from invoke or invokeAsync.\n\n\nAnd then it needs to think about the worker placement,\nfinding a worker that has the capacity to run your function,\nand then passing the event to the runtime on that worker.\nSo your runtime is running on a fleet of workers,\nand the Lambda service is going to pass it to you.\nYou need to pull it using this runtime API.\nSo there's a get method on a specific invocation next path\nthat you need to pull to get the next event.\n\n\nAnd you just do this one at a time.\nAnd this will just hang until there's a new event available.\nSo you might have to set a long timeout on this HTTP connection.\nWhen you're finished, there's also a post with an invocation response URL,\nwhere you can signal that the request has been completed,\nand then that's used to send the response payload back to AWS,\nso that it can use it for other downstream invocations.\n\n\nYou can actually use this API to do response streaming as well,\nwhich we discussed in a previous episode.\nAnd we'll give a link to that episode in the show notes,\nas well as the link to how to use this API for response streaming.\nAnother one to be aware of is the invocation error response URL.\nAnd that's a separate path that you need to use\nif you've got an error in your function,\nand you need to report that back.\n\n\nAnd then you can pass in special headers to report the specific kind of error.\nThe body of that will also contain error information and even a stack trace.\nThe fourth URL might be useful to know in the runtime API\nis one that you can use to report initialization errors\nin the initialization phase of your runtime.\nSo that's basically how the runtime API works.\nI think all Lambda runtimes are using this\nto communicate with the Lambda service just slightly different ways.\nBut one of the ways you mentioned Luciano\nis that you can create your own custom runtime,\nand then you can interact with this runtime API directly.\nSo if somebody's thinking about using a custom runtime,\nwhat do you have to do to ship that?\n\n\nLuciano: Yeah, I guess the question is you have built this integration using the specific runtime API that you just described.\nNow, how do you actually push it to production?\nAnd generally speaking, there are two options.\nOne is that you can zip the bootstrap file within your code\nand ship everything as one package, or you can create a Lambda layer.\nSo when you zip everything, it's more in the case\nthat maybe you're doing something that's like one-off kind of use case.\n\n\nYou are maybe doing something that you are going to be doing once.\nYou don't expect to be like a general use case within your company\nor even within kind of the open-source space\nfor other people, other customers.\nSo maybe it's just easier to do one zip file and ship it.\nAnd this is actually the case when you use combined languages,\nagain, like Go, C++, or Rust,\nbecause since you are producing just one binary\nthat contains the runtime code that is coming as a library\nand your own custom handler code,\nand eventually ends up everything together in the single binary,\nthat's pretty much the only way you have.\n\n\nYou just zip it and you ship it as one thing\nthat contains both the runtime and your own custom business logic.\nThe other option, as I mentioned, is a Lambda layer,\nand this is more convenient when, for instance,\nyou think you have a use case that is a little bit more common.\nYou might want to do multiple Lambdas pretty much using the same runtime,\nor maybe you are building something that can even be an open-source project.\n\n\nMaybe you want to support a new language\nand you expect other people to be willing to use the same runtime\nbecause they also want to play with that new language in Lambda.\nAnd the way you do this is actually pretty simple,\nbecause again, you just need to zip that bootstrap file\nand then you can publish it as a Lambda layer.\nAnd another case where this is very convenient\nis where you have interpreted languages,\nbecause once you have shipped it as a layer,\nanyone that wants to use that runtime,\nthe only thing they need to do is basically go,\neven from the web UI, they can just go on the Lambda service,\nthey can create a new Lambda, they select the custom runtime,\nthey select the specific layer that implements the runtime,\nand then they can just use the built-in editor to create script files.\n\n\nFor instance, if we have built a runtime that can support bash scripting,\nthey just need to select the layer\nand then you can just create a file called, I don't know, handler.sh,\nwrite your code there,\nand assuming that you are following the spec of the underfile directory\nas the runtime expects,\nyou can just run your Lambda from there\nwithout needing to do anything more complicated than that.\nSo this is actually convenient, again,\nin this case where you have either scripted languages\nor you want to do something a bit more reusable.\n\n\nBut one thing that is always worth mentioning\nwhen it comes to Lambda layers\nis that they are not a way to escape file size limitations\nthat you might have with Lambda,\nbecause the layers are basically added on top of the total 250 megabytes unzipped\nthat you can have for your Lambda package.\nSo if you have very, very big, I don't know, runtimes,\nbecause maybe you have something like a JVM or something very big\nthat includes lots of native libraries that your code can use,\nthis is something that generally can go easily\nin the order of hundreds of megabytes.\nSo in that case, you need to be very careful,\nbecause then you might, just the runtime might go over the 250 megabytes,\nor you might be leaving very little space for the user code.\nWhich brings us to the next topic,\nbecause these are actually not the only two options,\nzip in the code or Lambda layers,\nthere is also the option of using containers.\nDo you want to talk about that, Eoin?\n\n\nEoin: Yeah, I'm getting more and more, I'm warming more and more to the idea of container image deployments for Lambda,\nbecause they're showing a lot of benefits,\nand one of the huge benefits there is that you've got 10 gigabytes\nto include all of your layers and dependencies and everything else.\nSo everything we've talked about so far has been about zip packaged functions.\nWhen you have standard zip packaged functions,\nyou have the option of using the built-in or the custom runtimes,\nand in the case of the built-in runtimes,\nit's completely managed by AWS,\nand they are responsible for keeping it up to date and secure,\nand this is one of the big benefits of Lambda in general,\nand one of the reasons why people don't like custom runtimes,\nthey don't like container image deployments,\nis because you'll sacrifice that if you go with one of those.\n\n\nWith container images, you don't have the provided built-in runtimes\nlike you do with zip packaged functions.\nInstead, AWS maintains and provides base images\nthat you can use to build your container image\nthat you deploy for your function,\nand these are available for all of the provided runtimes\nthat you already mentioned.\nThe shared responsibility model here is going to be different, though,\nbecause although AWS is providing these base images,\nthey are not going to be automatically updated\nwithout you having to redeploy your function.\n\n\nYou will need to continuously build and deploy\nagainst the latest build image\nin order to stay secure and up to date.\nYou also have the option of going with a completely custom approach\nwith container image deployments,\nso it's similar to zip packaged functions,\nwhere you're using the very same Lambda runtime interface client\nthat could communicate with this runtime API,\nand you just add that client to your container image.\n\n\nSo you have a choice with the container image build.\nYou either start with one of the base images,\nand you add your subsequent layers,\nor you can start with your own image.\nIf you've got some machine learning image, for example,\nyou need all of its base components,\nthen you just add the runtime interface client\nand the entry point and everything at the end.\nSo if you're using Lambda container image deployment\nto take advantage of existing images you have and you don't,\nand you just want to use them with Lambda,\nit's possible that you'll just start with your own base image\nand add that runtime interface client,\neven if you don't have any need otherwise for a special custom runtime.\n\n\nWith container images, you also have the benefit\nthat you can use the runtime interface emulator,\nand that allows you to run your container image locally\nwith this emulator, and you just get an HTTP endpoint\nto post events to.\nAnd it behaves then a lot more like a real Lambda function,\nnot completely like a Lambda function,\nbut it's a nice local emulation thing\nthat you get for free with container images\nthat I think is sometimes nicer\nthan the other local emulation options you have.\n\n\nNow, container images, it's probably worth saying,\nmight even be a preferable way to deploy functions\nif you're trying to reduce the call start times.\nAnd I've been doing a bit of benchmarking\nof certain runtimes recently,\nparticularly runtimes that involve\ntypically heavy package dependencies.\nI'm talking particularly about the Python data science stack\nwhen you need pandas and pyarrow and numpy\nand all of these things,\nand you quickly run into a 250 megabyte limit.\n\n\nNow, AWS actually released a paper\nwhere they describe all of the special performance optimizations\nthat they made for container image deployment\nthat caches files that are used by multiple images,\neven by multiple different customers.\nSo the time to load a 10 gigabyte function\nmay actually be less with container images\nthan the time it can take to call start a 250 megabyte zip.\nAnd that's very counterintuitive, but it is the case,\nand I've definitely seen results that show that.\nAnd we'll link that paper in the show notes.\nIt's pretty short,\nbut it talks about the neat caching strategies\nthat the Lambda team put in place\nto make sure that container image deployments\ncan be really fast,\neven though you're talking about 10 gigabytes of storage.\nSo going back to runtimes and custom runtimes,\nthen Luciano, what is our recommendation for people?\nDo you need a custom runtime?\nIs this something people should be thinking about doing\nin their job for any particular reason\nor are there good use cases for it?\n\n\nLuciano: I've personally been playing a lot with the Rust runtime,\nso I kind of had to explore this space\na little bit more in depth,\nand I am very excited to understand more how Lambda works\nand to use Rust in the context of Lambda.\nBut if I have to be honest and think about\nkind of the generic use cases\nthat I've seen in the industry,\nI think the answer to the question,\ndo you really need a custom runtime?\n\n\nMost of the time, it's probably not.\nAnd the reason is because the official runtime\ngives you, basically gives AWS\nmore of that chunk of shared responsibility,\nand you are free to think more about the business value\nthat you want to provide.\nYou don't have to think about all these details\nabout the runtime, you just write your own code\nand everything should work out of the box for you.\nAWS focuses on keeping the runtime up to date,\nperformant and secure,\nand you just focus on writing your code\nand making sure that it's as bug-free as possible.\n\n\nAnd provider runtimes are also potentially more,\nI guess, optimized to avoid cold start times\nbecause AWS can easily keep the runtime cached\nin the local workers,\nwhich is not something you can do\nwith your own custom runtimes,\nbecause of course,\nevery time you are publishing the runtime,\nit can be different from customer to customer.\nMost likely it's going to be very different.\nSo there is really no point in AWS\ntrying to cache that locally.\n\n\nAnd the other thing it's in terms of pricing,\nbecause with the provider runtimes,\nyou don't pay for the cold start phase for the most part.\nAnd there is actually a very interesting article\nby Luc van Donkersgoed\nthat explains a little bit of the research\nthat has been doing,\nbut the summary of it is that\nif you do your own custom runtime,\nyou pay not just for the execution time,\nbut even for the cold start time.\n\n\nSo there might be an impact there\nin terms of additional cost\nif your runtime is not particularly faster\nthan what you could do with the built-in runtimes.\nAnd again, I think that the point of this episode\nwas more to try to understand a little bit better\nhow Lambda works under the hood.\nAnd then there might be cases\nwhen you actually might need a custom runtime.\nWhat can be those cases?\nOne case could be maybe you have some legacy stuff\nthat runs maybe in a very old Python,\nlet's say Python 2,\nbecause that's on the,\nthat I still see actually frequent enough,\nand you don't have time right now\nto move it to something more up to date\nand use the latest runtime.\n\n\nSo what you can do as a quick and dirty solution\nis just create your own runtime using Python 2,\nand then you can run your own code.\nOf course, this is far from ideal\nbecause you need to be aware\nyou are still exposed to a bunch of security issues\nbecause all the runtimes are probably not supported\nfrom a security perspective.\nSo this is only a very dirty hack\nthat you can do for a limited amount of time,\nand eventually you need to have a plan\nto migrate to the newer versions.\n\n\nSo a more interesting use case\nis actually when you want to be bleeding edge\nand you want to try very new runtimes,\nvery new version of runtimes,\nfor instance, Python 3.12,\nI think it was released last month,\nand I believe there isn't yet,\nthere isn't already an official version\nof that runtime supported by AWS.\nSo if for any reason you want to use\nmaybe some of the newest features\nor the additional performance gains\nthat that version provides,\nand you're willing to take the cost\nof building your own runtime in exchange for that,\nthat could be a very valid use case.\n\n\nAnd we can do a very similar conversation for Node.js 20,\neven though it seems that AWS\nis going to release that very, very soon.\nAnother use case which we have seen actually\nacross a bunch of people that were trying to experiment\nwith different JavaScript runtimes,\nmaybe they want to play with Deno or BUN\nin the context of Lambda,\neither to be able to run TypeScript more natively\nor because they want to compare\ndifferent performance characteristics.\n\n\nAnd because there is no official Deno or BUN runtime,\nthe only option you have in that case\nis to build your own runtime\nand basically package all of that that way.\nWe already mentioned other cases,\nlike you want to use compile languages like Rust, Go, C++,\nand this can be a good use case\nwhen you are looking for extreme performance\nor reduce latency to the very minimum,\nor maybe because you need to use some kind of native library\nthat only exists for these compile languages.\n\n\nIn those cases, I would recommend don't reinvent the wheel.\nAWS gives you these libraries that are really well maintained\nand they have really good developer experience.\nSo just use the library and that will cover 90%\nof what you need to do,\nand you can focus on the writing,\nactually the business logic of your own Lambda.\nAnd the last point is if you want to use a language\nthat is not supported yet,\nor maybe it's never going to be supported\nbecause it's kind of a niche language,\nthat's definitely a good use case.\n\n\nAnd a true story is that we once had a customer\nthat had a significant existing code base in TCL,\nor sometimes called 'TICOL',\nwhich is a relatively old language,\nbut apparently there is lots of software\nthat historically has been built with this scripting language.\nAnd as part of their migration strategy to the cloud,\nwe consider using a custom Lambda runtime\njust because some of their work was very event driven.\n\n\nSo creating a Lambda would have been very convenient\nfrom an architecture perspective,\nbut of course we were missing the runtime.\nSo we were considering,\nis it worth building the runtime or not?\nSo this is a consideration you might be doing\nwhen you're facing this kind of migration scenarios\nand you have things that might be well-fitted to run\nin a Lambda, but maybe just the language support\nis not there yet.\n\n\nAnd I think that's everything.\nSo we are at the end of this episode.\nI really hope that you found this particular episode\ninformative and useful,\nand we always look for your feedback and your comments.\nSo don't be shy, reach out to us\nand tell us what we can do better.\nWhat did you like, what you didn't like.\nAnd if you think this is useful,\nplease remember to like and subscribe,\nshare it with your friends and colleagues,\nand this way we can grow the channels together\nand always make sure we provide the best value\nwe can provide to you.\nSo thanks again, and we'll see you in the next episode.\n"
    },
    {
      "title": "105. Integration Testing on AWS",
      "url": "https://awsbites.com/105-integration-testing-on-aws/",
      "publish_date": "2023-11-24T00:00:00.000Z",
      "abstract": "In this episode, we discuss integration testing event-driven systems and explore AWS's new Integration Application Test Kit (IATK). We cover the challenges of testing events and common approaches like logging, end-to-end testing, and using temporary queues. We then introduce IATK, walk through how to use it for EventBridge testing, and share our experience trying out the X-Ray trace validation. We found IATK promising but still rough around the edges, though overall a useful addition to help test complex event flows.\n\nAWS Bites is brought to you by fourTheorem, the ultimate AWS partner for modern applications on AWS. We can help you to be successful with AWS! Check us out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources:\n\nsls-test-tools on GitHub\nSarah Hamilton’s article on Integration testing and how to use sls-test-tool\nOur previous episode on building a cross-account Event Bridge deployment\nOur IATK tests for the cross-account Event Bridge project\nIATK tutorial\nIATK examples\n\n",
      "transcript": "Eoin: Integration testing event driven systems is a classic hard problem.\nWith modern distributed applications, you have events filing off all over the place.\nHow do you write integration tests that check if events have been sent and received correctly?\nAs part of pre-Invent, AWS have just released a solution for exactly that.\nThe AWS IATK.\nWe've been taking it for a test drive and are going to share everything we found out.\n\n\nWe'll also talk about some of the alternatives.\nBy the end of this episode, you should have a good idea of how you can use IATK\nand we'll share a project where we have been able to use it to test\na cross account application with EventBridge.\nMy name is Eoin. I'm here with Luciano and this is AWS Bites.\nAWS Bites is brought to you by 4thErem, the ultimate AWS partner for modern applications on\nAWS. We can help you to be successful with AWS. So check us out at 4thErem.com.\nBefore we get into IATK and the alternatives, let's take a step back to Luciano.\nWhat is integration testing? How does it compare to other forms of testing?\nAnd why is it so difficult for event-driven applications?\n\n\nLuciano: Testing involves taking your code and testing it together with your application and including\nexternal systems. So if your code is communicating with things like a database, for instance,\nas part of your integration test, you need to include that database.\nAnd for instance, in a more complicated use case, maybe you connect to another base first,\nthen you create an email and then you send that email through an email provider.\n\n\nSo you can include all these external pieces as part of your integration test.\nAnd it's a little bit broader than what you might have been used to if you only have done unit tests,\nwhich typically are focusing only on small units of code in isolation.\nThe idea is to be very efficient, very specific, make sure that that one feature works really,\nreally well, but they don't cover how do you integrate that feature with the rest of the\napplication. So this is where integration tests come into place to make sure that you are actually\nwriting units that are correct, but then those units are actually still correct when they are\nput together and combined into your own software solution. So with AWS, it's tricky because often\nwhen you build a solution for AWS, you start to use all these very specific AWS services\nwith their own specific APIs, for instance, EC2, RDS, ElastiCache or DynamoDB, EventBridge.\n\n\nAnd of course, if you're writing unit tests, you can mock some of that and simulate their\nbehavior to make sure that part of your own business logic works well. But then at some\npoint you need to make sure that also your own mocks are correct and your implementation\nactually works with the real backend, with the real AWS services. So that's when writing\nintegration tests can help to increase the confidence that your application is actually\ngoing to perform well and be correct when you deploy to production. And it's very often that\nyou will find bugs not in your code itself, but just the way that you are integrating things\ntogether. Maybe a configuration option is wrong, or maybe you assume that certain API will work in\na certain way and then in reality works slightly different. So in your unit test, you didn't\ncapture that behavior because you mocked that API and made assumptions. But then when you run\nit against the real service, then you realize that there was a mistake there or an edge case that you\ndidn't include in your own logic. We usually like to mention as an example building e-commerce\nsolution, because I think this is an example that everyone can relate to. And in that particular\ncase, we can imagine that there is, I don't know, an order service and a delivery service.\nYou might be writing them independently as two totally separate services. They might have their\nown tests, but then at some point they will have to be integrated together. So for instance,\nwe might have the case where when you place an order, that order is pushed to something like\nEventBridge. And through EventBridge, there is a notification that gets picked up by the delivery\nservice and the delivery service knows that an order was created and it starts to process it and\ndo all the fulfillment procedure that you put in place for your own e-commerce. So that EventBridge\nis the tricky bit because how do you actually test it and how do you make sure that on one side you\nare producing the right type of event, on the other side you are picking it up and processing\nit correctly. So that's the question that we want to explore today.\n\n\nEoin: There's different ways, different approaches that people use for testing events, these kinds of applications. One is you\njust build in logging to the event system so all events are logged. And then in your integration\ntest, you can actually scan the logs and filter out for the ones that you're interested in testing\nand validating. That's a fairly simple approach, but logs aren't always reliable in terms of the\ntime to deliver them and the guarantees around delivering them and having to parse them. It can\nbe a little bit slow and inefficient. Another approach you can take is just focus on more\nend-to-end testing. So testing the final outcome like a record appearing in the database or an\nemail being delivered and then you don't have to think about testing the events at all. It might\nwork for some cases, but not all cases will have a readable outcome like this and you might want to\nfocus on just a smaller unit that you're integrating. And then the third approach\nis temporarily recreating an additional consumer for the event just for the purposes of your test.\n\n\nSo if you are testing SNS or EventBridge, this could involve adding a temporary SQS queue\nas a subscriber or target and polling that SQS queue for a limited period to check\nfor delivery of the expected message. I think this last approach with the temporary queue\nis probably the most reliable, but it requires a bit of setup. You also have to think about\nthe additional latency to create these test resources and also think about deleting the\nqueue when the test is finished, including in cases where the test exits before any tear-down\nphase has a chance to happen. Now when we're talking about the different approaches, we're\ntalking about integration testing and end-to-end testing, so it might be worth clarifying the\ndistinction between those two. Integration testing, as you said Luciano, is basically ensuring that\napplication components work well individually and together, including with external systems.\nEnd-to-end testing is broader. It's still integration testing because you're using real\nservices, but it's really everything altogether. So it evaluates the product as a whole from the\nuser's perspective, user flows, and that can include starting with your front end or an API\nor whatever the external user interface is. So given those three approaches we talk about,\nwhat are the tools out there to help you with this?\n\n\nLuciano: One of the tools that we have been using, and I think it's very relevant here, is called SLS Test Tools, which comes from this company\ncalled Alayos. It's basically a tool that extends Jest, so the famous Node.js test runner,\nand it provides a bunch of additional matchers, I guess I'm going to call them. I'm not sure it's\nthe right terminology, but it extends basically the capabilities of Jest, the built-in checks\nthat you can do to include checks that are very specific to AWS services, for instance DynamoDB,\nS3, Step Functions, SQS, EventBridge. So the idea is that you will create a test where you\nprovision some infrastructure and then as part of your test running you can use the specific\nmatchers or assertions to basically check that the infrastructure was created correctly and that\ncertain behaviors actually are apparent in the infrastructure after you executed the specific\ntests. There is a very good article by Sarah Hamilton that we're going to put in the show\nnotes which describes a little bit of a tutorial on how to use it and why it can be very convenient,\nbut I think the hype these days is into this new tool announced by AWS that we mentioned in the\nintroduction and I think we want to show a little bit more of that and maybe compare it with SLS\ntest tools. So how does AWS integrated application test kit compares with SLS test tools?\n\n\nEoin: I think that Sarah Hamilton article is actually a very good article on the general approach here, everything we're describing, and I wouldn't be surprised if it actually inspired some of the\ndesign of this integration application test kit from AWS. So it just has been launched and it's\nstill under public preview, so we're going to talk about the pros and cons, but we should be\nfair and say that this is just released. AWS releases things early so we can expect some\nglitches. This one is currently available for Python-based tests, although it's implemented\nmostly in Go, Golang for the core and it simply uses an RPC wrapper for Python. AWS says that\nthey will add other languages in time, so I think that's a good thing to see. It has a few\ncapabilities. We can kind of break them down into three parts. One is creating test events from the\nEventBridge schema registry that you can use in your integration tests. The second one is probably\nmore aligned to what we've been talking about in terms of challenges and that's the validating that\nevents have been received via EventBridge and have the correct structure. And the third one,\nwhich is kind of the most innovative piece almost, is checking the correct flow of events with X-ray.\nSo we'll go through how the process for all of these things work. Maybe Luciano, you can talk\nabout IATK and how it works from the perspective of EventBridge event testing.\n\n\nLuciano: So what we tested is basically a very simple example and in this example what happens with this IATK tool is that\nit creates a temporary SQS queue and a temporary rule on the bus which uses the same patterns as\nthe rule you want to test and that will allow you to basically put into SQS copies of the events\nthat are happening so you can expect them and make sure that they look correct from an application\nperspective. It doesn't allow you to specify any pattern. You have to specify a rule and a target\nwhen you create a test listener and this seems a little bit strange. Now I don't know why it needs\nthe target but we could be missing something that maybe is a little bit obvious to AWS and that we\nare not seeing here. Now the idea is that again you are trying to capture that event into SQS and\nthen analyze it after you have been executing the code that you want to test. So it provides\na number of helper functions for you to effectively clean up everything after you\nexecuted the test but also to inspect the state of the system after the test was executed. And\nyou can also do things like clean up first just in case that a previous execution maybe left things\nin a little bit of a dirty state. So this is something that's actually recommended by the\ndocumentation and then you run your test, you do all of your own assertions and then in the\nthird down phase you clean up again. Now before we go through all the features of this tool and how\nto use it, it's interesting to note that everything is available on GitHub including examples\nand what we did is basically we created an integration test for a repository that we call\nCross Account Event Bridge which is something we built previously and it basically allows you to\nexecute event bridge across accounts and share messages across accounts. And this is something\nwe mentioned in a previous episode, episode 39, and you can find the link in the show notes if\nyou want to know a little bit more about that specific use case and why we built it. Now this\nrepository uses TypeScript for the CDK but because right now this tool only allows you to use Python,\nwe wrote the integration tests in Python and we will also have a link to the specific test section in this\nrepository if you want to find a quick way to go and just see how we brought the tests. So let's talk\nmaybe a little bit about the process of creating this kind of test. Eoin, do you want to cover that?\n\n\nEoin: Yep, so what we did was we were using PyTest so you create your Python test then you use the AWS IATK Python module and instantiate it. Now immediately we kind of ran into an issue where\nit didn't pick up the credentials locally. The documentation says that it should pick up your\nAWS environment variables but I was just getting expired key all the time and I think it was\npicking them up from somewhere else like from a credentials file or config file. I don't use\ncredentials for files so I don't know why but that didn't seem to work as documented so we had to\nactually specify a profile argument in the constructor in order to get this to work. Once\nwe've done that then you just you need to know the event bus, the rule and the target. So IATK\nalso provides some utility functions for reading cloud formation resource physical IDs or reading\ncloud formation outputs so that you can get those values for your stack. You mentioned also the\ncleanup process for ephemeral resources so you can use the IATK remove listeners helper to do that\nand you will it will use a tag filter to identify the resources that it can clean up safely. So you\ncall that at the start of your test and then you also call it during your tear down. That's\nbasically how you do it so that it clean up at the end of the tests normally but it'll also run at the\nstart of tests in case there's anything dangling from previously aborted runs. So then you create\nan IATK listener so you give it the bus name, the rule name, the target ID and some tags and this\nwill allow you to start checking for events. So when you create this listener under the hood it's\ncreating an SQS queue and it's creating an event bridge rule to route events to that queue and it\nbasically copies the pattern from the rule you provided. Again the fact that they ask you to\nprovide the target ID as well doesn't make any sense to me because once you have a rule and a\nbus and you have a pattern that's all you need I think to do the test. So I didn't look into the\ncode to find out what was that what that was all about. I'm not very good at reading Golang so I\nwasn't going to go in there and try to figure out what was going on but yeah maybe somebody can\nexplain. I'm sure they have a valid reason for it. And then you have two options for actually\nretrieving the events and doing the validation. One is wait until event matched. This basically\njust waits for one event to come in on the bus and you provide an assertion function to check if the\nmessage is the one you expect. The other one is poll events. So this is a different model where\nyou basically say you poll for events for 20 seconds and it'll give you all the events that\narrive in that period and then you can go through them and filter them and check if they are valid\nyourself. In your tests teardown function then just remember to clean up the resources created by\nIATK. And you can see this example and we have in our cross-account event bridge e-commerce example\nwe have the event bridge testing approach but we also tried out the trace validation with x-ray\nwhich seemed pretty exciting. We got some mixed results but we were able to get it to work.\nLuciano do you want to describe that process?\n\n\nLuciano: Yes I think it's a good advice in general to have x-ray enabled when you're doing event driven systems because that gives you peace of mind that you can\ntrace exactly how requests flow through the system and which different components gets\nused based on your own requests. And of course this is something you can even leverage here\nfor testing and this is the one of the more innovative things that I think this tool brings\nto the table. So the idea is that you set up everything, you execute your test, meanwhile\nthe system is also collecting traces because you have enabled x-ray. And one of the things that\nyou can do in your own assertions is that you can actually fetch the traces as a structured object\nand then do assertions on the traces themselves. So basically that can help you to make sure that\nthe systems that you expect to be involved in that particular flow are actually being involved.\n\n\nIf there is some kind of ordering that might be important for you it's also something you can use\nto do assertion and make sure systems are actually propagating messages in the correct order.\nSo basically what the library allows you to do is gives you an helper function that's called\ngetTraceTree and with this function you can specify a tracing header as a parameter and then\nit gives you back this object which represents the tree of traces so it's a nested structure\nwhere you can follow the different branches to make sure that things are happening correctly.\n\n\nNow depending of course on the complexity of your code and how many systems are involved\nyou might have to write lots of code to do these assertions correctly like it's not like a plain\narray where it's easy to assert certain things you might need to traverse the tree so it might be a\nlittle bit tricky to test exactly what you want to test but you have the entire view of all the\nsystems involved there assuming that you enable and configure x-ray correctly. Now just to give\nyou examples of the kind of matching that you might be doing on the trace tree for instance\nyou might check if there are errors in any segment maybe a specific system was part of this\ntransaction and that system produced an error maybe it's not something you might realize\nimmediately by just looking at the final result of your execution but just looking at the tree\nyou might realize that some of the components was failing in some unexpected way so I think it's\ngood practice to try to traverse the tree and look for this kind of things and if you didn't\nexpect any error in the case you see an error maybe make the test fail and report that particular\nerror. You can also check performance metrics so make sure for instance if you have requirements\nin terms of SLAs regarding response times you might produce a warning or even fail the test\nif you see that certain numbers go beyond the thresholds that you expect as acceptable and you\ncan also check if specific components were actually part of the trace maybe there are\nsystems that don't really expose a behavior that you can assert at the end of your test but you\njust want to make sure that they received for instance some information and somehow they were\npart of this transaction so you can just assert that they were part of the trace tree at some\npoint in the tree and finally you can also do the inverse for instance you might know that there are\nonly three systems for example involved so you might assert that those three systems are there\nbut if by any chance you see a fourth system that's probably a symptom that you are doing\nsomething a little bit unexpected so maybe something else that you might want to write\nan assertion about and make sure that only the things that you expect to happen are actually\nhappening and nothing else is happening in the flow. So what are the things that we were happy\nwith and the things that we were disappointed with?\n\n\nEoin: I think we were curious to see this in action but when we did the testing there were a couple of issues with the x-ray approach.\nThe get trace tree function you mentioned gave us an error which basically said error building trace\ntree and it said that it found a trace with a segment that had no parent which was a very\nstrange one like how can how can you get a segment from x-ray with no parent so we looked into the\nraw data like in the x-ray console in cloud watch and we saw the raw data we saw the segment and we\nsaw that it had a parent so we figured maybe it was like an eventual consistency kind of a problem\nthat but at the time when the test was reading it maybe all of the data hadn't been fully settled\nso what we did was we filed a bug in the IADK repo and we did a workaround so there was that\nother function you mentioned that allows you to wait until a trace comes with a valid condition.\n\n\nWe still had to add in a sleep in order for this to work so when we switched over to that function\neven though the documentation says you should never have to sleep in order to get it to work\nwe had to get it to sleep and that would work. Now there are examples in the code of using\nsleeps with the get trace tree approach but not with the approach we eventually used so\nit seems like the documentation and the behavior aren't 100 aligned on this but luckily we were\nable to do it and we were able to get the traces in this example application we have we've got\nthree kind of services we've got this kind of global bus and then we've got an order service\nand a delivery service so the flow the trace is kind of interesting it's like an event bridge to\nlambda to event bridge event bridge to lambda to event bridge three times and we were able to\nassert that that's true and we were also able to put in some performance checks like an SLA that\nall of this should take no longer than 10 seconds for example which is pretty good for exposing any\nunexpected performance degradations just in your continuous build process so I think that was quite\na nice one. So that's we've talked about the event bridge testing approach the x-ray testing approach\nthere was one other feature in IATK do you want to say something about mock events?\n\n\nLuciano: Yes mock event is something else we we tried and it's an interesting feature that you have available\nand the idea is that if you're using event bridge in event bridge you can have a schema registry so\nwhat this library allows you to do is basically to generate a mock event starting from your own\nschema registry and then use it effectively as a source for your own tests and for instance you\nyou can say I want to generate a mock event that invokes a lambda or a step function and then\nthat's basically the starting point of your own test and this can be convenient for instance when\nyou have maybe a very complex flow and you want to break down the testing into individual parts\nmaybe you want to test one integration at the time it can make it a little bit easier to actually have\na clear starting point where you can craft exactly the event that gives you a good test case without\nhaving maybe to go through a bunch of additional steps that you might have in the entire flow\nso it's basically a way to make your test case start from an event in event bridge\nI don't think this was very applicable for our own testing so we kind of had more of a look at\nthe documentation and so that looks like an interesting feature when you have kind of a\nmulti-step approach maybe going through different rounds of generating events on event bridge\npicking up the event from there and doing something else but yeah in our case this wasn't\nreally applicable but nonetheless it's an interesting feature that you might find useful\nand it's great to have the convenience to be able to create an event not let's say from scratch but\nstarting with what you have already in your schema registry which should give you a little bit of\nconfidence that the event is matching what you will actually have in a production scenario\nso what are all our overall thoughts like what did we like and what we didn't like about this new tool\n\n\nEoin: on the good side I think this is a nice addition just fills a gap in tooling for integration testing there weren't a lot of options out there we had sls test tools people are rolling their\nown this is a good one it supports other languages which is a good thing I think as well once you're\nfamiliar with it it makes testing these pretty complex cases quite simple the fact that they\nallow you to clean up down resources easily it's very nice because that's one of the problems you'll\nface if you do this yourself and I think an advantage of that RPC approach is that we will\nhave support for additional languages in the future which is promising because it'll see\nbroader adoption and hopefully then more features without them having to rewrite different versions\nfor different languages what about the bad do you want to be the messenger for all the bad news\n\n\nLuciano: Luciano I can't bear it I can be the bearer of bad news so let's recap what we didn't like and just disclaimer this is probably due to the fact that this is a very new product it's still very early\nstage so everything we are saying here maybe it's not going to be applicable anymore in a few months\nas the product evolves and gets polished gets new feature gets bug fixes and so on so we already\nmentioned that traces didn't work the first time maybe our fault maybe we did something wrong but\nit wasn't really obvious how to do this just by looking at the documentation and copying their\nown example so definitely something to be improved either fixing bugs improving the\ndocumentation providing more examples and making this functionality easier to use correctly the\nother problem is that there is right now no SQS or SNS or Kinesis or Kafka support yet so really\nthis is applicable today if you're using EventBridge but we know that EventBridge is not the only option\nhere so depending on your architecture it might be disappointing to be able to test\nEventBridge but not to be able to test the other types of integrations documentation is there\nthere is actually a website with a bunch of pages and examples but it looks like it was put together\nvery quickly so it feels like there is a a lot more work to be done there for instance there is\na section called tutorial and in that tutorial you only learn how to install the tool and then there\nis a link to some examples so I had the feeling that they want to create a more kind of fully\nfledged tutorial that walk you through all the different steps and gives you a bunch of different\nexamples where every example is actually discussed what is the rationale behind the specific\nimplementation for that example but right now you have to kind of fill the gaps on your own and just\nlearn how to install it look at the code and the examples and figure out everything else in between\nnow this is also a project that is open source so maybe if people are willing to contribute they can\nspeed up the process of building this body of documentation and making the experience better\nfor everyone else so that wouldn't be the first time for AWS to receive open source contribution\nand make specific tooling a little bit better so probably something where actually we have a chance\nto contribute and makes the project a little bit better the last one is on the RPC approach that we\nmentioned we like on one side because it makes this project more likely to be fully supported\nacross languages in a consistent way even though today is all available for python the problem that\nwe expect to see there is that with this kind of abstractions often when you have an error that\nerror can be very obscure because it's just showing you for instance an issue at the python\nwrapper level but that error that you might see might be very generic and then the actual error is\nhidden in the go implementation which is abstracted away by the RPC wrapper this is something that we\nhave seen for many many times i guess when when we use cdk and we see the kind of js\nai errors appearing and it's always very hard to troubleshoot here because the design seems very\nsimilar we expect to have the same problem and this is maybe something that can be fixed by\nputting lots of attention in making sure that the rpc layer also propagates good errors and these\nerrors are actually displayed well by all the different wrappers but nonetheless is an effort\nthat ws needs to put into building the library and building the reporting tools when an exception\nhappens so we expect this to be a bit of a friction point for people using the tool now again\nit's worth remarking that this project is in public preview so we don't need to be too harsh on\njudgment here i think the starting point is absolutely positive and it's great to have this\ntool so i think that the future is going to be brighter and this is going to be a valuable tool\n\n\nEoin: for people to use and write their own integration tests with in general iatk looks very promising i think and we hope to see plenty of improvements before it becomes generally available aws as we\nmentioned tends to release products early so we won't dwell further on the shortcomings if the\nconcerns are addressed this should really be a valuable part of our toolkit but let us know what\nyou think if you've tried it out what alternative approaches might we have missed and if there's\nsome other features that you think should be added into iatk let us know and let the maintainers know\nas well. Until then, we'll see you in the next episode.\n"
    },
    {
      "title": "106. Luciano's reInvent Experience",
      "url": "https://awsbites.com/106-luciano-s-reinvent-experience/",
      "publish_date": "2023-12-01T00:00:00.000Z",
      "abstract": "In this episode, Luciano and Eoin chat about Luciano's experience attending AWS re:Invent 2023 in Las Vegas for the first time. They talk about the massive scale of the event, logistical challenges getting around between venues, highlights from the keynotes and announcements, and tips for networking and getting the most out of re:Invent. Luciano shares his perspective on the AI focus, meeting people in real life after connecting online, rookie mistakes to avoid, and why re:Invent is worth the investment for anyone working in the AWS space.\n\nAWS Bites is brought to you by fourTheorem, the ultimate AWS partner for modern applications on AWS. We can help you to be successful with AWS! Check us out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources.\n\nAmazon Q\nEfi Merdler-Kravitz's talk on &quot;Rustifying serverless&quot; with AWS Lambda (YouTube)\nElastiCache Serverless for Redis and Memcached\nThroughput increase and dead letter queue redrive for SQS FIFO\nStep Functions Workflow Studio in AWS Application Composer\nLambda scales 12x faster\nStep Function redrive from a failed state\n\n",
      "transcript": "Luciano: Hello everyone, Luciano here and we have a unique episode for you today.\nI am currently at reInvent in Vegas and I've been here since all the week and we wanted to have the\nchance to share the experience with you all and we're going to talk just about what did I do,\nwhat did I learn, people I met, different announcements that we have seen and we maybe\nwant to comment on a little bit. So I hope this is going to be an interesting episode and maybe\nsomething that is going to inspire you to look more at reInvent and maybe we'll see you here\nnext time, next year and we can chat together about all the different things that happen at\nreInvent. My name is Luciano and I'm joined by Eoin for another episode of AWS Bites.\nAWS Bites is brought to you by fourTheorem, the ultimate AWS partner for modern applications on\nAWS. We can help you to become successful with AWS and if you're interested check us out at\nfourTheorem.com.\n\n\nEoin: Hey Luciano, how's it going? So I'm curious to know how are you getting on over there? What is reInvent 2023 like?\n\n\nLuciano: Yeah, I think it's just crazy. So many people, so many activities, so many things to do. So it has been my first one so I'm totally anew but this I'm\ntotally improvising and trying to get the best of it but honestly I'm just getting by and trying to\ndo whatever happens every day without trying to plan it too much because it's challenging even to\ntry to come up with a plan. I heard that there are at least 60,000 people which should give you like\nthe scale of how busy it is. So yeah and it's just interesting even just the logistics of it is like\nspread across multiple hotels like these big casinos and you really need to plan like when\nyou want to go from one to the other. Of course there is transportation but like it takes time\neven to walk from one room to the next one if you're trying to attend different talks. So far\nabsolutely positive experience. I am amazed of on how many people I met and how many things I've\nlearned and new connections or also meeting people that have been connected with before but only\nonline and I finally had the chance to meet many of them in person. So that alone I think pays that\nthey experience big time.\n\n\nEoin: It sounds like meeting people is just more of an advantage than attending the talks. I guess you can watch those back on YouTube later whatever you've missed but since you\nmentioned it's your first re-invent. If myself, anybody else wants to go over there for the first\ntime do you think you've made any rookie mistakes so far that you wouldn't do again?\n\n\nLuciano: Yeah probably made many at this point but yeah the main one is probably trying to plan to attend talks but not\nrealize again how far away things can be. For instance yesterday I wanted to attend the power\ntools workshop and I had to go from the Venetian which is where I'm staying to Mandalay Bay which\nif you just look on Google Maps doesn't seem too far away and there are buses so I left like one\nhour early with the bus from the Venetian and it got stuck in traffic and it took slightly more\nthan one hour so when I got there and I found the room it was probably like five ten minutes late so\nnot too too late but the room was already packed and they just didn't let me in so it was a bit of\na shame that I spent like more than one hour trying to get there to attend the workshop and then\nI couldn't attend it. I was still able to meet the power tools team and have a nice chat after it\nso it was totally worth it to go anyway but yeah it was a bit of a rookie mistake I should have\nplanned that a little bit better and maybe go there a little bit earlier so I think that kind\nof stuff can happen and sometimes there are so many talks so you might not realize when you\ncreate your own agenda that maybe one after the other is just not feasible because you're not\ngoing to have enough time to to move from one place to the next one so someday definitely to\nkeep in mind and next year maybe try to have just a very few talks you want to attend and make sure\nyou have enough space to move from wherever you are to the to the venue where they're doing the\ntalk.\n\n\nEoin: Yeah and I guess all the talks are along the strip in Las Vegas right I'm fairly familiar with the layout of it and I know that it seems deceptively short like you would not think but\nit's actually like I don't know how many miles long it is but I'm looking here it looks like it's\nalmost like two and a half miles to get from the Venetian to Mandalay Bay even though you can\nkind of see Mandalay Bay nearly. Exactly. So yeah it's pretty but still an hour it's crazy for that\ndistance isn't it? Yeah the other thing is even if you're walking right you got to go up these steps\nand down the steps and through buildings and through casinos so it's it always takes longer.\n\n\nLuciano: I think the thing that is making it worse this year is that they just had the Formula One\nso you can see on the roads they are still cleaning up all the setup that they add so I\nthink that that's why traffic is a little bit worse than what probably would be in another\ntime of the year. Yeah watch you don't fall into any manholes that have been displaced. I hope not.\n\n\nEoin: Okay so you what have you been up to then you mentioned meeting people. Did you like schedule\na bunch of meetings with people and are you rushing around from place to place? Is it more\nkind of ad hoc? And have you managed to get into any good talks so far? Are you kind of at this\nstage thinking you've got a couple of days left are you going to just try and abandon talks all\ntogether and just focus more on the social and the networking side?\n\n\nLuciano: Yeah I've attended a couple of talks and they can be really interesting but I would say just go to the talks where\nyou really think you can get value on the face-to-face with the speaker and maybe other\npeople that have an interest on the same topic and then you can use the question time to socialize a\nlittle bit because again just the content alone it's something you can consume later on everything\nis recorded slides are available so it's not worth it to compromise the opportunity for more\nnetworking for just going and seeing something you can check out later on your own time and yeah so\nso be aware of that so I attended one about Lambdas in rust which is a topic that I'm really being\ncurious about and been experimenting a bit. I'm surprised you didn't give that one but go on.\n\n\nAnd yeah it was actually a really good talk by Efi and he is also another serverless hero and\nbasically he actually described three different ways to use rust in the context of Lambda so not\njust building on Lambda runtimes but even just packaging a library written in rust for python\nfor instance and then just using it in the python runtime or even building custom Lambda extensions\nwith rust and then using them with other runtimes like java or not js so a bunch of interesting\nlearnings there and the conversation after it was super interesting it was actually\nrelatively small crowd which wasn't surprising because it's a very niche topic still\nbut everyone that was there was like super excited and everyone was trying to share\nall the mistakes and all the lesson learned while trying to do this thing which is still\npretty new and not many people are doing so it was quite an engaging session because everyone was\nsuper excited to try to learn from each other and other than that has been mostly trying to meet\npeople I wasn't really sure how to schedule all of that again because my first time here I didn't\nknow what the spaces will look like what everyone agenda is going to look like so I was just\nconnecting before the event online on slack linkedin twitter and just asking people I'm going\nto be there let's exchange maybe phone numbers and then let's try to find some time to spend some\ntime together and that has been working out in in a mixed way I think in some cases it's working very\nwell in other cases I just was unable to meet the people because the schedules didn't really\nmatch but also there have been a few interesting events organized by AWS for the heroes or the\ncommunity builders so being able to to attend this focused events is generally the best place\nto meet people that more or less have the same interest and people that you might want to talk to and just share opinions.\n\n\nEoin: It's probably good to talk about some announcements since we're talking about re-invent I thought a few of them were pretty interesting there's been quite a lot so far.\nI haven't had a chance to really try out any of the new features or services that much.\nWhat about you from your position on the ground there, what have you picked up? What are your favorite announcements? We won't go through them all.\n\n\nLuciano: Yeah I think I only looked marginally because again here it you can be so busy that all the announcements at least for me become almost like a\nsecond order problem like yes I like to see the headlines but I'm not gonna\ndeep dive and see exactly what's going on so in that sense one that seems very exciting it's\nserverless memcache and it's something that people following the podcast know that we have been\nrequesting for at least two years I think it was one of our very first episodes where we mentioned\nthat it would be a nice thing to have for AWS and offer that kind of service so I'm really excited\nabout that I already heard disappointing opinions that maybe it's not really serverless or as\nserverless as we wanted it to be but it's still a step in the right direction I hope so I think I'm\ngoing to defer a final opinion while while I have the time to when I have the time to try it out and see really what it looks like to to use it.\n\n\nEoin: It looked pretty good to me as well because the pricing page hadn't been published fully when I\nlooked at it first. Then when I checked it looks like you pay per gigabyte storage which sounds\nfine if you store some data in there but there's a minimum of a gigabyte and that's like 90 dollars a month already. So that's a bit disappointing as you say.\n\n\nLuciano: Okay so it doesn't scale to zero at least in terms of pricing. Okay so let's let's put that disappointment aside for a second\nthe other one that is probably a minimal thing but I think it can be very interesting and can enable\na bunch of interesting use cases is the API for the free tier services which I think is going to\nallow lots of interesting dashboards and integrations and it might make AWS more accessible\nto people like students that might be very concerned about not overspending on AWS so that's\ndefinitely a welcome addition and again something that I want to play with a little bit more before\nhaving a final opinion but it's great to see that there is a little bit of movement in that direction\nwhere it should be easier for people to see what is actually being given for free and not fall into\nany mistake or trap and then have some kind of bill shock at the end of the month. What about you? What did you like so far?\n\n\nEoin: There are a couple of nice ones in the kind of serverless space like SQS FIFO throughput has had a massive increase and now you can do 70,000 transactions per second so now\nSQS FIFO queues used to be a lot more limited in terms of throughput now with 70,000 transactions\nper second you can theoretically put 700,000 messages through it in a second and also with SQS FIFO you\ncan now do QA drives for DLQs which was something that was missing before and then on Lambda\nthe scalability of Lambda has suddenly had another jump as well so everybody can get a thousand\ninvocations per minute after the burst concurrency we talked a lot about burst concurrency and Lambda\nin a previous episode but like in most accounts that we've used in eu-west-1, we can get 3,000\nimmediately and then it used to be 500 per minute. Now, that's a thousand per minute, so you can scale\nup really fast. And then on Step Functions, there were two announcements that I'm aware of. One was\nthe ability to be able to redrive from a failed state. I think that's a really good one so now\nyou don't have to run the whole function again you can just continue from the failed step. And\nthe other thing is something we actually asked for I think when we covered Application Composer\nwhich is the ability to integrate Step Functions workflow studio into Application Composer now\nthat's done so they're obviously listening thanks everybody what I think you caught a lot of the\nAdam's Selipsky keynote yesterday now there's a lot of AI as we could have predicted and I think even\nthough they're pretty impressive in a lot of ways all of the Amazon Q announcements we could have\npredicted them as well because it felt like AWS had to have an answer to all the announcements\nfrom the open AI developer conference from a few weeks ago do you have any thoughts I'm guessing\nyou haven't had a chance to try it out but have you heard from anyone is there a mood in Las Vegas on what Amazon Q can do? Is it skeptical or optimistic?\n\n\nLuciano: I think I spoke with a few people after the keynote and I maybe it's a bit of a bubble that I'm personally in but most people more I guess on\nthe developer side are a bit disappointed that there was so much focus on AI alone and they're\nlooking forward for the keynote from Werner just to see stuff that is probably a little bit closer\nto what developers are looking for so maybe it's just a different audience there in general I think\nit's it's impressive to see the amount of investment that was put in AI but again it's not surprising\nthat that's the main message that AWS wants to push because that's been the main message probably\nfor all 2023 and it only makes sense to come to re-invent with strong announcements in that field.\n\n\nEoin: I've had a very brief period just playing with the Q assistant in the AWS console because it's like a it's almost a bit like Clippy for AWS console it's like, \"I see you're starting an EC2\ninstance. I can help you with that!\" I've got mixed results so far but you know we know AWS tends to\nlaunch things a little bit early especially like they were it seemed like they were going to have\nto launch this no matter what I would give it time I'm sure it will improve models will improve\nanyway but I actually am impressed by the way they've integrated Q into AWS console visual\nstudio code AWS toolkit loads of other services like connect and quick site it must have been a\nmassive massive undertaking to get everything ready in time for re-invent so that is impressive\nand there's there's loads more like you can you can start your own chatbot application you can\nintegrate it with your own data sources like Gmail and Dropbox and Aurora, SQL server loads of stuff\nand it's it's an alternative which I would probably intuitively trust a lot more than\nany of the other offerings out there because I know about AWS security and how it works\nand I would have high confidence in it doing the right thing with my data\nI'd be much more comfortable building like an enterprise knowledge base retrieval augmented\nsystem with a bedrock or Q than I would with anything else and there's obviously a lot of focus\non code transformation as well they announced this Q code transformation that can migrate\nJava 8 applications to Java 17 applications I think in minutes now the Adam's claims were quite\nextreme I think it was something like thousands of applications migrated in minutes I'd love to\nknow how that works and how do you even test them exactly I don't mean that amount of time but\nlet's see how it goes. I've started playing around a little bit with the AWS toolkit in VS code so\nloads of stuff to play with there I think it's worth giving a try and I'm sure there'll be some\nrough edges at the start but as alway.s it'll improve over time so apart from announcements\nMaybe some advice, what would you recommend based on what you've seen so far other people to go?\n\n\nLuciano: I think in general yes it is a bit of an investment both in terms of time but in terms of cost because of course it's not depending on where you're coming from it might be expensive to\nfly here it is expensive to stay here the tickets to the conference are also not cheap so it's\ndefinitely something that you have to see as an investment and make sure that you get back the\nright value from it. But I think in general, if you are working in the AWS space it is the place to be\nto get not just the news which is something that you can get from your the comfort of your own home\nand just watch all the keynotes and all the announcements from remotely right but more to\nmeet all the people and exchange opinions exchange contacts solutions build relationship potential\nbusiness opportunities also because you might meet potential partners, you might meet potential\ncustomers and you might meet people that maybe you want to hire in your own company.\nSo it's definitely worth for that alone that there is lots of value and I would recommend people looking for\nthat kind of opportunities to to consider re-invent next year.\n\n\nEoin: Okay I think you've got a day or two left anything particular you're hoping for before the end that you haven't managed to do\nyet?\n\n\nLuciano: I think I'm just gonna focus on trying to meet a few more people there are other people that really want to meet and I know that they are here around and just haven't bumped into them and I\nreally hope I can manage to do that and just have a chat and yeah just get that opinion and get to\nknow them in person a little bit more. Okay you haven't got people fatigue yet that's good.\nYeah I'm feeling very extroverted this week which is not my usual thing but it's working out pretty well.\n\n\nEoin: Look it was good that we got a chance to catch up and share it with everybody. I know I'll be seeing you the day after you come back in person but you'll probably be too jet-lagged\nto talk about your experience then so it was nice to catch up.\n\n\nLuciano: Absolutely I hope I'm gonna be making some sense when we meet but I have lots of swag to share so be ready for that.\nAll right I think that brings us to the end of this episode I hope you enjoyed it and I hope\nif you have been to reinvent as well that you'd like to share your experience what are the things\nthat you learn or unexpected things that happen to you I think it's gonna be awesome to just share\nall of that stuff and because I am a noob if you have been a noob as well and that was your first\ntime again if there was anything that you didn't expect then you would suggest to other people to\navoid if they are going next year for the first time please share all of that information in the\ncomments and we'll be super happy to see that and share it with other people as well so thank you so\nmuch for being with us for another episode and we'll see you on the next one.\nBye!\n"
    },
    {
      "title": "107. Expert opinions from re:Invent 2023",
      "url": "https://awsbites.com/107-expert-opinions-from-re-invent-2023/",
      "publish_date": "2023-12-08T00:00:00.000Z",
      "abstract": "In this episode, we share expert opinions from AWS community leaders on their favorite announcements from re:Invent 2023, advice for those starting their cloud journey, predictions for the future of serverless, whether to go multi-cloud or not, and how AI will impact developers. Our guests provide insightful perspectives on getting hands-on experience, leveraging the AWS community, thinking through architectural decisions, and more.\n\nAWS Bites is brought to you by fourTheorem, the ultimate AWS partner for modern applications on AWS. We can help you to be successful with AWS! Check us out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources.\n\nAlex Kearns on Linkedin\nAWS Console-to-Code (Preview) to generate code for console actions\nEmily Shea on Linkedin\nEmily's talk: Getting started building serverless event-driven applications (SVS205)\nRan Isenberg on Linkedin\nRan's blog\nMaxime David on Linkedin\nDanielle Heberling on Linkedin\nJones Zachariah Noel N on Linkedin\nSam Williams on Linkedin\nAJ Stuyvenberg on Linkedin\nFaizal Khan on Linkedin\nHeitor Lessa on Linkedin\nChris Williams on Linkedin\nPraneeta Prakash on Linkedin\n\n",
      "transcript": "Luciano: Hello folks, I'm just back from re:Invent, the biggest AWS conference of the year and\nprobably the biggest conference where I've been so far.\nre:Invent is hosted in Las Vegas and it was on last week, so I just came back.\nI'm still a bit jet-lagged, but I'm super excited because I had the pleasure to meet\nmany experts and thought leaders in the cloud and AWS space and they decided to share some\nof their bold opinions with us.\n\n\nSo today we are going to get expert opinions about topics such as how to get started with\nthe cloud effectively, favorite AWS announcements, bold predictions for the future of serverless,\nshould you go multi-cloud or not, and of course there is going to be a little bit of Gen AI\nbecause you cannot escape that these days.\nMy name is Luciano and thanks for joining me for another episode of AWS Bites podcast.\nAWS Bites is brought to you by fourTheorem,\nthe ultimate AWS partner for modern applications on AWS.\nWe can help you to be successful with AWS, so check us out at fourTheorem.com.\nSo let's start with favorite AWS announcements.\nThe first opinion we have for you is from our friend Alex Kearns, Principal Solution\nArchitect at Ubertas Consulting, who has an astonishing collection of AWS certifications,\nwhich is 8 different certifications.\nSo we ask Alex, what's your favorite AWS announcement so far?\n\n\nAlex: We're only on Monday, but my favorite announcement from reInvent so far is the ability, or to\nat least in preview, the ability to take applications that you've built in the console and convert\nthem to infrastructure as code.\nSo I believe this is only in one region at the moment, in very, very limited public preview,\nbut super excited to see exactly how this works in practice.\nThings really important to encourage infrastructure as code and anything AWS can do to make it\neven easier to get there is a massive bonus in my eyes.\n\n\nLuciano: The next question we asked is if you could give one piece of advice to someone starting\ntheir cloud journey today, what would that be?\nI think this is a great question that can help all of you\nwho are just getting started with the cloud and AWS, but it can also help people that\nare involved in training or that are supporting you and your colleagues.\nSo we had 3 awesome folks giving us an answer to this particular question.\nAnd the first one is Emily Shea, Head of Application Integrations, Go-to-Market at AWS.\n\n\nEmily: So this is one of my favorite questions, particularly because I have my own personal experience\nwith it.\nSo when I started at AWS, I didn't have a tech background at all.\nAnd so I really kind of built that up as I went with some of the certifications, but\nthen also building hands on myself because I thought it'd be so much more fun if I built\nan actual kind of use case that I wanted for myself.\nSo I'm actually giving a session tomorrow that's all about that journey of getting started\nfrom a very simple use case to solve a problem that I had.\n\n\nMine was around wanting to get daily reminders to study Chinese language and then building\nup an increasingly complex application with serverless around that.\nMy recommendation to folks is definitely start really simple with maybe just like one or\ntwo services.\nI think it's nice to build something around an event coming in.\nSo for mine, I built a daily schedule event with EventBridge that would trigger a Lambda\nfunction, pull a word from S3 and then send that out with SNS as a text message.\n\n\nSo it's a super basic use case.\nAnd then I took that and added more complexity and more features over time.\nI think getting hands on and starting simple, maybe with just an event trigger and building\non top of that is a really nice way to get started.\nI also think that there's some really cool stuff that's come out recently around visual\ndesigners.\nSo maybe if you want to start messing around with Application Composer to build a serverless\napplication, or also the Workflow Studio for step functions that allows you to pull in\ndifferent services and figure out the workflow that you want those services to step through.\nThose are really cool, nice visual tools to get really get a feel for how the services\ncome together and then be able to export template to go and deploy that application.\n\n\nLuciano: So that's really a great answer.\nThank you, Emily.\nYou should actually watch Emily's session.\nThe link is in the show notes.\nNext up is Ran Isenberg, Principal Software Architect at CyberArk and a fellow AWS Server\nHero.\nSo let's hear from Ran what people should be doing when they get started with the cloud.\n\n\nRan: My advice to you is basically don't focus on certifications too much.\nI think it's great to know it.\nI mean, it's great to have knowledge.\nIt's a great in theory.\nBut in order to get a job, you need to have practical knowledge.\nYou need to know how to solve a problem.\nAnd you gain that only from actually working with the tools, right?\nIt's not with theory.\nSo my advice to you is basically try to build something, maybe solve a problem.\n\n\nTry to build the REST API in serverless, maybe use CDK, Terraform, whatever you want to learn.\nJust try that.\nSee how it works for you.\nWhen you really start to do something, you really understand the corners, the edge cases,\nand you know how to test, you can learn how to test it, how to build it.\nAnd that's usually what I do.\nI try to do some PoC.\nI read the documentation and just try to build it.\n\n\nAnd that's usually how I gain the most insights.\nThen I can write a blog post about it or recommend it to my work for a design or something like that.\nSo another option that you can do when you have more confidence and you have some knowledge\nis also to go to the road of code contributions.\nSo usually I look for an open source that I really care about.\nI look at the issue segments, I look for maybe a label for good for first contribution, and\nI look for it and try to solve it.\nSo you can talk with the team, write in the issue there.\nHey, I can help.\nMaybe I have an idea.\nMaybe I can help.\nI can do it.\nJust try to solve it.\nAgain, you're going to learn so much from just trying to solve a very specific problem\nin one of your favorites open sources.\nSo that's my advice.\n\n\nLuciano: Ran mentioned writing blog posts.\nHe's actually a very prolific writer.\nSo if you want to find out lots of interesting serverless material, make sure to check out\nhis blog, RanTheBuilder.cloud\nThe link is also in the show notes.\nNow we have Maxime David, software engineer at Datadog and AWS community builder.\n\n\nMaxime: So maybe try not to reinvent the wheel.\nAWS has a lot of different services, so maybe there's already something for you there.\nI would highly suggest you to reach to community builders, read blogs.\nAnd as soon as you learn something new, maybe you should write a quick blog post about that.\nFirst to get feedback on it and just to make sure that you understand it well, you're using\nit well.\nSo yeah, that would be my advice.\nStart to learn as quickly as possible and then try to share and receive feedback from\nthat.\n\n\nLuciano: Build, share and get feedback.\nThis is probably a good summary for all these awesome opinions.\nSo let's now move on into another topic, favorite and least favorite AWS services.\nWe have Danielle Heberling, senior software engineer on an healthcare company called\nEnsomata.\nSo Danielle is also an AWS Serveless Hero and a community builder.\nSo let's talk about Danielle's favorite AWS server services and of course the least favorite\nones as well.\n\n\nDanielle: Favorite thing about AWS by far is definitely the community.\nI do know that one of AWS's principles is customer obsession and I think they do\na really great job with the community in terms of more active community members getting recognition,\ngetting opportunities to talk to the service teams.\nMostly speaking about the community builders program and the heroes program.\nSo that's definitely my favorite.\nMy least favorite part about AWS is there's a lot of services and there's some services\nthat do the same thing yet slightly different.\nThinking mostly in terms of all the different types of databases, all the different ways\nyou can run a container.\nIt can be kind of challenging and daunting especially if you're just getting started\nwith the AWS ecosystem to know which solution to go with because it's confusing.\n\n\nLuciano: Indeed the AWS community is awesome.\nI can certainly say that after my experience at re:Invent.\nBut it's also true that there are so many AWS services and they can overlap quite a\nlot.\nI find particularly funny thinking about all the messaging and data streaming services\nsuch as EventBridge, SNS, SQS, Kinesis, Kafka, Amazon MQ, etc.\nIt can be really daunting to figure out which one of those offers the best trade-offs for\nthe specific problem you are dealing with.\n\n\nI suppose there is also a good thing in having variety because you will probably be able\nto find the perfect set of trade-offs if you really put your effort into it.\nSo that's kind of pros and cons of AWS and you might like it or dislike it.\nBut I suppose with experience things get a little bit better and more accessible.\nAt this point I want to talk about bold predictions for the future of serverless.\nAnd because you know I love serverless and I had to ask this kind of question, especially\nhaving these amazing thought leaders in the room.\nSo I actually had a few people volunteering to answer this question and the first one\nwe have is Jones who is a fellow AWS serverless hero and a senior developer advocate at Freshworks.\n\n\nJones: To talk about the future of serverless, well it's scary and it is exciting for me because\nit's scary first because we are losing the meaning of serverless as we go about with\na lot of things coming out.\nThe true meaning of serverless is being lost and that scares me.\nAt the same time the exciting things that happen and the future of serverless could\nbe more distributed, more granular in nature where if you would want to have a scheduler\nand monitor it maybe it's going to be much more simpler and much more easier to do it.\nYou don't have to go through EventBridge, right, write the cron job, stuff like that.\nIt's going to be much simpler.\nThat's just a simpler thing for instance, right.\nBut a lot of things is going to change.\nA lot of things is going to happen.\nSo I'm excited for that future but also scared about it.\n\n\nLuciano: Now let's compare this opinion with Sam Williams, founder of Complete Coding, a company that\nprovides training and content for serverless and AWS.\nSo let's hear from Sam what the future of serverless is going to look like.\n\n\nSam: In my opinion there's kind of two big things that are going to change going forward.\nThe first is that we may see an evolution kind of on top of infrastructure as code.\nSo at the moment you always define a very physical infrastructure and that is what you\nthen run your applications on.\nAnd I think we're going to find that there's going to be an evolution over the next couple\nof months or even years where it's more based on the functional requirements that the business\nhas or the company or the client has.\n\n\nAnd we define our requirements and there is kind of an interpolation layer from our requirements\nto our architecture.\nAnd the second is that with all of the AI stuff that's going on with all the LLMs, there's\ngoing to be some really cool functionality built into our serverless pipelines and our\nserverless development process, whether that's like with those requirements, running that\nthrough a large language model and that is going to generate our architecture for us.\nAnd then we can tweak it or being able to have tools that can kind of critique our designs,\ncritique our implementations.\nA bit like what we've got at the moment with CodeWhisperer and GitHub Copilot.\nBut for more of the architecture side of things.\nSo yeah, I'm really excited to see what happens and the new tools and new software and how\nmuch quicker and how much better we can make our applications using serverless with these\nnew tools that come out in the next couple of years.\n\n\nLuciano: Yeah, I'm also really excited to see what kind of innovation AI is going to bring to the table for serverless developers.\nBut we also have AJ Stuyvenberg, staff engineer at Datadog and AWS Serverless Hero as well.\n\n\nAJ: My bold prediction for the future of serverless is that one day we're going to solve some\nof the outstanding problems like serverless search in a true scale to zero, pay for what\nyou use, scale to infinity kind of fashion.\nThat is what I'm looking for when I use any managed service.\nI want to be able to deploy anything and not pay for the number of deployments I use and\nonly pay for the actual compute that I use or storage that I use when I use it.\nThat's what I'm hoping to get out of every single launch here at re:Invent and I'm excited\nto see what they launch.\n\n\nLuciano: I totally agree with this one and I hope AJ wasn't too disappointed about the new serverless\nElastiCache, but that's probably a topic for another time.\nNow I think it's time to discuss multi-cloud: yes or no.\nAnd this is something that we actually have talked about before, so I'm really curious\nto hear other expert opinions.\nAnd to answer this question, should you go multi-cloud yes, no and why, we have Faizal\nKhan, who is the founder and CEO at Ecomm and Xite Logic and he is also an AWS Community Hero.\n\n\nFaizal: To answer the question about whether multi-cloud or not, like all great answers, it really\ndepends on your particular use case.\nThere are certain advantages that you get in terms of redundancy if you need it or maybe\neven compliance where you need to ensure that you're using two completely different networks\nor two completely different providers, you can have that.\nBut it is not always a necessary thing.\n\n\nIt's kind of like the question of whether you need to go completely serverless with\nyour application or still run on servers.\nIt really depends on your use case.\nIt's not always a necessity because there's a lot of things that you have to do in terms\nof retaining your workforce, ensuring that you have adequate redundancy set up and all\nof that.\nSo there's a lot of additional work, expense, time that's going to go into that.\nSo if it is like a real requirement, then I would say yes, but otherwise just stick\nto a single cloud provider because it provides you almost everything that you need.\nSo you can actually in fact set up redundancy within the same providers as well if you need\nto.\n\n\nLuciano: I totally agree with Faizal and if you want to find out more about this topic, you can\ncheck out our previous episode on this and we have the link in the show notes.\nBut this year was the year of Gen AI.\nWe all know that.\nAnd of course, I wanted some opinions about Gen AI and especially how Gen AI is going\nto change life in a way or another for software developers and cloud developers.\nLet's start with Heitor Lessa, chief architect at AWS and also the mastermind behind Lambda\nPowerTools.\nSo let's hear it from Heitor.\n\n\nHeitor: If you know me working on open source, one of the features that amazes me in Gen AI is\nthat you can create documentation for people who are terrible at writing documentation\nfor developers, for data engineers and so forth.\nOne of the pieces that I noticed just now is that while it creates an amazing structure,\nit's not very well aware of the way people think and learn and the different personalities\nand personas when reading the documentation.\n\n\nSo one of the ways that we use right now at PowerTools is we generate the structure of\nthe text, but then we have to have the human element to think all the diversity and the\nplurality of customers using the product and how would they perceive?\nHow do we take someone who knows zero about AWS or barely anything about serverless or Lambda\nspecifically for the matter?\nSo we take this text and we start optimizing it, reviewing it and doing multiple editings.\nSo I would say roughly about 30 to 50 hours just goes into editing of that text.\nEven though the Gen AI gets you there very quickly, should remove that block of your\ncontent creativity.\nThe reviewing of those and trying to match to people's reality and people's limitations\nand their also plurality on how they think, which makes all of us special, it still requires\nhuman.\nI still cannot see a place where Gen AI will replace that.\nThat human element is so special to us and what bonds us together.\n\n\nLuciano: Yes, documentation is definitely one of those things where Gen AI can be a fantastic assistant,\nbut where I definitely agree that we still need the human element if we want to provide\na great experience to our users.\nSo let's talk a little bit more about this and this time we have Chris Williams who is\na developer relationship manager at Hashicorp, also a Cloud Therapist.\nBy the way, I love that job title, probably my new favorite job title so far and he's\nalso an AWS Community Hero.\nSo let's see with Chris what he has to say about Gen AI.\n\n\nChris: So how do I think Gen AI is going to affect me and is it going to help me as a developer?\nI think it is going to, that's a very nuanced question and I think that it is going to help\nfolks that have a little bit of experience to accelerate their development cycle, create\ntests more efficiently, create documentation more efficiently, create Git comments more\nefficiently.\nI do think that there is a danger in making it an easy button for folks that might not\nhave as much experience and that they will fall victim to not understanding the fundamentals\nbefore stepping into something and potentially making big problems.\nSo I think it's going to help.\nI am very excited to use it.\nIt is helping me a lot on my day to day and I do use it daily.\nBut I think that we need to make sure that we have a good set of guard rails around leveraging\nit and using it expeditiously.\n\n\nLuciano: That's really a fantastic opinion.\nSo let's move on to the last expert opinion for this episode.\nWe have Praneeta Prakash who is a senior product manager at AWS.\nPraneeta works on building tools to improve developer productivity, which is the best\nkind of tools if you ask me as a developer.\nSo let's see what Praneeta has to say about AI.\n\n\nPraneeta: Honestly, we've been thinking a lot about this on our side.\nAt AWS, we have this thing we call two-way door and one-way door decisions.\nFor us, a two-way door decision is reversible and a one-way door decision is when you need\nto make a change that cannot be easily reversed.\nFor me, AI comes into play where it can help you with those two-way door decisions.\nBut every time there is a decision that needs to be made that's a one-way door decision\nor it needs a human in the loop to approve it.\nAnd so how I'm thinking about AI is how can it help that human in the loop make better\ndecisions as part of their development journey.\n\n\nLuciano: I actually didn't know about this concept of two-way door and one-way door decisions.\nAnd I think this is actually a pretty cool idea and it's probably something that we'll\nhave to think a little bit more and maybe use it in the future because it seems like\na very good framework to think about architectural decisions or decisions that might have an\nimpact on the business in a way or another.\nBut I think this brings us to the closing of this episode.\n\n\nI hope you found all these answers insightful and inspiring as much as I did.\nAnd I've added all the links so that you can connect with all our guests on LinkedIn in\ncase that you want to reach out to them and maybe ask additional questions, clarify their\nopinions or maybe just engage with them and build new meaningful connections.\nBut how would you have answered all these questions?\nMaybe you have a dramatically different view on some of these questions.\nI really love to know.\nSo in that case, don't hesitate to reach out or drop a comment on YouTube.\nAnd as always, if you found value in this episode, make sure to like and subscribe and\nshare this podcast with your friends and colleagues.\nThank you very much and we'll see you in the next one.\nBye.\n"
    },
    {
      "title": "108. How to Solve Lambda Python Cold Starts",
      "url": "https://awsbites.com/108-how-to-solve-lambda-python-cold-starts/",
      "publish_date": "2023-12-15T00:00:00.000Z",
      "abstract": "In this episode, we discuss how you can use Python for data science workloads on AWS Lambda. We cover the pros and cons of using Lambda for these workloads compared to other AWS services. We benchmark cold start times and performance for different Lambda deployment options like zip packages, layers, and container images. The results show container images can provide faster cold starts than zip packages once the caches are warmed up. We summarize the optimizations AWS has made to enable performant container image deployments. Overall, Lambda can be a good fit for certain data science workloads, especially those that are bursty and need high concurrency.\n\nAWS Bites is brought to you by fourTheorem, the ultimate AWS partner for modern applications on AWS. We can help you to be successful with AWS! Check us out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources.\n\nOur blog post detailing our research on how to optimise Python Data Science in AWS Lambda\nThe repository with our benchmarks and related visualizations\nOn-demand Container Loading on AWS Lambda (AWS Paper)\n\n",
      "transcript": "Eoin: Python is one of the two most popular languages\nfor developing AWS Lambda functions.\nWhen it comes to data science, statistics,\nand machine learning workloads, Python is the undisputed leader.\nBut it has often been difficult, and sometimes even impossible,\nto deploy certain data science workloads in AWS Lambda.\nThe 250 megabyte packet size limit in Lambda\nhas been at odds with the heavy nature of Python data science modules\nlike NumPy and Pandas,\nnot to mention machine learning packages like PyTorch and TensorFlow.\nAnd this problem might actually occur with other runtimes as well.\nSo today we're going to talk about some benchmarking\nwe did on Lambda functions\nand present some really interesting findings.\nWe're going to talk about zip packaging, Lambda layers,\nand also talk about the trade-offs between zip, images, and Lambda layers.\nAnd by the end, you'll hear how container image packaging\ncan actually solve this problem and even provide superior performance.\nThis episode also has an accompanying source code,\nrepository, and detailed blog post.\nI'm Eoin, I'm here with Luciano,\nand this is another episode of the AWS Bites podcast.\n\n\nLuciano: So, Eoin, I'd like to start by asking you the question,\nwhy would you even consider Lambda as a target\nwhen doing data science with Python?\nBecause this is generally heavy workloads,\nso Lambda might not seem like the right solution,\nbut maybe I'm missing something important there.\n\n\nEoin: I think there's plenty of people using Python\nfor just API-based workloads\nand normal kind of data transformation on AWS on Lambda.\nBut when you think about data science,\nyou have to also think then about all the other options you have on AWS\nfor running those kind of workloads like Python Shell\nor PySpark workloads on Glue or Elastic MapReduce,\nyou have more as well,\nand then you have SageMaker, SageMaker Notebooks.\n\n\nSo you can also think about services like EC2, ECS, right?\nLambda, I would say, is best suited to two classes of workloads,\nso those that are really bursty,\nwhere you don't really have constant traffic,\nand those where you need a lot of concurrency very quickly.\nAnd that could be like data processing,\nit could be like high-performance computing,\nhigh-throughput computing, lots of data science,\nfinancial modelling, scale,\neven kind of HPC stuff like fluid dynamics\nor all sorts of scientific modelling.\n\n\nAnd the great benefit of Lambda there\nis that executions can start faster than any alternative.\nLike, you can get thousands of concurrent instances in a few seconds,\nand now since the new announcement at re-Invent,\nyou can increase by like a thousand every ten seconds.\nWhen you think about it,\nyou've got two classes of data science workloads, I guess.\nOn AWS, you've got ones that are highly coupled,\nand then you use something like Spark or Ray or Dask\nor one of those distributed frameworks\nto spread it across lots of stateful nodes.\n\n\nLambda isn't really suitable for that kind of workload,\nwhere each concurrent unit in a Lambda environment\nis not going to communicate with the others, right?\nIt's stateless.\nSo instead, Lambda functions are more highly isolated,\nand you would just run lots of them in parallel\nand just orchestrate them with step function\nor with other schedulers.\nBut there are plenty of good use cases for Lambda with Python,\nand we've even done lots of machine learning inference\non Python very successfully, where you don't need a GPU.\n\n\nI think we've talked about that in the past.\nSo, like, Python is definitely a great fit for Lambda.\nYou know, we use it for doing real-time data analytics for IoT,\nfor doing event-driven ETL or batch processing ETL,\npreparing data, machine learning, and aggregation in general.\nSo if you have a workload that is, I suppose,\nmore for data scientists to do ad hoc, hands-on work,\nthen you're probably going to use something like JupyterLab,\nSageMaker notebooks,\nand maybe use an orchestrator like Airflow or DAGster.\nThere's a load of them there in the Python ecosystem.\nLambda functions can run up to 50 minutes,\nwhich is usually plenty, and use up to 10 gigs of RAM.\nThat can be enough, but sometimes it's not,\nand then you might run for something like Fargate\nor AppRunner instead.\nSo it's always a trade-off.\nIf Lambda does suit your workload,\nyou can avoid a huge amount of infrastructure\nand all that kind of stuff.\nBut if you've got something that's long-running,\nthen you might just go with EC2 or a container instead.\n\n\nLuciano: You mentioned the difference between long-running, short-running.\nWe are very well aware of the limitation in Lambda,\nbut are there other limitations that are relevant\nwhen you want to do data science with Python\nand trying to target Lambda?\n\n\nEoin: When we started using serverless architectures\nand combining it with Python data analytics,\nwe were generally, this is going back a few years,\nso container image packaging wasn't a possibility,\nand you were always working within the 250-megabyte size limit.\nAnd that's everything unzipped, all of your layers all together,\nall of your code, there's no way to get around it.\nAnd when you look at even just a naive basic data science package\nwith Python, a lot of the stuff we would do would have NumPy,\nPandas, and PyArrow as well, and PyArrow is also a bit of a beast.\nSo if you've got these things, then by default,\nyou're already exceeding the 250-megabyte limit.\nAnd you might want to ship Boto3, your own version as well,\nand AWS Lambda Power Tools for Python is pretty indispensable as well.\nThose things aren't massive,\nbut even PyArrow is like 125 megs just on its own.\nSo that storage requirement becomes a really big problem.\n\n\nLuciano: Yes, so I remember doing lots of tricks to try to strip down the binaries\nthat sometimes you get from these packages,\nbecause behind the scenes, they try to use native packages\njust to be faster, and you can reduce the size a little bit.\nBut yeah, I guess at some point, there is always that limit,\nand the more libraries they use,\nthe more likely you are to bump into this limit\nthat you cannot bump in any way.\nSo that, I guess, brings us to the next question,\nwhich is, you mentioned already container deployments.\nWhat are the pros and cons of using zip packages\nversus container deployments?\nAnd I guess for this particular use case,\nis one of the two options more suitable?\n\n\nEoin: I'd say in general, zip packaging is still the preferred way,\nand the main reason for that is that it's the only way\nto make sure that AWS has the maximum amount of responsibility\nfor your whole environment.\nAnd I think when container image support was announced in 2020,\nI believe a lot of people were excited about it,\nbecause it allowed you to take your existing images\nand kind of shove them into Lambda in a weird way.\n\n\nBut I remember you had people like Ben Kehoe as well,\nwho's always quite wise,\nI would guess, in his assessment of these things,\nand he mentioned that, you know, once you have a container image,\nyou're essentially taking responsibility for the runtime,\nbecause it's like a custom runtime,\nand we talked about this in the recent runtimes episode.\nSo you suddenly need to maintain that runtime yourself\njust because you're using container image deployment.\n\n\nSo while it's giving you benefits of being able to use\ncontainer image packaging tools, like Docker and\nFinch,\na new one from AWS and Podman and all these tools,\nthat's really great.\nYou also get the 10 gigabyte package size limit,\nwhich is 40 times greater than the 250 megabytes you get with Zip.\nBut now all of a sudden, if you have a Java base layer,\nand there's a bug in the JVM,\nit's your responsibility for patching that JVM\nand releasing a new base image,\nwhereas with Zip packaged and one of the AWS supported runtimes,\nthey're responsible for that packaging,\nand it just happens while you sleep in the night, usually,\nwhich is a really great benefit.\nSo I wouldn't understate that benefit.\nSecurity is job zero for all of us, really,\nso it's pretty important.\nBut you have to make these tradeoffs, you know?\nAnd again, a lot of people are running container images anyway,\neven in other environments alongside the Lambda functions.\nSo they might say, well, look, I'm running container images,\nand I have my whole security tooling in place\nto do the patching and upgrades anyway.\nSo I'm not really concerned about that additional drawback.\n\n\nLuciano: But one of the things that might come into play here is performance.\nAnd I heard a few times from different people\nthat they are worried about going to containers\nbecause they expect worse performance.\nSo you mentioned that we did some benchmarks,\nand I'm curious to find out whether that's actually true,\nand especially in this particular case where we use Python\nand all these different data science libraries.\n\n\nEoin: Yeah, I mean, there's a lot of factors at play here.\nAnd the traditional way we just solved this problem\nbefore we had container image support,\nthere was, with serverless framework,\nthere was a very popular plugin called serverless-python-requirements,\nand it would do exactly what you mentioned.\nIt would take all the tests out of your Python packages.\nIt would remove your readme and any documentation,\nand then it would strip any native shared libraries as well.\n\n\nSo there'd be no debug symbols.\nIt was also common to remove the PYC bytecode files as well,\nthe precompiled bytecode, just to save on that extra space.\nOf course, that might result in a performance hit.\nSo at every step, you need to think about the tradeoff, right?\nSo when you're using Lambda, you're supposed to be just writing the code\nand let AWS manage everything else. That's the promise.\nAnd if you end up having to do all this heavy lifting\nto strip out your packages, you kind of wonder,\nare you really realizing that benefit?\n\n\nSo with container images, when they came out first,\nI suppose 10 gigabytes, the assumption\nand a lot of the observation was that cold starts were faster.\nAnd that's kind of intuitively something that makes sense\nbecause you say, okay, well, it's going to take longer\nto cold start a 10 gigabyte function than it is a 250 megabyte function.\nWhat we did was we started to do some proper benchmarks on this\nand measure cold starts.\n\n\nAnd we put together some benchmark code base\nwith lots of different permutations of Lambda deployments.\nSo we had zip package ones, we had zip package ones with layers.\nAnd the whole idea of using layers is actually\nthat there's a provided Lambda layer from AWS\ncalled the AWS SDK for Pandas layer.\nAnd that already has Pandas, PyArrow,\nand NumPy already stripped down and optimized for you.\nYou can look into their build process\nand they're like compiling PyArrow with all the minimal flags\nand they're stripping out debug symbols, and that's how they do it.\nSo layers doesn't really give you a benefit inherently,\nbut the fact that somebody else has gone to the trouble\nof doing the stripping for you kind of gets around that problem.\nSo that was why we tested the layers option.\nAnd then we tested images as well.\nWe tested it with lots of different memory configurations\nbecause we know that memory configuration\ncan affect Lambda performance as well.\nSo you mentioned that you did some benchmarks.\n\n\nLuciano: What are the results of these benchmarks?\nIs it container the favorite option in terms of performance\nor is it still zip in terms of delivering better?\nMaybe call starts versus execution time as well\ncould be another dimension to explore.\nSure, yeah.\n\n\nEoin: So in this benchmark application, we've got the CDK project that deploys all these permutations\nof the Python functions we mentioned.\nSo we try four different supported Python versions,\nfour different memory configurations from like one gigabyte up to 10 gigs.\nAnd then we execute them all from a cold start situation.\nSo we deploy it into a region\nwhere we haven't deployed these things before,\nand then we start invoking the functions.\n\n\nSo we have a script that basically just invokes\nall the functions in parallel 2000 times,\nor however many times, we just say 2000 times.\nAnd then we extracted all of the inner durations\nfrom the logs of every single invocation.\nAnd we started plotting these using Jupyter Notebook and Matplotlib.\nSo since we're talking about Python data science,\nwe're using all the familiar tools here.\nNow, the initial results we get from the first invocation\nare pretty bad for images.\n\n\nAnd this kind of proves the suspicion that most people have\nthat the first time you end up with significantly worse cold starts\nfor images.\nAnd the difference we're talking about is that for zip package functions,\nwe're getting about four seconds of cold start.\nBut for image package functions,\nit's more like nine seconds to begin with.\nSo it's significantly worse.\nSo this makes sense, I guess, but we run it again.\n\n\nAnd the second time we run it, now we can force a cold start again\nby changing the environment variables of the functions.\nBut we did that, but we also waited 90 minutes as well,\njust to be sure and let things settle.\nAnd the second time we invoke it, the results are completely the opposite.\nSo this zip package functions invocation times\nare still three to four seconds, pretty much the same as the first one.\n\n\nBut the image package functions cold starts have gone down\nto one to two seconds,\nmostly in the one to one and a half second range.\nSo this is completely different.\nSo we decided, okay, let's leave it.\nAnd the wait overnight and try it again the next day.\nAnd the next day we try it again.\nEverything is cold starting from the start again,\nand we get the same results, images way faster,\ndown to one second, one and a half seconds,\nwhereas the zip package functions,\nthey are still between three and four seconds.\n\n\nAnd by the way, the one with layers is a little slower\nthan the one without layers when it comes to zip packaging.\nSo I think this confirmed some of our suspicions in both senses,\nbecause we had also been hearing from other members of the community\nand other AJ Stuyvenberg has been doing a load of great research\naround cold starts as well.\nSo it wasn't totally a surprise to us.\nIt was kind of showing us that if you want to optimize your Lambda functions\nand you're doing heavy dependencies in Python or any other language,\nthat image deployments seem to actually be a great fit.\nYou just have to exclude that first batch of implications\nafter you deploy your function.\n\n\nLuciano: Yeah, I'm still a bit surprised about this result.\nAnd I'm wondering if you try to figure out exactly\nwhat's going on behind the scenes\nto justify this better performance in terms of cold start of containers\nafter, let's say, a second round of cold starts.\nSo I'm wondering, what have you tried to try to understand\nwhat's really going on?\nMaybe you tried different memory configurations.\nMaybe you tried to understand,\nis it something at the file system level that is happening?\nAnd maybe in general, you can walk me through the process\nof how did you do these benchmarks?\n\n\nEoin: Yeah, I mean, the benchmark gathering process is pretty simple in that we had just an illustrative Lambda function\nthat's generating some random data, doing some aggregations on it\nwith pandas and then uploading it to S3\nafter using PyArrow to convert it into Parquet format.\nAnd this is pretty standard, I guess, in the Python data science world.\nSo when it came to understanding the benchmarks\nand why we were getting this performance, bear in mind as well,\nsome of your data analytics workloads might take 30 seconds\nor 90 seconds to run,\nin which case your 4 to 10 second cold start\nmay not be that big of an issue.\n\n\nThis workload I had was taken about 500 to 1,000 milliseconds to run,\nso the cold start was massive.\nWe were able to figure out what was going on\nand why images were giving us better performance\nbecause luckily, the Lambda team at AWS\nwrote an excellent paper on it,\nand it's called On Demand Container Loading in AWS Lambda.\nThe link will be in the show notes.\nIt's not a very long paper, but it is pretty interesting to read.\n\n\nAnd the gist of that paper is that they've put together\na whole set of optimizations around container image deployment\nthat doesn't exist yet for zip package deployments.\nSo I guess when they were building in the container image support,\nbecause they had to support larger file systems,\nthey had to figure out how are we going to make this work\nwithout creating 30 second cold starts for people.\nAnd how they do that is pretty clever.\n\n\nSo when you deploy a container image function,\nit's not like a normal container runtime environment.\nThey take all of your container image layers,\nyour Docker image layers,\nand they flatten them all out into a flat file system.\nAnd then they chunk all of those files into like 500K blocks.\nAnd those blocks are encrypted, but they're encrypted in a special way\nthat if two customers deploy the same 500K block,\nbecause they're using shared similar dependencies,\nthen they'll be able to be cached once, which is really cool, right?\n\n\nBecause your private blocks are still going to be private,\nbut they will recognize if people have got\nthe exact same binary block of 500K.\nAnd that way, it doesn't have to deploy all of the common stuff\nthat everybody's going to have across base images.\nYou can imagine Linux shared libraries,\nother utilities, Python modules, node modules,\nwhatever else it is, Java jars, they'll be cached,\nand everybody can benefit from that cache.\n\n\nAnd within this whole caching system they built for container images,\nthey've got a tiered cache.\nSo caches exist in each Lambda worker on the actual node\nwhere Lambda is running the code,\nbut they also have an availability zone cache as well.\nSo if chunks are not in the worker cache, it'll go to the AZ cache,\nand if the AZ cache doesn't have the block,\nit'll go to S3 only then, right?\nAnd the paper reports cache hit rates of 65% in the worker cache\nand actually 99% in the AZ cache.\nSo this is why we're getting this massive performance benefit.\nSo even though your container image has 10 gigs,\nand it can benefit from this cache all the time,\nand it also benefits from the fact that in a 10-gigabyte container image,\nyou don't need to load most of the files most of the time.\nThere's only a subset you'll ever need.\nSo they've got this virtual overlay file system\nthat takes advantage of this, reads from the caches,\nand makes it highly performant.\n\n\nLuciano: That's an amazing summary, and yeah, it's also very interesting to see that AWS has gone to the length of publishing a paper\nso that we can learn all these new techniques\nand see how they are building systems at scale,\nhow they are building this amazing caching strategy.\nSo I guess going through to the end of this episode,\ndo you have a definitive recommendation\non whether people should use zip or containers?\nMaybe not necessarily just in the data science space,\nbut more in general?\n\n\nEoin: Yeah, I still think zip packaging isn't as developer-friendly\nas container image packaging,\nbut because you get AWS still taking the responsibility for your runtime,\nI still think it's the best preferred way initially,\nand only if you have a need such as this to go for something beefier.\nThen you've got a lot of module dependencies.\nMaybe you've got even machine learning modules in your image.\nThen you can think about using container images,\nand I would say it's not as bad as we thought it might be,\nand as long as you've got the security situation covered\nand you're patching and upgrading your base images regularly enough,\nI think it'll do fine.\n\n\nIt's probably also worth pointing out\nthat because we did multiple memory configurations,\nwe were able to see if there was much of an impact\nfrom changing the memory size on performance,\nand the answer is that there wasn't really,\nand there's probably an easy explanation for that.\nWe know that when you run Lambda, you set the memory size,\nand the amount of CPU and network I.O. and disk I.O.\nyou get is linearly proportional to the memory you set.\n\n\nSo I think it's 1,769 megabytes of memory\nis equivalent to one virtual CPU,\nand if you use less than that, you'll get less than the CPU.\nIf you use 10 gigabytes, you'll get around six CPUs,\nbut actually in the cold start phase of a Lambda function,\nthey always give you two CPUs.\nEven if you've only allocated 256 megabytes of memory,\nyou'll still get two CPUs to kind of give you that extra boost\nwhile your function is cold starting.\n\n\nSo the memory configuration didn't really affect cold start performance.\nThere were some minor discrepancies, but nothing impactful,\nat least for this set of benchmarks we ran.\nWe also kind of looked at the individual module load time,\nso we broke the benchmarks down into the time\nto load individual modules.\nPandas, I think, is one of the worst ones for load time.\nIt can take up to four seconds on Lambda without any optimizations.\nSo it's meaningful, all this load time.\nWe also looked at things like the time to initialize Boto3\nand the time to initialize power tools.\nThese all had an impact.\nYou can see the exact numbers in the benchmark reports.\nWe've got lots of visualizations and tables\nof all the results in the blog post,\nbut I think you don't have to worry too much about memory configuration.\nFor cold start, that's more of an issue for the handler execution itself.\n\n\nLuciano: You mentioned already a few interesting resources, like the paper published by AWS.\nIs there anything else worth mentioning other than that paper,\nour source code, our blog post?\n\n\nEoin: Yeah, actually, just before we published it,\nI was able to catch Heitor Lessa and Ran Isenberg's talk\non pragmatic Python development in Lambda from Reinvent.\nIt gave me some insight into some more tools\nyou can use for Python performance analysis.\nI'd definitely recommend you check that out.\nIf I'd seen that beforehand,\nit might have changed the approach on this benchmarking.\nI mean, I think the results are still the same,\nbut they have some nice tools for visualizing import times in particular\nand runtime analysis as well.\n\n\nI think another thing to think about with all this\nis that when we see the import time performance for Python,\nand we just looked at pandas and PyArrow and that sort of thing,\nbut other modules can be even more significant.\nWe didn't even deal with like Scikit or PyTorch or any of those.\nWhen we think about Snap Start,\nthe new feature that was launched for Java Lambda functions last year,\nif that support comes in for Python as well,\nthen you can imagine a world where when you deploy your function,\nit can go through this module import time\nand then checkpoint it and freeze it.\n\n\nThen when your function is invoked,\nthe cold start won't have to include the module load time anymore.\nThat's going to make a big difference\nif the Lambda team can introduce that support for Python as well.\nWe've got a good few resources there.\nLuc from Donkersgoed has written a couple of great articles\non cold start performance, which we'll link in the show notes.\nAnd we've also got the blog post,\nwhich gives all the details that we've gone through here,\nbut a lot more as well on all the visualizations of data,\nas well as the source code repository.\nSo we do recommend that you check those out.\nAnd with that, please let us know if you've got any tips and tricks\nfor optimizing Lambda functions, especially Python Lambda functions.\nThanks very much for joining us, and we'll see you in the next episode.\n"
    },
    {
      "title": "109. What is the AWS Project Development Kit (PDK)?",
      "url": "https://awsbites.com/109-what-is-the-aws-project-development-kit-pdk/",
      "publish_date": "2024-01-12T00:00:00.000Z",
      "abstract": "This episode of the AWS Bites Podcast provides an overview of the AWS Project Development Kit (PDK), an open-source tool to help bootstrap and maintain cloud projects. We discuss what PDK is, how it can help generate boilerplate code and infrastructure, keep configuration consistent across projects, and some pros and cons of using a tool like this versus doing it manually.\nIs PDK something you should use for your cloud projects? Let's find out!\n\nAWS Bites is brought to you by fourTheorem, the ultimate AWS partner for modern applications on AWS. We can help you to be successful with AWS! Check us out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources.\n\nThe official PDK website (and documentation)\nOur previous episode 16. What are the pros and cons of CDK?\nOur previous episode 93. CDK Patterns - The Good, The Bad and The Ugly\nProjen's official website: https://projen.io/\nIntroduction talk to Projen at CDK Day 2020\nOur previous episode 70. How do you create good AWS diagrams?\nBuilding a shopping list app with PDK (tutorial)\nPDK in-depth developer guides\nOpinion about PDK by Vlad Ionescu on X\nYeoman\nCookieCutter\nTerraform project generation example\n\n",
      "transcript": "Luciano: Ever find yourself bootstrapping new cloud projects, endlessly copy-pasting config files and drowning in repetitive tasks?\nWhat if you could ditch the copy-paste chaos and channel all of that energy into real value, crafting your project business logic itself?\nToday we are going to discuss a new AWS open source project that aims to address this problem.\nIt's called AWS Project Development Kit, aka PDK.\nIf you stick around, you will learn what PDK is, what kind of features it supports, how to get started with it, and finally we will also disclose our opinion on it,\nwhat are the pros and cons, and whether you should be using it or not, depending on the kind of company you work with.\nMy name is Luciano and I'm joined by Eoin for another episode of AWS Bites podcast.\n\n\nEoin: We have discussed CDK in the past, but what is PDK?\n\n\nLuciano: Yeah, it's another three words acronym, so it might be more confusing than ever with all the variety of tools that exist in AWS.\nWe have a new one with another acronym, so the name PDK stands for Project Development Kit, and it's relatively new.\nI actually checked the first commit was like more than two years ago, so I think it has been going on for a while, but I think only now it's starting to get a little bit of traction.\n\n\nMaybe it's becoming a little bit more stable.\nWe only discovered it like last week, so it came under our radar very recently.\nSo the idea is that if you actually have the problem that we discussed in the introduction, so you spin up projects and projects, you work in a very active company with lots of stuff going on,\nyou end up probably copy pasting a lot and lots of boilerplate being somehow managed, and this tool is trying to give structure to that kind of problem.\n\n\nSo how do you consistently put up new projects? That's the goal of the tool.\nAnd just to give you an example, if you start a JavaScript project, you probably know how many configuration files do you need.\nYou probably need a TypeScript config, linting config, coverage config.\nYou will have some kind of CI CD, and when you start to add on top of that, also infrastructure with CDK configuration and all that kind of stuff, it only compounds,\nand you have all this massive configuration file that you have to manage over and over and over again,\nand then every time there is a change in your organization, good luck trying to keep everything standardized and up to date.\n\n\nSo that's kind of the problem space that this tool tries to address.\nAnd the example that you get from the website is basically you can build a monorepo, so that seems to be kind of one of the main cases that the tool tries to address.\nSo when you build a monorepo, you can also have additional things like how is the build pipeline going to work?\nMaybe I only want to build the projects that are actually changed from my latest commit, and you need to have some kind of mechanisms to do all of that consistently.\n\n\nGenerally, you also have caching. You might have dependencies between the different projects that are part of your monorepo.\nSo this tool also tries to give you all the tools to manage all the different problems in a consistent way.\nIt is built on top of CDK and Projen, so two very interesting technologies.\nWe have been speaking about CDK before, and we have an entire episode dedicated to that, so we will have a link in the show notes.\nBut if you've never heard of CDK, it's pretty much a way to do infrastructure as code using programming languages.\nSo rather than using YAML, you can use TypeScript, Python, Java, Go, and so you can think of it as terraform or cloud formation,\nbut using those languages and variables and classes and instantiating constructs rather than defining the clarity code.\nOkay, so yeah, I mean, we like CDK for a certain number of cases.\n\n\nEoin: I think we've both got our reservations for when it comes to CDK wider adoption, but we know why it shines.\nIt's really good for bootstrapping and prototyping and building dynamic infrastructure as code,\nbecause you just create those classes and it can create a load of boilerplate for you.\nSo I can kind of understand that if you've got project templates and all that boilerplate that you have with new projects,\nespecially with full-stack projects and monorepos, it gets to be a lot, right?\nAnd copy-pasting everything every time is really very difficult to get a handle on it\nand then keep it up to date over time as some of those dependencies change.\nSo I'd say, okay, there's definitely a problem to be solved here.\nI guess I haven't seen a perfect solution to this. We've seen some odd things in the past.\nAnd you mentioned Projen. So Projen is something I hear of behind CDK experts.\nI hear Matthew Bonig and other CDK Day speakers talking about Projen from time to time.\nDo you know much about it? Can you describe how it fits into the ecosystem?\n\n\nLuciano: Yeah, it's probably worth detailing a little bit more what Projen is.\nBy the way, it's spelled Projen as P-R-O-J-E-N.\nSo it sounds like project, but it's Projen instead, which I always find it a little bit of a funny name.\nAnyway, it's a project that was started. It's an open-source tool.\nIt was started by Elad Ben Israel.\nAnd there is actually a very interesting talk with a demo that summarized very well in just 15 minutes what the project does.\n\n\nAnd you can find it from the... It's one of the talks from a CDK Day from a couple of years ago.\nSo we will have the link in the show notes if you want to just 15 minutes intro of what it does and how it works.\nBut the idea is that it basically allows you to synthesize project configuration files.\nWe discussed before, if you start a JavaScript project, probably you need a Packet JSON, TS-Config JSON.\n\n\nYou will need some kind of Git ignore, ESLint configuration, Jest, and all that kind of stuff.\nSo the idea is that it can synthesize all of that stuff for you in an opinionated way, but you can probably pick and choose from different templates.\nAnd the more revolutionary idea is that it doesn't just do that once.\nLike you don't just bootstrap your project and that's it, and then you take it from there.\n\n\nIt is a way to manage all these files going forward as well.\nSo these files that are created, you can kind of consider them read only.\nSo you're not going to attach them manually anymore.\nBut instead you're going to be using a more, let's say, consistent configuration,\nwhere that configuration gives you a programmatic interface that looks like the one you get with CDK.\nSo you write typed codes with classes and variables and so on.\n\n\nAnd in this code, you actually try to describe exactly what this that you want to generate.\nFor instance, if you want to generate a front end with React, it will know how to generate code for all these configuration files that are suitable for that particular environment.\nAnd of course, you can tweak things.\nSo you can change configuration options that maybe you want to change, that may be different from what you get by default.\n\n\nAnd it's basically a CDK, but for generating project templates with all the related configuration.\nSo if you use something like Projen, you can spin up multiple projects in your organization.\nAnd then if there is a change that you want to apply across the board, you can easily just change one line of configuration and reapply to all the projects.\nAnd basically will align all the configuration across all of them, which seems very powerful.\nSo again, it's just a tool to generate configuration, but also tries to give you a way to keep that configuration consistent,\nwhich is something that is missing in most of the other tools for generating boilerplate or configuration or just bootstrapping projects.\n\n\nEoin: Okay. This is the kind of project do you think that where you might have like a platform engineering team or something maintaining this, these constructs, this Projen stuff?\nAnd they would say, okay, there's new versions of all of this front end stuff or even backend libraries coming out all the time.\nSo we'll have a maybe a test project and a build pipeline that will test the generator for the project.\nWe'll check that it all works and builds together, and then people will have an easier time upgrading and aligning to the latest version\nand keeping kind of evergreen all of their dependencies in the individual projects in a bigger company.\nYeah. I haven't used it myself outside that just a quick demos, to be honest, but I can see organization using it that way.\n\n\nLuciano: And you can also take it to the next step and even automate that.\nSo you can have pipelines that will look for updates on, I don't know, common dependencies or maybe look for vulnerabilities\nand automatically try to apply the updates around all the tests.\nAnd if everything is green, they can just submit a request.\nSo some kind of like the bot you get with GitHub to apply to your project configuration rather than just do your own dependencies.\nSo that could be a cool use case, but I haven't seen it, to be honest.\nIt's just making it up on the fly because it kind of makes sense.\nOkay. It sounds kind of valuable.\n\n\nEoin: Every time I go back to a project I've written even like a year or two years ago, full stack project, I find there's so much that has stagnated or decayed because everything moved on and node versions are out of LTS\nand frontend libraries have all these vulnerabilities in them.\nSo you can't just leave code alone anymore.\nYou have to constantly refresh it with the latest dependencies.\nSo I can see how that might help.\nHow do we then get started with Projen? What's the first hello world we need to think about?\nYeah. So Projen will be part of your PDK.\n\n\nLuciano: So you don't really have to think about it too much because PDK is, let's say, an additional layer of abstraction that uses Projen and CDK.\nSo if we look at the quick start guide that you can find on the PDK documentation, the idea is that you can start by creating like a monorepo\nand then on that monorepo you can add additional things.\nSo some of the things that you can do are, for instance, you can create an API backend and you can use either OpenAPI or Smitty.\n\n\nIt can also generate documentation in a variety of formats for your own APIs, for your own frontend.\nIt can generate infrastructure as code.\nFor instance, if you want to ship your API as Lambdas, which I think is the default, it will generate all the infrastructure as code using CDK for that API layer to be deployed in Lambda.\nIt can also generate libraries.\nFor instance, one thing I found interesting is that it's very easy if you create an API and you want in the future to create a React frontend.\n\n\nIt can also generate a library that contains React hooks that are built with all the auto completion and all the models that you have in your own API.\nSo it makes it much easier for you to then call that frontend because you don't have to think about how do I do this specific API call.\nYou just use the hooks and you have a much more expressive experience that way.\nIt can also generate the boilerplate code for your Lambdas handler, React frontends for single page applications, deploying them on S3.\n\n\nIt can also use Cloudscape, which is the design system published by AWS.\nAnd finally, one of the cool things that I think are very interesting is that it can also generate diagrams.\nSo for your own architecture, based on the CDK code that you have, as you build, it also spits out diagrams that you can visualize to make sense of the architecture that was generated.\nSo to get started, what you need to do is you need to have Node.js installed.\n\n\nAnd then at that point, you can run npm install aws-pdk.\nAnd that will just install the CLI tool.\nIt might require some additional dependencies.\nOf course, you need to have the AWS CLI installed.\nIt might need to use Git, depending on the modules you use.\nIt might also need to use Java and Maven, again, depending on the kind of modules that you're going to be using with PDK.\nSo you might need to install additional dependencies.\n\n\nBut generally speaking, having a JS and a WCLI should be more than enough to get started.\nThen you can scaffold a new PDK project.\nAnd this is a bit of a meta statement.\nYou can use a generator to generate a generator by saying PDK new in the template.\nFor instance, PDK new mono repo dash ts will generate the basic PDK project to manage a TypeScript-based mono repo.\nAnd once you've done that, you can already browse around.\n\n\nSo it will generate a bunch of files for you.\nAnd you can see, for instance, that it created a Projen RC file, which is the starting point where all the configuration leaves.\nAnd that you can modify to add additional bits and pieces and then make changes to the project structure.\nSo at this point, if you run PDK, it will actually do all of that.\nIt will generate all the files.\nBut then you can also run PDK build, which is kind of a wrapper around all running build on all the packages, basically.\n\n\nAnd if you do that, since it's going to create also CDK stuff, you can also go inside.\nI think it's going to be packages slash infra.\nAnd you can see all the CDK stuff that was generated and built.\nAnd inside that folder, you will also find the diagrams for the architecture that was generated.\nSo this is one more way to generate diagrams.\nWe have a dedicated episode on how to generate diagrams for AWS.\n\n\nIf you use this tool, it's an additional way.\nSo if you're interested in this topic, we'll link to the previous show where we talk about creating diagrams for AWS.\nAnd then finally, there are additional commands.\nFor instance, this is only local so far, but at some point you will want to deploy all of this code to AWS.\nSo PDK as a wrapper gives you a high-level command called PDK deploy, which can deploy in dev mode using CDK or swap.\n\n\nOr for production.\nAnd basically it will deploy everything that needs to be deployed.\nAnd finally, if you are just testing things out and you want to destroy everything, there is a PDK destroy command that will clean up all the deployed infrastructure.\nSo I guess the next question will be where do you go from here?\nSo this is just a quick run through on how do you get started and what you can do with it.\n\n\nBut if you look at the documentation, there are more examples.\nThere is an interesting guided tutorial where you're going to be building an API in the front end.\nThen add login using Cognito to this front end.\nAnd finally, you can also add more advanced capability.\nIt's kind of a shopping list application where it's a little bit more interactive and you get to see all the moving parts.\nAnd there is also a developer guide that covers specific modules.\nSo you can see, for instance, what the static website module does in detail and all the other modules that are available by default.\nSo we will have links to all these tutorials and the detailed documentation in the show notes if you want to take it from there.\nOkay, cool.\n\n\nEoin: And I've already fallen into the habit of mixing up Projen and PDK.\nBut if we're looking at PDK, then I just tried, as you mentioned, I just tried NPM installing it.\nAnd I can see that when I want to do a PDK new, it gives me options of doing a React website, infrastructure projects, and then the Monorepo.\nYou mentioned Monorepo TS, but there's also it looks like Monorepo Java and Python.\nSo those are the languages that are supported. Is that right? Java, Python, and TypeScript?\nI think there is a difference between the code that you write, which is probably depending on what Projen supports, and then the code you actually generate, which probably there you can create your own modules and generate pretty much anything if you really want to make an effort.\n\n\nLuciano: So, yeah, in a sense, I think we need to distinguish between the code that you can use to manage PDK and Projen and the code that you can generate.\nAnd I think, yeah, you probably have options on the first one, but on the other one, it's pretty much up to you.\nYou will have good defaults if you stick with Python or TypeScript and standard ones. But if you want to go wild and use, I don't know, Elixir, you can probably still do that as long as you put the effort into generating all the modules for that.\nOkay, nice. Yeah, it looks like yeah, those are the Projen supported languages from what I can see anyway, those three.\n\n\nEoin: Okay, which I guess will cover bases for a lot of people, but there's no C sharp, it seems like which is is supported in CDK. So maybe that's something that will come down the tracks.\nI think that was a pretty good overview. What about your opinion then? What's the good and bad? What are the pros and cons as we look at the current state of PDK?\nYeah, let's start with the good things. I think it's great to have a tool that tries to standardize the way that you bootstrap projects, but not just bootstrap, but also maintain them over time, which I think is where most of the other tools fail, because it's easy to generate a template and just copy paste it.\n\n\nLuciano: There are millions of tools that you can use to do that and customize things a little bit. But then after that, you're pretty much on your own and then projects will drift over time and then it becomes harder to to to add that level of consistency.\nA tool like this can actually be a solution to that problem. Now, this is a vague statement intentionally because we haven't used it to the level that we can say that it actually fulfills that objective, but it seems very promising.\n\n\nThe other thing is that it's also focusing on giving you some degree of standardization when it comes to documentation through using open API, other tools to generate documentation, generates diagrams.\nAnd this is one of the big pain points that I see, especially going to larger organizations. Documentation is always very inconsistent. Sometimes it's lacking. It's not up to date.\nSo a tool like this can also address that kind of problem and encourage people to be consistent, make it very easy actually to have that level of consistency.\nAnd the other thing is that if you like CDK, I think it's a very similar experience, so it will feel familiar enough. So I'm going to put that on the good side. That is not going to try to reinvent a new paradigm, but it's giving you something that if you have used CDK, it should be pretty familiar.\nDo you want to ask?\nOkay, so that's the good part of PDK. Are there any cons? What do you think? What would stop people from using it today at least?\n\n\nEoin: Yeah, this is a very opinionated take based on my quick experience with PDK. So I hope it's not going to sound too harsh because that would be unfair. But the first thing that kind of made me a little bit, I don't know, not enjoy the experience is that it feels very opinionated.\n\n\nLuciano: There are lots of starters and all the starters are very opinionated in a way that is very AWS heavy, which is not necessarily a bad thing. But it feels like the tool wants to encourage people to use more of these AWS things that maybe are not necessarily the best in class for the specific topics of interest.\nFor instance, you get to use CDK for infrastructure as code, which we know is like a tool that does the job, but it's AWS specific for the most part. So AWS is already pushing that kind of tool onto you if you use PDK.\nSimilarly, Projen is probably a little bit more generic, but has a lot of ties with CDK and other AWS tools. So you will probably see Projen more in the AWS space than in other ecosystem.\nThen there is Meety, which is the internal tool from AWS to generate types and models and then create APIs based on that. You can also use OpenAPI, but Meety seems to be kind of the default.\nThen there is Cloud speed.\nAnd does that support GraphQL or is GraphQL outside of this completely? Because I know there's a whole, I knew everything that you said Smithy could do seems to be already supported in another GraphQL ecosystem and tooling like Apollo, etc.\n\n\nEoin: Might be. Honestly, I haven't looked in detail. I expect that there will be modules for that to make it easier. I don't know if there are already built in or if you can easily create your own.\n\n\nLuciano: The other one is Cloudscape. So there is this new designs, relatively new design system that was published by AWS. And you might argue that is not the most famous design system, at least not yet.\nBut of course it's a tool coming from AWS. So they had to pick the design system from AWS. And then there is Cognito for user management. So again, it's just that it's very opinionated in a too much AWS heavy way.\n\n\nSo you probably need to go the extra mile if you want to use other options out there and create all your own starters, starter modules and use, I don't know, maybe you don't like Cloudscape.\nYou want to use Bootstrap or material design. You're probably going to be a little bit on your own to figure out how to migrate that original Cloudscape module into something else.\nAnd then there might be a little bit too many dependencies. So you need to have Node.js, CDK, probably Java, Maven, Maven, AWS CLI.\n\n\nSo you already need to buy in into the entire developer ecosystem of AWS plus additional new things if you want to use this tool.\nSo maybe the starting point, if you're just starting from scratch, might feel like there is a little bit too much work involved into just getting the development environment ready.\nAnd I think this is the main one that I have now is that it can feel quite complex, especially at first.\n\n\nAnd if you think about that is because there are many layers of abstraction. So when too many layers of abstraction is too many, I don't know.\nBut it feels like we are getting to a limit here because you basically have a project to manage projects and you basically have to bootstrap this project to manage projects.\nAnd then every time you have to do a change, you will have this PDK making change into CDK, which will make changes, for instance, into generated cloud formation.\n\n\nSo there are so many levels of generating code. So if you have an issue, first of all, good luck trying to figure out where the issue is, then figuring out at which level you actually need to apply the fix and how to do that.\nAnd then I expect that there will be so many escape arches sometimes just because maybe something is not supported at one level and you need to figure out exactly how to do a workaround.\n\n\nSo, yeah, I guess having too many layers just adds complexity. And with that level of complexity, the development experience might not always be nice, especially when you have to deal with bugs and things to fix.\nSo my final, I guess, judgment is that it feels like something that AWS wanted to have internally because they probably built lots of examples, lots of projects, and they have their own preference on how to do all of that.\n\n\nAnd that's probably why all the main tools that are coming as default ones are AWS specific tools. And they just made it open source because other people might find it useful.\nBut right now the state of the project makes me think that this started as an internal AWS thing, as a convenience thing, and now it's maybe getting into general adoption.\nSo all my opinions might get better over time as maybe with more general adoption, there will be more variety in that ecosystem. But right now, we just feel something fresh off of AWS that only AWS can really get value off.\nGot it. That makes sense. I think there's probably also a benefit in that, the fact that it came, if it is internally used, it probably means that it's more likely to be supported and grow in the future.\n\n\nEoin: And that there are people inside AWS who are going to be passionate about it. And it's not like some sort of side project that they've launched as open source and they're looking for kind of a solution, looking for a problem.\nIf they're already using it, you know, I've seen Smithy pop up as a dependency in the new SDKs. So they're obviously using this ecosystem that might actually bode well for the future. Who knows?\nWhat's the general community sense around PDK then and adoption? Have you seen much traction?\nI've seen a conversation on Twitter, mostly started by Vlad Ionescu with AWS Hero. So we will have the link in the show notes if you want to read all of that.\n\n\nLuciano: He seems to be very skeptical and I'm not going to try to quote his opinion too much because I don't want to paraphrase it.\nBut yeah, I guess the feeling is that this is not necessarily a tool that he wants to use, but the trade is interesting because other people chime in and they bring different opinions as well. So I think it's worth reading if you're looking for other perspective on the tool.\nOkay, fair enough. And if people don't feel like adopting PDK, is it the only show in town or would you recommend any alternatives if people have the same problems that we're trying to address here?\n\n\nYeah, I guess in terms of bootstrapping projects, the most famous ones that I know are Yeoman, which was, I'm going to say was because I'm seeing it less and less in the last few years.\nBut it was the tool of choice for all things front end. So all things like JavaScript projects, front ends, APIs, and so on. While in the Python space, probably people will be a lot more familiar with something like Cookie Cutter, but the concepts are the same.\n\n\nYou can define a template, you can define some kind of configuration. And then when you install the template, you can combine that template with your own configuration and will generate a project with some degree of customization.\nAnd I think both projects after that step, like you are on your own, you need to figure out how to keep it going and how to keep that consistency. So in that sense, PDK might be a little bit better trying to address also what is the next step after you generated the project?\n\n\nHow do you guarantee that degree of consistency? So interesting different approaches there, but the tools are somewhat similar. Then we might also talk about specific generators. For instance, I don't know if you do React, there are many ways to generate React projects.\nThe most famous is create React app, but there are lots of other alternatives. Or I don't know if you do Lambdas, there are many generators. For instance, if you use Rust, we spoke before about Cargo Lambda.\n\n\nSo that's another very domain specific tool that allows you to generate projects. And then there are other approaches that are probably the most common that I see all the time when people just create a repository on GitHub, they publish it as a template.\nAnd then in GitHub itself, you can easily just say use this template and it's going to clone that repository for you in your own space. And then from that point on, you are on your own and you have just a starting point.\n\n\nAnd there are CLI tools like the Git, which basically does the same thing. It's just going to clone a repository for you and clean up all the Git artifacts, all the Git history. So you just have a clean slate starting from whatever it was published in a specific repository that you are starting from.\nAnd one more alternative that I believe is worth mentioning is Terraform. One of our colleagues, Connor Meyer, did a very good demo. We will have the link in the show notes showing that you can use Terraform to also bootstrap projects and have some degree of consistency.\n\n\nBecause there are modules, for instance, for Git to create repositories, integrate with GitHub or other repository. And then from there, you can add a bunch of additional things. You can bootstrap scripts that, for instance, will install dependencies and so on.\nSo even a tool like Terraform can be used to do something like this. And it will be a little bit more general purpose because you can use it also to bootstrap other types of projects that don't have necessarily to be AWS specific.\nOkay, nice. Yeah, I could see that looking at the list of existing tools like Yeoman and Cookie Cutter, I've used them and they're great for bootstrapping projects, but not so great at keeping them up to date and retrofitting upgrades into an existing project.\n\n\nEoin: And I can see how PDK is trying to address that a little bit, but seems like there's a lot of complexity. Kind of makes me ask the question, do you want to use these tools at all? Or are you just better doing it by hand, trying to communicate within an organization and provide references and templates for people just to copy paste and then just use some other good hygiene or automation to keep things up to date? Do you need a project manager tool like PDK?\n\n\nLuciano: I think the answer as every good technical question is, it depends. And I will answer with another question, which is what is the value here? What are we trying to achieve? I think the most important thing is trying to reach that level of global consistency.\nAnd by that, I mean that you probably want to get new projects started very quickly. You want to embed best practices in your templates. You want to reduce the amount of choices that developers have to face every time they are starting a new project.\n\n\nAnd also you want to create projects that are similar enough that it should be easy for developers to cross collaborate, or maybe if they have to change team to make the transition as easy as possible.\nSo basically you want to have a tool that keeps people productive, and it makes people focus on the things that matter, which is probably the business logic and not all this layer of configuration that you are repeating over and over and changing maybe slightly enough to create surprises.\n\n\nBut the other question is, when do you really need all of this? Because if you are a small scale company, or maybe a company that just started, you probably don't need all of this. You actually really want to go through the trouble of building projects from scratch to really understand what is the setup that works best for you?\nWhat are the technologies that you prefer to use for the specific domain you are in? And if you use this kind of boilerplate or starters, they will have their own opinions that maybe are not very suitable for your specific organization.\n\n\nSo you might be biased into using something that is not necessarily the best option for you. So that's kind of one of the risks that if you are a new organization, maybe is not the best option.\nSo you probably want to spend a little bit of time figuring out yourself what is the best way for you to build software.\nBut even for larger organizations, I think there is a little bit of a risk that you might end up stagnating in your technology choices.\n\n\nYou might end up just doing projects always in the same way. You might not embrace new technologies or new ways to build software. Or even worse, sometimes you need to build something very specific.\nAnd just because you have a boilerplate to build an API in a certain way, you just go with that without even thinking what are the trade offs.\nAnd sometimes you end up just using a hammer for everything, even though sometimes you don't necessarily need that hammer. You might need another tool.\n\n\nSo I think the risk with all these tools is just it's good when you need to do something very similar to avoid all the boilerplate and all the repetition.\nBut we know that software projects are always different and in subtle ways sometimes. So having a little bit of freedom and picking the technologies and the approach that can be more suitable for the specific domain you're working on.\nI think it's something that you need to figure out how to guarantee anyway if you want to keep building high quality software.\nAnd the final thing is that one good use case where this might be very valuable is when you have compliance obligations. Because in that case, I think it's actually important to have some kind of template where you are sure that all the best practices are built in and you don't have to go through a compliance review over and over every time you do something new.\n\n\nEoin: That sounds like really good advice. And thanks for a very thorough overview of PDK this year. Is there anything we wanted to say to wrap up?\n\n\nLuciano: I think in general, this is a very, I guess, personal opinion space. I think it's easy to have people with very different opinions. Some people actually enjoy the process of starting a new project and picking up libraries and maybe checking out what are the new things and whether they are worth using.\nI think there is actually a space there where you can be productive in ways that you generally cannot be productive when you're working on an existing project.\n\n\nSo some people might actually enjoy all of that work that might feel just as busy work, and there is value there for sure. So I'm really curious to hear people opinion and see, like, do you actually do all of that work or you'd rather just have a template that has made all of the decisions for you and just get over with and focus on the actual business logic that you're building.\nAnd if you've used this tool or other tools, like, do you like them? What do you like? What you don't like? Maybe there is space for something else that we haven't covered today that can solve the problems we are discussing about.\nSo I think that brings us to the end of this episode. I hope that you enjoyed. As usual, if you enjoyed it, please give us a like, subscribe, share it with your friends and colleagues, and we hope to see you soon in the next episode.\n"
    },
    {
      "title": "110. Why should you use Lambda for Machine Learning?",
      "url": "https://awsbites.com/110-why-should-you-use-lambda-for-machine-learning/",
      "publish_date": "2024-01-19T00:00:00.000Z",
      "abstract": "In this episode, we discuss using AWS Lambda for machine learning inference. We cover the tradeoffs between GPUs and CPUs for ML, tools like ggml and llama.cpp for running models on CPUs, and share examples where we've experimented with Lambda for ML like podcast transcription, medical imaging, and natural language processing. While Lambda ML is still quite experimental, it can be a viable option for certain use cases.\n\nAWS Bites is brought to you by fourTheorem, the ultimate AWS partner for modern applications on AWS. We can help you to be successful with AWS! Check us out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources.\n\nEpisode 46. How do you do machine learning on AWS?\nEpisode 108. How to Solve Lambda Python Cold Starts\nggml (the framework)\nggml (the company)\nllama.cpp\nwhisper.cpp\nwhisper.cpp WebAssembly demo\nONNX Runtime\nAn example of using whisper.cpp with the Rust bindings\nProject running Whisper.cpp in a Lambda function\nAWS Lambda Image Container Chest X-Ray Example\nEpisode 103. Building GenAI Features with Bedrock\n\n",
      "transcript": "Eoin: More people are getting into AI and running their own machine learning models.\nWhether it's generative AI, image processing, recommendation, or text-to-speech,\nthe need for somewhere to run machine learning models is increasing.\nFor many, they'll use hosted services and pre-trained models using something like OpenAI\nor AWS, but others want more control, improved data privacy, and might want to manage their\nperformance, scalability, and cost at a more fine-grained level.\n\n\nToday, we wanted to cover a slightly controversial choice for running machine learning models,\nand that's AWS Lambda. By the end of today's episode, you should have a clear idea of how\nand when Lambda can be used to run machine learning predictions and when to go for something\na little bit more traditional. I'm Eoin, I'm joined by Luciano, and this is the AWS Bites podcast.\nAWS Bites is sponsored by fourTheorem, an AWS partner with plenty of experience running machine\nlearning workloads in production. If you want to chat, reach out to Luciano or myself on social\nmedia. All the links are in the show notes. Now, back in episode 46, which was, \"How do you do\nmachine learning in AWS?\", we talked about the various ways to run machine learning models in AWS,\nand we briefly covered there the idea of using Lambda for inference, and this is the specific\ntopic we wanted to dive into in more detail today. As always, we should start with why.\nWhat are the use cases? Why do you need to run machine learning models?\n\n\nLuciano: I think it's important to clarify that generally when we talk about machine learning infrastructure, there are two different categories of workloads. One is when you need somewhere to\ntrain and test models, and the other one is where you are thinking about, you have a model, I need\nto run this model somewhere. And when we say run models, we mean having some kind of infrastructure\nthat can take inputs and run predictions or inference. So today we're going to focus only\non the second category, which is inference. So for training is generally something a little bit\nmore complex. You need more specialized infrastructure like many GPUs most of the time,\nand you can also do training with CPU, but it's generally much more limited.\n\n\nIt's going to take probably much longer, and depending on the type of model you are trying to build, it might limit you\nto the size of that model itself. So generally GPU is kind of the way to go when you're thinking\nabout training, especially the more complex the model, the more you'll need to invest in\ninfrastructure with lots of GPUs. So we focus today instead on inference, and it's something\nthat can also benefit a lot from GPUs, but it doesn't always require using GPUs. You can also\nuse, for instance, CPU. But let's talk about some use cases. One common use case is medical imaging.\n\n\nFor instance, if you want to run an automated diagnosis of an X-ray scan on demand, and maybe\nyou have a few images every hour, maybe running a model on a CPU against a particular image may take\none minute, and I think one minute delay on having a response is probably acceptable in that particular\nuse case. You don't need an instantaneous answer with the diagnosis for that picture.\nYou can probably wait one minute. Another use case is audio transcription, for instance, for video calls. Maybe\nyou are using a system that records your video calls in your company, and you want to have a way\nto have automated minutes like transcriptions and summaries of that meeting. And also in that case,\nit's probably acceptable to have some delay. Maybe a process running on CPU takes like half an hour\nto produce all of that summary and transcript. It's probably okay to receive an email half an hour\nafter the meeting with that document attached. Again, it's not a use case where you need an\nimmediate answer for that particular task. And finally, I have another example, which is,\nfor instance, you want to use word embedding models to index documents. Maybe you are building\na SaaS platform where users can attach different kinds of documents, and you want to make this\ndocument searchable. And maybe you want to make it searchable through, for instance, like a chat\nUI where you're using some kind of Gen AI capability. And of course, you need to index all\nthe documents in such a way that then the Gen AI can access it. So you're going to be using specific\nmodels to do all of that. And that might take a time, sometimes like, again, half an hour, one hour.\nSo the documents will only be available for search after a little while. But for most use cases,\nthis is probably acceptable as well. By the way, I mentioned the word embeddings. It's one of the\nnew terms that comes around a lot when we talk about Gen AI. If you don't know what it is, don't\nworry, we'll cover that during this episode. Now, I said that these applications generally\nneed specialized hardware, for instance, GPU. Should we spend a little bit more time clarifying\nwhat is the advantage that a GPU brings when compared to CPU?\n\n\nEoin: Yeah, the state of the art for most machine learning models uses deep learning. And deep learning is essentially employing deep\nneural networks. Neural networks have been around as an idea since, I think, the 1950s.\nAnd deep neural networks are kind of an evolution of that, that has become more popular in the last\ndecade or so. And the idea essentially is trying to model how we think humans' brains work, or\nactually any other animal brains work, simulating the idea of neurons and synapses and connections\nbetween nodes in our brains. So deep neural network architectures can have thousands or\neven millions of nodes. And as they're trained, at least at a basic level, the connections\nbetween nodes obtain weights, right? And it's those weights that are the most important and\nthe bulk of the parameters for a model. So when you hear people talking about model parameters,\nweights are generally the biggest number of them. And they need to be stored in memory.\n\n\nSo large memory requirements are typical. And the storage format are generally multi-dimensional\nvectors. You hear the term tensors a lot, and that's basically vectors of many different\ndimensions. And these are used to represent those networks and their parameters. And the operations\nthat need to happen in the CPU or the GPU, these are generally fast floating point matrix mathematics\nthat you need to occur. And that's why GPUs came into play, because GPUs originally developed for\ngraphics. That's where the G comes from. We've all had GPUs in our desktop PCs and laptops over time,\nparticularly popular with gamers, then became popular with Bitcoin miners. Now the biggest\ndemand is coming from machine learning, because GPUs have thousands of cores optimized for this\nkind of calculation and can run the many calculations in matrix operations and in\nparallel as well. So they're highly optimized for this particular application. And they also have\nhigher memory bandwidth for loading large data sets, which is quite important for performance\nin this space as well. We talk about GPUs, but of course, there are other AI chip technologies\nevolving that are not GPUs. AWS has its NeuronCore and NeuronCore 2 architectures,\nwhich you can utilize if you use the Tranium or the Inferencia instance types in EC2. And these\nare not GPUs, but are basically designed for more cost-effective and more power consumption optimized\nmachine learning. So I think we might see a lot more of that. Google also has TPUs, the tensor\nprocessing units. So we may see more adoption of those options instead of just pure GPUs,\nas people are kind of making trade-offs and optimizing for the availability of hardware,\nthe cost and power consumption critically as well. So hopefully we've outlined why GPUs and\nother special cores are really well suited for machine learning. So then why are we doing a\nwhole episode based around AWS Lambda and CPUs only? Why would you bother with CPUs?\n\n\nLuciano: Yeah, that's one of our current complaints about Lambda that doesn't support GPU, at least not yet. But that doesn't stop us from running machine learning inside Lambda,\nwhich is what we are going to be covering for the rest of the day. But let's talk a little bit more\nabout what is the trade-off with CPU versus GPU, because CPU compared to GPU is generally widely\navailable and much cheaper. And when you run things in the cloud, you definitely have lots\nmore options when you think about CPU compared to GPU. You can run things locally as well,\nand generally easier to run on many developer environments because CPUs are a lot more widely\navailable and standardized than GPUs. And it can scale much faster in a way that if you need to go\nthrough levels of concurrency by spinning up multiple instances, like multiple machines or\nLambdas or whatever, it's generally easier to do that if you think just about CPU. Because when\nyou bring GPU into the matrix, generally either the cost becomes more prohibitive, or maybe you\nhave limits that will stop you from spinning up thousands of instances, or even the provisioning\ntime might just be much higher than it is with provisioning CPU-based instances. So, again,\nthis is kind of the trade-off. While with CPU you don't have the power of GPU for parallel matrix\nmath, there are other things that you can take and use with CPUs to still have decent levels\nof performance. For instance, recent advancements in CPUs have brought us SIMD, single instruction\nmultiple data, which is a CPU extension that allows you to run vectorized operations. And\none example of that is AVX2, which is also available in Lambda. This is an Intel CPU\nextension and has been in Lambda since 2020. So if you write software that can take advantage of this\nkind of capabilities, you're still going to have pretty good performance on a CPU, and you don't\nnecessarily need to use a GPU. There are other examples. For instance, for ARM processors, you\nhave NEON, which is another extension that allows you to run SIMD. Now, even though you have a CPU\nmodel execution, it's not always obvious to say that GPU is always going to be faster. I think\nit really depends on the use case that you are trying to address and the amount of data that\nyou might want to process in terms of actual size of the single unit of data, but also in terms of\nhow much data can you actually parallelize in one go. And we can make an example. For instance,\nlet's say that we have a neural network that can process between\none and 100 images in parallel into a limited seconds having a GPU. Let's use this as a baseline.\nNow, if you take the same thing and put it in a Lambda, maybe you can run one inference with that\nLambda in two seconds. So it is a little bit slower, but the advantage of Lambda is that then you can\nmuch more easily run thousands of instances of that Lambda than it is of running, for instance,\nSageMaker instance with a GPU. Also, if you run a SageMaker instance with a GPU, that instance is\ngoing to take minutes to spin up, while when we think about spinning up a Lambda, that generally\ntakes seconds. So that gives you an idea that there might be cases where you can just take the\npower of parallelization and fast bootstrap times of Lambda, and you might end up with something that\ncan be even more convenient than just having one or a few instances with a GPU that are going to be\nmuch faster to do the single inference, but maybe all the bootstrapping time and the scalability\nis going to be overall slower. So it's not always obvious to say that GPU is faster than CPU.\nI think there are lots of use cases where you can make traders, and if you use Lambda with CPU,\nyou can still come up and win the race of this is actually going to be a better approach than\njust spinning up GPUs. So what do you need to get up and running? Are we going to run,\nare we going to think for instance about Python Lambda functions with PyTorch or Tensorflows or\nsomething else?\n\n\nEoin: Well, Python is supported by pretty much every model and framework out there, so it's probably your go-to when you're getting started. As we mentioned in a very recent episode,\na lot of the Python libraries are very heavy and can then lead to longer deployment times and\ninitial cold start times, so we'll have that link in the show notes. I would say that the space is\nfast evolving. It's almost like the machine learning framework space is a little bit like\nfront-end frameworks about five years ago where it's just moving so fast and new ones are coming\nout all the time. But maybe before we get into that tooling, we can talk about an extreme example\nand kind of play with this idea a little bit. Since Gen AI is all the rage, can you actually\nrun large language models on AWS Lambda? I mean, surely not is probably the default response to\nthat, but there are lots of open source models out there and people might want to take advantage of\nopen source models to run things in a private way just for their own experimentation or to\nreally focus on data privacy and security, and they will be thinking about how to optimize the\ninfrastructure then. And we're talking about open source models like Llaama from Meta or Mistral or\nthe new Microsoft one, Phi2, or even stable diffusion for images. Yes, generally the\nrequirements for these models to run them are huge, but not always. And when we hear people\ntalking about these models, they generally talk about the number of parameters in their model.\n\n\nWhen Meta released the Llaama2 model, this is an open source large language model comparable to\nGPT 3.5, GPT 4 in some ways, it was released with three different parameter sizes, 7 billion,\n13 billion and 70 billion. And there are models out there with hundreds of billions of parameters.\nSo what does that mean in terms of resources you need? Well, it depends on the numerical precision\nof the model. So you might have a model that's using 32 bit floating point values. So that's\nfour bytes per parameter. So then your memory requirement is going to be the number of parameters\ntimes four. So if you have that 70 billion parameter Lambda model with 32 bit precision,\nthat's 140 gigabytes of memory to run it. So you need a pretty high end GPU or you need to start\nthinking about parallelizing over multiple GPUs. And for this, even for the 7 billion parameter\nmodel, you're talking about 14 gigabytes. So it's quite a lot. But since resources are constrained,\nand not everyone who's enthusiastic about this space has access to GPUs with that kind of memory,\nthe community is putting a lot of effort into getting pretty good results with fewer parameters\nand lower precision parameters as well. So if you imagine using four bit integers instead of 32 bit\nfloating point, this is a process called quantization, where you can convert it into a\nlower precision model, all of a sudden, you can take the 70 billion parameter model, or sorry,\na 4 billion parameter model and run it in two gigs of RAM. So by tweaking both of those factors down\nby a significant amount, you can still get pretty good performance. And when I'm talking\nabout performance, I mean, accuracy of the models and the inference results. The just because you're\nscaling down by a factor of 10 or more, it doesn't mean that you're scaling down accuracy linearly,\noften you can get almost as good accuracy, depending on the use case and the model.\n\n\nSince GPT and chat GPT came out, a lot of the most exciting developments in the whole LLM space has\nbeen the development of these quantized models and the performance you're getting. Now, of course,\nrunning it on CPU is rarely going to be as fast as GPU. There's a lot of factors that can affect\nperformance. So it's difficult to say with any certainty, but 100 times slower performance,\nlike in the example you gave Luciano on CPU, it's not unexpected. That's quite typical.\n\n\nBack to the tooling then. So we talked about Python and we know about TensorFlow and PyTorch.\nWe talked a bit about those in the previous episode, but a lot of work now has been done\nin creating native frameworks and implementations. So machine learning frameworks that don't need\nall of the Python interface or a much lighter Python interface. And llama.cpp was one of the\nfirst one of these, and this was started by Georgi Gerganov, who then also went on to create ggml,\nwhich is a pure C machine learning library designed to make machine learning models\naccessible on commodity hardware. So if you want to run machine learning models on your Apple Mac\nARM processor, like an M1 or an M2, you could really look into this because it's got really\ngood support as well as for just CPU execution, also good support for Apple silicon GPUs. And\nthe ggml framework is now a company, ggml.ai, and it has funding to develop it further, which is good\nnews for us, I think. And I think it seems like a pretty good fit for Lambda because you can build\nreally small package sites that are really fast to deploy, pretty good on cold start time as well.\nAnd then there's bindings available for different languages. So you're not glued into the Python\necosystem, if you don't want all that heaviness, or you're just not a Python fan. And this ggml\nframework will adapt to different CPU and GPU architectures, depending on where you want to\nrun it. So it's easily portable from your local development environment into runtimes like\ncontainer runtimes or Lambda. And Georgi Gerganov has also created a lot of quantized versions of\nthe models in the format acquired by ggml. So you've, instead of having the 32 bit or 16 bit\nfor floating point versions, you have four or five or eight bit integer versions that you can\nuse to reduce your memory consumption. There are other alternatives apart from ggml, like the ONNX\nruntime, but we haven't used it directly. We have been working with ggml and experimenting with it\nover the past few months. And we found it pretty useful. And the results, while I wouldn't say\nwe're ready to deploy it at scale in production, we've had some pretty interesting results. So\nLuciano, would you want to take us through some of the, I guess, examples of Lambda for\nmachine learning we've been doing, at least publicly citable ones over the past few years?\nYes.\n\n\nLuciano: One of that we mentioned before in this podcast is the way we create the transcripts for our podcast, which is basically using SageMaker. And the startup performance of\nthat is a little bit of a pain. It's not a deal breaker because again, we don't really need\ninstantaneous response, but we were a little bit curious of checking what is the difference? What\nare the trade-offs if we try to run the same thing on Lambda? Can we do anything better? And\nwhat are the results? It's going to be cheaper. So what we did was basically experimenting and\ntrying to figure out exactly what we could achieve and what kind of results we could get.\n\n\nAnd we were very pleased to see that Gerganov also created a version of Whisper, which is the\nmodel that we use from OpenAI to do the descriptions. And this version has been\nimportant as well to C++ and with bindings for lots of languages, for instance, Node.js,\nWebAssembly, Rust. There is actually an amazing demo that Gerg created of running this model on\nthe browser using WebAssembly and it's totally available online. We will have the link in the\nshow notes if you want to play with it. So that kind of shows that once you bring the model to\nC++, it opens up a bunch of use cases that are not always so easy to access when you just have models\nin Python. And this is a use case that we work with. And I think we were very pleased with the\nresults. Seems a good tradeoff of performance is a little bit slower to do the inference,\nbut of course, it's much faster to bootstrap the environment. And I think we might spend a little\nbit more time in future episodes talking through the details of this experiment. This is still very\nearly on for us, so we're still trying to figure out exactly if the tradeoffs are convenient or not\nfor this particular use case. There are other use cases that are actually something interesting that\nwe have been playing with. For instance, you can use LLM models, for instance, Llama in Lambda.\nAnd this is something that basically you can try to ask questions and it generally takes about 30\nseconds to give you a response. So maybe it's not necessarily the best use cases because when you\nuse LLM and try to create kind of a chat-based interface, you want to have a more real-time type\nof answer. And with Lambda, it tends to do everything kind of in a batch approach where\nit processes everything, it creates that response objects, you get the response object that then you\ncan use in your frontend so you can see those 30 seconds of delay and it's a little bit painful\nto use for that particular use case. And there are other use cases that we have been working with.\nActually, the oldest one was four years ago when Lambda container image support was announced.\nWe were able to create a demo where we were embedding one of these models to do x-ray analysis.\nAnd we were able to run 120,000 x-ray images in about three minutes, which is pretty impressive.\nWe have a repository with all the examples and the code to run all of that and we will have a link\nfor that in the show notes. Is there any other use case that comes to mind, Eoin?\n\n\nEoin: Something that we've been looking at recently a lot actually is within the Gen AI space, retrieval augmented generation,\nor RAG, and it's becoming very common and it's one of the areas where Lambda might play a role.\nJust a quick overview of what RAG is. We mentioned also the concept of text embeddings and promise\nthat we'd define it. So the reason RAG has become popular is that LLM models like ChatGPT,\nthey have a limited context window. So the input size you can put into a prompt.\n\n\nSo if you want to query all of your company data and get a factual response processed by an LLM,\nso it's got good language in the response, you can't just put all your company data with a\nquestion into a prompt. It's too much data. It's not going to work. So RAG is one of the solutions\nto address this. Instead of putting all the data into the prompt, you retrieve relevant sections\nof your company data from a knowledge base and then put those sections as context into your LLM\nprompt, allowing you to get effective answers and summaries.\n\n\nAnd because you're using a real knowledge base as your context, there should be a much lower chance of hallucination or fiction\nin your response. Now, in order for this RAG to work, you generally need to index your documents\nfirst and put them in a repository. So this could be a traditional lexical search like with Elastic\nSearch or similar, but a more common approach now is to use a word embeddings LLM model. And this is\nsimilar to any other LLM model, but it's basically just creating a vector, multi-dimensional vector\nrepresentation of text in documents. And by having that multi-dimensional vector stored, you can then\ndo a semantic search on all of that textual data because it's a numerical format. You can do like a\nKNN search just to find similar terms to the question in documents and then retrieve those\nsnippets of documents, then take them and put them into the context as part of the prompt. And that's\nthe whole idea of RAG or retrieval augmented generation. And now the OpenAI, Bedrock and many\nmore have text embedding models for you to do that, that you can then use with the other, like\nwith the chat models. And when you use a model to create a vector embedding, you'll then store it in\na vector store. Like you could use Postgres with the PG vector extension. You can use just S3 with\nMeta's FAISS storage mechanism. Then there's other third-party solutions like Pinecone, Memento,\nand then you can perform those semantic searches when you have a query. So the LLM chat part of\nthat is fairly straightforward, but you need to think about what do you do when you've got lots of\ndocuments coming into your company's knowledge base and you need to asynchronously process them\nand add them to your vector store. And so this is kind of a sporadic bursty activity that doesn't\nreally require real-time performance and a Lambda with a reasonably sized text embeddings model could\nwork pretty well for that. So I think this is one of the areas where you might find Lambda being\nused, even though you might end up using a fleet of GPU instances or similar for the knowledge-based\nsearch or for a chat interface for your company's knowledge base, you might end up being able to\noffload all of the embeddings generation to a Lambda. Now of course this is a little bit of an\nadvanced optimization. Bedrock on AWS can do text embeddings and LLM predictions in a serverless\nway. We talked about that in our Bedrock episode, but you're limited there, right, to available\nmodels and the need to consider pricing and quotas. So if you want to use an open source model\nit's a little bit more difficult and that's why you might go more of a custom route. So check out\nthat previous Bedrock episode if you want a similar solution.\nWe know I suppose that these Lambda experiments we're talking about are kind of specialist, they're quite experimental and not\nnecessarily ready for the prime time, but it's still really interesting and I definitely recommend\nfor people who are just interested in the space to check out those frameworks like ggml, llama.cpp,\nwhisper.cpp, if you're running stuff on a Mac especially or your laptop in general. If you\ndon't want to go put all your data in open API there's also other great frameworks on top of them\nlike private GPT and local GPT which can run pretty well on a Mac or similar hardware and give\nyou that chat GPT-like experience but all within the safety of your own development environment.\nI think that's generally the conclusion time for this episode and while these experiments are\ninteresting and a little bit of fun it kind of remains to be seen whether Lambda can be\nan important service in the Gen AI space but for other more tried and trusted ML applications\ndoing inference in Lambda can definitely simplify, save costs, give you great scalability and\nperformance as well. But let us know what you think as always. Are we losing the plot a little\nbit going left of field with Lambda for ML or have you also had good results? Thank you for listening\nand we will catch you in the next episode.\n"
    },
    {
      "title": "111. How we run a Cloud Consulting business",
      "url": "https://awsbites.com/111-how-we-run-a-cloud-consulting-business/",
      "publish_date": "2024-01-26T00:00:00.000Z",
      "abstract": "In this episode, we discuss how we work as a cloud consulting company, including our principles, engagement process, sprint methodology, and focus on agile development to deliver successful projects. We aim to be trusted partners, not just vendors, and enable our customers' business goals.\nBy the end of this episode, you will know what working with a cloud consulting company like fourTheorem could look like and you might learn some strategies to make cloud projects a success! We will also digress a little into the history of software practices, common misconceptions, and what we believe should be the right way to build software.\n\nAWS Bites is sponsored by fourTheorem, an AWS Partner with plenty of experience delivering cloud projects to production. If you want to chat, reach out to us on social media or check out fourtheorem.com!\n\nIn this episode, we mentioned the following resources.\n\nWorking with fourTheorem\nAI as a service, book by Peter Elger and Eoin Shanaghy\nMajority of developers spending half, or less, of their day coding, report finds (TechRepublic article)\n2023 software.com Future of Work Report\nManaging the Development of Large Software Systems, Dr. WInston W. Royce, 1970\n\n",
      "transcript": "Luciano: At fourTheorem, the cloud consulting company where we work, one of the things that we get asked a lot,\nboth from our potential customers, but also from our peers in the industry is how do we work? And\nit seems like a simple question, but in reality, we can cover a bunch of different topics. Things\nlike what's our unique selling proposition and our target customer base? What the first engagement\nwith a potential customer is going to look like? Do we ever say no to potential customers? How do\nwe gather requirements? Why are software projects somewhat unique? How do we make plans, including\na desired architecture, estimates and success criteria? What do we do when the work starts and\nhow do we keep iterating over it? And what happens after the delivery? There are so many different\nways of working and methodologies to deliver software and cloud projects. We believe we have\nour own unique way of doing that and today we want to share it with you. Hopefully by the end of this\nepisode, you will know what working with a cloud consulting like fourTheorem could look like and you\nmight learn some strategies to make cloud projects possibly more successful. We will also digress a\nlittle bit on the history of software practices, common misconceptions and what we believe should\nbe the right way to build software in the cloud. I hope this is going to be a fun ride. My name is\nLuciano and I'm here with Eoin for another episode of AWS Bites podcast.\nAWS Bites is sponsored by fourTheorem, an AWS partner with plenty of experience delivering cloud projects to production. If you\nwant to chat, reach out to us on social media or check out fourtheorem.com. All the links will be in\nthe show notes. So why don't we start by giving a little bit of an introduction to our audience,\ntelling them what fourTheorem is, a little bit of the history, the people and some of our past projects made.\n\n\nEoin: fourTheorem started in 2017, actually that's already a good few years.\nThere are three founders in the company, myself, Peter and Fiona. And I guess the interesting thing about\nus as the initial team is that we've all been through a lot of startups in the past,\nlike our own startups, working in startups, bootstrapping, funding, building new products,\ngetting them to market, succeeding and failing. That kind of helps shape, I think, our culture\nand our mission a little bit. One of the founding principles is that we don't want to just become a\nbody shop. In the consulting world, it's pretty common. There's almost an inevitable gravitational\npull towards just throwing people at problems. And that's customers and consultants kind of\nenable this behavior a little bit instinctively because when things get tough, people tend to just\nsay, okay, let's add resources. We don't have enough resources to solve our problems. So let's\njust add more people. And this suits the business model of consultancies very well, because they're\njust earning money based on time and materials on a day rate. So you just times the number of days\ntimes the number of people, and you end up with a nice revenue at the end of it. But the problem\nwith that is that it doesn't really solve the core problems in general, the core engineering problems,\ncultural problems, etc. And we try to do it a little bit differently. So rather than just\nadding people, we try to focus on the minimal number of people almost to do projects and\nfocusing on the quality of the people and the quality of the work. And so far, we've been able\nto stick to that after almost seven years, six or seven years, which is good going so far, I think.\n\n\nAnd so we focus on a small team. We're deeply technical, I would say, but with a focus on\nbusiness because of, I guess, our previous startup experience, we optimize for delivering early and\noften so continuous improvement where possible, we were bounded with a big focus on AI and data\nscience. That was one of the reasons myself and Peter wrote the book on, you know, using Amazon\nmachine learning services in the beginning, just kind of stake a claim to that as a direction of\ntravel. We're seeing that a lot now these days with Gen AI and other machine learning fields,\ndata science, modern data architectures, production, those in a really optimal way\nwas going to be a big differentiator. And the rest is just focusing on more managed services. So\ntrying to remove a lot of the old school operations and maintenance that legacy systems\nhave seen a lot of in the past so that people can do more again with fewer people. We help people\nof all different sizes. One of the nice things about working for fourTheorem, I guess, as an\nengineer of any kind, or no matter who you are, is that we work with startups, but also larger\nenterprises. That kind of has a, there's a virtuous cycle within that because when you're working with\nstartups, you tend to move very quickly, iterate quickly. And therefore, as an engineer, you learn\nquickly and course correct quickly. In enterprises, the appetite for speed of innovation is less\nbecause there's more risk associated with it. So the pace is naturally different. But when people\nmove from a startup environment into one of our enterprise environments, they bring that innovative\nculture, lots of new learnings. And then when you move from an enterprise client onto a smaller\nclient, like a startup one, you kind of bring that rigor and process and compliance and governance\nand all of that stuff that's more common in the enterprise world. So it kind of benefits our\nclients in a virtuous cycle kind of way, but also our people. No matter what we're dealing with here,\nbig customer, small customer, I think the first engagement is kind of similar. Do you want to take\nus through the discovery part when we, okay, we've wooed a new client or we've been another client\nhas referred a friend of theirs in and we start talking to them for the first time. How does that work?\n\n\nLuciano: Yeah, this kind of meeting is what we generally call a discovery session.\nAnd it's, there is no commitment from either parties to proceed with future work. So it's just an\nexploration to try to figure out exactly what is the problem that we are discussing and whether we\ncan help solve that problem or not. It generally requires a mix of people. It could be a mix of\nbusiness people and tech people. It really depends on the company and the type of project. But\ngenerally speaking, you need people that understand the business and the problem,\nboth from a technical perspective, but also from a business perspective. And of course, we are going\nto bring our experience as well in that evaluation of the problem. But before doing that, we need to\nunderstand what the company looks like, what the problem space looks like, what is the project in\ndetail and how accurate is the understanding of the problem itself. Because sometimes companies\njust have a very vague idea and then they haven't really developed a reason for why this idea is\nimportant for the business because they haven't dug deep enough to really understand all the\nnuances of the project. So by asking very specific questions, we can try to figure out exactly what\nis the level of understanding of that problem and whether the business really needs to address that problem or not.\n\n\nOr maybe they need to focus on something else. So I guess focusing on the business\nchallenges is a very important element to this phase of the conversation with the customer,\nwhere we are really trying to understand, do you need it for the business? Is this going to be an\nenabler of some kind? Or maybe something you want to do for innovation, but there needs to be a\nstrong value. If the project is completed, the business is going to have a value pack.\n\n\nSo in a way, there needs to be some kind of investment and a return of investment as part of framing the\nconversation around the project. And of course, there will be also very specific technical\nchallenges that we will need to understand. For instance, if this is something that the business\nis developing to compete with other competitors in the same industry, what the other competitors\nare doing is also very interesting. What kind of characteristics their solution has? Do we need to\nbe better? Do we need to be the same? Do we need to have something maybe similar but slightly\ndifferent? Maybe it needs to be more performant or simpler. So all these kind of details will\ninform the type of architecture and will frame also from a technical perspective, what the problem\nand the solution will look like. And also it's very important to understand, is there a tech\nteam? And this is actually something very interesting because when we work with startups,\noften there isn't even a tech team or if there is, it's very small. Sometimes it's just one person.\n\n\nSometimes it's even the founder themselves are doing a little bit of business and a little bit\nof tech and they just need more support. And other times they don't have a tech team at all.\nSo we will become their tech team if we decide to help them. While when we work with bigger\nenterprises, of course, often they have their own tech team. And it's important to understand\nexactly what that tech team looks like because our engagement is going to be very different\ndepending if we are going to become an extension of an existing team or if we are going to become\nthat kind of interim team that is going to develop maybe an MVP to get the company to the point where\nthey have a point where they can basically keep growing the business and it makes sense to start\nto invest in creating their own internal tech team. And then another interesting question that\noften comes up because our expertise and I guess our fame a little bit comes from being AWS experts.\n\n\nSo sometimes not all the problems are problems that you should solve with AWS. So also evaluating,\neven if we are brought in as AWS experts, does that problem really require AWS?\nI think it's a very fair question to ask and assess very honestly. Don't just try to put AWS in there\njust because you are the expert on it, but make sure that it's something that is really needed\nthat is going to make the difference for the success of that project. So basically,\nat this point, we want to come out of this meeting with two possible outcomes. It's generally like an\nalpha day, so it's not super intense. And the idea is that at the end, we have a quite good\nunderstanding of what's the problem space, the people involved, what everyone expects as success\ncriteria. So we might decide that we can help and that's kind of a positive outcome or that\nmaybe we are not the right fit for that particular company and problem. And at that point, we just\nsay, okay, this is not going to work. Hopefully, you got some value anyway with all this conversation\nand try to describe it and nail down the requirements of the project. But if we decide\nto start to help the company, what we are going to do is create a proposal. And that proposal is\neffectively a document that says this is our understanding of the problem and this is how we\ncan help you out to develop that solution further. And there were also other interesting\ncases that happened to us in the past where we decided not to be a good fit for that company\nbecause we didn't really believe on the structure, on the framing of that particular project.\nSometimes we realized that a founder is really early in the implementation. Maybe they need to\ndo a little bit more market research. So even though we could build a product for them, that\nproduct is still likely to fail just because there isn't enough research.\nSo we feel like it's... We are obliged to say, well, make sure you really understand the problem sets before you\ninvest money into this project and then come back to us later and we can reassess together whether\nwe feel we are going to be more likely to be successful after all of that. So I think it's\nalso important to call out that you need to be a little bit honest that way as a consultant,\nbecause we feel that's part of our job. If we have that expertise where we can give a useful\npiece of advice to our potential customers, even though that means losing business in the short\nterm, I think that can be much more valuable in the longer term. Now, I guess another question\nthat comes very often when we have this kind of conversation and maybe when we present a plan to\nour potential customers is this looks very different from what I expected. I was expecting\nmaybe, I don't know, lots of planning and then maybe a very clear roadmap going forward.\nAnd then you give me a very precise deadline where everything is done. But then when we present our\nplan, it might look very different from that expectation. So I think it's worth spending a\nlittle bit of time discussing our opinion and how do we perceive the history of software development\nand why we think that there is a large misconception, especially from people that are\nnot really into the software industry or now software should be built.\n\n\nEoin: It's probably worth reminding everybody from time to time about how software projects are fundamentally different in\na few different ways to other projects. And that could be a non-technical founder, but also a\nbusiness leader in any company. But I think we also forget and we often have a false sense of\nsecurity around times and deadlines and budget. We kind of forget that software is a relatively\nnew field with only about 70 years of history and a lot of change has happened in that time.\n\n\nThere's a bit of a bad reputation around software as well because of lots of horror stories around\nscope creep, bad estimates, delays, significant cost or time overruns, quality expectations,\nbeing poor and then from a team perspective, burnout, death march projects. So those things\nare quite common, right? It is common, let's face it, for software projects to ultimately fail and\nwe want to avoid that. So we have a strategy to make sure that we do avoid that. And we just think\nthat it all comes down to the fact that software projects are inherently just more complicated,\nmore complex than people might think, especially when you compare it to maybe more of a traditional\nengineering field like construction engineering or mechanical engineering. So I think one of the\nanalogies we've been using recently is if you compare what it takes to build a mobile website\nversus what it takes to engineer a coffee grinder. And if you can imagine more of a mechanical\nengineering example and let's look at it from a few different angles. So in terms of specification,\nif you're building a device like a coffee grinder, generally you have a clear set of functions. You\nwant to grind coffee to various sizes. The requirements are pretty clear and they don't\nusually change once the design is finished. Whereas with software, like you're building a\nmobile web website, even a simple one, the requirements are always dynamic. It needs to\nadapt to a variety of different user needs and expectations, and those change rapidly.\n\n\nUser expectations are always changing and user behavior is changing. You might need to update\nthe website frequently to meet new user demands and expectations, or even just to stay ahead of\ncompetitors. The process of understanding requirements, communicating them clearly,\nand making them a reality in software is really complex. And it's actually one of the major areas\nwhere potentially great projects can falter at the very beginning. And when you get your first\ndelivery, then looking at the environment in which these two products operate, you know,\ncoffee grinder will work in a kitchen and it'll have a stable power supply. The environment is\ngenerally controlled and predictable. Once you ship software, it must function across lots of\ndifferent devices, smartphones, tablets, PCs, and different screen sizes, operating systems,\nbrowsers, which all constantly change, specs change, they have continuous updates. So it's\na very different context from that point of view. And then users, when they use a coffee grinder,\ngenerally they put in the beans, press the button, empty it out, hopefully they clean it once in a\nwhile. With software like a mobile website, it has to function across, I suppose, contexts where\nyou didn't anticipate. Users do unexpected things all the time. Simple things like they put a\nspecial character in the first name field that you didn't think of and didn't expect. And then\nat the other end of the scale, you have malicious users, actors who are trying to damage the system,\nsteal data, hack the system. There's so many variables in terms of user actions.\n\n\nAnd I think as well, the constraints are quite different. With your physical device, the design and\nfunctionality, ultimately the constraints come down to the laws of physics, which I think are\ngenerally well understood and reasonably stable over time. In software, you're building a mobile\nwebsite, the constraints are constantly evolving, technologies are evolving, practices are changing,\nuser behaviors are changing. What's cool and trendy today might be obsolete in just a couple\nof years. Security context is changing very frequently. So you need to constantly stay up to\ndate, stay secure and adapt. And that's why software projects are always kind of living, breathing things.\n\n\nThey're never really complete. If you leave software, we talked about this, I think in the last\nepisode or the previous one, you can't just leave software alone and expect it to come back to it in\ntwo years and for everything to work because the whole environment, the technology landscape has\nchanged. So it's a living, breathing thing. You need a high performing team of domain experts,\nsoftware engineers working together to tackle this, right? And to address the different nature\nof software projects, to distill it all into prioritized tasks and deliver on the tasks in a\npredictable way. So this is, I suppose, worth reminding ourselves because when we talk about\nhow we do things and our process and everything, it's really to address the extraordinary,\nexponentially growing number of variables that you have to deal with in software projects that\nyou don't have in other environments where you can use engineering principles, deterministic\ncalculations, and then come up with a Gantt chart and be reasonably confident that you'll meet the\ndelivery. Software has to be much more adaptable and much more agile. How do we then manage software\nprojects? Do you want to share some of our process?\n\n\nLuciano: Yeah, I think speaking of the Gantt chart example, another thing that comes to mind is the waterfall approach, which is generally the idea that you\nhave a specification, you do some kind of analysis, from that you do some kind of design,\nthen you move to an implementation phase, then you have some kind of ready product that you just\nneed to test a little bit before you can confidently release and you're done. So it's\nlike step by step from beginning to end, it's kind of a straight process and it's called waterfall\nbecause generally it's represented with this kind of diagram where things are moving down to the\ncompletion phase. And this approach is really interesting because I think there is this\nmisconception that most people believe this is how anything is built, including software,\nbut you could argue that this model is not really something you can use. In reality,\nit's kind of a conceptual model to understand what are different phases of a project,\nbut this is not really how you move things from zero to completion. It's not like one single\niteration and the phases are very distinct. And even if you think about building physical\nproducts, if you start to compare this model with the agile methodologies that we are going to talk\na little bit more about in the second part of this episode, you can see that the agile\nmethodologies actually started in more traditional manufacturers, not something that was invented in\nsoftware, it's something that actually came out from Toyota. So even in traditional manufacturing,\nthere is a clear need that you need to have a more dynamic process where you can adapt to change,\nyou can adapt to things you didn't expect, you can adapt to new requirements and try to minimize\nwaste at every step of the process. So it is really interesting to see how even in traditional\nmanufacturing, this kind of conceptual model, it's good to understand the basics of the different\nphases of a project, but it's not realistic. This is not really how you move a project forward.\n\n\nSo I think it's very important for us to try to demystify, especially with some customers that\nmaybe are not so experienced with the way of building projects or software projects specifically,\nthat we need to take a different approach and an approach that needs to be more agile. So what do\nwe mean now by agile? It's not really easy to define what agile is. There are many different\ndefinitions, many different interpretations of different methodologies. And I think ultimately\neveryone has a slightly different way of doing agile. So it's kind of a set of principles,\nand then eventually everyone comes up with their own framework for taking some of these principles\nand adapting to their way of working. So our idea of agile is building things in small iterations\nand have frequent chances to reassess the scope and the landscape. And if there is something that\nchanged, you have the time to course correct things. It's never too late to do a change\nbecause you are trying to reassess things frequently enough that even if you took\nthe wrong turn somewhere, you are still in time to come back and try a different approach.\n\n\nAnd this is one of the main reasons why the waterfall approach doesn't work,\nbecause if you spent a huge amount of time building a project and invested tons of money,\nand then you come at the end phase, and when you are there, everything changed in the landscape,\nyou're understanding of the problem, what the customers would expect, you just wasted tons of\nmoney and tons of time. And going back basically means let's start from scratch and let's redesign\nagain. And then let's hope that next time things don't move as fast as they do, as they did. But\nwe know that most likely they're going to move again. So the only way really is to find a process\nthat's flexible enough that allows you to understand what's going on, understand if you are\non the right path, and if you need to change something, have the time to do that as soon as\npossible. So the way that you can do that is using a process that is divided in small chunks of\niterations, and we generally call them sprints. A sprint can be a period that can vary between\none week and four weeks, typically. And again, this is just arbitrary amounts of times, you just need\nto pick one size. And ideally, it should be small enough that gives you frequent chances to reassess,\nbut also big enough that gives you enough time to have a small objective in mind that you can\ncomplete during that timeframe. So it doesn't make sense to have a sprint of one hour because\nrealistically you're not going to have time to do anything meaningful in one hour. But if you start\nto think, for instance, about two weeks, you can probably achieve something useful for the business\nin two weeks. I think the other side effect of this approach is that it reduces the risk of experimentation.\n\n\nBecause if you are trying to do something and you're not really sure\nwhether a possible solution is actually going to be the right one, it is less risky to actually try\nit out. Just say, okay, we are going to spend one sprint maybe trying the solution. It might not work,\nbut the worst outcome that can happen is that it doesn't work and we learned new stuff. And now we\nspend two weeks and we can consider those two weeks of learning, even if the product didn't\nreally evolve. But our understanding of the problem space evolved and the next sprint, we can actually\nuse our time even better. And probably ultimately that's going to allow us to be even more efficient\nin the long term because we had the chance to try a number of different things and we understood\nmuch better what works and what doesn't work. So I think it's also important to mention that\nthis process to work needs everyone to be involved and to understand the project.\n\n\nIt's not just something that technical people do in isolation. It's something that requires all the different\nstakeholders to really understand the process and be fully involved in all the different phases of\nthe process. For instance, it's very important that everyone understands things are flexible\nand iterative. We don't have all the answers straight away. We will find out the answers as\nwe go and we will figure out what are the most important questions to ask at every single step.\n\n\nAnd there is a mindset of continuous improvement in that sense, again, because you are knowledge\nthat you are starting from a stage and you are trying to get better and better as you go doing\nthis kind of an iterative approach. It requires great communication skills.\nThis is actually one of the common problems that we generally see that if people are not sharing all their learning,\nthey cannot communicate effectively, what are the challenges, what are the expectations,\nthat's where you start to see problems and this process breaks down a little bit.\n\n\nSo it's very important to make sure that everyone is confident in terms of discussing how they\nunderstand the project, what is their progress, if they have any blocker and what are their\nexpectations maybe at the end of a sprint. And this is something you need to keep doing as much\nas possible. And sometimes other communicating is probably a little bit better than under\ncommunicating in that sense. And you can have tools that can help you in that sense.\n\n\nFor instance, you can have tools that will help you to make sure that there is space during a\nsprint to have this kind of useful conversations. Sometimes we talk about retrospectives, which is\nsomething that you can do at the end of a sprint to try to have an open conversation of what you\nthink went well and what you think didn't go so well and maybe you want to try to improve in the\nnext sprint. Or you can have, for instance, tools that try to measure the output, for instance,\nin terms of the quality of what you deliver, in terms of how many tasks did you actually\ncomplete compared to the ones that you wanted to complete during the sprint. And all of that stuff,\nit doesn't mean that you deliver poorly. It means that maybe your understanding of the problem space\nwasn't as accurate as you thought. And then in the next sprint, you can take everything\ninto consideration to try to plan a little bit better for the next sprint. Now, talking about\nmeasuring the output of a project, I think there is a common complaint in the industry\nthat developers are not really productive. And there are a number of studies that often come up\nand they all make very interesting claims, like a developer, a developer doesn't write more than\n100 lines of code per day, or it doesn't spend a certain number of hours coding per day. So I guess\nwhat do you think about that? Is it really important to measure the output of developers\nthat way? Or is there more that developers do in their daily job?\n\n\nEoin: Well, when you learn that code is not just a measure of productivity, but it's also like an, it's an asset, right? When you build\na product, you want the work that your team are creating to be an asset that's valuable and\ngrowing. But at the same time, code is a liability. I think we hear this expression quite a lot,\nwhich means that every time you have a line of code, it's something you must maintain into the\nfuture, every line. And every line of code is a line that can have a bug. So what you want to do\nis get the balance right, maximizing the benefit in terms of business outcome, by minimizing the\namount of code it takes to create that. And there's lots of ways we do that. We talked about\none of our principles being leveraging experts like AWS as partners to manage our infrastructure,\nso that we have to write less code. And this is a constant journey, trying to remove the code you\nhave and offload things that aren't really specific to your business. We talk about this\nquite a lot. So let's think about software development. And we've seen some studies that say\nan average software developer, if you have an eight hour working day, they're spending anything\nfrom one hour to four hours coding. And it might seem like you're unproductive. But when you look\nat what the function of a software developer is, it doesn't seem so bad at all. Because a software\ndeveloper, you don't generally want to be coding all the time. It's a bit of a red flag if they are,\nbecause it probably means they're a little bit isolated from the larger mission.\n\n\nIf we look at the all of the activities of a developer, they need to understand the customer and the business\nneed, they need to be constantly honing their skills, because the landscape is changing quickly.\nSo they need to be learning the right tools to do the job and ensuring that the approach is optimal.\nNo matter how experienced a developer you are, learning is a daily part of the job,\nthat's never going to change. Then you have things like analyzing bugs and constraints in existing\nsystems, often systems that were implemented by others who aren't available anymore. And this is\na really advanced skill almost in software development, which is understanding software\nthat you don't have full control over. It's a lot easier to start from scratch with a blank canvas\nall the time. A lot of companies out there that will do that, but there aren't a lot that will\nkind of understand the value in your existing systems that are already working pretty well\nand generating revenue. Then you have things like updating stakeholders, communicating with them so\nthat everybody's on the same page, sharing your learnings and ideas with peers, learning from your\npeers, collaborating with maybe QA staff, user experience, designers, project managers, and lots\nof others. Then you have just other collaborative activities like creating, reviewing, and updating\ndocumentation. This is pretty important if you don't want lots of problems in the future,\npeer reviewing code, and then writing and even rewriting code tests infrastructure. So the coding\npart almost comes last on the list once you've got everything else right. That's the way it\nshould be, getting the balance right between the focused implementation time, you know, this kind\nof stereotypical view of a developer, headphones on, furiously coding away. That's important,\nright? But it only makes sense if you have the right context to set, the mission is clear,\nand you've communicated with everybody else, because otherwise you could be in isolation,\nfuriously coding in the completely wrong direction, something that will have to be rewritten or thrown\naway later. That's not good. So the other thing is too many meetings, interruptions can kill momentum.\n\n\nSo you have to be conscious about these things and try and get the balance right between focus time\nand collaboration. Do a little communication will lead to a lot of misunderstandings, rework,\nhuge amounts of waste. So overall, I would say the amount of code produced, it's not a good measure\nof productivity at all. That's a mistake. And it can lead to over engineering and creation of large\namounts of technical debt. I think, generally, developers understand the concept of technical\ndebt for other stakeholders, it's a little bit less tangible. And I think it can be thought about\nas any other kind of debt, really, if you imagine that on your journey to developing a system of\nvalue, every time you might need to make a trade off between getting something done quickly, like\ngetting it implemented today, versus getting it done, right, maybe you have to wait for some\nexternal dependency, maybe you have to do some extra research, or you just have to put a lot of\nrework into the system in order to get it done right in a way that's really stable for the future.\n\n\nSometimes you have to make a trade off and say, okay, let's just take some sort of shortcut today,\nthat's essentially borrowing some technical debt, you're borrowing some future time,\nreally for the for right now to get something done. And a degree of debt is inevitable, technical debt\nis inevitable. But if you accumulate it without paying it off, just like any other kind of debt,\nyou will eventually become technically bankrupt, essentially, you'll end up with a system that is\nunmaintainable, you can't add features to, you're constantly fighting bugs on, and that will have\nenormous cost and productivity impacts down the line.\n\n\nMaybe we before we go on to our sprints, and all that process, we can summarize all this by saying that effective software development comes\nfrom having skilled experts and domain experts from a technical perspective, and a domain\nperspective, a whole team approach where everybody is working together, you have clear mission and\nsuccess criteria. And you can get metrics from those criteria, then you've got your requirements,\nwell defined, broken down into small pieces that can be delivered in fast iterations. And then\nyou've got clear communication feedback mechanisms without any gatekeeping or silos between the\ncustomer and those tasked with delivering the product. Lastly, I think permission to fail,\nlearn and improve is really critical. You can't have agile processes with continuous improvement,\nunless you have a very dynamic organization that has a permission to fail, learn from those\nfailures and improve. And that's where you really get the best outcomes. So with that in mind,\nIxiano, what does the first real day one working with the customers, sprint zero look like?\n\n\nLuciano: Yeah, we actually call the first, the very first iteration sprint zero, just because it's a little\nbit special than what we will be doing for the rest of the project. And it's just because the\nfirst phase requires a little bit more preparation than the other phases where you already have a\nmomentum and you have a little bit more clear planning of what's going to happen, and you keep\niterating that way. So with sprint zero, we basically, it's something that we do for a shorter\nperiod of time. We generally prefer a sprint of two weeks, but for sprint zero, we generally go\nfor just one week. And again, it's just because it's a preparation for then the more regular\nsprints. And it's kind of a workshop and then we prefer to do it in person where we try to go\nthrough all the available documentation, what is the output of the delivery. And in terms of\nactivities, there are a few things that we try to do. It will be kind of a full time\nseries of days, and we have different project stakeholders. There could be domain experts,\narchitect developers, UX, UI experts, really depends on the type of product that we're\nbuilding and the type of people involved in that project. But again, whoever has a perspective and\nan expectation on that project needs to be involved in this phase because they're going\nto have to define exactly what is their expectation, what is their success criteria.\n\n\nAnd we need to figure out how to align everyone with that particular expectation in mind.\nSo mission and objectives are at some point mutually agreed. Everyone is say,\nokay, this is what we want to achieve. And this is how we think we should be proceeding for the\nnext sprints. So that basically helps us to create a product backlog where we try to define all the\nfeatures that we need to build. And those features are not just in a random pile where anyone can\njust pick one, but it's very important to try to prioritize the ones that you want to do first,\nbecause generally they are the ones that are either going to enable other things or deliver\nthe most value straight away. So it's very important that you focus a bit of time trying\nto prioritize what is really important for the business, because that's going to drive the next\nphases. So you definitely want to finish first the things with the most value or the things that are\ngoing to block more value for the future. The other thing that we need to do is if there is any\ndependency on existing systems or legacy software that is used in the company, that needs to be\ntotally analyzed and documented, because this is generally where you can find surprises, where you\ncan find blockers. And if you need to create some kind of integration, you really need to understand\nwhat that integration could look like and how much work there is involved in creating that\nintegration. Sometimes you need access to systems like that. You need access to documentation and\nmight not be an immediate thing to get that access. Maybe you need to go through a process\nwhere you need to request access, where you need to request access to documentation. So it's really\nimportant to identify this kind of dependencies straight away and make sure you try to unblock\nthem as soon as possible, very early on in the process. At this point, we should have a clear\nenough idea to be able to design a first architecture. And we generally try to do that at two\nlevels. One level is very high level. It's kind of a logical architecture. Like what do we expect the\nsystems to do and how they are integrated with each other? The other one is a lot more detailed,\nwhat we call it physical level, where we actually describe what the implementation of the systems\ncould look like. Are we going to use, for instance, DynamoDB or a SQL database and which other systems\nare going to connect to that database? Maybe we need multiple databases. So it's really trying to\nnail down also the technologies that you're going to be using for implementing this architecture.\n\n\nAnd that leads to having a good technical vision for the engineering team. And you might also have\nwireframes if you are working, for instance, with designers. So it's really important to try to\ncombine not just the architecture, but also what the product should look like. So it's really good\nwhen you can work with a team of people that can take care of designing wireframes and really define\nwhat the user experience should look like. So that's definitely part of the delivery in that\nsense. And we will agree on some KPIs. So metrics that are going to help us to assess, are we being\nsuccessful in this implementation? Is this product really delivering the value that we imagined at\nthe beginning to customers or users in general? We also got to mention if there are risks or if\nthere are things that we need to do to reduce risk and all of this stuff is logged and it's something\nwe can reuse later on in the following sprints as a something to double check. Like, are we really\nseeing this risk? Are we doing something to mitigate the risk? Or maybe this risk is not really\nthat worrying after all, because we figured out or we learned something else during the sprints that\ncan help us to be more confident that we can avoid certain risks. But it's really important to have\na list of those risks because you always need to make sure you are assessing against them.\n\n\nAnd finally, once we have all of this, so we have a clear understanding, we have an architecture,\nwe have a list of tasks, there is the $1 million question, which is how long is this going to take?\nEveryone wants to know that. And of course, you can never have a precise answer, especially with\nan agile approach. You try not to have a super accurate answer because you are always learning.\nAnd as you learn more, you can be more accurate, but you need to have some kind of estimate anyway,\nbecause you cannot just tell the customer, we don't know this is going to take potentially\nforever. No, you need to be able to provide some value in a specific timeframe. So what we do is\nwe generally assign a size to all the tasks. And we call this exercise T-shirt sizing, because we\ngive a small, medium, large, extra large kind of sizing. And then we have a very simplistic model\nthat allows us to see, depending on the size of cards and number of people involved and how easy\nit is to parallelize certain tasks, how much more or less that project is going to take. And it's\njust a ballpark figure that you have as a feeling to see, are we talking about weeks? Are we talking\nabout months or years? And of course, the longer it is, the more you need to make an effort to try\nto reduce the scope so that you can come up with something valuable in a shorter amount of time.\nIt's not uncommon to see that a project might take, I don't know, two years after you do all\nof this research. And that's when you have to go back and see, okay, this is not a realistic project\nthat we can be successful in. We need to reduce the scope. We need to trim down, focus maybe on\na smaller area, try to deliver that one first, reassess everything. And then eventually maybe\nyou come up with like, this might be a three months worth of project. And at that point,\nyou have the risk a lot, the success of that investment, because you are much more likely\nto come up after those three months with something valuable rather than investing years into something\nthat might not turn out to be so important after the three years. So what happens next? At this\npoint, we are ready to roll. How do we continue?\n\n\nEoin: When people think about this approach to software development, it sounds fuzzier and maybe less of a commitment. Maybe it's just more of like a\ncop out from the developers, because you're not committing to a rigid timeline. In fact, to do it\nright, it all it means is that you're doing more planning, you're just spreading it over time,\nand you're constantly planning. Now there's that old saying that says, plans are useless, but\nplanning is critical. And that's what this really is about. Rather than say, okay, here's a plan\nthat gives us very fixed scope in a fixed period is we time box our milestones. So we say, okay,\nwell, here's what we're trying to achieve. But let's make an initial time box. And for a new\ncustomer, first greenfield project, or actually any type of customer, we try to limit the duration\nof the first engagement to like six to eight weeks, so maybe three or four sprints. And that\ngives us and the customer enough time to see a lot of value. And it's generally a good enough\namount of time to set the foundation and deliver something that's really valuable from a business\nperspective to production that you can iterate on from that point. And it seems like a short\namount of time to a lot of people. But if you're really focused and you've got a very high\nperforming lean team, you can do a lot in that amount of time. After that, sprint zero, which\nis critical foundation, you generally have a good idea of what that value is. So you can kick off\nyour regular sprint, sprint cadence. Sometimes agile teams work in sprints. Sometimes it's more\nkind of Kanban kind of continuous taking tasks off. More often than not, we work in two weeks\nsprints. And the idea of that is you've got, that's the kind of feedback loop in terms of\ntalking to your stakeholders and end users and understanding how to course correct your plan.\n\n\nThe main activity within these sprints, of course, is coding, development, delivery of prioritized\nfeatures and tasks, including all of the best practices like continuous deployment to your\nproduction environment. That's something we put in from the very start so that it doesn't become\na big effort later. It's always good to get all of those production deployment, best practices,\nobservability, quality control in from the very start. It's much cheaper to do it at the very\nstart and then just increment them over time. So it involves programming, the creation of\nvirtual infrastructure in a cloud environment, documentation, tests, all of that. Studies have\nshown that waiting until the end of a sprint or, you know, monthly or three monthly release cycles\nto deploy software, that's been shown to slow down high-performing teams. So we follow the\npractice of continuous delivery. So it just becomes a habit that you do without even thinking\nabout it. That's automated, highly and tested. Once you've got good planning in place,\nhopefully the items on your backlog, those features, whatever it is, ideally they should\ntake between like a half a day or three days to deliver. The reason for that is that if you've\ngot unexpected hitches along the way, external blockers, sometimes as a developer, we end up\nspending a day or two resolving an unanticipated issue. But if your units of work are small enough,\nthen the impact is less overall and the time to adapt and of course correct. It doesn't have such\nan impact on the overall delivery schedule. Two-week sprints, there's always meetings,\nof course. Some people live by these meetings, some that when they work well, they're great for\neverybody and they get a lot of satisfaction. They can become a pain for a lot of developer teams\nwhen these meetings are not done well, because they think it's just a bunch of managers distracting\nyou from the work you're supposed to be doing just to talk rubbish. Believe me, I've worked\nin companies where agile software has been done really well and I've been lucky enough to be in\nthat position. And when it works, it really, really works and makes everybody more productive.\n\n\nSo we try to get a good balance ration, try to keep the meetings focused and short and not take\ntoo much of people's times. Planning meeting at the start of a sprint with the product owner,\nthe delivery team, that's about reviewing the top of the backlog, checking your priorities,\nadjusting, getting more clarity on specific features and just having a good sense of what\nyou're going to try and achieve over the next couple of weeks. Then you have a daily check-in,\nwhich is about raising and tackling any blockers, reviewing your work done, and just basically\nself-organizing the people who are involved in delivering, self-organizing to get that day's\nactivity done. And it should not take more than 10 or 15 minutes. If it is, there's a red flag\nthere. It's typical at the end of the sprint then to review. You have a demo with your stakeholders\nwhere you showcase everything you've done. You get feedback on it. They say whether it meets the\nneeds, fulfills the mission. You look at your KPIs, your checklists, look at your risks and\nyour actions. Now it's pretty common in like agile methodologies like Scrum to have that end of sprint\ndemo. What we like to do is actually a weekly demo at least, and also ad hoc demos to stakeholders\neven, because I think two weeks to wait to show them the work you did maybe 10 days ago,\nit's quite a long feedback loop and doesn't give you a chance to course correct quickly enough.\nIt also often causes you to rush the demo and maybe do a little bit of a polished demo where\nyou're just trying to show all of the nice things you've built. A demo is much more valuable for\neverybody where you show the stuff that didn't work as well as the show the stuff that did work.\nIt goes back to that transparency principle we talked about at the start that we try to adhere\nto. It establishes a lot more trust. You can show things a lot more detail and as well it means\nthere's not a lot of pressure for everybody to be there and to be super engaged in the demo at the\nend of every sprint. You can maybe miss one every once in a while and it's not such a big deal.\nSo I encourage people to think about more regular and more honest and raw transparent demos as well.\nSo getting the value in those meetings measured, checking that you're not wasting time,\npeople aren't just phoning it in and checking in and nodding along to the meeting while they keep\ncoding. That's important because what's the point if that happens? You have to make sure that they're\nvaluable and I always think if you're not either contributing value to a meeting or getting value\nfrom a meeting just don't show up. You're better off doing something else and you're better off\nkind of raising it and saying okay how can we make this more valuable for everyone who is there.\nSo we've gone through sprint zero. We've gone through the regular sprints. Luciano,\nshould we talk about milestones and releases, things that are less discussed in agile processes?\n\n\nLuciano: Yeah, I think it's really important to clarify that this process might seem like something that doesn't really have a clear vision but it's actually quite the opposite. Meaning that\nyou are only trying to minimize the risk that something changes and you are not ready for it\nor there is that you misunderstood something and you realize that too late but you are still driven\nby business needs. And the business will probably need things like get new customers, get specific\nfeature out that customers are going to be paying for or maybe there is a marketing element to it.\n\n\nSo you'll need to have certain things ready before a marketing campaign can be kicked off or maybe I\ndon't know you are discussing with investors and you have agreed certain things with investors so\nyou need to meet certain goals or features or number of customers to be able to secure another\nround of funding. So all these things are something that somehow needs to be taken into account.\nSo even though you are following this approach that is very agile, you are still driven by\nmilestones and releases that are very closely tied to what the business needs to effectively\nsurvive and be successful. So it's interesting to be able to find a good balance between keeping\nthat vision in terms of business but also keeping a methodology that is flexible enough but it's\ndefinitely doable as long as you keep both things in mind. And I think this is very important. This\nis why it's very important to have both business people and technical people involved in the process\nand have them to be able to communicate effectively together and understand together what is\neach other's responsibility and how can they work together rather than working against each other.\n\n\nSo I think it's interesting that there might be different scenarios. For instance, when you are\ncreating a new platform for Scratch, what we try to do in those six to eight weeks is generally\ncome up with an MVP that we have in production. So it's something that we can show to people,\nwe can show to customers, we can show to investors and that clearly communicates this is why this\nproject makes sense. And this is why either as a customer you should buy this product or maybe as\nan investor you should invest in this business to move it to the next phase. So I think it's\nvery important for us to agree with our customers. This is what we want to deliver and to give them a\nchance to make sure that they can get that value straight away. The sooner we do that, I think that\nthe easier it's going to be for our customers to evaluate this kind of partnership and decide what\nto do next. And what to do next is always an interesting question. There might be different\noutcomes. Sometimes we realize and our customer realize that even if we did an excellent job,\nthe customer might be better off continuing on their own. Maybe they can build their own\ntechnical team. Maybe they can just totally onboard something that we built together with\nthem and continue developing it with their own internal team that they might already have.\n\n\nSo that's totally an option. And when that happens, it doesn't mean that we haven't been successful.\nActually means that we've been extremely successful and we are going to help the customer\nto just move on to the next phase and continue building things on their own. In other cases,\nwe can still decide to continue the partnership. Maybe the customer decides that they want to build\nmore stuff with us, maybe expand that project, build more features, maybe build the next phase\nof that project, or even just move us to other projects because in bigger companies, they always\nhave a number of projects going on at the same time and we might be helpful in other projects\nas well. Or maybe there are other projects that they want to start as more experimental activities\nand we might be the partner that helps them to try to build something new. So in summary, today,\nwhat we did is covering what are our principles, how do we work as a company, and we try to be\ntrusted partners for our customers. We don't just want to build hours of engineering time,\nbut we want to make sure our customers will succeed with their own business goals. And we\nare the enablers from a technical perspective to make sure that they're going to deliver the best\ntechnical stuff that can fulfill that specific business need. We also spoke a little bit about\nthe software world and how there are so many misconceptions on how you should be building\nsoftware, even though it's not a perfect science and everyone has its own kind of incarnation of\nwhat good software development looks like. I think it's important to recognize that there are some\nprinciples, some guiding principles that are universally recognized and you should be using\nthem. Then as long as you have a clear process and you can work effectively with your customers,\nit's fine to probably have slightly different take on how you actually organize the day to day.\nWe have our own way that we described today, and hopefully you found that interesting.\nBut we are also, as usual, very curious to hear it from you. Do you use a very similar process when\nyou work with technology projects? If you are also a consultant, what are we saying? Does it make any\nsense or do you do something entirely different? I think it's really important to have a healthy\nconversation in our circle to compare all the different processes and learn from each other.\nWhat does it really work? What doesn't work? How can we grow together and get better at this craft?\nDefinitely let us know what you think. Leave us a comment either on YouTube or reach out to us\non socials. All the links will be in the show notes as usual. Thank you very much and we'll\nsee you in the next episode.\n"
    },
    {
      "title": "112. What is a Service Control Policy (SCP)?",
      "url": "https://awsbites.com/112-what-is-a-service-control-policy-scp/",
      "publish_date": "2024-02-02T00:00:00.000Z",
      "abstract": "In this episode, we provide a friendly introduction to Service Control Policies (SCPs) in AWS Organizations. We explain what SCPs are, how they work, common use cases, and tips for troubleshooting access-denied errors related to SCPs. We cover how SCPs differ from identity-based and resource-based policies, and how SCPs can be used to set boundaries on maximum permissions in AWS accounts across an organization.\n\nAWS Bites is sponsored by fourTheorem, an AWS Partner with plenty of experience setting up AWS accounts and Service Control Policies. If that's something you'd like some help with, reach out to us on social media or check out fourtheorem.com!\n\nIn this episode, we mentioned the following resources.\n\nEpisode 96: &quot;AWS Governance and Landing Zone with Control Tower, Org Formation, and Terraform&quot;\nEpisode 40: &quot;What do you need to know about IAM?&quot;\nConor Maher's repo with some SCP examples\n\n",
      "transcript": "Eoin: Access control policies in AWS are one of the most difficult concepts to grasp.\nAnd yet this is something that everyone has to deal with and understand at some level.\nIf you understand how all the different policy types work and how to troubleshoot them,\nit's like having an AWS superpower. You'll be able to stay more secure,\nyou will save tons of time and you'll have a very sought after skill.\nSo how do you level up your skills in this area?\n\n\nWell, today we're going to help you in some way by covering one of the least understood policy types in AWS.\nSCPs or service control policies. By the end of this quick episode,\nyou should know what an SCP is, how and when to use them,\nand you should be better equipped to troubleshoot access denied errors.\nI'm Eoin, I'm here with Luciano for another AWS Bites.\nAWS Bites is sponsored by fourTheorem. If setting up service control policies seems like too much hard work\nand you'd rather let somebody else deal with it, give us a shout or check out fourtheorem.com.\n\n\nBack in episode 40, we talked about identity and access management or IAM\nand quickly covered the high level concepts around the different policy types.\nThere's lots of different policy types and if you've ever created an IAM role,\nyou'll be familiar with some of them. You might have used resource policies in the past as well,\nlike an S3 bucket policy. As an AWS developer or user,\nyou often have the rights to create those kinds of policies,\nbut there are other different types of policies that are usually taken care of by the people\nwho manage the AWS accounts and administer the whole organization structure.\nAnd that includes permissions boundary policies\nand these service control policies that we're talking about today.\nAnd because they're maintained by administrators, usually they're less well understood,\nprobably for that reason. So to understand them,\nwe need to think a little bit about AWS organizations\nbecause SCPs are actually a feature of organization service\nrather than IAM as you might expect.\nSo let's maybe start Luciano with a quick run through of AWS organizations.\nWe covered it in more detail in episode 96,\nbut we should remind ourselves so that we could talk about SCPs with a bit more context.\nYeah, of course.\n\n\nLuciano: I think it's important to discuss what organizations are because it's something that is becoming more and more prevalent in the industry.\nMost companies would have more than just one account.\nAnd this is for two reasons mainly. One is security, which is probably the biggest reason\nand we'll talk more about that during this episode.\nBut the other one is also quotas because with different accounts,\nyou are not going to have quotas in one account affecting the other.\n\n\nFor instance, an application running in, I don't know, a development account,\nmaybe where you're doing some load testing is not going to affect your production account\nand maybe stop things from scaling up because you are running out on your quotas.\nSo having multiple AWS accounts is something that it's actually done a lot in the industry.\nIt's very important. You will see that more and more, especially going to bigger companies.\n\n\nAnd the thing that is interesting is that AWS kind of recognized the need for seeing all this pattern,\nthe need for allowing companies to effectively manage multiple accounts inside an organization.\nSo they built a service called organization,\nwhich should make it simpler to manage all of these accounts consistently.\nSo if you go to the organization panel,\nyou will see that there are a bunch of different features.\n\n\nAnd if you start to read about what can you do with organization,\nyou'll see also a bunch of other interesting stuff.\nThe main one from an accounting perspective is that you can have consolidated billing,\nwhich basically means that you can pay from one bank account\nfor all the different AWS accounts that you have enabled under your own organization.\nAlso, you have a hierarchical structure.\nSo you can imagine like you have a file system with folders and files,\nwhere the files are kind of the actual accounts,\nbut you can also have folders which you can call organization units.\n\n\nAnd the idea is that you can group accounts logically.\nAnd you also can have centralized management of auditing and compliance.\nFor instance, you can have things like CloudTrail, AWS Config working at the organization unit level.\nSo you are kind of centralizing the management of all of that.\nAlso, you have resource sharing across the organization.\nYou have the ability to create new accounts within the organization\nor invite other accounts into the organization.\n\n\nYou can do centralized access for identities,\nso users using SSO, which is something called IAM Identity Center.\nSo the idea is that as a user of that organization,\nyou will have a dashboard that you can log in using your own company SSO,\nand then it's going to show you all the different accounts and roles that you have access to.\nAnd then you can easily jump from that to the specific account you need to work with.\n\n\nAnd finally, we get into organization policies.\nOrganization policy also includes SCPs, which is the topic of this episode.\nAnd these are policies that are applied to organization unit within an organization.\nSo in order to use SCPs, you need to select the enable all feature within your organization.\nAnd the other option just gives you consolidated billing only.\nSo it's not a feature that you get out of the box.\n\n\nIt's something that you need to explicitly enable in your account,\nwell, in your organization.\nOnce you have enabled that, you will have the management account\nand possible other accounts as well,\nwhich are going to become part of your organization structure.\nJust to give you an example, you have at the top,\nyou can imagine again, you have this kind of tree structure,\nand at the top, you have the management account,\nwhich you can also call the root account.\n\n\nAnd then you can have organization units like security or workloads.\nInside security, you can have an account where you are storing,\nwhere you are managing things like guard duty or other security services.\nYou can also manage accounts for auditing.\nSo maybe where you're going to store logs that you might need\nin case that you need to perform some kind of auditing.\nAnd then if we go back to the workload units,\nthese are going to be things where, for instance, you run applications\nand you might have a production account, a development account,\nyou might have, I don't know, a testing account or maybe a QA account\nthat really depends on the way you ship software from maybe a development to production.\n\n\nYou can also choose to organize your accounts by department or project.\nIt's not uncommon to see that if in a big company you have kind of big applications\nand they are all part maybe of a microservice architecture,\nyou might have different services running in dedicated accounts,\nwhich makes it simpler to organize, for instance, the deployment pipelines,\navoid that again you have conflicting quotas\nor even just make it simpler to have very specific policies\nand control exactly what kind of accounts, what can they do account by account.\nSo the structure that you choose is highly influenced by the kind of company you work with\nand the way that you actually ship things to production,\nbut also what kind of service control policies you want to have\nbecause defining service control policies is going to affect what you can do in every single account.\nSo I guess at this point we should say what SCPs are and what they are good for.\n\n\nEoin: If you've ever written a policy for an IAM user or a role,\nyou'll probably have written or created policy statements,\neither doing this directly in JSON or YAML or with the AWS console.\nAnd in a policy statement, you have things like an effect,\nwhich says are we allowing or denying action\nand then the properties of the statement that is often referred to as the PARC model, P-A-R-C\nand PARC stands for principal, action, resource and condition.\n\n\nSo the principal part is the identity and this is an account or a user or a role\nand you only normally have that in resource policies,\nnot in role or user, not in identity based policies because the identity is already implicit.\nThen you have the action, like something like S3 list buckets or easy to start instances.\nThen you have the resource or resources,\nwhich are ARNs of the resources you're trying to allow or deny access to.\n\n\nAnd then you have conditions, which is a bit of an advanced feature,\nbecoming more and more common where you're providing restrictions on the action,\nthe allow or deny based on attributes of either the identity or the resource.\nNow, I think there's a few important things to know about service control policies\nthat are a little bit different to identity policies or resource policies you may have seen in the past.\n\n\nThe most important one is probably that service control policies never grant any permissions, ever.\nSo that might sound surprising for an access policy,\nbut a service control policy is really just a limit on the actions you can perform in an account.\nYou still need to have an identity based policy or a resource based policy\nto do the actual granting to do the allow part.\nSo if you think about an identity policy and then a service control policy within the same account\nand those two sets of permissions are in a Venn diagram with the identity policy in the SCP,\nthe action needs to be allowed by the intersection of the allows in those two policies.\n\n\nAnd another important thing is that everything is denied by default in an SCP implicitly.\nAnd for this reason, when you enable service control policies in your organization,\nAWS will create a full access service control policy for you by default.\nAnd this is basically action star, resource star, effect allow.\nAnd that pattern is pretty familiar to anyone who's ever seen administrator access managed policy.\n\n\nIt looks exactly the same.\nAnd this seems like a very permissive thing to have by default from AWS.\nBut of course it isn't permissive because as we just said,\nSEPs don't actually grant any permissions.\nThey just set the boundary for all the permissions within an account.\nSee the idea here is that they give you everything by default\nand it's up to you then to start creating service control policies\nto scope down the boundary for all of the organization units or individual accounts\nand refine it as you need for your organization structure.\n\n\nAnd the last other important point about SEPs is that they don't affect the management account.\nThey only affect the member accounts within the organization.\nSo you talked about that hierarchy, Ushano.\nWe started with the management account at the root.\nThe SEPs are applied to OUs which are beneath that.\nSo that's important to realize.\nAnd that's exactly how you apply them.\nOnce you create these policies, you attach them to organizational units within the organization.\n\n\nNow you could do this at one level of the hierarchy\nor every level of the hierarchy at OU level if you want.\nAnd the effect of that then is that for an identity to have permissions in an account,\nif there are multiple SEPs within the hierarchical structure all the way down to that account,\nthat the user is in, you have to explicitly allow the permission in all of the SEPs.\nSo because they're denied by default,\nyou have to make sure that the permission you're looking for is denied in each one of them.\n\n\nThere's no like trickle down from the top SEP down to OUs beneath that with additional SEPs.\nIt has to be allowed in all of them.\nAnd if there isn't that explicit allow in all the applicable SEPs,\nyou'll get an error and the errors are getting a little bit better here,\nbut usually it says something like the identity is not authorized to perform an action\nbecause no service control policy allows the action.\n\n\nThat's basically saying it's implicitly denied\nor it'll say it has been denied because of an explicit deny in a service control policy.\nAnd that basically tells you that there's a deny somewhere.\nAnd if you've got a deny in a policy in the whole IAM service control policy evaluation logic,\nan explicit deny beats everything.\nSo that's why it's denied.\nAnd service control policies can be one of the more difficult ones to troubleshoot\nbecause as a developer, an engineer in an AWS account,\nyou don't have visibility often into the service control policies.\nSo sometimes you just get a hint that it comes from the SEP.\nThen you might have to go to somebody else or look in the infrastructure as code for those policies\nto see where the error is coming from.\nIt's not as explicit as with other cases.\nNow that we know how they work pretty much, what are they good for Luciana?\nWhat are the use cases and why should people be interested in applying them to their organizations?\nSEP are used as very broad set of controls on the maximum permissions as you very well explained.\n\n\nLuciano: So they are rarely very fine-grained.\nSo they're not going to really talk about specific resources,\nit's more groups of things that will allow or deny certain specific actions.\nSo the idea is that you still should do fine-grained controls, of course,\nbut you should do them somewhere else.\nSo you should probably do them as you have done them before,\ninside specific accounts, create specific types of policies and identity policies for specific resources.\n\n\nSo when you think about SEPs as kind of the macro level, then everything else,\nall the other type of policies that you know well are probably more on the micro fine-grained level.\nThere are some typical use cases for SEPs.\nFor instance, one that we see a lot is specifying which regions can be used for a given account or sets of accounts.\nAnd this can be important because you know that different regions might have different costs.\n\n\nAnd generally, as an organization, you might be using a few regions,\nyou're not going to end up using all the regions most of the time.\nSo it makes sense to limit things that you can do only on the regions that you are actually really intending to do, to use.\nThere is a little bit of a gotcha there that certain services will require specific regions to work.\nFor instance, US East 1 for AEM.\nSo just be aware that you cannot just say I want to use one region, maybe you, Central 1,\nbecause that's the closest to you, and forget about all the others, certain services will require specific regions to be enabled.\n\n\nAnother example is somewhat related where certain services are only available on a subset of regions.\nSo for instance, if you want to use Bedrock right now, it's limited only to a couple of regions, I believe.\nSo if this is something that you want to use, you still need to enable those regions as well.\nOr at least one of the available regions as well.\nAnother good use case is prevent root user access, which is, I think, a very good practice to have.\n\n\nYou can also prevent accounts from leaving the organization if somebody might accidentally do that at the account level.\nThis is something you can prevent with an SCP.\nYou can also enforce data governance with S3, for instance, you can have specific SCPs that block public access at the global level,\nso nobody can create a bucket that has public access.\nOr you can prevent people from disabling something like S3 object questioning.\n\n\nYou can even force all the buckets to have encryption enabled.\nSo things like that will make sure that when you're using a service like S3,\nyou actually follow all the data governance rules that you might have within your company.\nYou can also limit services that people can use inside the organization.\nFor instance, as an administrator, you might say, okay, I don't want people to use Lambda,\nbecause maybe you only really want to do easy to use, you are old school that way,\nyou can disable access to the Lambda service.\n\n\nYou shouldn't probably do that.\nBut if you really want to do it, it's something that you can do.\nAnd another thing you can prevent people from deleting or modifying resources,\nand you can have, in a way, those resources centrally managed.\nFor instance, you might have administrator access in development accounts,\nbut still, there are things that you can limit even to people that have administrator access in those accounts.\n\n\nFor instance, you don't want them to be able to delete organization managed roles,\nor turn off config rules or crowd trail logging,\nbecause you can guarantee that they won't be able to do this using specific SCTs.\nAnother use case that I've seen quite commonly is that you can limit the type of EC2 instances that can be launched.\nAnd this is something that is done because there are some EC2 instances that are very expensive.\n\n\nImagine the ones with like tons of GPU, or the very advanced ones,\nmaybe for, I don't know, graphics use cases, or AI model training use cases.\nThose are very expensive, and you rarely need them,\nunless you're really trying to cover that particular use case.\nSo I think it's a good practice to try to disable those using an SCP,\nbecause that's going to make sure that you're not accidentally going to spawn up\nEC2 instances that might end up being very expensive.\nSo at this point, you should have a good idea of what SCPs are, what they're good for.\nNow, how do you start creating them?\nWell, you can create them in the organization section of the AWS Management Console.\n\n\nEoin: Ideally, you should use some form of infrastructure as code.\nAWS Control Tower will create default SCPs for you with some sensible first best practices.\nSo that's a good first option, and that's kind of, I suppose, a gateway for a lot of people in this area.\nTerraform, again, we seem to be mentioning this repository almost every episode now,\nbut Conor Marr, our colleague, has some nice examples in his Terraform landing zone repo.\n\n\nLink will be in the show notes.\nAnd if you're using CloudFormation or OrgFormation as well,\nthe OrgFormation repo has some examples.\nAnd this is all stuff that we covered in episode 96.\nSo we don't have to go into too much detail around how to create them.\nEssentially, we're talking about infrastructure as code for organizations,\nwhich was the exact topic we covered in episode 96.\nSo if that's of interest to you, feel free to go back and check out that episode.\n\n\nMaybe before we finish, it's probably worth mentioning that troubleshooting SCPs,\nas we alluded to, can be a bit difficult.\nFor other policy types, you can use the IAM policy simulator,\nand usually it will be able to highlight the area that denied or didn't really allow access well.\nSo it can highlight denies and specific policies.\nBut with service control policies, you don't get to see in the IAM policy simulator\nwhat is being explicitly or implicitly denied.\n\n\nIt just says denied by AWS organizations, which can be a bit frustrating.\nSo it's one of the areas where I think SCPs scare me a little bit,\neven though in general they're really good practice and everybody should use them,\nas long as they're pretty coarse grained and you don't go to town on them.\nBut maybe this is a general appeal.\nIf anybody has any great tips for troubleshooting SCPs,\nwe're pretty interested to hear about them.\nSo please share them with us.\nAnd let us know as well if you have any other neat use cases for service control policies.\nAnd with that, thanks again for joining us, and we'll see you in the next AWS Bites.\n"
    },
    {
      "title": "113. How do you revoke leaked credentials?",
      "url": "https://awsbites.com/113-how-do-you-revoke-leaked-credentials/",
      "publish_date": "2024-02-09T00:00:00.000Z",
      "abstract": "In this episode, we discuss what to do if you accidentally leak your AWS credentials during a live stream. We explain the difference between temporary credentials and long-lived credentials, and how to revoke each type. For temporary credentials, we recommend using the AWS console to revoke sessions or creating an IAM policy to deny access. For long-lived credentials, you must deactivate and rotate the credentials. We also touch on using tools like HashiCorp Vault to manage credentials securely.\n\nAWS Bites is brought to you by fourTheorem, the AWS consulting partner that doesn’t suck. Check us out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources.\n\nGist with example policy\nRevoking IAM role temporary security credentials (official AWS docs)\n\n",
      "transcript": "Luciano: Imagine this scenario. You were live streaming your AWS application build on Twitch,\nand because you are amazing and people love you, you have thousands of people that are watching you,\nand everything is going great.\nThen for a split second, you accidentally show your audience your environment variables,\nand that includes amazing things like your temporary AWS role credentials,\nlike access key, AD, secret access key, and session token, which is basically everything, right?\n\n\nAnd you start to be worried, your heart is racing, and you are sweating, and you are wondering,\nokay, what do I do now? Because in a matter of minutes or seconds,\npeople are going to actually try to use these credentials to do something nasty with my AWS account.\nSo what do you do now? Fear not, because today we're going to give you a super quick tip\non how to revoke AWS credentials. And by the way, if you're thinking that you want to get the AWS Solution\nand you want to get an architect certification exam, this is one of the questions that might come up.\n\n\nSo be ready to take notes. My name is Luciano, and I'm here with Eoin for another AWS Bites podcast episode.\nAWS Bites is brought to you by fourTheorem, an AWS consulting partner that doesn't suck,\nor rarely suck at least. So if you want to check us out, go to fourTheorem.com,\nand also feel free to reach out to us. We have all the links in the show notes.\nOkay, let's talk this through. Maybe you have been following our 100-plus episodes,\nso you know that you shouldn't be using hardcoded credentials.\n\n\nYou should be using temporary credentials. Still, you can make a simple mistake,\nand you can still accidentally commit them to Git, or maybe if you're doing a screen share\nduring a video call, you can still leak them. So what is the problem here, right?\nIs that a big deal? Because temporary credentials, as the name suggests,\nis something that's going to be temporary, but how long are they going to last?\n\n\nThey might last hours, right? So is that going to be a problem?\nWell, probably yes, right? Because if you have hours, there is still a lot of damage\nthat you can do in a few hours, especially if that role is something that has very broad credentials.\nMaybe, I don't know, it's an administrative credential, probably is the worst use case,\nbut you might have a role, for instance, that you use for deployment reasons,\nso you might be able to provision new Lambdas or EC2s or create VPCs,\nall things that can lead to actually giving pretty broad permissions to an attacker\nthat might be able to get those credentials.\n\n\nSo what do you do in this case? Because the first worry that you have is that\nyou might have a massive AWS bill if somebody tries to use these credentials,\nand they can do other kinds of damage that might affect the reputation of a company\nor might destroy some of your environments that you use for work.\nSo yes, what do we do here now, right?\nAnd the first thing that you might realize is that this kind of credentials cannot be invalidated.\nYou don't really have an API or a panel where you can say, well, invalidate this particular set of credentials,\nas you can do with the un-coded ones.\nSo the only thing it seems that we have at end is the expiration time,\nbut we don't want to trust that because it's still pretty open.\nSo what do we do now, Eoin? Any idea?\n\n\nEoin: Yeah, like you say, you can't invalidate them, they exist until they expire.\nSo you need to think about what permissions really these credentials are granting\nand how can you change the permissions level.\nAnd still, the valid credentials will still be usable,\nbut maybe just prevent what they can do with those credentials in the time until the expiration occurs.\nSo the only thing you can do really is change the role and the policies within it.\n\n\nI suppose you could delete the role, that would be a brute force way of dealing with the problem.\nOr you can also update the role and maybe just remove all of its permission,\nso it doesn't have any permissions then.\nThis would have the immediate effect of blocking out anyone with the temporary credentials.\nActually, I say immediate effect, you should also be aware that IAM is eventually consistent.\n\n\nSo it might take seconds or even minutes sometimes for the effect of your changes to occur.\nThat's something you don't really control, because IAM has to be reflected out in all of the regions\nand all of the nodes it's distributed across all of AWS's massive infrastructure.\nAnd that doesn't happen in the blink of an eye.\nSo by doing that, you've blocked out your attacker, but it also has side effects,\nyou might not want any valid holders of credentials are also blocked out,\nincluding anyone who just gets new credentials for the role after you've made that change.\n\n\nSo it's not ideal.\nYou could also attach a policy with an explicit deny.\nSo you could say, deny everything on every resource, and just attach that to the policy.\nAnd we know that an explicit deny always wins in these policies, it will trump everything.\nAnd it's better because you don't have to remove all the other permissions,\nyou don't have to delete the role, but it still has that problem that it blocks out valid identities too.\n\n\nSo the solution to this really is to attach a policy with a conditional deny.\nAnd there's a special clever condition that you can use that denies access to identities\nthat assume the role before a specific timestamp.\nAnd this is basically a way of saying, deny everybody who assumed this role before the current timestamp right now.\nAnd there's a condition key called date less than the condition predicate.\n\n\nAnd then the attribute you're putting into that predicate is the AWS token issue time.\nSo if the token issue time is less than the current timestamp, then deny them everything.\nWhat that means is that valid and invalid, like spurious attackers have no permissions for all sessions until the current time.\nBut it does allow valid users to get a new session.\nAnd the new session will be valid and will continue to work.\nAnd business will operate as normal.\nOr if it's a service linked, a service role, then the service can get a new role as well.\nAnd the session will be valid.\nBut anyone with the leaked credentials is locked out.\nSo that's your problem solved.\nYeah, all of this, the way we've described it, it sounds a bit complicated.\nAnd if you're in a situation where you're panicked, it's difficult to remember it all and do it correctly.\nSo how do we simplify this process so we don't have to remember everything in that real situation?\n\n\nLuciano: So you described already that the policy very well.\nAnyway, we're going to make that policy available in a gist.\nAnd you will find the link in the show notes just if you want to have a more easy to follow reference.\nAnd also if you need to copy paste the structure of it.\nBut of course, if you need to remember it from scratch, from the top of your mind, and you're panicking,\nit might not be the easiest thing to do ever.\n\n\nBut luckily, there is a feature which is built in the AWS console.\nAnd basically, if you go to your role in IAM, in the page for that specific role, you can select the there is an item called revoke session.\nAnd that revoke session will give you something like revoke active sessions.\nAnd this will actually do something very similar to what we described, because it's going to create an inline policy called AWS revoke all the sessions,\nand it's going to inject the current time into that condition.\n\n\nAnd we also going to have a link in the show notes that basically document this particular feature.\nBut it pretty much works in a similar way to what we described.\nSo while this is something that is good to have available in the AWS console, is it's good to know how it works under the wood.\nSo it's something that if you ever want to automate with your own script, or maybe you want to add, I don't know, something that you can easily do from a CLI or a lambda,\nbecause it's maybe easier for you to trigger it that way, you can recreate that feature by yourself.\nNow, that was the answer for temporary credentials. What about if you have long lived credentials? What do we do?\nDo we try not to use them? But we know that the reality they are often something that exists in companies and deployments.\nSo what can we do in those cases?\nThe fact that it's harder to do this with long lived credentials is one of the reasons why you should avoid them all together.\n\n\nEoin: Because your only option is to just deactivate that access key and create a new one.\nSo it's like a revocation, but if you have long lived credentials, by their nature, that probably means that they're hard coded in multiple places,\nthen you have to go updating those places. And that might mean you're updating an application, there might be some downtime involved or a period where the application isn't working fully.\n\n\nSo I would say avoid them if possible. But basically, your only option is to delete the access key, and recreate a new one and update any references to it.\nThat would go for IAM user access keys, but also dreaded root user access keys as well, which hopefully you shouldn't have.\nThere are some organizations that are still using HashiCorp Vault to manage AWS secrets.\nAnd I suppose it's worth mentioning because it sometimes uses long lived secrets under the hood, but they're not really long lived in the same sense,\nbecause Vault is managing the lifecycle of those secrets for you, and is able to rotate them and revoke them automatically.\nSo it's not much of an issue there, you're just relying on Vault to deliver that feature for you and manage your security.\nSo if you're worried about us complaining about long lived credentials, but you're using Vault, don't worry, as long as you're following the best practices. I don't think there's much of an issue there.\n\n\nLuciano: This brings us to the end of this short episode, I hope that now you have a little bit more peace of mind that even if you accidentally leak your credentials,\nat least you know what to do and where you can find some kind of solutions to remediate quickly for that particular issue.\nIt's important, of course, to think about if those credentials are part of an application, if you can guarantee zero downtime to that application while you are revoking the credentials.\n\n\nIt's, of course, something you should be doing, but it's not always possible if you're using long lived credentials.\nSo just keep that in mind. And maybe one more reason to think, am I using long lived credentials? Do I really need to use them? Maybe I can transition to temporary credentials.\nThat's probably going to be a good thing to do anyway. So if you have other tips about how to rotate credentials, or if you have any horror story that happened to you,\nmaybe you leaked credentials and something funny or not so funny happened to you after that, it might be interesting to know.\nIt might be a nice story, might teach us something that we didn't know about. So please share all of that stuff either in the comments or if you don't feel like particularly happy to share this stuff publicly,\nyou can always reach out in private and we can have a nice conversation about that and maybe learn from each other.\nSo that's all we have for today. Thank you very much for following along. If you found value, always remember to share or like. This stuff always helps a little bit.\nSo thank you for that and we'll see you in the next episode.\n"
    },
    {
      "title": "114. What's up with LLRT, AWS' new Lambda Runtime?",
      "url": "https://awsbites.com/114-what-s-up-with-llrt-aws-new-lambda-runtime/",
      "publish_date": "2024-02-16T00:00:00.000Z",
      "abstract": "In this episode, we discuss the new experimental AWS Lambda LLRT Low Latency runtime for JavaScript. We provide an overview of what a Lambda runtime is and how LLRT aims to optimize cold starts and performance compared to existing runtimes like Node.js. We outline the benefits of LLRT but also highlight concerns around its experimental status, lack of parity with Node.js, and reliance on dependencies like QuickJS. Overall, LLRT shows promise but needs more stability, support, and real-world testing before it can be recommended for production use. In the end, we also have an appeal for AWS itself when it comes to investing in the larger JavaScript ecosystem.\n\nAWS Bites is brought to you by fourTheorem, the AWS consulting partner with lots of experience with AWS, Serverless, and Lambda. If you are looking for a partner that can help you deliver your next Serverless workload successfully, look no further and reach out to us at fourtheorem.com!\n\nIn this episode, we mentioned the following resources.\n\nEpisode 104. &quot;Explaining Lambda Runtimes&quot;\nLLRT official repository on GitHub\nQuickJS official website\nLambda runtimes performance benchmark by Maxime David\nRichard Davidson on GitHub\nFabrice Bellard on Wikipedia\nQuickJS-ng fork on GitHub\nQuickJS issue where users debate whether the project is dead or alive\nWinterCG initiative\n\n",
      "transcript": "Eoin: AWS has recently launched LLRT, the low latency runtime,\na new experimental Lambda runtime for JavaScript.\nNow, you might be thinking one of two things,\neither this is amazing, we've got a new runtime for JavaScript,\nit's going to be faster and cheaper than the existing ones,\nI'm going to rewrite all of my Lambda functions right now.\nOn the other hand, you might be thinking, oh, no,\ndidn't we just stop publishing new JavaScript frameworks every week,\nonly to start publishing new JavaScript runtimes every week?\n\n\nOr maybe you're just somewhere in between.\nSo if you're curious today, we're going to give you our perspective\nabout LLRT.\nThere's a lot to talk about with LLRT.\nThere's a lot to love about it.\nBut there are also some concerns that are worth highlighting.\nAnd we'll try to describe these in more detail\nand talk about what LLRT is, how it works,\nand what the specific problem is that it's trying to solve.\n\n\nSo let's get into it. My name is Eoin,\nand I'm here with Luciano for another episode of the AWS Bites podcast.\nAWS Bites is brought to you by fourTheorem,\nthe AWS consulting partner with lots of experience\nwith AWS, serverless and Lambda.\nIf you're looking for a partner that can help you deliver\nyour next serverless workload successfully, look no more\nand reach out to us at fourtheorem.com.\nJust to set the stage, let's just do a quick overview\nof the AWS Lambda service and talk again about what a runtime is.\n\n\nLambda is a serverless service,\nand it's a service that's built on the AWS Lambda platform.\nSo let's go back to the service and talk again about what a runtime is.\nLambda is a serverless compute service\nin the category of functions as a service.\nYou can write your code in the form of a function\nthat can respond to specific events,\nand AWS will take care of provisioning\nall the necessary infrastructure to run that function\nfor when the event happens.\n\n\nLambda supports a lot of different programming languages,\nand it does that using the concept of runtimes.\nAnd every language and language version has a dedicated runtime.\nAnd this is logic that AWS maintains for specific languages,\nstrap your Lambda function, orchestrate events and responses,\nand call your code in between.\nA Lambda runtime also includes the specific runtime binary,\nNode.js, Python, et cetera.\n\n\nFor example, with the Node.js one, you'll get the Node.js binary\nand all the system libraries it needs as well.\nNow, it is possible to build custom runtimes,\nfor instance, to support more esoteric languages\nor specific language versions that are not officially supported.\nAWS itself uses custom runtimes to provide support\nfor compiled languages such as C++, Go, and Rust.\nSo this should give you a reasonable base\nto understand more about LLRT as we go on and have this discussion.\nBut if you're curious to know more about Lambda runtimes\nand how they work, and even how to build your own custom runtime,\nwe have a dedicated episode for that, and that's episode 104.\nThe link will be in the show notes.\nSo given our context, we've talked about Lambda runtimes as you know,\nyou've been looking into LLRT in some more detail.\nWhat have you found out?\n\n\nLuciano: Yeah, I think a great place to start is the LLRT repository,\nand we'll have the link in the show notes,\nbecause it gives, I think, a very good introduction\nto what this runtime is about, why it exists, and a bunch of other interesting things\nthat we are going to try to cover today.\nSo first thing is that this is a JavaScript runtime\nthat is built specifically for Lambda.\nSo it doesn't try to compete with the likes of Node.js,\nDIN, or BAN, which are much more generic purpose.\n\n\nSo this is kind of a very important leader\nbecause some of the design trade-offs\nmake a lot of sense looking at it from this perspective,\nthat it's not competing with all the other ones.\nIt's something very, very specific that makes sense in the context of Lambda.\nSo the first trade-off is that it tries to be very lightweight,\nwhich means that the final runtime package that you get\nshould be as small as possible,\ngenerally in the order of kilobytes\nrather than in the order of megabytes,\nwhich is what you get, for instance, with Node.js, DIN, or BAN,\nyou will have 20, 30, 60, 80 megabytes of runtime itself\nrather than a few kilobytes,\nwhich is the case, for instance, with LRRT.\n\n\nNow, why is this important in the context of Lambda?\nI think we need to remember that Lambda is a very dynamic environment.\nAs you described very well,\ninstances are started only on demand and shut down when not needed anymore.\nSo AWS is going to be provisioning all these necessary resources\nall the time, bootstrapping and killing those,\ndepending on requests arriving into our account.\nSo it is very important that AWS can do all of that as quick as possible,\nbecause every time that you are starting a new instance of a Lambda,\nthe whole process of bootstrapping the infrastructure is called cold start,\nand it's something that's going to affect the latency of your application.\n\n\nSo the choice of runtime is something that is very relevant\nwhen we discuss about how to improve cold start.\nAnd the bigger the runtime package, of course,\nthe more time is required for AWS\nto download all the necessary files and load them into memory.\nSo the bigger the runtime, most likely, the longer is going to be the cold start.\nSo the choice of trying to make the runtime as small as possible,\nof course, is something that tries to reduce the cold start,\nwhich is one of the biggest problems that people always talk about\nwhen we talk about problems with Lambda and serverless in general.\n\n\nSo this is definitely a step in the right direction in that sense,\nand it's a trade-off that makes a lot of sense.\nAnother interesting aspect is that it is built using Rust and QuickJS\nas the JavaScript engine, and these are two very interesting choices.\nSo I'm going to try to give you a little bit more detail about both of them.\nRust is actually not too unusual,\nbecause if we look, for instance, at Deno, it's also built in Rust,\nbut if we also look at Node.js, it's written in C++,\nwhich is somewhat similar to Rust\nin terms of most of the trade-offs that the language takes.\n\n\nAnd very similarly, if we look at BUN, it's written in ZIG,\nwhich is another alternative to C++ and Rust.\nSo in that sense, it's nothing special, I guess,\nbut it's still important to try to understand\nwhat Rust brings to the table in this particular case.\nAnd the first one is that Rust is a language\nthat is built for performance and memory efficiency,\nand these two dimensions are very, very important in the context of Lambda,\nbecause, yes, on one side, you might argue\nthat nobody likes memory-hungry software or slow software,\nbut in the context of Lambda, this is even more important,\nbecause these are the two dimensions that are going to affect price.\n\n\nAnd it's worth remembering that with Lambda,\nyou pay a unit amount that depends on how much memory\nyou allocate for your Lambda function,\nand then you have to multiply that unit amount\nto the number of milliseconds that are used by your Lambda\nwhilst doing something useful.\nSo while your Lambda is running, you take the number of milliseconds\nand multiply for the amount of memory\nthat you have pre-allocated for that Lambda.\n\n\nSo of course, if you can keep the memory footprint very low,\nand you can be still very, very fast at doing the execution,\nthat means that you are going to be using Lambda\nin the most effective way from a pricing perspective.\nSo your CFO is probably going to be very thankful,\nlooking at the bill and checking that there was maybe\na quite significant reduction in cost\nwhen it comes to the Lambda item in the bill.\n\n\nSo faster startup, by the way, is not only to be seen\nfrom the perspective of price, which is important,\nbut I think there is another very important aspect\nthat is power consumption.\nThis is something we are becoming more and more aware in the industry.\nProbably we should do even more.\nWe are still at the very beginning of the conversations.\nBut I think it's important to realize that everything we run in the cloud\nhas a cost not just from an economic perspective,\nbut also in terms of environment and sustainability.\n\n\nSo we need to be very mindful that we might be able to do something\nto reduce that kind of footprint.\nAnd every time we have the chance, we should probably take the chance\nbecause it's something that we will need to eventually care\nand be more responsible.\nSo it's important to see that perspective as well.\nAnd having a runtime that can give us very, very efficient compute,\nit's something that goes in the right direction in that sense.\n\n\nAnd to be fair, serverless is also a very sustainable technology in general.\nSo if we can make it even more sustainable,\nit's another win that we take from this particular set of trade-offs.\nNow, it's also worth mentioning that the idea of using Rust or C\nin order to make code more sustainable\nis generally kind of a double-edged sword.\nOn one side, you get that effect that you become more sustainable.\n\n\nBut on the other side, there is a huge investment\nin terms of teams having to learn these technologies,\nespecially if you have teams that are more versed with technology\nsuch as Python or JavaScript.\nThat's going to become a very big investment to do.\nSo here, there is an even more interesting trade-off\nbecause the promise is that you don't need to learn\na new low-level language like C, C++, Rust, or Go.\n\n\nYou can stick with JavaScript,\nwhich is probably something much more well-known in the industry,\nand still get very good trade-off\nand very good performance and energy efficiency.\nSo this is definitely one of the areas\nwhy LLRT shines in terms of a very interesting approach.\nNow, speaking about QuickJS,\nthis is quite of a novelty in the JavaScript runtime space.\nWe have a link to the QuickJS website\nwhere you can find a bunch of details.\n\n\nAnd it's probably worth looking into it\nif you've never heard about QuickJS.\nBut I'm going to try to explain very quickly what it is\nand what kind of trade-offs it provides.\nSo QuickJS basically implements JavaScript,\nmeaning that it's able to interpret and execute JavaScript code,\nand it does it in such a way that it's almost like a library\nthat you can take and embed in other programs.\nSo it doesn't really give you any core library, so to speak.\n\n\nIt's just able to understand the JavaScript syntax\nand execute it correctly.\nAnd this is something that every JavaScript runtime needs\nin a way or another, but the big ones, Node.js,\nDeno, and BUN, none of them use QuickJS.\nIn fact, Node.js and Deno both use V8,\nwhich is the Google Chrome JavaScript engine,\nwhile BUN uses JavaScript Core, which comes out from WebKit,\nwhich is the project that's by Apple that is used in Safari.\n\n\nSo QuickJS is somewhat novel in the space\nof JavaScript runtimes,\nand the reason why I believe it's being used here is, again,\nbecause it tries to fulfill that promise that it needs to be\nas small as possible in terms of inventability\nand as easy as possible to embed in an application.\nIt's also quite modern and feature complete.\nIn fact, already supports ECMAScript 2023,\nincluding ECMAScript modules, including other advanced features\nlike async generators, proxy, begin, there are even extensions to have,\nthings that are not even in the ECMAScript specification yet.\n\n\nAnother interesting trade-off is it doesn't have a just-in-time compiler,\nand this might seem like a negative thing\nbecause I think all the modern runtimes are expected\nto have a just-in-time compiler,\nand generally something that helps a lot with performance,\nbut I think it's important here to understand the trade-off.\nSo let's try to explain quickly what a just-in-time compiler is.\nGenerally, with interpreted languages,\nwhat you do is as you scan the code, you try to evaluate it,\nand that's basically run in the program.\n\n\nAnd of course, this is not going to be extremely efficient\nbecause most of the trade-offs that dynamic languages have\nis that you don't necessarily have strict typing,\nso the runtime needs to make a lot of assumptions\nto be as generic as possible\nand to support a lot of dynamic range of functionalities.\nSo generally speaking, the interpreted languages will at some point\nintroduce a just-in-time compiler that tries to,\nas you read the code and process the code,\nfigure out what are the patterns and try to generate machine code\nwhich is much more optimized on the fly\nand start to swap out part of your scripting language\nwith actual compiled code\nthat can run much faster on your specific architecture.\n\n\nNow, while this is very good in the long term,\nso if you have computation that needs to run for a long time,\nif you have a computation like in the context of servers\nwhere you're trying to optimize for small event-driven pieces of computation,\nsometimes it's a little bit of a waste to do all of this optimization\njust to shut down your computation after a few seconds\nor even milliseconds in most of the cases.\n\n\nSo here it's a very interesting trade-off\nbecause we are giving up on that just-in-time capability\nbecause we know that most of the time we are going to prefer\nto have very small and fast lambdas\nthat are going to do something very quickly, mostly glue logic,\nand therefore we don't necessarily need that level of optimization,\nwhich comes with a little bit of upstart price\nthat you have to pay to do all of that compilation up front.\nSo I think this is something that makes a lot of sense\nin the context of all LRT,\nbut I guess we can start to discuss about\nhow much performance are we really talking about?\nCan we figure out what are some numbers\nor maybe some comparison with Node.js?\n\n\nEoin: Well, we haven't had the chance to try it ourselves in any great detail,\nbut there is an interesting benchmark on the LLRT repository,\nand it's based on a fairly simple lambda function\nthat puts a record into a DynamoDB table.\nSo even though it's minimal, there's a bit more realism to it\nthan the usual hello world style benchmarks,\nand it compares the performance of running this function\non an ARM architecture, so graviton-based lambda\nwith 128 megabytes of allocated memory,\nand the other side of the comparison is Node 20.\n\n\nSo LLRT results, if we look at the...\nThe results are kind of presented with, you know, P100, P99,\nso you can see the maximum cold start time and the maximum run time, as well as like P50, so the 50th percentile,\nand we can see that for the 95th percentile with LLRT,\nyou're getting 76 millisecond cold starts, which is pretty good.\nOn Node.js 20, they're reporting 1600 milliseconds of cold start time\nfor 95% of the cases, the maximum,\nand then warm start executions are looking at 33 milliseconds\nfor this function with LLRT compared to 100,\njust over 100 milliseconds with Node 20.\n\n\nSo the full tables and set of benchmarks is available on the website.\nIt's kind of interesting that it's only comparing ARM,\nand it's only using Node 20.\nI think it would be great to have a more comprehensive set of benchmarks,\nbut in general, what this is showing is that in this permutation,\nat least LLRT is noticeably faster than Node 20,\nparticularly when it comes to cold starts.\nThere's another very well-known benchmark,\nwhich we've mentioned, I think, before on a few episodes,\nthat tries to compare the cold start memory footprint\nand the execution latency of different runtimes,\nand they recently added support for LLRT in their test suite.\n\n\nLLRT scores very well in most configurations there,\nand it's generally the third fastest runtime behind C++ and Rust.\nIt's even faster than Golang in this case.\nOf course, you have to bear in mind that C++ and Rust\nare very mature ecosystems, comparatively go as well,\nand this is still an experimental beta product.\nIn the benchmark, we can also see the difference in memory usage,\nand if we compare LLRT to Node 20,\nwe have 24 megabytes versus 63 megabytes,\nso it's about a third of the memory needed for the same Lambda function.\n\n\nIf the performance is the same, it might mean\nthat you can reduce your memory allocation\nand save cost even further.\nSo this seems pretty exciting,\nand I've been using Node.js for a long time,\nso the idea of this kind of explosion in runtimes\nis a little bit exhausting to think about, to be honest,\nbecause so much investment has gone into Node.js,\ninto JITs, into optimizing.\nI mean, whenever I hear people from V8 team or the Node team\ntalking about the amount of effort they put into optimization\nof single functions and single libraries,\nI think, how can these runtimes ever get that same level of maturity?\nBut maybe if they focus on a specific problem,\nmaybe there is a use case where we should be thinking about them.\nSo, Luciano, you're a Node.js aficionado.\nHow does this make you feel?\nDoes it make you think that you should use LLRT\nfor every single Lambda function now, or where do you stand?\n\n\nLuciano: Yeah, I think that's a great question,\nand it's a bit difficult to give you a 100% answer.\nI think we will see as we go what happens to the project,\nbut as it stands today,\nthere are a few things to be a little bit concerned about.\nFirst of all is that the project itself is labeled as experimental,\nand we don't know exactly what that really means,\nbut we can make some assumption and also try to interpret\nwhat we can see in the repository.\n\n\nSo the repository marks the release as beta.\nSo, again, not really indicative of any kind of promise,\nbut it gives us a first idea that is not something\nthat we can consider stable right now.\nSo maybe let's not use it for everything we have in production just now.\nMaybe let's wait to see when it becomes a little bit more stable in that sense.\nAlso, the repo says that it is subject to change,\nand it is intended only for evaluation purposes.\n\n\nSo, again, don't use it for your most important production workload.\nMaybe if you have a very secondary workload\nand you want to use it with something that is a little bit more relevant\nto your business, that could be one way of approaching it,\nbut definitely use it for the most sensible business case that you have\nbecause you might have unexpected surprises.\nAnd I think there is, in general, no guarantee that AWS\nor the current maintainers are going to invest more on this project\nas it stands today, and even if they do,\nmaybe they will change everything,\nor they will change a significant amount of the code base\nthat might require you to do a significant amount of change on your side.\n\n\nIf you want to keep using the project.\nSo that's definitely something to keep in mind as a starting point.\nThere is another problem that is also very important,\nthat this project is not Node.js.\nSo it's not packaging Node.js in a smarter way.\nIt's just a totally different implementation of a JavaScript runtime.\nAnd the reason why this is important is that on one side,\nit doesn't come with all the baggage of Node.js,\nand this is why it can be very fast and very performant, as we described,\nbut on the other end, it doesn't have all the ecosystem of libraries\nthat Node.js has, and that has been for over,\nI think, almost 15 years at this point.\n\n\nSo what that means is that you don't have\nthe full Node.js standard library at your disposal,\nand that means that you might have problems with some of your code.\nEven if you're using third-party libraries,\nthose third-party libraries might rely on some functionality\nthat exists in the standard library of Node.js\nthat doesn't exist in LLRT yet.\nAnd when I say yet, it doesn't mean that there is a promise\nthat eventually LLRT is going to have future parity with Node.js.\n\n\nActually, if you look at the readme, they state very clearly\nthat this is not a goal. They are not going to try to compete\nfor future parity with Node.js.\nThey have some degree of support,\nbut there is no promise that they will try to improve\nthe percentage of coverage in that sense.\nSo I guess for the foreseeable future,\nwe only have a partial implementation of the Node.js standard library,\nand another thing to keep in mind is that even that implementation,\nthere is no guarantee that it's matching 100%\nthe same level of functionality that we have in Node.js.\n\n\nYou might have surprises, for instance,\nsubtle differences on how certain APIs actually work in certain edge cases,\nand that means that all the code you write,\nyou need to be very careful testing it specifically in the context of LLRT\nand not just run Node.js tests with Node.js\nand assume that everything is going to work as expected\nwhen you package it into LLRT.\nNow, speaking of libraries, you might think,\nwhat about the AWS SDK, right?\n\n\nBecause most likely, this is the main library\nthat you will need to use in a Lambda.\nAnd actually, interesting enough, this runtime comes with many AWS\nSDK clients already baked into the runtime.\nThere is a list on the repository.\nLast time we counted was 19 clients supported,\nplus the Smt library from AWS.\nSo if you need to use one of these 19 clients or the Smt library,\nyou don't need to install it yourself.\n\n\nThose are already prepackaged in the runtime.\nAnd actually, the repository goes as far as saying\nthat it's not the standard package itself,\nthe one that you would get from npm,\nbecause there are extra optimizations that the authors have put in place,\nreplacing some of the JavaScript code\nthat exists in the standard version of the library\nwith some native code, supposedly Rust, I imagine.\nSo I guess that could give you an extra boost in performance\nwhen you use these libraries.\n\n\nNow, they also say that not all the methods are supported.\nFor instance, if you try to get a stream from a response\ncoming from the SDK, maybe...\nI haven't tested this very thoroughly,\nbut I imagine if you're trying to read a big file from S3,\nthat might be a little bit of a problem\nif you cannot really stream that output into your program\nand you need to patch all the data into memory\nbefore you can actually access to it.\n\n\nI'm not really sure if this use case is supported or not,\nbut there might be similar cases like that\nwhere not being able to stream the response coming from the SDK\nmight become a limitation in terms of the memory usage,\ndepending on your use cases.\nSo again, it might work in most cases.\nIt might actually be even faster in some cases,\nbut you have to be really careful\ntesting all the use cases that you have in production.\n\n\nNow, last thing, what about tooling?\nBecause this is always the main thing\nwhen it comes to new programming ecosystems.\nIt takes a while before the tooling is good enough for you as a developer\nto have a very good experience and be productive.\nSo what is the starting point that we get here?\nIt's actually not too bad,\neven though we haven't played enough with it\nto be confident in saying that.\nBut just looking at it and just playing with it a little bit,\nthere are a few things in place that are already quite useful.\n\n\nFor instance, there is a Lambda emulator\nthat you can use to actually test the runtime locally.\nSo all the code that you write, you can immediately execute it locally\nand see if it's performing and be adding exactly as you expect,\nwhich is great because it kind of reduces the feedback cycle\nof always having to ship to AWS\nto be sure that your code is actually working as expected.\nThere is also a tool that allows you to package all your code\ntogether with the runtime into a single binary.\n\n\nSo you are effectively building a custom runtime\nthat includes not just the runtime, but also all your code into one binary.\nAnd this is actually the preferred and recommended approach\nto deploy Lambdas written using this runtime.\nAnd the reason why this is convenient is because that's going to more likely\nimpact performance positively because it needs to load only one file\nand then everything is already in place and ready to start.\n\n\nAnd finally, there is also a Lambda layer available.\nIf you prefer to take a little bit of a more experimental approach\nwhere you say, okay, I'm just going to put this layer into the web console\nand just going to play around with it this way,\nthat could be another approach to start using OLRT\nand see what that looks like.\nNow, again, it's worth remembering that this is not an officially supported\nLambda runtime, it's a custom runtime.\n\n\nSo what you deploy is effectively a custom runtime\nand you are responsible for it,\nmeaning that if there is a new update or if there is a security concern\nand maybe you need to install something to patch a security issue,\ndoing all of that work is on you.\nSo you need to be ready to take over that additional burden\nthat you don't have, for instance, when you use the official Node.js runtime.\nSo what is our recommendation again?\n\n\nJust to try to summarize all of that.\nI think this is a great initiative,\nso it is definitely worth playing with it and see what it looks like.\nAnd for your specific use case,\nhow much performance can you squeeze out of them?\nBut again, because it's so early and experimental\nand it's not really clear what is going to be the future of this project,\nuse it with cautious, use it with the idea\nthat you're not going to re-implement everything with this runtime.\n\n\nMaybe you're just going to implement a few functions that you use a lot,\nbut they're not the main ones for your business.\nSo I guess if all goes well,\nwe would have gained major performance benefits\nwithout having to switch to C++ or Rust,\nwhich would be a big win for the serverless and the JavaScript community.\nBut again, we have to be seeing exactly what is going to happen.\nIt's also an open source project,\nso if you are really excited about this kind of initiatives,\nyou can contribute to it.\nAnd at that point, you are also a little bit responsible\nfor the success of this initiative.\nSo this is always a good call to action to people\nthat if you feel like you want to contribute,\nyou want to see this project successful,\nyour contribution is definitely going to be useful\nto achieve that larger goal.\nNow, what other concerns do we have, Eoin?\n\n\nEoin: Well, we already mentioned that it's experimental,\nand I think that's fair enough because they state that explicitly.\nAs well, if you look at the contributions,\nit's built mostly by one person.\nAnd I think we have to credit the amazing engineering effort here.\nBut Richard Davidson is the amazing developer\nwho has done an incredible job here.\nBut there's obviously a risk associated\nwith having only one main person behind the project.\n\n\nSo let's see if AWS decides to invest more on the project\nand form more of a cohesive internal team as the project evolves.\nIt's good to see that in a few weeks since its public release,\nthere have already been contributions\nfrom open source members of the community.\nSo we can expect to see that grow, and that will be a healthy thing.\nThe lack of feature parity with Node.js\nand other runtimes is going to be a concern.\n\n\nAnd there isn't really an intention to reach parity,\nso you just have to be aware of that.\nYou mentioned as well, Luciano, there is some AWS SDK support.\nI kind of wonder, since there's already the C-based common runtime\nfrom AWS that's highly optimized, as well as the C AWS SDK,\nI wonder why LLRT wasn't able to leverage those\nto get complete service support.\nI suppose as well, QuickJS,\nbeing one of the main dependencies, may also be a bit concerning.\n\n\nIt has an interesting history as a project.\nIt was mostly written and maintained by another outstanding engineer,\nFabrice Bellard, and Fabrice is also the same author\nof other great projects like QEMU and FFmpeg.\nAgain, same problem with single owner projects.\nThere's a risk with it. In fact, the Qix.js project hasn't received,\nwell, it didn't receive any great updates in the last few years, and the project really looked to be stagnant\nwith a lot of forks emerging in the open source community,\nmost notably Qix.js NG. There has been some activity of late,\nbut there is an interesting community conversation on,\nI suppose, whether this project is alive or dead,\nand we can link to that conversation on GitHub in the show notes.\n\n\nSo there has been a recent spark of activity,\nas I mentioned, in the repository,\nand Fabrice has introduced some significant new features,\nsuch as support for top level of weight,\nand a couple of new releases have been published.\nSo hopefully, a larger community will form around the project,\nand that will help to guarantee long-term support,\nbecause I think it's interesting.\nPreviously, there were various different JavaScript runtimes.\nThere was JavaScript Core, you had V8.\nMicrosoft had their brave effort for a while with the Chakra Core,\nbut the idea was that Node.js could use any of these runtimes,\nthese JavaScript runtimes.\nThat seemed like a healthy thing with good competition,\nbut it seems like everything has kind of converged\non the Chromium ecosystem,\nand that's not a great thing for the future of JavaScript, I feel.\nLuciano, you've kind of given your recommendations,\nbut what's your final assessment?\n\n\nLuciano: I think, in general, I'm very happy to see\nthese kind of initiatives coming out from AWS,\nbecause everything that can make Lambda more efficient and powerful\nfor JavaScript developers is absolutely welcome.\nI think everyone should be happy about that.\nIt is a very ambitious project, and if it becomes stable,\nand there is a team maintaining it consistently,\nit's going to be a win, definitely, for the server landscape as a whole.\n\n\nBut I think we need to talk about another problem,\nwhich is the JavaScript ecosystem fragmentation.\nIt's something that we have been seeing a lot\nin the JavaScript community for I don't know how many years at this point,\nand it seems like it's getting worse and worse rather than getting better.\nSo this...\nSometimes it's called the JavaScript fatigue.\nIt's definitely real, and it was associated\nwith the idea of frameworks and libraries.\n\n\nNow it's being associated even with runtimes,\nwhich only makes things worse.\nIt's already hard to pick and learn a single runtime like Node.js.\nImagine if you also have to learn Dino or BUN\nwith all the different core libraries and characteristics,\nand now there is also another Lambda-specific runtime,\nwhich will have its own characteristics and things to learn\nand mistakes and patterns.\nBut even imagine that now you are a JavaScript library author,\nand you want to build a general-purpose library\nthat you might want to make available across all of these runtimes.\n\n\nNode.js, Dino, BUN, the browser, and maybe now even OLL or T, right?\nBecause why not allowing people to even use your library\nin the context of a Lambda?\nHow much work there is involved in just testing\nthat everything works with F3, just fine-tuning all the edge cases,\nmaybe patching for all the missing libraries\nand different behaviors that exist across different runtimes.\nSo this is a problem that's just going to keep getting bigger and bigger\nif the ecosystem doesn't converge into kind of a more comprehensive standard\nthat all the different runtimes will adopt.\n\n\nThere are some efforts in that direction.\nFor instance, the Winter CG that we can link in the show notes\nis an initiative that tries to figure out exactly\nwhat is a common set of APIs that every runtime needs to have,\nespecially the ones running in the cloud and on the edge.\nSo there might be, I guess, a bright future there\nif this kind of initiative is successful.\nBut as it stands right now, as a developer,\nit's just a very confusing landscape,\nand there's a lot to learn and so many edge cases.\n\n\nSo that's definitely a problem.\nAnother point that I have, and this is more directed to AWS,\nit's great to see this kind of initiative emerging from AWS,\nbut at the same time, I would love to see AWS\ninvesting more on the larger Node.js ecosystem.\nWe know these things that are not super nice to see.\nFor instance, if you look at the performance of the Node.js\n16 runtime and compare it with the Node.js 20 runtime,\neven though Node.js itself is generally considered faster\nin the Node 20 version, when it comes to Lambda,\nsomehow the runtime is a little bit slower than Node 16,\nwhich is very disappointing because it looks like they didn't take advantage\nof the new advancements in Node.js,\nand maybe they did something suboptimal on their side.\n\n\nNow, I'm not really sure what's going on there,\nso I'm not going to comment too much in detail,\nbut I think the message there is that I wish that AWS would invest more\nin making sure that Node.js has a bright future ahead\nbecause it's effectively one of the most used languages\nwhen it comes to Lambda,\nso definitely a big revenue stream for AWS,\nand it would be nice to see AWS\nreinvesting some of that revenue into the project itself.\nAnd it's not just something that relates to Lambda itself\nbecause Node.js gets used a lot even in other kinds of applications,\nnot just serverless, it will be used in containers,\nso something like that in ECS, Fargate, but also in EC2 or AppRunner.\nSo if Node.js gets better,\nI think AWS is still going to benefit from it.\nSo this is kind of a final call for consideration to AWS\nif somebody's listening there to think about this problem\nand maybe decide to invest a little bit more into the Node.js community.\n\n\nEoin: Yeah, we're seeing lots and lots of different ways\nto optimize code starts and runtime performance.\nI'm thinking of Snap Start, currently available in Java,\nand it might come to more runtimes,\nand then we see like with .NET,\nyou've got the new ahead-of-time compiler,\nwhich is essentially compiling it to native code.\nI wonder if the AWS Lambda team are thinking about how Snap Start\ncould be used to optimize existing Node.js runtimes\nand give us the kind of amazing code start times\nwe've seen with LLRT or even better,\njust with existing Node.js and all the compatibility it offers.\n\n\nSo it's definitely a space to watch, and regardless of what happens next,\nI think we can agree that LLRT is already\nan amazing software engineering achievement,\nand a lot of credit has to go to Richard\nand also to Fabrice, the QuickJS author, too.\nSo if you're a JS developer interested in LLRT,\nit is important to check compatibility\nand measure performance with meaningful workloads.\nWe're just seeing, I think, the first set of benchmarks here.\nBut if you have seen some results and you've got some success\nor you've decided to abandon it for now, let us know what you think,\nbecause we're really curious to learn more ourselves.\nSo thanks very much for watching or listening.\nPlease share with your friends, like and subscribe,\nand we'll see you in the next episode.\n"
    },
    {
      "title": "115. What can you do with Permissions Boundaries?",
      "url": "https://awsbites.com/115-what-can-you-do-with-permissions-boundaries/",
      "publish_date": "2024-02-23T00:00:00.000Z",
      "abstract": "In this episode, we discuss Permission Boundary policies in AWS IAM. A permissions boundary is an advanced feature in which you set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions allowed by its identity-based policies and its permissions boundaries.\nIn this episode, we discuss this concept a bit more in detail and we show how it can be used to give freedom to development teams while preventing privilege escalation. We also cover some of the disadvantages that come with using permission boundaries and other things to be aware of. Finally, we will give some practical advice on how to get the best out of Permissions Boundary Policy and get the best out of them.\n\nAWS Bites is brought to you by fourTheorem, the AWS consulting partner with lots of experience with AWS, Serverless, and Lambda. If you are looking for a partner that can help you deliver your next Serverless workload successfully, look no further and reach out to us at fourtheorem.com!\n\nIn this episode, we mentioned the following resources.\n\nEpisode 112. &quot;What is a Service Control Policy (SCP)?&quot;\nIAM Policy Simulator\nThe famous RSA paper that introduces Alice and Bob in the world of cryptography\nA biographical backstory on Alice and Bob\n\n",
      "transcript": "Luciano: In episode 112, we talked about SCPs, or Service Control Policies.\nSCPs are about setting a maximum boundary for permissions in an account or in an organization unit within an organization.\nThere is another kind of policy that is important to understand, and this is the Permission Boundary Policy.\nIf you don't fully know how this works yet, you can run into all kinds of problems.\nBut once you know, you have a tool that allows you to enforce further security, prevent something called privilege escalation,\nand overall will let you sleep a little bit better at night.\n\n\nToday, we will try our best to give you an effective introduction to the Permissions Boundary Policy.\nMy name is Luciano and I'm joined by Eoin for another episode of AWS Bites podcast.\nAWS Bites is sponsored by fourTheorem. fourTheorem is an AWS consulting partner.\nSo if you need help to start your journey on AWS or build ambitious projects on AWS, check us out at fourtheorem.com.\nOkay, back to the topic of Permission Boundary Policy, what it is, let's try to give a little bit of an overview.\n\n\nThere are additional policies that can be attached assigned to identity policies.\nSo what do we really mean by this? That means that we can attach them to IAM users, we can assign them to IAM users,\nwe can assign to IAM roles, we can assign them to IAM Identity Center Permission Sets, which are still roles under the hood.\nBut the idea that we need to clarify is that they don't really have an effect on resource based policies.\nSo like SEPs, they never grant any permission, it's just another mechanism that allows you to set a maximum boundary\naround all the possible permissions that you can have in a given account.\nSo the question might be, if we already have SEPs, why do we have another thing in place?\nWhy do they really exist? What is the use case for other kinds of policies?\nYou might say that the reason for these permissions boundaries to exist is all to do with the whole idea of security shifting left.\n\n\nEoin: So what does that mean? Well, I suppose IT security used to be an area exclusively for operations teams to worry about.\nBut the trend in recent years has been to shift left the responsibility so that development teams and architects are part of the approach to operations and security\nand it becomes a collective responsibility.\nThere's an understanding that in order to be productive now teams need more control over infrastructure and security\nand don't just throw stuff over the wall to another team.\n\n\nThis is really what the DevOps movement was all about.\nThe approach really has the benefit of removing barriers and blockers to innovation.\nThe other benefit of it is that you have greater awareness of security requirements and practices throughout the whole organization.\nSo to shift security left you need to give teams power to administer things like IAM roles and maybe even users within their domain.\n\n\nBut you also have to limit the potential risk, right?\nBefore the introduction of permissions boundaries, if you gave a developer role the permission to create other roles,\nthey could actually use that privilege to create a role with more permissions than they had themselves.\nAnd then they could just assume that role and become like a super user.\nThat is what we call privilege escalation and it's something security admins would be very careful to avoid.\n\n\nSo it's not just about escalation for developers, but obviously any attacker that might gain access to the developers credentials could do that as well.\nSo then to explain how we prevent privilege escalation by using permissions boundaries, we might think of an example.\nSo let's say Bob is a forward-thinking but security conscious AWS account administrator\nand Bob wants to delegate IAM administration within a development account to Alice, Alice the developer,\nand giving Alice permissions to create roles within that account.\n\n\nAnd I think a lot of people hopefully have the experience of doing that as AWS developers.\nIdeally Bob would like to be able to give Alice the permissions to create, edit and remove roles and users within that account.\nBut there are a couple of things you would prefer that Alice is not able to do either intentionally or accidentally.\nAnd those are things like Alice should not be able to update or delete her own role and its policies because that's managed by somebody else.\n\n\nAlice should not be able to update or delete roles that are created by the administrators\nor be able to assume a role with greater privileges than Alice's own role.\nAnd Alice should also not be able to pass a role with elevated permissions to an AWS service.\nAnd if you think of an example, let's say you have a role exists and EC2 is a trusted service in its trust policy\nand has the administrator access permissions policy attached.\nAlice might have permissions to start EC2 instances and could then specify this role as the instance's instance profile.\nAnd then Alice could then log on to that EC2 instance and perform actions that are not permitted by the role Alice has herself.\nSo this pass role idea here with EC2 is also something you need to be careful of with privilege escalation.\nAnd this whole area is something that permissions boundaries are designed really specifically to help.\nSo Luciano, would you like to describe how permissions boundaries are the solution to privilege escalation?\n\n\nLuciano: Yeah, let's say that Bob now needs to fix this problem, right? What can I do?\nAnd the idea is that Bob can create a permission boundary policy, which is just a normal policy in AM.\nBut it will allow him to restrict specific actions like create a role, but make sure that Alice must assign a permission boundary to it.\nSo the idea again is that Alice will have a policy that permits her to create a role, but that role cannot be unconstrained.\n\n\nIt needs to have a very specific constraint, which is it needs to allow to assign specifically this particular permission boundary to that role that was created.\nThen, of course, it denies the ability to update or delete role when that particular role is managed by the administrator.\nSo it's kind of sealing the role in that way. It denies changes to the boundary policy itself.\nSo it's also sealing the boundary policy. And then it denies the deletion of centrally managed policies.\n\n\nFinally, also prevents IAM pass role with a role that doesn't have the same permission boundary.\nSo again, it's just trying to make sure that the permission boundary is kind of a common denominator in all the roles that are managed this way.\nAnd pass role is what mentioned that is a special IAM action that doesn't have any specific API call associated to it in the SDK.\nIt's just another way to prevent privilege escalation by basically defining or controlling who can pass a specific role to an AWS service.\n\n\nSo you have a way to express this concept of an entity passing a role to another AWS service, and therefore create restrictions around that particular action.\nNow, when Alice creates a role, she can basically give it whatever permission makes sense for the particular context.\nShe also has to set the permission boundary on this role to the REN of the boundary policy set by Bob, which basically means that the policy itself will follow whatever the boundary policy is limiting.\n\n\nAnd this will restrict the action that the role can perform.\nSo Bob at this point can say you cannot spin up an EC2 instance.\nAnd at that point, this way, the new role is in editing that limit action.\nAnd this is also going to prevent elevation of permissions or messing up with the roles in ways that were not intended for.\nNow, what are the disadvantages of boundary policies?\nBecause it seems like they solve a very interesting problem that might happen, and it might be very tricky to even realize that you might have this vulnerability in your AWS architecture.\nBut is there anything else that we would need to be concerned when it comes to boundary policies?\n\n\nEoin: Yeah, they're quite a nice thing to be able to allow somebody the freedom to create a role with a policy like administrator access attached, and then worry that you still got some boundary that would prevent them from doing really serious things like privilege escalation.\nBut there are, like you say, some disadvantages.\nAnd really, it comes down to the fact that you have to attach them everywhere you create a role or a user if you want to use them effectively.\n\n\nAnd that can become a little bit of friction.\nIt's just an extra step when you create that role and you have to remember, oh, what is the ARN of this permissions boundary I'm supposed to attach everywhere now.\nIt's easy enough to do in the AWS console, or with CloudFormation and Terraform.\nSome tools, however, make it a bit of a pain.\nThere are a lot of serverless framework plugins that create roles implicitly, just for convenience.\n\n\nBut a lot of them don't really have good support for permissions boundaries, or they've got inconsistent support for attaching boundary policies.\nAnd I've had the case in the past where I was working in an environment where I was explicitly forced to use permissions boundary policies.\nBut I ended up having to define that in like three or four different configuration formats in the one serverless YAML file just to get it working.\n\n\nAnd in other cases, you might have to do some hacky stuff if it doesn't support the boundary policy.\nSo I think most tools are getting better at it, but it can be inconvenient.\nSo just watch out for that.\nAnother thing is that having multiple policy types in your organization, like we talked about SEPs, now we're talking about boundary policies.\nYou've got resource based policies and endpoint policies, as well as the principal policies, and having permissions defined in all these different places can make troubleshooting more complicated.\n\n\nLuckily, the IAM policy simulator can help with this.\nI think we mentioned in the SCP episode is that we mentioned the fact that the policy simulator wasn't great for SCP errors,\nbut it is good at pointing out where permissions boundary policies are restricting you from performing an action.\nBoundary policies can be abused as well, have become too fine grained.\nLike service control policies, they should really only be used for broad permissions and really not much more than privilege escalation.\nOtherwise, they'll become very maintainable and really slow productivity down, which is not what you want.\nBut maybe as a quick aside, Luciano, we keep talking about Bob and Alice.\nCan you explain to the audience and to us who are Bob and Alice in the first place?\n\n\nLuciano: Yeah, I think if you have done any cryptography at college or in general, because maybe you like to read this kind of stuff, you might have come across Bob and Alice.\nBut there is an interesting story behind it, and I think it might be nice to just mention it.\nAnd basically the idea is that when people were trying to describe cryptographic algorithms, you generally have at least two entities and they were called A and B.\n\n\nSo Alice and Bob are kind of an evolution of that.\nThat was introduced by Rivest, Shamir and Adleman when they wrote the famous RSA encryption paper.\nRather than just using A and B, they introduced Alice and Bob.\nSo they kind of created characters, which was, I guess, something interesting to do in a scientific paper.\nBut it was well received by the community to the point that everyone is doing that now.\n\n\nAnd there are also interesting side stories that gets created around the life of Alice and Bob.\nAnd we will have a link in the show notes if you're curious to look more into that.\nSo these papers can be fascinating because sometimes you have this kind of Easter eggs that end up becoming kind of a standard across the entire literature.\nOK, what is the summary for today?\nSo today we try to give developers a way to control IAM to some degree while still retaining administrative permission.\n\n\nSo we want to give freedom to developers to be able to set the permissions that make sense to them.\nBut of course, there needs to be a boundary somewhere so we can use these new tools that we talked about today.\nAnd this also allows us to prevent privileged escalation.\nSo if you set them up correctly, they will help you to make sure that as a user, while you still have all the freedom that you need to do your job as a developer,\nthen you still are not going to be able to do something nasty and escalate permission and suddenly become an administrator or delete things that you were not supposed to delete.\n\n\nSo permission boundaries are the way to achieve all of this.\nAnd also the concept of IAM pass role is very important to understand to really master how to craft permission boundaries and how to use them.\nThe idea again is don't abuse them, keep them at very high level and mostly focus on the privileged escalation use case.\nIf you try to be over specific, it's going to become very, very hard to maintain them.\n\n\nAnd you will always have tickets opened against you as an administrator because somebody needs to do something and suddenly they don't have the right permission to do it.\nAnd that will slow down the entire development chain.\nSo just be very aware of all of that.\nAnd this brings to the end of this episode.\nI hope you found it interesting and I hope that complements all the other episodes that cover permissions and IAM and policies.\nAnd hopefully we are giving you over time a good overview of what are the best practices or the tools that you need to use to really put your AWS accounts under control and make sure you have proper security and power permissions set up.\nIf you have any other suggestions on topics that we might cover, just let us know.\nOr if you have any interesting story around SCPs or boundary policies, also something we'd like to know and maybe talk about.\nThank you very much and we'll see you in the next episode.\n"
    },
    {
      "title": "116. What is RAM (Resource Access Manager)?",
      "url": "https://awsbites.com/116-what-is-ram-resource-access-manager/",
      "publish_date": "2024-03-01T00:00:00.000Z",
      "abstract": "In this episode, we discuss AWS Resource Access Manager (RAM) and how it can be used to securely share AWS resources like VPC subnets, databases, and SSM parameters across accounts. We explain the benefits of using RAM over other options like resource policies and assumed roles. Some key topics covered include how to get started with RAM, how it works from the resource owner and resource participant side, and common use cases like sharing VPC subnets, Aurora databases, and SSM parameters.\n\nAWS Bites is brought to you by fourTheorem, the AWS consulting partner with lots of experience with AWS, Serverless, and Lambda. If you are looking for a partner that can help you deliver your next Serverless workload successfully, look no further and reach out to us at fourtheorem.com!\n\nIn this episode, we mentioned the following resources.\n\nSharing Aurora Databases with RAM (Conor Maher's article)\nBlog post &quot;VPC Lattice: The Future of AWS Networking Explained&quot;\nOur previous episode dedicated to VPC Lattice\nVPC Lattice sample code base on GitHub\nSharing AWS Systems Manager Parameters official announcement\nOfficial documentation for what can be shared with RAM\n\n",
      "transcript": "Eoin: Sharing is caring and if you are following the current best practices of using multiple AWS\naccounts, you'll end up having to share resources between accounts. For some things, you might be\nable to use resource-based policies to grant access or assume a role in the target account.\nThis isn't suitable for everything, however, but luckily there is another way. Today, we are going\nto chat about Resource Access Manager and by the end of the episode, you should know what it does\nand how to use it. I'm Eoin, as always, I'm here with Luciano and this is AWS Bites.\nAWS Bites is brought to you by fourTheorem, the AWS partner who works with you to build successful\nprojects in the cloud. Check us out at fourtheorem.com. Luciano, maybe you can start and tell us all\nwhat kind of problems is Resource Access Manager designed to solve?\nYes.\n\n\nLuciano: So AWS Resource Access Manager or RAM, which is not to be confused with random access memory, by the way, it's the same acronym. So this is a tool that is designed to solve the problem of\nsecurely sharing AWS resources across different AWS accounts within an organization or even with\nexternal organizations. It is basically designed to do this while it tries to reduce the cost\nby eliminating, for instance, resource duplications. You can simplify centralized\naccess to resources and it can achieve security and compliance. So RAM allows you to share\nresources with, as I said, AWS accounts, including the ones outside or inside the organization.\nIt can also share resources with accounts in a specific organization. So depending on the resource\ntype, you can share things with, for instance, the organization itself and organization units with\nall the accounts inside of it or specific accounts. And you can also share resources with\nidentities like users, roles. The pricing is interesting because effectively there is no\nadditional costs. You only pay for the services that you are actually sharing. So you are creating\nresources that you share and you're going to be paying for those resources. Now, what are some of the use cases?\n\n\nEoin: The most common one you're likely to come across with RAM is VPC subnet sharing.\nNow, before Resource Access Manager, VPCs in different accounts, if you wanted to share them,\nit generally meant using something like VPC peering or transit gateway. If you share subnets\nin a VPC with RAM accounts, then get access to implicit routing in that VPC. So if you have a\nVPC in the owner account, you share it with participants and participating accounts, then\nthey can use that subnet and it allows them to implicitly route within other resources already\nin that VPC. The IP address allocation then within the centrally managed subnets\ncan be managed by a network team. So that's one of the advantages of this shared services VPC model.\n\n\nAccounts with access to the shared resource then can access databases or VPC endpoints or other\nservers running in the subnets. So how does it work? Well, if you've got this shared services\naccount, you create your VPCs and subnets in it, then you share the subnets with other accounts.\nWithin an organization and participants can then see those shared subnets in their account.\nNow let's go back to the resource owner account. If they start an EC2 instance in the shared VPC,\nonly they can see it. You're not sharing all of the things in the subnet per se.\n\n\nParticipants can't make networking changes. So only the owner can make changes to the subnets,\nbut participants can provision resources they own into shared subnets. Like they can start\ntheir own EC2 instances in the account in the shared subnet. The participant will then own the\nEC2 instance, even though the shared owner account owns the subnet. I suppose it's worth saying that\nsharing subnets is very different solution to using VPC peering and Transit Gateway.\n\n\nAnd you might use them in combination actually, because sharing with RAM means that you have\naccess to an existing subnet. It doesn't really provide any additional routing. It just allows\nyou to launch resources into existing subnets. If you wanted to have routing into this shared\nservices VPC, you still need to do something like VPC peering or use Transit Gateway to route from\none VPC to another, just like you do between any two VPCs. So RAM is not a routing solution,\nand that's something to be aware of, but you can combine it with things like Transit Gateway and\npeering. So that's the most common use case, I think, VPC sharing. Apart from networking,\nthere are many other things you can share with RAM. AppSync APIs are one thing. There's code\nbuild projects. You can share capacity reservations, which is interesting for\noptimizing compute capacity costs. You can share FSX volumes. You can share GLUE catalogs, tables,\nand databases. So if you've got these catalogs in GLUE for access to data on S3, you can share it\nthis way rather than having to duplicate those resources in many accounts. If you have a private\ncertificate authority with the AWS Private CA, you can share that too. One of the things that\none of our colleagues, Connor, wrote an article about was using RAM to share Aurora databases.\n\n\nSo he has a really good blog post on that. We're going to link that in the show notes.\nAnother topic for where we wrote a blog was VPC lattice. And VPC lattice is a really cool topic.\nAnd if you haven't discovered it or haven't seen our previous episode, I'd recommend you check it\nout because it leverages RAM highly to share services and service networks. We have the episode\non that. We have a blog post, and we have a sample application code. And the links for all three will\nbe in the show notes. Transit gateways themselves are things you can share with RAM. And hot off the\npress, there is a new resource since last week only, I believe, which is sharing Systems Manager\nParameter Store parameters, otherwise known as SSM parameters. This is something which people have\nbeen very excited about to hear because previously, you would really have to replicate those\nparameters. And there wasn't any way to share them across account. Now, if you've got parameters,\nlet's say for custom AMI for an EC2 instance, and you want to share that across multiple accounts,\neverybody can discover the latest version of the AMI. You can now do that with parameter store.\nThere's a couple of gotchas to know with SSM parameter sharing. It only works for advanced\ntier ones. So those are, I'm afraid, the more expensive ones, about $0.05 a month each.\nIt doesn't work for the standard tier, which is the free option. So it definitely seems like AWS\nslipped up in allowing that standard tier to be free. And they're just trying to push all\nof the features into the advanced tier from now on as a correction measure.\nLuciano, those are some of the use cases. There are other services you can share with\nRAM. You can check out the full list in the link in the show notes. But let's say people\nwant to use it. How do you get started?\n\n\nLuciano: So there are a few steps that you need to follow. So first of all, you need to create a resource share in Resource Access Manager.\nAnd by default, every account you share with, so you share something with an account, that account\nreceives an invitation. And to avoid this, you can also enable sharing with the entire AWS\norganization in the AWS Management Console. Normally, owner accounts have full access control\nand the shared principles have limited subset of permission. So again, it's still the idea that\nyou are not giving up ownership. You are just allowing another account to see those resources\nand use them in different ways, depending on the type of resource. When you create a share,\nyou need to specify a few different things. First of all, the resource type, for instance,\nis it a subnet? Is it a blue catalog? Is it an SSM parameter or something else?\n\n\nThen you see the list of the resources of that type and you can pick the specific resources that you want to share\non that list. And then you need to define the permissions that the recipients will get on the\nresources you have selected. And this will be either AWS Managed Permission or for some resources,\nyou can even create your own custom managed permissions. Each resource type has different\noptions for sharing within RAM and the supported options vary depending on the resource type.\n\n\nOptions can be things like whether you can share with an account outside the organization or you\nare maybe limited only to the account inside the specific organization. Whether you can share with\nIAM users and roles, whether you can share with AWS Service Principles or whether you can use\ncustom managed permissions. So again, there is a guided process if you use the AWS Console and as\nyou go through, you will see different screens and different options depending on the resources that\nyou have selected. We will link to the documentation for more details just to get a better overview of\nwhat's possible, which resources can be shared and what options are available for every resource.\nBut just to give you an example, if you want to share a subnet, all of the options that we\nmentioned are not available for this particular resource. What you cannot do, for instance, you\ncannot share with accounts outside the organization. So if you're sharing a subnet, that's going to be\navailable only for the accounts inside your organization. You cannot share a subnet with\nspecific IAM users and roles, and you cannot share it with Service Principles. And you have to use\ndefault AWS Managed Permissions. So effectively, you can allow participants to launch instances\nwithin the subnet, create EAPs and things like that that are predefined in AWS.\nYou cannot really just create your own set of permissions and decide in a fine-grained way what people can do with that subnet.\n\n\nEoin: I guess you could, if you wanted to restrict that, use a Service Control Policy. One of the things we\ntalked about recently, if you wanted to narrow down those permissions, but I don't know what\nyour reason for that might be, maybe people can give some suggestions.\n\n\nLuciano: Yeah, that's a really good point. But I guess the next question would be, once you share something from the ownership side,\nso you are sharing from a specific account, what would that look like from the participant side?\nSo whoever is receiving access to that particular resource or set of resources.\n\n\nEoin: Yeah, so from the participant side, then if you go into RAM, you'll get a complete overview of all the shares and the\nresources shared within your account. So you can see what people have shared with you.\nAnd then when you go to see specific resources, like if you go into the VPC console, select a subnet, you will\nsee the owner account column in the table. And you can see that the owner account is a different\nidentifier than your account. And that's how you know it's an externally managed resource. If you\nshare subnets, you can also see the associated VPC in the VPC console, which is maybe a little\nbit unexpected, because you haven't explicitly shared the VPC itself. The VPC as a thing is not\na shareable resource with RAM. And participants can tag resources with separate tags to the shared\nresource, which is also probably unexpected, because you might expect that tags are an\nimplicit part of the resource. But tags are almost like a separate resource that's linked to the\nsubnet itself. So when you share a subnet, you don't share the tags, you can give us new tags in\nthe participant account. Another unexpected thing you might see when you start sharing subnets\nacross accounts is that you might share a subnet in us-east-1a, right, availability zone. And when\nyou look at it in the participant account, you might see that it's called us-east-1b. And you\nwill wonder what that's all about, and how they seem to have moved from a different data center 30 miles away.\n\n\nAnd the reason for that, you may know, but in case you don't, it's worth\nstating it. When you create a new AWS account, AWS randomly assigns these availability zone labels\nto you in a just in a randomized way. So everybody's us-east-1a may be different.\nAnd if you want to see if they're actually in the same AZ, there's another identifier called the AZ ID.\nAnd this has a format like EUW1-AZ2. And these are physical AZ identifiers, and they do not change\nper account. So the whole idea with randomizing them is they don't want everybody to just pick\nthe first AZ when they're launching something, and then have unbalanced load across their\navailability zones. So that's why to randomize them. If you are using availability zones names,\nthen remember, they don't necessarily line up from account to account.\n\n\nI think one thing you mentioned as well Luciano, which is worth highlighting again, is that sharing within an\norganization, if you want to make it easier, in fact, for some resources, you have to do this\nanyway. So it's probably worthwhile doing it, you just go into RAM in the management account,\nand enable sharing inside the organization. And then when you share resources, the acceptance is\nautomatic, the participating accounts don't have to accept an invitation. So I think RAM is pretty\npowerful. It's probably a lot less complex than using resource policies or assume role for a lot\nof different services. And it avoids you having to duplicate resources and change identity as you have to with assume role.\nAnd I think that's all we have for today on Resource Access Manager.\nLet us know what you think. If you have any other neat uses for it. Apart from that, don't forget\nto share the podcast or the YouTube channel with your friends and colleagues. So thanks very much\nfor listening again, and we'll catch you in the next episode.\n"
    },
    {
      "title": "117. What do EBS and a jellyfish have in common?",
      "url": "https://awsbites.com/117-what-do-ebs-and-a-jellyfish-have-in-common/",
      "publish_date": "2024-03-08T00:00:00.000Z",
      "abstract": "In this episode, we provide an overview of Amazon EBS, which stands for Elastic Block Storage. We explain what block storage is and how EBS provides highly available and high-performance storage volumes that can be attached to EC2 instances. We discuss the various EBS volume types, including GP3, GP2, provisioned IOPS, and HDD volumes, and explain how they differ in performance characteristics like IOPS and throughput. We go over important concepts like IOPS, throughput, and volume types so listeners can make informed decisions when provisioning EBS. We also cover EBS features like snapshots, encryption, direct API access, and ECS integration. Overall, this is a comprehensive guide to understanding EBS and choosing the right options based on your workload needs.\n\nAWS Bites is brought to you by fourTheorem, an AWS Partner that does CLOUD stuff really well. Go to fourtheorem.com to read about our case studies!\n\nIn this episode, we mentioned the following resources.\n\nEBS Official Documentation\nEBS Direct Access API\nEBS internal configuration is implemented as “millions of tiny databases” (paper)\nEBS Pricing examples\n\n",
      "transcript": "Luciano: If you have run an EC2 instance on AWS, you are very likely to have used EBS storage.\nStorage may be a dual topic, but if you scratch under the surface, EBS is actually a bit of a\ntechnical partner. Its design is even based on the Portuguese Man O' War, (not the metal band),\nbut a stinging jellyfish-like sea animal. We'll try and explain why and explain why EBS\nis an understated genius amongst AWS offering. This is AWS Bites and Luciano, joined today by Eoin.\n\n\nAWS Bites is brought to you by fourTheorem, an AWS partner that does cloud stuff in a really,\nreally good way. Go to fourtheorem.com to read about our case studies and feel free to reach out to us.\nSo if you use EC2, you're going to need some sort of storage, something like a disk drive or a\nremote storage, and you would use some kind of physical server that needs to store information,\nof course. Some instances support what they call instance store, which is something that is\nphysically attached to the instance itself, which is also really fast, but they are also ephemeral,\nwhich means it's temporary storage. So if your instance gets stopped, terminated or hibernated,\nall the data gets lost. So for these reasons, they are very good for cases where you need fast access\nbut not persistent data like caches or temporary storage. But for almost everything else, imagine\nyou have databases or files that you need to keep around, there is something that's called EBS,\nElastic Block Storage. So what is really EBS? Eoin, do you want to try to take a stab at it?\n\n\nEoin: It might be worthwhile starting with block storage. What is block storage? It's used by\nphysical disks as well as SANs, so storage area networks. And with a block store, data is stored\nin fixed sized blocks. That's how it gets its name. If you imagine old school traditional spinning\ndisks, these blocks related to physical sectors, and then with software or network-based storage,\ndividing a storage volume into blocks can actually be used to achieve resilience and performance if\nyou optimize how those blocks are read and written. So when you create a file system on top of a block\nstorage device, file reads and writes are translated to reads and writes of individual\nblocks. And the volume might also have some form of caching built in. So what about EBS then? Well,\nEBS is a kind of block store, hence the name, and it's a service that allows you to create\nstorage volumes in AWS that can be attached to EC2 instances. And now actually ECS containers. So\nwe'll talk a little bit about that later. The volumes themselves exist independently of the\nEC2 instances. So they can be detached and reattached to other instances. And they give you\na really good performance. They also give you different options so you can balance the cost and\nperformance factors. In terms of size, they can go from one gigabyte all the way up to 64 terabytes.\n\n\nAnd that's just for a single volume. So you can also have multiple volumes and you can even use\nthings like RAID to create arrays of volumes. You can also easily change volume types. So you've\ngot elastic options with EBS as well. You can change the type, the size, as well as other\nfeatures. Volumes can even be resized dynamically without any downtime on attached instances,\nwhich is pretty cool. Now they're encrypted. And one of the amazing things about them is that\nthey're encrypted without that encryption incurring any performance impact. Because of\ntheir design, they also have a reduced failure rate when compared to physical disk drives,\ntypically, which is something that is really an advantage when it comes to choosing EBS over\na traditional physical medium in a data center. And they're highly available within an AZ. And\nI think that's one of the important things to understand about EBS volumes. They are related to\navailability zone. Within that AZ, there is redundancy when it comes to the storage and\nfailure of components of the EBS volume, but they're still within one availability zone. So\nif you want to make them more resilient, you can do that by taking EBS snapshots. And these\nare backed by S3. So then they're stored in multiple AZs. Another point to mention is that\nwe're talking about EC2. EBS volumes are of course also used by RDS for storage. So when you're\ncreating databases in RDS, you will choose EBS volume types when you're creating them. It doesn't\napply to Aurora because Aurora has its own very special storage mechanism, which we might talk\nabout in another future episode. One of the neat features of EBS is snapshots. And we just\nreferred to them there, but it's worthwhile going into them in a bit more detail. What do you think?\n\n\nLuciano: Yes.\nSo snapshots are point in time backups, and that backup will contain the entirety of your EBS volume. And they will be stored in S3. So that means that by default, you just get the 11 nines\nof durability that S3 gives you. It needs to be monitored for cost because of course, if you have\nlarge volumes, you are effectively replicating all of that data into S3 and that comes with a cost.\nAnd if you are not careful, that cost is going to build up, right? But there is actually an\ninteresting feature there because these backups are incremental. So only the data that has changed\nfrom the previous snapshot is actually stored. So that can make them more efficient. And you still\nneed to be careful of course, but generally speaking, the incremental approach will make\nso that you are not replicating all the data all the time, but just the first snapshot and then\nthe changes layer on top of that. You can create new volumes starting from a snapshot.\n\n\nSo the idea is that basically you might snapshot a volume from a machine. From that snapshot, you can maybe\nbootstrap another machine. And other things that are relevant are that you can create copies of the\nsnapshot within or across regions. So maybe you can use this approach for instance to spin up\na new machine in another region. And you can also share snapshots with other accounts. So you can\neven use this approach to share data across different accounts, or even you can create\nsnapshots that are publicly accessible. So if you have information that somehow needs to be\nshared across pretty much everyone that might need that information, maybe it's like a large\ndata set that you want to make public, and people might need to spin up an EC2 and load that\ninformation easily into the EC2 instance, you can use this approach as well.\n\n\nSnapshots can also be archived, which is something that can be convenient when you don't expect to necessarily access that\nsnapshot frequently, maybe just for extra peace of mind. That will help you to reduce the cost.\nSo you just take the snapshot, archive it, and it's going to be there for 90 days or more, for example.\nSo apart from attaching an EBS volume, is there any other way to access the data? You might be\nwondering, okay, is that the only use case? There is actually a very interesting API that is something\nthat I came across only very recently. So it's a little bit of something that is not necessarily\nwell known to everybody, and it's what is called the direct API. We will have a link in the show\nnotes with all the documentation, some examples, but generally speaking, it's kind of a lower level\nAPI that allows you to access directly the EBS snapshot data, so without having to mount a volume\ninto an EC2 instance. And this is probably a bit of a niche use case that is used maybe if you are\na vendor that wants to provide some kind of backup, DR kind of extra facility, they can use this\napproach to be very efficient in the way they can access the data and read and write data directly\ninto EBS snapshot data. You don't necessarily have to create volumes. That's kind of the idea.\nSo if you have a use case where you want to control the data directly and you might do that at large\nvolumes, this might be an API worth exploring a little bit more. It will come at a lower price.\nThat's kind of the idea. We mentioned that EBS itself is a bit of a technical wonder in AWS. So\nwhat is this magic? How does it work under the hood?\n\n\nEoin: Well, I can't explain all the details because the details and if I did, I probably wouldn't understand them. But one of the interesting\nthings I suppose fundamentally is that it's not directly attached to physical storage.\nIt's accessed by instances over a network, but it appears to the operating system like a physical\ndisk. Now there is a paper, there is an academic paper published by AWS on how they evolve their\ninternal configuration management system. And the paper is called \"Millions of Tiny Databases\",\nwhich gives you a hint as to how that's architected. The idea is that in the\nconfiguration management system for EBS, it uses cells, which are full tolerant units that manage\na small portion of EBS data. And these little cells are replicated across the storage nodes\nto give durability and availability. And each cell is actually a seven node Paxos cluster.\n\n\nThis cellular database design is known as Physalia after the Latin name for the Portuguese\nMan o' War. So the Portuguese Man o' War is, it looks like a jellyfish, but it's not technically\na jellyfish because it's not a single animal, but a colony of millions of individual organisms.\nSo I guess the team thought that this was a good fit for the Physalia EBS architecture,\nwhich is built basically on a colony of individual distributed database cells. So it's\ndifficult to really wrap your head around the idea that this high performance storage system is\nactually backed by millions of tiny distributed databases that give you durability, high\navailability and consistency that you need. And if you have a look at that paper, like a lot of\ndistributed systems papers, it'll talk about the CAP theorem, which is the famous theorem that it\ndescribes in a distributed system, you can have two out of the three when it comes to consistency,\navailability, and partitioning. Now consistency and partitioning are non negotiable in the context\nof EBS, you have to have consistency when it comes to block storage. And the architecture basically\njust optimizes to achieve the maximum possible value of the third one, which is availability.\nSo it's they're just essentially achieving availability with a very high probability,\nwhich is the best trade off you can achieve for a system like this. Now you often hear AWS people\ntalking about the nitro system, which is their custom, very extensive hardware that underpins\na lot of the modern AWS infrastructure. And this the nitro system actually has specific\nfeatures built into it to facilitate EBS volumes. And to enable this encryption, we talked about\nwithout having any performance overhead, you might also have heard of EBS optimized easy to instances,\nbecause it's a network based file system, these instances actually have dedicated bandwidth,\nso they give you more guarantees around the bandwidth you have when accessing EBS. So that's\nas much I think, as I can say about the internals of EBS, feel free to check out the paper if that\nkind of thing floats your boat. When it comes to actually using them as an end user, though,\none of the things I find is that if you're not used to EBS, suddenly, all you want is a disk,\nbut you're confronted by all these different terms that seem very confusing. And there's a lot of\ndifferent options. And people talk about IOPS, and it's very easy to get lost and confused.\nSo should we try and do our best to explain these concepts at least at a high level without boring\neverybody to death?\n\n\nLuciano: I agree. And it's something that to be honest, I still get confused about every once in a while. So it's definitely good for me to also go over all of it again and try\nto finally memorize all of these different concepts. So we have at least three different\npieces of terminology that can be confusing and something that we need to know. And we are talking\nabout volume types, throughput, and IOPS. And those are really important, not just because you\nneed to make the choice, right? As you said, you just need a disk, but you don't just get to pick\na random disk, you need to decide based on these options. So definitely, you need to be aware.\n\n\nBut the other element is that it has a massive correlation with the cost. So cost is definitely\nbased on the different values that you pick when it comes to these options. So it's important to\nunderstand the meaning and the associated pricing so that you can avoid some kind of random bill\nshock just because you provision a disk and you didn't know what you picked. So let's start with\nIOPS. What is an IOPS? It basically means IO, input output per second. And it identifies the\nnumber of IO operations that you can perform every second. So what is an IO operation, you might ask?\n\n\nAnd this is basically either a read or a write of either 16 kilobytes, 64 kilobytes, or one megabyte,\ndepending on the EBS volume type. So you are effectively reading or writing a certain amount\nof data. That's an operation. If you have any meaningful production workload, it's generally\na good idea to really understand how IOPS can become kind of a bottleneck and make sure that\nyou fine tune the number of IOPS to make sure it matches your needs for that particular workload.\n\n\nJust always be aware of cost. Don't just push that to the maximum because you might end up\njust paying a lot of money for something that you don't really need to use. So instead, what are\nvolume types? This can be a little bit confusing because there are many different options, but\nlet's try to cover the broad categories and how to think about them so that you can choose somewhat\nconsciously. So you have one category of volume type, which is basically SSD, so solid states,\nlike, I don't know, the flash memory that you have in your phone, or probably if you have a very\nmodern laptop, they will have some kind of SSD inside. Then the other option is HDD, so the old\nschool spinning mechanical disks. It's another option and there are slightly different trade-offs\nin terms of performance and cost, but you can make this option. With SSD, you have general\npurpose SSD and provision IOPS types. So this is a kind of a subcategory when you pick SSD.\nThen if you go with HDD, you can pick either throughput optimized or called\nHDD. Now again, we might kind of deep dive a lot more, but let's just try to keep it high level.\nAnd we will put a link in the show notes about some pricing examples that are part of the\nofficial documentation when it comes to pricing to really understand how these different choices\nwill affect pricing. Again, the pricing model as sometimes happens with AWS is not very linear.\nDepending on the choices, it changes the formula. So I think the examples are a very good way to try\nto figure out some common use cases and how to think about pricing for those specific use cases.\nEoin, do you want to try to give us a little bit of a more complete overview on what the types are and how to think about them?\n\n\nEoin: Yeah, it's probably worth starting with the default option.\nSo I think the default option, if you don't listen to anything else in this section, it's probably worth listening\nto the fact that GP3, so the general purpose SSD type known as GP3 is probably the default option.\nAnd if you're not sure or you can't be bothered learning the rest of them, I would say go with\nthat one. This one is the one that measures IOPS with 64 kilobyte I O units. So when we talk about\nIOPS, the different types can have different I O unit size. So you need to think about two things,\nthe number of I O operations you might want to perform per second, but also the throughput.\n\n\nAnd these things are two different dimensions. They're slightly related, but you need to think\nabout them separately. Now the GP3 is relatively new and it gives you a decent balance between cost\nand the ability to control IOPS throughput and size independently. So there's a lot of flexibility\nthere too. That's why I think it's a good default choice. The cost is generally good as well\ncompared to the other options. And by default, the baseline is that you get 3000 IOPS and 125\nmegabytes a second of throughput. So this is different to the older type, which we'll talk\nabout in a second, but the fact that you get that baseline, which is pretty good in its own right,\nbut then if you need more, you can adjust those levers. That's nice to know. And it's reassuring,\nI think the GP2 one was the default for a long time. It uses smaller 16K I/O units,\nbut the baseline is very variable. It depends on your volume size. So they give you three IOPS per\ngigabyte. So it makes it a little bit more difficult to calculate, but the unique thing\nabout GP2 is that it also gave you first of all IOPS so that it was pretty good.\n\n\nIf you didn't know what you were doing, you had something that generally didn't need that much IO, but from time\nto time, like even on boot, it would need a burst of IO or for variable workloads. But then again,\nyou could also exhaust that first of all limits and you needed to be careful to watch out and\nmonitor for that. I think it's still a good side issue there just to recommend to monitor your IO\nconsumption against your allocation for any type. So GP3 is generally cheaper on the order of 20%\nin most cases. So that's why we'd say if you're going for general purpose SSD, go with GP3 and\nyou should be pretty safe with that option. And there's a calculator to indicate what savings\nyou'll get. And we'll give a link to that in the show notes. You can by the way, upgrade GP2 volumes\nto GP3 and you don't need to restart the attached instances, which is pretty cool.\n\n\nNow, if you're a performance hungry workload, then you can go for the provisioned IOPS. So you can imagine that\nyou've got something like you're running a database. That's the typical example that you\nget, or you've got something else that's just really read and write heavy in terms of IO. Then\nyou've got a few options there. They used to be IO1 and IO2. IO1 would give you up to 64,000 IOPS,\nwhich was pretty good. IO2 was available up until the end of last year. And it's actually been\nreplaced now by a new one called IO2 Block Express. IO2 is essentially legacy. Now IO2\nBlock Express has a whole bunch of additional features, but basically compared to IO1, it\nallows you to get up to as many as 256,000 IOPS. Now that'll depend on the instance side, because\nthat needs specific instance characteristics to support it, but it'll give you four times\nhigher throughput than IO1 as well. Other benefits of the IO2 Block Express are higher durability.\n\n\nSo you get five nines instead of three nines, and you get lower latency as well. Again,\nGP3 is probably your go-to. If you know you need more IO, you can always move to one of the\nprovisioned IOPS options. Worth quickly talking about the HDD types, but these are more niche\nthese days. Generally much, much slower. The IO size is one megabyte, so it's completely different\nphysical architecture. If you need sequential access, people who have used hard disks and SSD\ndisks and physical machines in the past may know that SSD is good for random access, because you\ncan read at a consistent speed anywhere on the device, but physical disks that have a robotic arm\nneed to actually move to the location you're reading from. So they're always much better for\nlinear sequential access. So that can be workloads like big data, things like MapReduce, Kafka,\nwhere you've got streams that are sequential and logs. You're only going to get 500 megabytes max\nthroughput, even on throughput optimized ones, and 500 IOPS. Now those are one megabyte IOPS,\nbut you won't use it for a lot of individual read or write operations. Cold HDD, the last one we\nhave to mention, it's really slow, 250 megabytes a second, but that's something you'd only use for\ncold storage. So things you aren't going to read and write option. And that's the cheapest of them\nall. Luciano, what do you think? How would you decide?\n\n\nLuciano: Yeah, I think I can give a very high level decision framework. So a bit of a shortcut that doesn't really take into account maybe all the\nintricacies of different workloads, but it might be just a good reference framework if you either\ndon't want to spend too much time investigating all the possible options, or maybe as a sanity\ncheck just to make sure that your investigation makes sense at the high level. So the idea is that\nif the throughput is more important than IOPS, you should probably go for an SD1 HDD type.\n\n\nIf it's a really cold storage that you are looking for, then SC1 is the one to go for. Instead,\nif IOPS is more important, and this is in my experience, in most cases, that's what it is,\nright? You care more about IOPS rather than throughput. In that case, you need an SSD.\nAnd if you really need the maximum IO performance possible, you need to go for I02 Block Express.\nOtherwise, choose the best all around, which is GP3. And as we said, it's generally the safest\ndefault these days. So again, maybe your decision tree starts with GP3, and then you try to look for\nreasons not to use GP3, and you can look at the other points we mentioned as a way to steer your\ndecision. Now, I think there are also some other interesting features that we should quickly\nmention before we wrap up. The first one is that with EBS, you can have multi-attachment.\n\n\nThat basically means that when you create a volume, that single volume can be attached to\nmultiple EC2 instances. And there is a limit, of course, and the limit is 16 EC2 instances,\nwhich basically means that you can share data in a volume across 16 different machines.\nOnly works with provision IOPS though, so you basically are forced to use IO1 or IO2 if you\nwant to use this particular feature. And of course, it's something that you need to use\ncarefully because you know that reading and writing from different machines in the same disk\nmight create consistency problems. So one way to avoid those consistency problems is to use a file\nsystem that is designed for this particular use case, like a cluster or file system. So don't just\ngo for the standard Linux one like ext or XFS because these ones might not guarantee you any\nconsistency. There are of course other options if you want to do something like this. Probably you\nare thinking about NFS or EFS on AWS or maybe FSX, and these are designed for sharing data across\nmany devices. So probably these are more scalable options. Generally speaking, they might be more\nsuitable for this kind of use case, but we thought it was worth mentioning the idea of multi-attachment.\nMaybe for specific use cases, you maybe have a cluster of a few machines,\nit might be an easy way to just be able to share information across these machines.\nAnd the last thing that I want to mention is that there is ECS support, which basically means that you can configure\nyour tasks or services to create EBS volumes when they are launched or deployed, but you cannot\nattach an existing EBS volume. The thing that you could do is that you could do a snapshot and then\nyou can create an EBS volume from a snapshot. So again, in the use case where you might have\ndatasets that you have created before and you want to be able to consume those datasets from\nan ECS cluster, this could be one way of giving your cluster access to that data.\nAnd of course, this works in Fargate too, so something to be aware. And this is everything we have on this\nJellyfish EBS episode, so hopefully you enjoyed all of that and you find it valuable. But of course,\ndon't forget that if you don't clean up your old volumes as snapshots, you are going to get stung!\nSo thank you very much for being with us and we'll see you in the next episode.\n"
    },
    {
      "title": "118. The landing zone: Managing multiple AWS accounts",
      "url": "https://awsbites.com/118-the-landing-zone-managing-multiple-aws-accounts/",
      "publish_date": "2024-03-15T00:00:00.000Z",
      "abstract": "In this episode, we provide an introductory overview of AWS's best practices for managing infrastructure using multiple accounts under an organization. We discuss the advantages of this approach and how to get started creating your own multi-account environment, or &quot;landing zone&quot;.\n\nAWS Bites is brought to you by fourTheorem, an AWS Partner that does CLOUD stuff well, including helping you set up your AWS organisation! If that’s something you are looking for, go to fourtheorem.com to read more about us and to get in touch!\n\nIn this episode, we mentioned the following resources.\n\nAWS Definition of Landing Zone\nSeries of articles &quot;Managing AWS accounts like a PRO&quot;\nAWS Organizations service (docs)\nIAM Identity Center service (docs)\nControl Tower (docs)\norg-formation\nOur previous episode &quot;AWS Governance and landing zone with Control Tower, OrgFormation and Terraform&quot;\ngranted.dev\nAWS SSO util\nLeapp\nCloud Glance\n\n",
      "transcript": "Eoin: Today, we're diving into a fundamental practice for managing your AWS infrastructure using\nmultiple accounts.\nWhether you're a seasoned cloud expert or you're just dipping your toes into AWS waters,\nthis best practice can really make a difference to the way you handle your business applications\nand your data.\nAnd if you've ever explored the AWS Well-Architected Framework, you might have come across mentions\nof the benefits of multiple accounts from enhancing operational excellence to bolstering\nsecurity and cost optimization.\n\n\nAt fourTheorem, with our work, we've seen firsthand how using this approach can really make a\nhuge difference in organizations' cloud strategies, regardless of your cloud maturity level.\nBut how do you actually implement this strategy without getting overwhelmed?\nWell, that's where we come in.\nAnd in this episode, we're hoping to give you an introduction on everything you need\nto know when it comes to managing AWS organizations and accounts.\n\n\nAnd we'll also share a number of AWS services and tools that you can use to make your own\nlife easy and do all of this management using infrastructure as code.\nWelcome to another episode of AWS Bites with myself, Eoin and Luciano.\nAWS Bites is brought to you by fourTheorem, an AWS partner that does lots of cloud stuff\nreally well, including helping you to set up your AWS organization.\nIf that's something you're looking for, go to fourtheorem.com to read more about us and to\nget in touch.\n\n\nUsing multiple accounts under an organization, no matter what size you are, it's often referred\nto as a landing zone.\nAWS uses this term landing zone quite a lot, and they've got a definition which says that\nlanding zone is a well-architected multi-account AWS environment that's a starting point from\nwhich you can deploy workloads and applications.\nAnd it provides you with a baseline to get started with multi-account architecture, including\nidentity and access management, governance, data security, network design, and logging.\nThere's a link to that full verbatim description in the show notes.\nBut what are the advantages of having multiple AWS accounts?\n\n\nLuciano: There are several advantages.\nThe first one that I would like to start with is isolation.\nAnd I mean isolation of resources because every single AWS account operates independently\nand it provides effectively a boundary for resources.\nSo this kind of isolation allows you to get better security compliance and also the way\nyou access these resources, you have effectively more controls around who is allowed to do\nthat and what kind of access level they have.\n\n\nDifferent teams in fact can have their own accounts.\nFor instance, if you organize your teams by business domains, you could create accounts\nin AWS that represent those business domains and that reduce the risk of interference.\nFor instance, different teams might not be worried about, for instance, exhausting each\nother quotas.\nMaybe, I don't know, the usual example that we make is the number of Lambdas that they\ncan run at a given point in time.\n\n\nDifferent teams will have their own quota to manage and they're not going to be conflicting\nwith each other, which is a great thing to have when you're thinking about running lots\nof workloads at scale.\nAnd that is also important in terms of security standards and it can make easy to comply with\nregulations such as GDPR, HIPAA, or PCI and things like that.\nBecause since you have more isolation, it's easier to prove that you are basically not\nleaking data and you are complying with all the standards that this kind of regulation\nwill impose on you.\n\n\nAnother similar aspect is what we generally call environment segregation.\nThis is more on the idea that even within the same domain, you might still want to provision\nmultiple accounts because you want to use different accounts for different environments.\nFor instance, if you have, let's say, the usual e-commerce example, you might have a\ndomain that is called, I don't know, maybe fulfillment.\nIn that domain, you deploy specific workloads.\n\n\nYou might want to use different accounts, one for instance, for development, one for\ntesting, one for production.\nAnd this is really useful because you are effectively minimizing the risk of disruption\nto those systems while you are experimenting, for instance, because you want to build a\nnew feature or maybe you want to try maybe a different database.\nYou can do all these things in environments that are dedicated to testing and building\nbefore you actually move all this stuff into production where you might create disruption\nif you didn't have a chance to test things properly.\n\n\nAnd this approach also allows you to separate testing data or development data from the\nactual production data, which is another great thing, for instance, when you have applications\nthat are managing customer sensitive data.\nIn that case, you probably don't want to give access to that production data with customer\nsensitive data to the other environments where you are effectively just testing and lots\nof developers are going to need to access that data to create all sorts of different\ntest cases.\n\n\nAnother advantage is short-lived developer credentials.\nThis comes because when you have multiple accounts and you are using IAM identity center\nand AWS organizations, effectively you have to pick a specific account and a specific\nrole to work with every time you need to do something programmatically.\nAnd this is very convenient because you can prepare specific roles for specific accounts.\nFor instance, you might have a role that only allows you to read logs from S3 and it's a\nread only type of role.\n\n\nAnd you are going to assume that specific role when you need to read S3 logs.\nAnd that's basically limiting the plus radius.\nIf accidentally you do something else, maybe because you are writing a script and try to\nautomate something, you are not going to risk that maybe you can delete files or maybe you\ncan access a database that you're not allowed to access to.\nAnd of course, you can take that example and use it for other use cases.\n\n\nMaybe I don't know, not just logs, but you can think about managing budget.\nYou can think about accessing databases.\nYou can think about security roles where you can go and check maybe, I don't know, guard\nduty or other things like that.\nSo all of this stuff is very powerful.\nAnd when it comes to programmatic credentials, you basically don't use anymore the long-lived\ncredentials because you can use tools that will give you temporary credentials only to\naccess the specific role that you need to access to for one hour, two hours, four hours,\nor something like that.\n\n\nIt's not going to be long lasting credentials that are going to live in your machines and\nmight be easy to steal and abuse for other things in the future.\nAnd finally, another point is cost management because by separating resources into different\naccounts, you have a natural bucket, which is that account in terms of collecting and\naggregating all the costs for the meaningful purpose of that specific account.\n\n\nSo of course you can use tags, but you know that with tags you need to be very diligent.\nAnd if you want to aggregate cost by tag, you might be forgetting about tagging certain\nresources or you might be using different tags and they don't necessarily match all\nthe time.\nFrom the account perspective, it's much easier to just say, give me all the cost in this\naccount.\nAnd you can also easily set up budgets and alarms to say, for this particular account,\nmaybe it's a development account.\n\n\nI don't expect to be spending more than $200 a month maybe.\nAnd if you go higher than that, just send me an alarm.\nSo you can easily create these kind of utilities that will tell you whether you are spending\nmore than you are supposed to be spending on a specific account.\nSo I guess the final point is that doing this structure, creating effectively a landing\nzone with all sorts of different accounts can be a little bit challenging.\n\n\nAnd hopefully after this episode, it's going to feel a little bit less scary, but there\nare lots of benefits that are really, really important.\nSo I will say that don't be scared, just get on board with it.\nAnd the sooner you do all of this, the better it's going to be.\nAnd I guess that leads me to another question, because maybe you are not a big company with\nthousands of employees.\nYou don't necessarily need hundreds of accounts.\nYou don't have multiple workloads.\nMaybe you are just a solo developer.\nMaybe you are a student.\nMaybe you are just learning about AWS.\nIs it worth for you to go through all this journey and learn all these different things?\nOr you can just be better off with long-lived credentials, one account, and just keep to\ndoing things in the standard way, so to speak.\n\n\nEoin: This is probably a matter of opinion, but I think we believe that everyone using\nAWS should invest a little bit of time learning how to set up the organization and effectively\nmanaging multiple accounts.\nAnd one of the main messages I'd say is that it's not that complicated.\nIt doesn't take a huge amount of time to do it.\nAnd it is worth the investment because everybody is generally moving towards multiple accounts\nanyway.\n\n\nSo if you're not exploring a little bit how that all works, then you might be left behind\na little bit and you might start getting a little bit of FOMO.\nSo why would you do it if you're a solo developer or just experimenting in AWS?\nWhy would you bother setting up accounts?\nWell, you're going to learn, right?\nSo I'd say for learning purposes, even if you're hosting some side projects, there's\nlots of benefits for you.\n\n\nWe already mentioned some of the benefits you went through with Luciana.\nAnd they're even valid when you're running small workloads on AWS.\nAnd it helps you just to be a bit more organized when you're deploying lots of different hobby\nprojects or your own blog or whatever it is to AWS.\nBut if at some point you decide to invest a little bit more on AWS and maybe you're\ndoing freelancing or you're working for a company doing a lot on AWS, having your own\nmulti-account environment is definitely going to help you there.\n\n\nSo maybe if one of your side projects starts to pick up and you decide to turn it into\na commercial project, it could be the start of a new SaaS service.\nIf you've put a little bit of manners around your landing zone to begin with, then that's\ncertainly going to help you.\nOr if you want someone to give you a help at some point, you need to hire somebody else\nto work on the code with you or get some more contributions.\n\n\nThen it's easier to give them access to your organization if you've got a little bit of\nsegregation between all of the stuff you've deployed in there.\nMaybe eventually you want to grow your efforts into a proper company and you have multiple\nteams and multiple departments and they want to interact with an AWS organization.\nOr maybe eventually you'll sell the whole lot for billions and you'll be a startup unicorn.\n\n\nI don't know if you care, but if you were to do that and if you were to retire and go\nand live on a Caribbean island, you might want to know that what you've left behind\nis somewhat well organized for the people who have to keep maintaining it.\nSo I suppose the point here is that investing in setting up the landing zone, it's just\nlike creating a great foundation.\nAnd it doesn't matter if you want to start small.\nIt really doesn't take very long.\nYou can turn on organizations and start creating new accounts very quickly and then start adding\nsome of the more advanced governance and compliance type stuff later on.\nAnd it really just helps you to grow your AWS presence and get off to a good start.\nAnd you get all those benefits like temporary credentials and sign on with your Google account\nor whatever identity provider you want to use, even the built-in one.\n\n\nLuciano: Yeah.\nIt's worth reminding that an account at the end of the day is just a container for resources\nwithin AWS.\nAnd another interesting detail that I think you will start to realize very quickly when\nyou create your second account is that every single account is uniquely tied to an email\naddress.\nSo you might need to start thinking also how to get aliases for all the different accounts\nso that you are effectively managing all of them under one email.\n\n\nOtherwise, it might get a little bit messy.\nSo that's another concern.\nAnd if you use things like Gmail, it's easy to generate aliases.\nYou can use, for instance, the plus syntax to just say, I don't know, Luciano plus production\nat gmail.com, something like that.\nSo this is just a practical tip, something you will start to face very quickly when you\ncreate multiple accounts.\nBut the other question you might have is, okay, but how many accounts do I need to create?\n\n\nAnd what is a way to organize them that kind of makes sense?\nBecause initially you might just be lost in trying to think, okay, right now I've been\nusing one account for everything.\nMaybe I can see a future where I might need a second account for another workload, but\nis there any best practice on how to organize accounts in a more scalable way that allows\nme to grow even 10 years down the line?\nYou have a structure that can fulfill what you need to do.\n\n\nAnd really there isn't one way to do that.\nAWS gives you a bunch of different tools so that you can actually put a structure in place.\nAnd I think it's worth mentioning a little bit more what is the naming that AWS gives\nyou to the different entities that you can use to organize effectively your organization\naccounts.\nSo generally there is a root entity, which is kind of a high level container for your\naccounts.\n\n\nAnd initially when you open your first account, you effectively have this root and only your\nown first account, which is generally referred to as the management account, because it's\nthe one that will allow you to control all the other accounts.\nAnd then you can create additional accounts directly in the root level.\nSo they will all be affected at the same level of depth.\nImagine maybe a file system can help you out.\n\n\nIt's like you have only one folder and then you have a bunch of files into that folder.\nBut you can also decide to create sub levels.\nAnd the tool that AWS gives you is called organizational unit or OU.\nAnd the idea is that effectively that's a group that you can create and inside the group\nyou can create additional accounts and you can even nest additional organizational units.\nSo again, in the analogy of file system, it's like you can nest multiple folders and\neventually you will have files into this kind of structure, hierarchical structure.\n\n\nSo at this point, you know that you can organize all your accounts under groups and create\nthis kind of logical hierarchy where somehow you try to divide things in a way that makes\nsense.\nBut what kind of groups would you want to create?\nAnd again, this is totally up to you, but there are some sensible common practices.\nFor instance, it's very common to go with a security OU and this is going to be a group\nthat contains all the accounts where you might want to deploy security related services,\nfor instance, GuardDuty.\n\n\nThen very common, you would have a sandbox OU and this is where you do all your experiments.\nMaybe if you want to try a new service, deploy like something random quickly just to have\na feeling for what that new service looks like, you will have accounts like for doing\nworkshops or to just do experimentation.\nAnd you might even use automation there to just delete every single resource in those\naccounts, maybe every weekend or at the end of the day, just because these are very disposable\naccounts where you just do stuff for trying things around.\n\n\nYou don't want to for things you deploy there to stick longer.\nThen generally you would have some kind of workloads, organizational unit.\nAnd this is probably one of the most important ones or the ones that you will be using the\nmost because this is where you deploy your workloads.\nAnd you might want to organize accounts there by environment as we suggested before.\nSo for instance, you might even have multiple levels.\n\n\nFor instance, inside workloads, you might have another set of OUs to divide your different\nbusiness domains.\nAnd then inside every OU for the business domain, you might have multiple accounts for\ndevelopment, testing, pre-production, production, QA, whatever makes sense to you in terms of\nenvironments.\nAnd then you might have other utility OUs or account.\nFor instance, I've seen commonly organization will want to have a place where they can test\ndifferent policies that maybe they want to put in place.\n\n\nSo it could be common to have either an account called policy testing account, or maybe an\norganization unit where you might deploy multiple accounts.\nOr you might also have a suspended OU where you might move things that you want to delete\nor that you are suspending for whatever reason makes sense in your organization.\nAnd this is generally useful because when you want to delete an account, you cannot\ndelete it straight away.\nThere is a period of time where the account is going to stay there pending deletion.\nSo it might make sense just to move it somewhere else where you can easily control access and\nmake sure that people are not going to go there and create new resources as you are\nin that process of deletion.\nSo now that I think you should have a little bit of a better idea of what are the capabilities\nand what are some common ways of organizing the accounts, where can we get started?\nWhat is the next thing that people should be doing?\n\n\nEoin: Well thankfully, we have your excellent step-by-step guide you wrote on the fourTheorem blog\nLuciano that shows you exactly how to set up IAM identity center and AWS organizations\nand use those to create your perfectly crafted landing zone.\nSo we don't have to repeat all of those steps here, but we'll link the article in the show\nnotes for you.\nWhat might be more interesting is just to give you a quick overview on the different\nparts of it.\n\n\nSo the different services we used in that article and explain why they exist.\nNow we've already talked about AWS organizations.\nThat's your account management service that really enables you to consolidate multiple\nAWS accounts together.\nAnd it gives you that account management, consolidated billing, which is very important\nif you want to avoid getting hundreds of invoices from AWS every month.\nAnd as an administrator of an organization, you can create accounts in it, invite other\naccounts into the organization, and you can also do various things with other services\nthat give you budgetary security and compliance needs as well.\n\n\nSo other services are becoming more and more integrated into AWS organizations as we push\nmore towards this model of doing things.\nAnd the next really important one and very interesting one, if you're just setting it\nup for your own personal account is IAM Identity Center.\nIt's pretty much distinct from IAM and it used to be just called AWS Single Sign-On.\nSo we're just still getting used to the new name.\n\n\nAnd this is for really managing human user access to AWS resources.\nAnd it's a place where you can assign users, create users, link users to an external identity\nprovider and manage the level of access.\nSo you can configure an IDP, which will allow you to log on with one of your existing login\nproviders to an AWS organization.\nAnd then you can link your users to the roles and the permissions they will get with individual\nAWS accounts or groups of AWS accounts.\n\n\nNow the article we mentioned shows in a lot of detail what all the steps are that you\ncan follow to go from zero to a fully functional landing zone with multiple accounts and user\nroles.\nWe'd really recommend trying that out.\nAnd if you don't have an external identity provider for your own personal account, you\ncan use the built-in one, which is nice.\nYou don't have to use Google Workspaces or Azure AD.\n\n\nThere's a built-in one which allows you to get up and running pretty quickly.\nAnd then you've got nice enterprise level temporary credentials and all of that good stuff.\nNow moving on to Control Tower, it is available in the management console.\nAnd the idea of it is that it gives you a way to set up and govern that landing zone.\nIt's AWS's solution to creating a landing zone really.\nIt is the easiest way from an AWS native perspective to get up and running.\n\n\nWe talked about the pros and cons of this versus alternatives in one of our previous\npodcasts which we'll be talking about again and link in the show notes.\nBut it's really a Control Tower is a higher level AWS service that allows you to set up\nthat landing zone and then provision more accounts and provision resources in those\naccounts.\nControl Tower is generally used as a AWS management console feature.\nIt's good to get familiar with the concepts.\nThose of us who like infrastructure as code don't necessarily think it's the best solution\nfor long-term management and we prefer to use infrastructure as code, utils.\nControl Tower does integrate with some infrastructure as code tools, but it's really management\nconsole first.\nSo Luciano, what options do we have around infrastructure as code?\n\n\nLuciano: Yes.\nThe main tool that we use most of the time and we are seeing most of the community referring\nto is called OrgFormation.\nAnd this is an open source tool developed by the community.\nSo it's not an official AWS tool service or solution.\nIt's an open source project maintained by the community.\nAnd it's a really interesting project because it allows you to effectively fill some of\nthe missing features of CloudFormation.\n\n\nBecause if you would want to use CloudFormation to manage all these things with infrastructure\nas code, you very quickly will bump into the problem that when you deploy a stack with\nCloudFormation is effectively tied to one specific account.\nSo you will need to write all your orchestration to switch between accounts, use different\ncredentials just to try to deploy different resources.\nSo OrgFormation tries to backfill for that kind of missing feature and it does a lot\nmore.\n\n\nThere is actually on the repository a very interesting PDF file that you can download\nwith 50 different examples showcasing the capabilities of OrgFormation.\nSo definitely recommended to look at that just to have a feel for what else can it do\nmore on top of CloudFormation.\nBut at the end of the day, still using CloudFormation behind the scenes to deploy resources for\nyou on the different accounts.\n\n\nSo the idea is that once you set up OrgFormation, you can apply changes automatically and reduce\nmanual work and inconsistency and mistakes.\nSo you're not going to go to your management account, log in in the web console, and then\nstart to click around to change things.\nBut rather you'll be maintaining YAML files that describe everything that represents the\ncurrent state of your landing zone, all your accounts and permissions, resources, policies,\neverything that you want to manage in a more centralized way with Infrastructure as Com.\n\n\nAnd just to give you some example, what you could do, for instance, is just in your YAML,\nand by the way, you have a command at the very beginning that allows you to import the\nshape of your organization unit and accounts.\nSo the first command that you generally would run is that one, and then you have a good\nstarting point.\nYou don't have to rewrite everything from scratch the first time that you decide to\nuse OrgFormation.\n\n\nBut once you have that YAML file, maybe you would want to create a new organization unit\nand a new account under it.\nThat generally means changing a few lines of YAML, adding maybe a few extra things in\nthere, and then you can just apply that template and it's going to deploy everything for you,\nmaking sure to apply the diff correctly and deploy only the changes that you actually\nhave applied to your template.\n\n\nAnd you can also create things like SCPs.\nBy the way, we have another episode dedicated to those if you're curious to learn more about\nSCPs, so we'll add the link to that episode as well.\nYou can also create arbitrary resources and provision them under various accounts.\nMaybe you would want to have an event bridge in every single account because you might\nwant to use that to communicate across accounts.\n\n\nYou can easily do something like that with OrgFormation.\nAnd another cool thing, and that is actually a very good example in OrgFormation, in the\nset of example provided by OrgFormation itself, is that you can add tags to every single account.\nAnd for instance, one thing that you can do with that is use a specific tag that represents\nwhat is the budget threshold for that specific account.\nAnd then you can easily provision alarms that will notify you if you are going over that\nthreshold in specific accounts.\n\n\nSo it makes it also very easy to control costs and you can just follow the example and provision\nthat kind of stuff very, very quickly.\nNow is this the only tool?\nOf course not.\nThere are other tools that you can use.\nIn our previous episode that you mentioned, or in episode 96, we made a little bit of\na comparison between OrgFormation, Terraform and Control Tower.\nSo Terraform is another contender and it is generally a really good one because it\ngives you a really good diff, for instance, of what is going to change before you apply\nthe changes, which is definitely something you want to do, especially for bigger organizations.\n\n\nBut the one problem that Terraform has is that it might be a little bit tricky to write\nvery abstract code just because the way that Terraform manages different accounts is by\nconfiguring different AWS providers.\nAnd when you provision a resource, you always need to prefix that resource with the provider\nname.\nIt's not something that you can easily generalize.\nSo it might be a little bit difficult sometimes to create very reusable modules that you might\nprovision into different accounts and you might end up doing a little bit of copy paste.\n\n\nNow maybe there is a solution for that, but I'm not entirely aware of that kind of solution.\nMaybe something that Terraform can make it easier in future releases.\nBut as it stands today, that's one of the complexities that people bump into.\nStill, it's a good tool if you're used to use Terraform.\nProbably it's not the end of the world to do a little bit of copy paste because most\nlikely you're not going to change the structure of your organization every single day.\n\n\nBut just be aware that there are trade offs there.\nNow I guess if you want to know a little bit more about all these details and differences,\nagain, we will put a link in the show notes to our previous episode because that one has\na little bit more depth when it comes to describing the different tools and what are the different\nstrengths and weaknesses.\nNow I think one last thing that we might want to cover is a little bit more detail on programmatic\naccess.\nYou mentioned that it is something that you will get some advantages in the way you can\nmanage programmatic access, but should we share in practice maybe some of the tools\nthat can help with all of that?\n\n\nEoin: Yeah, if you're moving to managing multiple accounts, then the first question is how do\nI manage multiple credentials?\nHow do I know which account I'm using when I set up credentials?\nWhat happens if you've got multiple AWS organizations and then you're managing multiple AWS accounts\nin multiple organizations?\nIt can become a bit of a minefield.\nSo there are plenty of tools, luckily, now that make it easier to manage all of that\nstuff.\n\n\nSo let's say, for example, you just want to play around and deploy something in a sandbox\naccount.\nHow do you get programmatic credentials with a CLI for that specific account?\nThere's a few tools that can be used and a lot of these tools, as well as offering programmatic\naccess, will allow you to launch into the AWS console with the right credentials for\nthe account you want to use as well.\nAnd one of our favorites is granted.dev.\n\n\nLuciano's article we mentioned at the beginning, we show how to set that up.\nSo that's all covered there and that's a really nice one.\nThere's other alternatives like AWS SSO util by Ben Kehoe, which we've used quite a lot\nin the past.\nThere's another couple which I haven't used personally, but look pretty good.\nOne is Leapp by Noovalari (and that's Leapp with two Ps.)\nAgain, there'll be a link in the show notes and Cloud Glance.\n\n\nAnd Leapp and Cloud Glance will also offer you a visual UI.\nSo there might be great alternatives if you're looking for more of a visual approach rather\nthan a terminal approach to managing the craziness of multiple AWS accounts in multiple organizations.\nIf you want to check any of those out, the links will be in the show notes and you won't\nregret it.\nIf you haven't used any of those tools, I don't think you'll regret adopting them.\n\n\nI think at this point we can just summarize and wrap up.\nI think today we highlighted really what the importance of setting up landing a zone and\nusing multiple accounts in the first place for different business applications and data.\nIt's something you should be doing regardless of the size of your team or the level of cloud\nmaturity.\nWe also mentioned the advantages, isolation, environment segregation, short-lived credentials,\ncost management and security.\nWe also give you a few examples of how an organization structure could look and which\ntools and AWS services you can use to create them.\nThen we finally talked about some of the tools you can use to simplify short-lived programmatic\ncredentials and access to the console.\nDid we miss anything important?\nDo you have any additional cool setup in your landing zone or have any additional tips to\nshare?\nOr if you haven't even done it yet, what's holding you back?\nLet us know in the comments below and reach out to us on our social channels because we'd\nlove to know what you think.\nUntil the next time, goodbye.\n"
    },
    {
      "title": "119. The state of AWS 2024 (AnsWeRS community survey commentary)",
      "url": "https://awsbites.com/119-the-state-of-aws-2024-answers-community-survey-commentary/",
      "publish_date": "2024-03-22T00:00:00.000Z",
      "abstract": "In this episode, we provide commentary and analysis on the 2024 AWS Community Survey results. We go through the key findings for each area including infrastructure as code, CI/CD, serverless, containers, NoSQL databases, event services, and AI/ML. While recognizing potential biases, we aim to extract insights from the data and share our perspectives based on experience. Overall, we see increased adoption across many services, though some pain points remain around developer experience. We hope this format provides value to listeners interested in cloud technology trends.\n\nAWS Bites is brought to you by fourTheorem, an AWS Partner that does CLOUD stuff well, including helping you set up your AWS organisation! If that’s something you are looking for, go to fourtheorem.com to read more about us and to get in touch!\n\nIn this episode, we mentioned the following resources.\n\nThe 2024 Answers for AWS Survey results\n&quot;GitHub Actions Feels Bad&quot; by fasterthanlime (video)\nDoing serverless with Terraform\nOur event services series (YouTube playlist)\nOur previous episode about machine learning and SageMaker &quot;How to automate transcripts with Amazon Transcribe and OpenAI Whisper&quot;\n\n",
      "transcript": "Luciano: What do people think about AWS when it comes to topics such as infrastructure as code, CI-CD,\nserverless containers, NoSQL databases, event-driven architectures, AI and machine learning?\nThese are some of the topics covered by an AWS yearly survey called AnsWerS, which ran from\nJanuary 16 to February 16 this year. The results are finally available on a website called answersforaws.com/2024\nand today we want to go over this data and provide some additional opinions.\n\n\nMy name is Luciano and I'm joined by Eoin for another episode of AWS Bites podcast.\nAWS Bites is brought to you by fourTheorem, an AWS partner that does cloud stuff really,\nreally well. If you're curious, check us out at fourtheorem.com.\nSo this survey is run by Peter Sankauskas, I hope I'm pronouncing the surname correctly,\nand it's also built together with the AWS community, so it's kind of a community\nendeavor. And Peter is an hero since 2014, a community hero, so he's trying to kind of get\na sense for what people think about AWS, what are areas for improvement, what people do when it\ncomes to different topics such as we mentioned infrastructure as code, CI-CD, serverless\ncontainers. So we are going to have our screen share here, but if you're following on the audio\nonly podcast, we'll try to describe of course all the data as best as we can, but if you want to see\nthe full video, the video is also available on YouTube. So Eoin, where do we want to start?\n\n\nEoin: Before we dive into the different areas, maybe we'll just talk about how the data is gathered.\nI think it's mentioned that this is inspired by the state of JS survey and also the Datadog\nstate of serverless survey. I know that the Datadog one, as far as I'm aware, uses Datadog data,\nactual customer usage data to compile that report. This one's a little bit different because there's\na survey and it's essentially spread through word of mouth and network and social media.\n\n\nSo already I think it's very interesting to look at this and see, okay, how does this compare to\nmy biases and what can I learn new from what other people are doing out there, what's trending,\nbut also kind of bear in mind that there might be a bit of selection bias here because\nthere might be a bit of a filter bubble and this might be capturing the voice of maybe a noisy\nmajority and maybe doesn't capture the full gamut of people building stuff on AWS. So we can bear\nthat in mind, but it's still going to be interested to dive into it. Are we ready to go into the\ntopics Luciano, or is there anything else you wanted to say first? No, I think that's fair.\n\n\nLuciano: Maybe if we go through the demographics first, maybe another comment I have is that demographic\nis a really good section to kind of get a feeling for what the audience looks like,\nbut we only get percentages. We don't really get like a total number of participants. So again,\nthis doesn't give us any insights on how statistically relevant this could be. So just\ntake that as a thing to bear in mind while we go through this. And of course, we're going to try\nto provide our own opinions, which are of course biased as well. So let's try to see what we can\nlearn, but of course this is not going to be representative for all the use cases of AWS.\n\n\nEoin: And right away we can see we've got 80% of respondents are from either North America or\nEurope, roughly in half. So maybe already there's an issue with language, I don't know,\nor just region network here, because if it's an English only survey that may skew the results\nand we've got 0% respondents from Antarctica. So that's a pity. I'd love to know what they think\ndown there. All right. And age-wise it's interesting, I guess, because it kind of shows\nthat the age profile for these surveys, I think, is shifting a little bit, right? We have more\npeople in the kind of middle age bracket. Quite a small number, I would say, in the 19 to 24 year\nold range, which is maybe a little bit surprising. I mean, that's what, 2%, 1% and nobody 65 years\nolder.\n\n\nLuciano: Honestly, I don't know if I'm surprised by that initial number, 1924, because one thing we have mentioned a few times is that AWS doesn't look very friendly to students or people that\nare just starting their career just because you can be scared about massive build shocks.\nSo people tend to stay away from AWS. So maybe that's a reflection of that feeling.\nYeah, that's a fair point.\n\n\nEoin: But what it's also saying is that I think that, let's see, we've got like 70% almost of respondents who've had perhaps 15 to 25 years of experience in the\nindustry. So that's going to inform the results so much as well. And that's specifically here.\nWe could see the majority is six to 10 years of experience with AWS alone. So this seems to be\npossibly weighted towards AWS fanatics in some way, or at least people who've been using it\nfor a long time. It would be nice to hear from people who are less experienced in general,\nbecause we can't just be building for the people who are already in the community.\nThe next one is company size. So a lot of big companies and they're actually 25%,\nbut that's quite a broad range. And almost half of the respondents are actually providing software\nand internet services or products in some way and professional services as well. So that would be...\nI did respond to this survey actually, Luciano, did you answer the call for this survey?\nYeah. So we're both in here in this 13.1%, I guess. And then financial services is the\nbiggest industry sector, which is definitely interesting.\nIt's also interesting to see that there is a quite significant slice there of healthcare and...\n\n\nLuciano: What was the other one? Government and education. So I don't know, a bit unexpected if you ask me,\nbut it's interesting. Maybe this has been growing more in the last few years because it might be\nthat these are generally industries that are always a little bit behind the curve and trying\nto catch up. So promising to see that it's a significant slice that it's almost 5%.\nYeah. I think there's something here.\n\n\nEoin: I think we were kind of surprised still, especially with modern AWS applications, how a lot of enterprise companies have really taken to them in a lot of\nways. Now there's still a huge number of companies who aren't even on the cloud, aren't just thinking\nabout moving to the cloud, but we might expect that startups are all over this technology and\neverybody else is lagging. But I think financial services showing that they're... I think there's\na lot of cases where they need the scale of the cloud, want to get rid of a lot of infrastructure\nmaintenance. So it's interesting to see that reflected here. Okay. And then most people\nare developers, got some architects. What else? Some business executives as well. Sales and\nmarketing even. There are some students as well, but that's the minority looks like, right? 0.5%.\nYeah. Okay. Yeah. You're right about that. Okay. Let's dive into the areas where we start with\ninfrastructure as code. I'm interested in this and we can see in terms of usage, obviously there's\na bit of overlap here because people might be using multiple infrastructure as code platforms\nas we do ourselves. What do you think of this? Yes.\n\n\nLuciano: I honestly was surprised to see that cloud formation are such a big share because generally speaking, I mean, cloud formation works and it's\nkind of the default AWS tool to do infrastructure as code. But in my experience, when you talk to\npeople at conferences or events, they generally try not to use cloud formation like Verbatim.\nThey generally use something like SAM or they use CDK and maybe they use Terraform and avoid\ncloud formation altogether. So I was a little bit surprised to see that there is an 84.7%\nof people saying we do use cloud formation. And again, this might be a reflection of people saying,\nyes, I know that down the line I'm using cloud formation. So they picked that option as well,\nbut I don't know if that means they use confirmation directly or through abstraction\nlayers like CDK or SAM. Yeah. It must be, I'm guessing here we see that 57.\n\n\nEoin: 3% are using CDK, which is massive for CDK, I think. But I think that that is also reflected in the 84.7% cloud\nformation usage because we don't see SAM or serverless framework or any of the other like\narchitect or chalice frameworks here. They're just not there. So I'm assuming that's all coming in\nunder cloud formation. What really surprises me here though is that Ansible is 55.2% and also that\nit's regarded as an infrastructure as code tool rather than configuration management.\nYeah.\n\n\nLuciano: And to be fair, that might be the case where people are provisioning EC2 instances, maybe through cloud formation, Terraform or whatever. And then they also use Ansible on\ntop of that to actually provision software inside the specific machines that they're provisioning.\nSo again, it's not that these tools are mutually exclusive. So it's a little bit difficult,\nI guess, to draw any conclusion here. Yeah, for sure.\n\n\nEoin: AppsWorks is also interesting because AppsWorks, as I understand it, is end of life as an AWS service for a chef and puppets. So\nthere's 16.4% of respondents who will have to migrate to something else at some point.\n\n\nLuciano: Maybe Ansible. Terraform is actually top for retention. So I guess this is...\n\n\nEoin: I'm assuming that this is based on previous year's results. Is that your interpretation here?\n\n\nLuciano: I'm not really sure. I don't remember if the questioner was asking you whether you were\nstill using these tools or if it's just data interpolated from the previous years.\n\n\nEoin: Okay. Fair enough. And then Terraform is top for awareness as well. Yeah. Okay. So most people are\naware of most of them and a lot of people are interested. Most people are interested in CDK.\nWe're not using it yet. So that is interesting. And then we can see the trend. So Pulumi is\nincreased on the increase, which is a bit of a surprise. Ansible as well.\nAnd cloud formation is going down, right? Which is also interesting.\n\n\nLuciano: Relatively, but it's still 35% for interest.\n\n\nEoin: Then if we go back to usage, we can see cloud formation has actually increased a little bit in terms of usage. So retention, the calculation is\nactually given here. It's the ratio of used it and would use it again over used it and would or would\nnot. Okay. So it's the percentage of people who used it and would use it again. Okay. I think the\nmain takeaway for me here is that CDK is the one that people seem most interested in and have\nadopted really quite significantly. I thought it was a little bit more divisive than this,\n57%. But the happiness index is actually one that I've heard people comment about the most.\nI'm not sure why this isn't showing up correctly.\n\n\nLuciano: Yeah, I think it was happiness in general on the whole area, not on specific tools. Okay. Right.\n\n\nEoin: So what I'm trying to get to is the fact that I believe we heard that most people would not, or a lot of significant number of people\ntry cloud formation and would like to not use it again. Not surprising.\nI saw this earlier. I'd love to dig deep into this a little bit more, but that should be reflected\nin the retention, but I'm not seeing it here. So maybe we'll come back to it. But yeah, I'm always\ninterested when I hear that because I know there's a lot of, I think there's a lot of mythology\naround as well. Cloud formation definitely needs a lot of improvement, but I also feel that there's\na bit of mythology around there from people who tried it maybe years ago and got rollback\nfailed errors, got stuck, had to delete everything and vowed to never use cloud formation again.\nBut I think things have improved a lot. And I think if you're serious about AWS,\nit's really worthwhile learning. So that's my take on it.\n\n\nLuciano: The other thing, if you scroll at the very bottom is that there is a mention of kind of up and coming tools, which is interesting. And we have there SSD and Wing Lang, which are tools\nthat I'm hearing a lot about. So it's interesting to see them there. I haven't heard of Crossplane\nor Troposphere, so I cannot really comment on those.\nYeah. Wing Lang, we talked about a while back. SSD, we've mentioned in the past.\n\n\nEoin: This was, has been built on CDK, but I believe they're moving away from CDK as the underlying\nimplementation. The Crossplane and Troposphere are things I'll have to check out.\nAll right. Well, we do see ICD. Let's do that.\nOkay. Do you want to take us through this one Luciano?\n\n\nLuciano: Yes. So my reading of this one, I wasn't surprised to see GitHub Actions there at the top,\nbut I was a bit surprised to see it as the first one because I think it's relatively new. For\ninstance, the second one is Jenkins and Jenkins has been around since, I don't know, since I can't\nremember. So probably at least a decade. And so GitHub Action is much more recent than that. So\nit's interesting to see that there is such a competition between the two and they are very\nclose. Now, also not too surprising because if you ask people, generally they're not very happy\nwith Jenkins while generally they're very happy with GitHub Actions. So probably there is going\nto be a gradual migration from Jenkins to other tools. And right now GitHub Action is probably\nthe biggest contender out there. We also have CodeBuild and CodePipeline in here and CodeDeploy\nas well. So I'm not really sure how the fact that this, I guess, the functionality spread across\nthree different tools, how is this affecting the numbers? And maybe people are only using some of\nthe tools and not all of them together. So I guess it's really hard to read into the numbers here and\ncomparing the AWS offering with other offerings. And also interesting to see that GitLab is the\nthird one non-AWS service to be there. And then we have CircleCI and that's it. So no other,\nI guess, non-AWS CI offerings.\n\n\nEoin: See people moving to GitHub Actions because the developer experience and the speed is just\nsuperior in general in my experience. And unfortunately for GitLab and CircleCI,\nthose, I think we're leading the space before GitHub Actions came along in a lot of ways,\nbut I think GitHub Actions is eating their lunch quite significantly at this point.\nJenkins won't die. That's an interesting aspect here. Still 75% usage of Jenkins.\nI recently-\nIt's that one legacy product Jenkins built that you can't shut off because nobody knows\nhow it works.\nYeah.\n\n\nLuciano: I recently watched an interesting video by Faster than Lime who did quite a thorough review of GitHub Actions and the flows that GitHub Actions has. So it's a little bit critical against\nGitHub Actions, which kind of shows an interesting thing that there is still a lot of room for\nimprovement in this space, even though I think everyone I talk to, they think the GitHub Action\nis the best thing ever. So this is probably giving all the competitors a little bit of room\nfor improvement and maybe catching up with GitHub Actions. I think if they can fix some of the flows\nthat GitHub Actions currently has, maybe we will see different numbers in the coming year. So maybe\nthere is hope there for CircleCI and GitLab. Okay.\n\n\nEoin: Well, GitHub Action is, people don't move away from it. It looks like 95% retention. GitLab also so popular, but the AWS ones aren't doing\ntoo bad here. And yeah, GitHub Actions is like CDK for infrastructure as code. That's the one\npeople are most interested in. So what can we see when we look at the trends year on year,\nif we look at usage, Jenkins is diving a little bit, but everything else is the same.\nYeah. Maybe another thing to...\n\n\nLuciano: It's not a lot of movement.\nAnother thing to mention is that once you have a CI CD pipeline set up and it works and it does\nall the things you need to do, you really need to have a pretty strong reason to migrate to\nsomething else. So maybe that justifies the retention for pretty much any tools.\nAnd I guess the dangers might be when you build a new project, if you can get to production with\nall the things you need quicker, then probably you have a reason to adopt another tool.\n\n\nEoin: The positivity around GitHub Actions and the lack of negativity is reflected in this positive\nnegative split chart, Jenkins, poor Jenkins, but they still can't switch it off because it's a 75%\nusage. CodeStar, I've never used CodeStar except for when you need to set up a CodeStar connection\nfor code pipeline. And Proton. That's interesting actually, because Proton we didn't mention yet,\nbut Proton is kind of last on the list in terms of awareness, usage, retention, everything,\nbut the people who do use it don't seem to like it very much.\n\n\nLuciano: Yeah. I cannot comment on that one because I don't have a lot of experience with it.\nYeah. It had a lot of hope. Okay. Most people are pretty happy. Yeah.\n\n\nEoin: There's very few people who are seriously unhappy with CI CD services. Argo CD build kit. Okay. Okay. We've covered two\ntopics and I think now to one, which is a special interest to us, which is the serverless space.\nAnd now we get into some of the tools we talked about earlier. The fact that AWS Sam is now top\nof the list when it comes to tools and the serverless space in terms of usage, that's\nalso a little bit surprising, I would say. If we go back to the infrastructure as code chart,\nand we looked at CDK's popularity, CDK is actually lower than AWS Sam here. So I don't know\nhow that can happen, but with serverless framework is also there, thereabouts. So those three are all\naround the 50% usage mark. 36% of people are using Amplify. What do you think of that? Does that\ncome as a shock to you? A little bit, to be honest.\n\n\nLuciano: Maybe again, this is my bias in not having been a huge user of Amplify for different reasons, but maybe people that use it, they\nreally like it and makes their life simpler. So we're not using it more, I guess.\n\n\nEoin: It would be interesting if you could actually correlate the demographics and the tooling here, because I still\nsee Amplify as a tool that helps you to get started and prototyping applications when you're early on\nin your journey learning AWS. Difficult to use it over the longterm. So I wonder which demographic\nthis correlates with. And 12.7% application composer, which is still a pretty new tool.\nSo CDK is top for retention. Serverless framework is top for awareness, so it still has the biggest\nbrand recognition, it seems. Lambie is one I have never come across. Lambie for Ruby on Rails. Have\nyou seen that one before, Luciano?\n\n\nLuciano: I have seen it, never used it, and I'm surprised that that's such a big awareness, to be honest, which is like 33 point something percent. Yeah.\n\n\nEoin: Maybe the link to the survey got shared around their Discord or something. Okay. And in terms of interest,\nthere's not much to separate these. Everybody's 40 to 50%, 40 to 60% interested in all these tools.\nNow the trend here is that CDK is, well, even though it has, in terms of interest,\nless than the last time, it's actually top now because everything else is so close to it. And\nin terms of usage, yeah, AWS Sam usage is on the rise as is CDK, significant jump for CDK.\n\n\nServerless framework, still in there. It's just been passed out by CDK and Sam at this point.\nOkay. So what do people think of these tools in general? CDK is very positive, would use again 46%.\nWould not use again 11%. Okay. I'm just remembering how to interpret this chart now.\nSo yeah, CDK, very positive. I'm surprised that there aren't more people who've been burned a\nlittle bit by CDK and constructs and version incompatibilities and issues with upgrading stacks\nhaving changed CDK modules. So it's a little bit surprising. So Sam is very popular, but a little\nbit more negative. Serverless framework has a bigger chunk of would not use again. 24% of\npeople say they would not use it again, but still more positive than negative. Okay. So many people\nare unhappy with the tools used to build serverless applications. Not very positive at all.\nReally 15% with five stars. So I think what we're seeing here is there's a lot more to do.\nWe take four or five stars, that's 50%, but there's still quite a lot of people who think\nserverless tooling is a bit meh.\n\n\nLuciano: And I don't disagree to be honest, because if you take the experience end to end for building a serverless application, I think that there are a\nlot of gaps there to be filled to just give you a more cohesive experience. There isn't like one\ntool that can do everything for you. You need to jump across different tools. You need to figure\nout which tool is the best. And then every tool has its own pros and cons, and you need to learn\nexactly how to use them, what works, what doesn't work, where are the dangers. So I think that there\nis definitely lots to improve there and hopefully it's going to get better over the next few years.\nAnd the developer experience in itself is going to get better as well.\nYeah.\n\n\nEoin: I suppose this is why we're seeing SSD and Wing Lang and Amped and all these other tools coming out and trying to really change the system because look, people are dissatisfied in general.\nSo I understand that. I agree. And other things not mentioned then are architect,\nwhich I would be interested to hear from the architect users because it looks like a really\nnice developer experience. I know that the use cases it targets are more specific. It\ndoesn't try to do everything. SST, Wing Lang, serverless.tf. That is-\nThat's I think Anton. Yeah, exactly.\n\n\nLuciano: Which is terra for modules that are more fine tuned for serverless use cases as far as I know.\nOkay. Interesting. Yeah. I'd like to hear more about those actually.\n\n\nEoin: I wonder what we have to do to reach people so that we can get data for those. Okay. Containers, Luciano, what are you seeing in\nthe container when it comes to trends? This is not my main area of expertise.\n\n\nLuciano: So I'm going to try to give my reading, but feel free to chime in and give me your opinion as well.\nSo the main thing we can see here is that ECS is number one in terms of usage with more than 80%\nfollowed by Fargate. And again, definitely some overlapping there because Fargate, you can\nconsider it as part of ECS if you want to. I don't know if it's really clear what is the difference\nthere to people participating to the survey. Are we assuming that ECS is only when you provide your\nown EC2 instances and create your ECS cluster and Fargate is only the serverless version, or if you\nuse Fargate, you are also using ECS. Yeah, it's very difficult to read into those numbers.\n\n\nAlso, we have Lambda for running containers, but we know that you are not really running containers\nin Lambda. So that's another confusing piece of data there because...\nIt's a recent number.\nExactly. I mean, you can package a container image to provide Lambda, all the files that are needed\nfor your runtime, but you're not really running a container in itself. So definitely source of\nconfusion there. The interesting one is that EKS is 56%, which is not necessarily a small number,\nbut I was expecting it to be a little bit more just because in bigger enterprises, I've seen\nthat they tend to prefer Kubernetes as an abstraction layer because generally they might be\nmulti-cloud or they might be a little bit more concerned of portability. So Kubernetes is generally\nkind of a more agnostic default that you can pick just to... We're not really committed to the AWS\noffering 100%. So it's not bad.\n\n\nEoin: We do have self-managed Kubernetes as well, actually. So this is represented as 40%.\nSo would you assume that these groups are more or less mutually exclusive,\nthe EKS group and the self-managed, or there's a bit of overlap?\n\n\nLuciano: I mean, they should be mutually exclusive, but just looking at the numbers, probably people interpret that as overlapping, right? Or maybe people would have bought EKS and self-managed\nKubernetes, so they tend to pick both.\nOkay. 50% on Beanstalk is a surprise to me. Yes, maybe agencies, right? I remember,\nI mean, initially that was the target use case for Beanstalk. So maybe they still have lots of\nwebsites and web applications running on Beanstalk.\nAnd more on LightSail than there are on AppRunner.\nWow. Which is also interesting, yeah. I\nthink LightSail is also a little bit more... I mean, AppRunner is a little bit more recent\nthan LightSail. So maybe it's just a catching up game. Maybe we'll see that on the retention\nor awareness chart, like how do they compare. The other interesting thing is that we have read that\nOpenShift, which is as far as I know, another distribution of Kubernetes. So is that an\noverlap with the self-managed Kubernetes or not? Good to know. Yeah.\n\n\nEoin: Good question. I think people are using Lambda then. Okay. Maybe we can exclude Lambda,\nbut people who are using Lambda are still using Lambda. But Farghout, EKS, people are pretty\nhappy with, it seems like. People would use it again. And AppRunner is also popular, but let's\nsee. If we go to awareness and interest, are we seeing more... People know about AppRunner. More\npeople know about LightSail than AppRunner. That really surprises me. But look, again, that's my\nfilter bubble. And then interest, yeah, AppRunner is higher. So we can expect AppRunner to grow a\nbit. I know we're planning to do a deep dive on AppRunner. We've been planning it for a while,\nso stay tuned. Okay. So trend is, let's see, a lot of ECS, 85% of people now instead of 77%\nlast year. Not a huge amount of movement overall. But also EKS is growing.\n\n\nLuciano: So it's not like people are necessarily peaking one over the other. Probably it's just different workloads.\nYeah, that makes sense. Okay. Yeah. Okay. So let's look at positive-negative split.\n\n\nEoin: Where's the... The negativity is most interesting to talk about, right?\n\n\nLuciano: I think so. Yeah. It's interesting to see Red Hat to be absolutely negative. I mean,\nI never use it, so I cannot comment on... But that's...\n\n\nEoin: 64% of people are not interested in it, which is fine because you have to be in a specific space. But only 11% would not use it again.\nThat makes more sense. Only 2% would use it again.\nSo yeah, maybe they've worked to do their beanstalk. People want to get away from it.\nIt looks like 37% would not want to use it again. And self-managed Kubernetes is 28% would not use\nit again, which is pretty high, versus say EKS, which is 13% would not use it again. So I think\nECS, Fargate, there's a lot of love for them in general. EKS also pretty good, but ECS and Fargate\nseems to be the way things are going. I suppose Fargate as well... I'm just thinking Fargate can\nbe on EKS as well as ECS, can't it? So that could span the two spaces you can have people using\nFargate with EKS. Okay, so in general, people are pretty happy with containers, given that there are\nhow many are there? Many ways are there to run containers. I lost count. AWS 19, was it?\n\n\nLuciano: We need to ask Cori Queen, what's the latest count?\nYeah, yeah. Code build doesn't come up here. All right. What have we got to say about NoSQL?\nYes, that's another area where I don't have a huge amount of knowledge, but it's also pretty broad because in this category here, we have all kinds of databases that are not SQL. So we even\nhave Redis- All sorts of services that aren't databases maybe?\n\n\nEoin: Through that, yeah.\n\n\nLuciano: For instance, you have Neptune or Timestream or MemoryDB, which of course you can use them as databases, but they are very specific use cases, I would say. Not like general purpose\ndatabases, so to speak. Yeah. Would you agree with Redis being classified as a database?\nI know that some people use it as a database, but again, there are constraints there which are important to know. I wouldn't call it a general purpose database.\nYeah, yeah. I'd put it in a separate category, I think in general.\n\n\nEoin: OpenSearch is being shoehorned in as a database. And I know a lot of people do use Elasticsearch as a database, but let's look\nat the data. So 87% for DynamoDB, I'm not surprised because a lot of people are using it just\neven as a key value store for some small amount of data. You don't have to be all in on it.\nI think Redis is a fantastic product and so useful in so many different scenarios. I'm also not\nsurprised to see that feature highly. DocumentDB is quite far down the list, isn't it? 38%. But I\nsuppose a lot of people using MongoDB might use Atlas or a different hosted solution or self-managed.\n\n\nLuciano: Which is also interesting that it's not here in this list because, I mean, with Atlas you can\nrun it on AWS, right? So it could have been on this list, right? Yeah, exactly. Yeah.\n\n\nEoin: I guess this is provided by AWS, so maybe that's the difference here. And ElastiCache is the most\npopular. MemoryDB we haven't tried, but I think it just got such bad press for the pricing.\nThat put a lot of people off. Have you tried QLDB?\nQLDB, no, no. I skipped. Well, yeah, we did see some projects in the blockchain space,\nbut QLDB seems definitely more like a pragmatic solution for a lot of applications that ended up\nusing blockchain because it's focused on the immutable ledger, but no, I haven't had an\nopportunity to use it properly. Yeah.\n\n\nLuciano: The other one that I'm curious about is Neptune because I heard very mixed feelings, so I'm curious to see what is the love and hate kind of chart there.\n\n\nEoin: Okay. All right. Some people are most interested in DynamoDB. Yeah. This is kind of interesting,\nbut I think because DynamoDB is really the only truly no SQL database from Amazon that they\nprovide themselves, it would be nice just to see some more non-AWS solutions in this analysis.\nYeah. There's not that much change in trend when it comes to usage. Okay. The positive-negative\nsplit. There's not a huge amount of would not use it again, actually. Open search is the highest,\nwhich is not surprised because it's difficult, I think, to use and manage,\neven though it's incredibly useful.\n\n\nLuciano: I'm surprised that everyone loves DynamoDB because I personally always have mixed feelings when I use DynamoDB. It's really, really good in\ncertain circumstances. In other circumstances, it might bite you back pretty badly. I don't know.\nI don't remember exactly which vote did I give to the questions here, but I will probably use\nDynamoDB again, of course, but with caution. There you go. Yes. Okay. That's fair enough.\n\n\nEoin: And I think that would reflect a lot of people's views because there has been a bit of negativity,\nnot around DynamoDB per se, but around a bit of pushback against a single-table design,\nprinciples recently. I think we saw some ETA discussion on Twitter and I think Alex DeBrie\nmentioned that he would use it a little bit with more caution and overused it in the past.\nAnd I think it was interpreted by some people that you should just not use single-table design\nand Rick Houlihan got involved again. I think it's a topic maybe we should cover in a future episode,\nthis whole idea of single-table design and DynamoDB. But in my view, it was always something\nthat's very difficult for everybody to grasp, especially for everybody on a team. So it should\nbe used with caution, but DynamoDB itself has so many other applications and uses. I would be\npositive in general. Absolutely.\n\n\nLuciano: It's just that I don't know if I would consider a totally general purpose database. You need to be careful with the use cases. If you pick a good use case, it's\nan amazing tool, but for other use cases, it might be very difficult to achieve your goals with\nDynamoDB, or it might end up being very expensive or you need to write a lot of custom code.\nYeah. Okay. Generally, most people seem pretty happy and memento.\n\n\nEoin: Yeah, it's a little bit strange, right? Because this section is titled ranking of the NoSQL databases provided by AWS,\nwhich is fine. And it sticks to services that are on AWS and then others. Other NoSQL databases\nprovided by AWS, not mentioned above, include memento. But of course, memento is not provided\nby AWS, it's provided by memento.\n\n\nLuciano: Maybe because some of them are ex-WS engineers, deserves to be that. I don't know. Okay.\n\n\nEoin: And NoSQL databases is its own category, but we don't have a category for SQL databases, SQL databases.\n\n\nLuciano: Which is also interesting, yes.\n\n\nEoin: Because I'd love to hear people talking about Aurora serverless. Yes. How many people love it, right? And will keep using it.\nYeah. Now we talked about... We had a whole series about event services in AWS. We did one,\nan individual episode on SNS, SQS, EventBridge, Kinesis, Kafka. Anything else did I miss? I think\nthose are the main ones. And then we did another episode comparing them all. So if those interest\nyou, feel free to check them out. They're always popular from the back catalog. So Luciano,\nif you had to guess beforehand what the most popular event service was, what would you have\nguessed? I was expecting SQS to be the first one just because, I don't know.\n\n\nLuciano: In my personal experience, it's something that always comes up in almost every architecture. But to be fair,\nSNS also has so many use cases that it's not surprising either to see that. Even just for\nnotifications. Yeah. Yeah. Alarms. And EventBridge is actually a bit surprising because it's still a\nrelatively newer service compared to the other ones. So it is surprising that there's so much\nshare, but to be fair, it is a great service. And it's so easy to start using it that I can see why\nthere might be an uptake even though it's a relatively recent service.\nWhat's Step Functions doing in here? Yeah. That's a bit unexpected, to be honest.\n\n\nEoin: Yeah. Maybe they felt they had to put it somewhere. Because in some ways it's the opposite\nof event driven, isn't it? Even though you can have asynchronous execution patterns in there.\nI'd surprised that only 57.9% of people are using Kinesis. People should make more use of that.\nAnd 10% of people are still using the simple workflow service SWF, which is maybe again,\nit's just use cases they can't turn off. Okay. DynamoDB streams. Yeah. People like,\nI guess for people doing any kind of triggers, change data capture on DynamoDB,\nthat's the way to do it or with Kinesis. Yeah.\n\n\nLuciano: Here it's pretty equal the spread of retention. I think, yeah. I think every one of the services has its own specific use cases and\nit works really well for those use cases. So I guess if you need a queue, you're going to use\nSQS and you're probably going to stick to it. If you need an event system, you probably use\nEventBridge and you stick to it. Kinesis is more if you need high throughput, batching,\nthat kind of stuff. So it makes sense that every one of them has a high rate of retention awareness\nand interest. It's probably more making sure that you use the right tool for the right use case.\n\n\nEoin: Some of these things I'd love to see the raw data. Is it possible, I wonder, to publish\nanonymized raw data for this? Is there a risk? Because 10% of people are using SWF,\nsimpler workflow services, which is basically deprecated by AWS for 99% of use cases in favor\nof step functions. So 10% are using it, but half of the people are interested in using it.\nYeah. That's a bit unexpected.\nA deprecated service. All right. Any major change here? More people are using step functions.\n\n\nDynamoDB Streams is a new entry on this list. It obviously wasn't in the survey last year.\nAnd EventBridge is on the increase, but every... Yeah. Adoption of event-driven services seems to\nbe up everywhere. Nothing has gone down. Okay. So where's the positivity? SQS and SNS are\npositive. It's hard to fault them, really. I think they're just... They're the simple services that\nlive up to their name along with S3. They're just fantastic services. Everything you want from a\nserverless service in those things. I think they just work and there's very little complexity to\nthem. Yeah. Kafka. I suppose Kafka is a bit more of a complex beast, even with MSK. Even with MSK\nserverless, we reviewed it, we talked about it, we've used it, and there's still quite a lot to\nmanage and monitor there. Incredibly powerful, but I can understand why people would be turned off if\nthey've used it and didn't really get a great experience without putting in the effort to learn\nit fully. All right. Yeah. People are mostly happy. I don't know if these happiness charts are\ntelling us too much. So our last category is AI and ML, Luciano. Yes. Let's run through it.\n\n\nLuciano: Yeah. And it's interesting that the first one is OpenAI, which is not even AWS, but that's fair.\nNot every category is only AWS-specific services. Then we have SageMaker, which is 35%,\nRecognition34, Textract, again, close to 34, Bedrock, which is interesting to see that almost\nas 30% being a very new service. And yeah, then Poly, Comprehend, Transcribe, Translate,\nPersonalize, Inference, Forecast, and Amazon Augmented AI, which I don't think I've come\nacross it before, so not really sure. But my feeling is that these are, again, very different\ntools for very different use cases. So I don't know if there is really a comparison that you\ncan make between them or is more, again, which kind of use cases are people having the most\nand do they pick up the right tool for that use case or not? Yeah.\n\n\nEoin: If we could influence the kind of questions or the design of the survey for next year, it would be interesting to get,\nwell, what are you using these services for? What's your application? Because I think, yeah,\na lot of this could be like OpenAI, 58%. That could be just people playing or using\nChatGPT a few times a week, I guess. Yeah.\n\n\nLuciano: Not necessarily integrating it into applications, right? Yeah. Yeah. The retention on all these services is quite high, isn't it?\n\n\nEoin: People are using them. People who use them tend to keep using them or want to keep using them. Awareness is\npretty good across the board for these AI services, even things like Forecast. Yeah.\nAnd I'm surprised a little bit, like 50, 60% of people are using OpenAI, but only the same number\nare interested in using it. So it's like, is there a split of people who are just completely\nuninterested in OpenAI? That surprises me in this day. All right. So what are the trends here?\nYeah, OpenAI is new there. Wasn't there last year, which makes sense. Same for Bedrock.\nOkay. But everything, machine learning usage seems to be on the increase in general. Amazon Translate,\nComprehend, Transcribe, they're all increasing for this audience.\n\n\nLuciano: Now let's see what people don't like SageMaker is probably- Yeah. SageMaker would not use again 9%. Yeah. I can understand that.\n\n\nEoin: Yeah. It's a bit of a mixed bag SageMaker, I guess, and some of the modes of execution\nare more complicated than I'd like them to be. They can be a bit slow to start up SageMaker\nendpoints, for example. We talked about this when we talk about in our episode about how to run\nmachine learning. Yeah. So there's a lot more negative sentiment here, isn't there? Overall,\n50% of people are three stars or less. That's interesting.\n\n\nLuciano: Another thing I would ask is that if there is a free form for future surveys, if there would be a free form on why are you\nnot satisfied with it? Because otherwise it's just too difficult to extrapolate why people might be\nunhappy with these tools. I understand it would be really difficult to aggregate this kind of answers,\nbut maybe they could do some kind of manual selection to try to extrapolate what is the\nsentiment there. Yeah, I agree. And look, I think it's well done on putting this stuff together.\n\n\nEoin: I mean, it's difficult to get this sort of output and make sure it's statistically relevant and\nunbiased. It's impossible, I would imagine, but it's been running for a few years now. I guess\nthis is the second year. And if you want to, if you didn't know about it this year, if you want to\nbe included next year, you can add your email for the 2025 and you'll get a notification when that one comes out next January.\n\n\nLuciano: Yes. And I think this brings us to the end of this episode.\nI hope that you like this slightly different format from what we do generally. This was a little bit more\nopen for us. We did do a little bit less preparation. So it's more of our real unfiltered\nopinions rather than preparing an entire set of things that we wanted to say in advance.\nSo hopefully that comes across well, but if not, let us know and we'll try to stick to what you\nlike the most and get better in the next episode. So as usual, leave us comments, reach out to us on\nsocial media and let's engage because that's what helps us to try to make this podcast better and\nbetter every single time. So thank you very much. And we'll see you in the next episode.\n"
    },
    {
      "title": "120. Lambda Best Practices",
      "url": "https://awsbites.com/120-lambda-best-practices/",
      "publish_date": "2024-04-05T00:00:00.000Z",
      "abstract": "In this episode, we discuss best practices for working with AWS Lambda. We cover how Lambda functions work under the hood, including cold starts and warm starts. We then explore different invocation types - synchronous, asynchronous, and event-based. For each, we share tips on performance, cost optimization, and monitoring. Other topics include function structure, logging, instrumentation, and security. Throughout the episode, we aim to provide a solid mental model for serverless development and share our experiences to help you build efficient and robust Lambda applications.\n\nAWS Bites is brought to you by fourTheorem, an AWS Partner that specialises in modern application architecture and migration. We are big fans of serverless and we have worked on quite a few serverless projects even at a massive scale! If you are curious to find out more and to work with us, check us out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources.\n\nEp. 110 &quot;110. Why should you use Lambda for Machine Learning?&quot;\nEp. 108 &quot;How to Solve Lambda Python Cold Starts&quot;\nEp. 104 &quot;Explaining Lambda Runtimes&quot;\nEp. 92 &quot;Decomposing the Monolith Lambda&quot;\nEp. 64 &quot;How do you write Lambda Functions in Rust?&quot;\nEp. 65 &quot;Solving SQS and Lambda concurrency problems&quot;\nEp. 10 &quot;Lambda or Fargate for containers?&quot;\nEp. 4 &quot;What language should you use for Lambda?&quot;\nEp. 6 &quot;Is AWS Lambda cheap or expensive?&quot;\nArticle: &quot;What do you need to know about SNS?&quot;\nArticle: &quot;What can you do with EventBridge&quot;\nArticle: &quot;What do you need to know about SQS?&quot;\nYan Cui's video about using Lambda Destinations instead of DLQs\nProfiling functions with AWS Lambda Power Tuning\nBlog post by Luca Mezzalira about hexagonal architectures for Lambda\nEp. 41 &quot;How can Middy make writing Lambda functions easier?&quot;\nSLIC Watch for automated dashboards and alarms\nEp. 35 &quot;How can you become a Logs Ninja with CloudWatch?&quot;\nEp. 34 &quot;How to get the most out of CloudWatch Alarms?&quot;\nEp. 33 &quot;What can you do with CloudWatch metrics?&quot;\n\n",
      "transcript": "Eoin: Lambda functions have revolutionized the way we build\nand deploy applications in the cloud,\nbut are we really harnessing their power efficiently?\nToday we're going to uncover some of Lambda's best practices,\ncombining insights from AWS documentation,\nbut also our own experiences to help you\nto optimize your serverless architecture.\nOverall, we want to make sure you have a great time\nwhen working with Lambda.\n\n\nI'm Eoin and I'm joined by Luciano\nfor another episode of the AWS Bites podcast.\nAWS Bites is brought to you by fourTheorem,\nan AWS partner that specializes in modern application architecture\nand migration.\nWe are big fans of serverless\nand we've worked on a lot of serverless projects,\neven at a massive scale.\nIf you are curious to find out more and to work with us,\ncheck us out on fourtheorem.com.\nOne of the things that's worth starting with\nis how Lambda works under the hood,\nand we've covered a few different episodes in the past about AWS Lambda,\nbut it's worthwhile having a brief refresher.\nSo, Luciano, do you want to tell us how does Lambda work?\n\n\nLuciano: I'll try my best and I think this is one of the interesting topics about Lambda that is often not covered enough or not discussed enough.\nSo let's try to discuss how Lambda works under the hood,\nbecause I think it's important to come up with a good mental model.\nAnd once we do that, I think it's going to be easier to understand\nhow to write Lambda code that is more optimized\nfor that particular kind of environment.\n\n\nSo the first thing to clarify is that Lambda as a service,\nby definition, is serverless.\nSo that means that if you're not using that particular service,\nfor instance, if you don't have anything to do,\nlike you brought an application, but nobody's using that application,\nideally, nothing should be running at that moment in time.\nAnd that also means as a nice side effect,\nthat you don't get charged for it.\nSo it's not like you have a server spinning out 24-hour seven,\nand you're going to be paying for even though maybe nobody's\nactually using your service.\n\n\nWith Lambda, if nothing is running, you don't get charged for it.\nIt's also event-based, which means that the computation starts\nwhen an event is triggered.\nAnd when that event happens,\nAWS will need to be able to run your Lambda code somewhere.\nAnd this is where things get a little bit interesting,\nbecause if there is no instance of your Lambda at that moment in time,\nor maybe the caveat there is if you have other instances,\nbut they are all busy, AWS will need to figure out,\nokay, where do I run the code for this particular event?\n\n\nAnd this means that AWS needs to spin up some infrastructure,\nwhich generally means loading the code from S3,\ncreating some kind of lightweight virtual machine,\nand make sure that that virtual machine can run your code.\nAnd of course, doing all of that extra setup takes a bit of time.\nAnd the time can vary between milliseconds,\nor even a few seconds, depending on the kind of runtime\nthat you pick for that Lambda function.\n\n\nAnd because this is extra time that you are spending\nnot really executing any code from the Lambda function,\nthis is generally called a cold start.\nSo you need to take to give AWS time to bring up all the infrastructure\nso that you can run your own code. And this is what's called a cold start.\nNow at some point, you take that event, execute that code,\nyou do something useful with it,\nand you provide some kind of useful response to the system.\n\n\nSo in a way, you can say that that particular Lambda execution is completed,\nbut the infrastructure related to that Lambda function\nis not immediately disposed by AWS,\nit will be kept around for a few minutes.\nIt's not absolutely clear by AWS how much is that time,\nwe have seen varying between five minutes and 15 minutes.\nBut it's just in the order of minutes.\nSo you need to be aware that your Lambda might stay around\ndoing nothing for a few minutes before it's disposed.\n\n\nAnd the reason why that happens is because if a new event comes in\nduring that period, AWS can be smart about it\nand recycle that instance,\nrather than recreating an entire new one for every single event.\nAnd of course, this is an optimization and it's called a warm start,\nyour code can be executed immediately\nwithout waiting for another cold start.\nNow, why it is important to understand all of this?\nI think in practice, once you understand the lifecycle of a Lambda function,\nyou can write code that is way more optimized for this environment.\n\n\nAnd just to give you an example, I think at this point,\nit should be clear that there are more or less two phases of your code.\nOne is what you can call the init phase,\nand the other one is what you can call the event handling phase.\nAnd the init phase generally happens only one for every Lambda function\nthat gets instantiated.\nAnd generally, you can use this init phase\nto create all the expensive resources\nthat you might want to recycle across multiple invocations\nof the same Lambda instance.\n\n\nFor instance, if you need to establish a database connection,\nif you need to create some kind of client,\nmaybe another AWS SDK client,\nand all of these resources should be made globally available\nso that multiple execution of your handler can access those\nand they don't need to be recreated over and over for every single event.\nThis is something that might be done slightly different,\ndepending on your choice of runtime and language.\nBut pretty much with every language and runtime,\nyou have ways to have this kind of initialization phase,\nand then you have handler logic\nthat can be called over and over once for every event,\nand it can access some kind of global space\nwhere all these instantiated globals are available,\nand you can just use them rather than having to recreate them\nwithin your handler code.\nSo what can we cover next?\nI think one interesting topic is maybe clarifying the different ways\nyou can invoke a Lambda function.\nShould we try to describe all of that, Eoin?\n\n\nEoin: Yeah, briefly we definitely should, because this is a really important thing\nwhen it comes to understanding why best practices\nare good practices in the first place.\nAnd let's break it down.\nYou've got synchronous invocations, asynchronous invocations,\nand then you've got the third type,\nwhich is polling-based or event source mapping.\nSo with synchronous invocation, you have something waiting\nfor the Lambda function to complete and issue a response,\nand that applies to services like API Gateway\nand Application Load Balancer,\nand it also applies to some other more niche cases like CloudFront,\nLambda at Edge functions, Cognito Lambda function integrations,\nand you can also synchronously invoke things from the API or the CLI\nif you choose request response mode.\n\n\nNow, the best practices when it comes to synchronous invocation\nis probably obviously enough to try to keep the execution time\nas short as possible.\nFor example, if it's an API, you should return a response\nbefore an HTTP timeout will occur.\nAPI Gateway will give you a 30-second timeout,\nso you should probably set your Lambda functions timeout\nto 29 seconds or less.\nAnd when it comes to synchronous invocation,\nthen another good practice is to monitor concurrency and throttling,\nbecause otherwise a lot of synchronous invocations come in,\nand then you'll end up in trouble,\nand your users are going to experience those errors,\nand you can't automatically handle that seamlessly for them.\n\n\nSo if you're not able to spin up new Lambda functions,\nyou'll directly affect those end users.\nNow, when it comes to asynchronous invocation,\nthis is probably the most common, actually,\nbut with async invocation, you're not waiting for the response.\nIt happens in the background.\nNow, the execution in this case can take longer,\nalways up to the Lambda global limit of 15 minutes.\nIt also means that Lambda is going to retry if there's a failure,\nso this is something you can't do with synchronous invocations\nwithout handling it yourself.\n\n\nBut with async invocations,\nLambda will retry up to three invocations total per event\nin the case of an error.\nAnd it does that by queuing up invocations internally\nfor up to a maximum of six hours\nif it doesn't have enough concurrency available to it\nwhen the event comes in immediately.\nSo Lambda has its own internal queue,\na bit like an SQS queue, but you don't see it.\nIt's all internal, and that's how async functions work.\n\n\nNow, when it comes to failure handling with async events,\nyou can configure a dead letter queue or a DLQ\nto automatically receive events that have failed,\nnow up to three times,\nand then you can inspect them for future investigation,\nand you can also redrive them back into the function.\nDLQs are only supported for SNS and SQS.\nNow, there is another way to do this kind of failure handling,\nand that's with destinations.\n\n\nSo destinations are a newer and probably better way of doing it\nthat is more recommended these days,\nand you can have a success destination and a failure destination.\nAnd then that destination can go to EventBridge, SQS, SNS,\nor onto another Lambda function.\nAnd one of the advantages of failure destinations\nis that it gives you additional metadata\nabout the event that caused the failure.\nNow, when it comes to async event invocations,\nservices like S3 and SNS are examples of services\nthat will invoke asynchronously. EventBridge is another one.\n\n\nOne of the additional best practices that you should really consider\nwhen looking at asynchronous invocation\nis implementing idempotency.\nAnd in event-driven architectures in general,\nidempotency is a very important thing to understand,\nand it basically means that the result of invoking a function\nmore than once should have the same outcome\nas just invoking the same function once.\nSo if you end up sending the same event to a destination,\nto a target for processing multiple times,\nyou shouldn't end up with any additional side effects\njust because you received that event multiple times.\n\n\nAnd the reason why idempotency is important\nis that most of these delivery modes\nhave at least once processing delivery semantics,\nso they don't guarantee that they'll only deliver it to you exactly once\nbecause that's a really hard thing to do.\nYou should be prepared to expect more than one invocation\nof the same event, and there are tools that can help you\nto implement idempotency like AWS Lambda power tools,\nwhich we'll talk about a little bit later as well,\nespecially when we talk about monitoring.\n\n\nWhen it comes to async events,\nthere's some nice new metrics you get like the async event age,\nwhich will tell you the time between the event coming into Lambda\nto be queued and ultimately being invoked.\nAnd if that turns out to be more than you expect,\nthere's probably a concurrency or a throttling issue\nor also a failure you need to look out for.\nAnd you'll also get a metric that tells you\nhow many dropped async events there have been.\n\n\nSo if events aren't processed\nbecause they stayed in the queue for more than six hours,\nthat metric will tell you all about it.\nSo that's synchronous invocations and async invocations.\nAnd the third one is polling invocation,\notherwise known as the event source mapping.\nAnd it's called event source mapping\nbecause there's a separate piece called an event source mapping\nthat takes the function from a source\nand essentially synchronously invokes that for you,\nbut it's managing the failures and retries\nin this event source mapping feature.\n\n\nAnd it applies to things like DynamoDB streams,\nKinesis streams, SQS queues, and Kafka topics as well.\nWhen it comes to best practices, idempotency applies here equally.\nYou should also monitor the size of the queue\nor the iterator age of the stream, because if it's growing\nand you're not processing items fast enough,\nthat can result in a problem for you.\nThen when it comes to Kinesis,\nit's important to make sure that you have a process\nthat keeps track of repeated failures.\n\n\nSo items are usually processed with Kinesis in order per shard.\nAnd if you want to know more about that,\nyou can look back at our previous Kinesis episode\nwhere we dived deep into it.\nAnd if you can't process an event in an ordered stream,\nAWS by default will retry indefinitely\nand basically block up the whole stream from being processed.\nSo that's something you'll need to configure and handle accordingly.\n\n\nAnd then another thing with things like Kinesis,\nyou can receive batches of like up to 10,000 events.\nSo if you receive batches of messages and one of them fails,\nyou don't necessarily want them all to fail.\nSo there's a few different ways to tell AWS\nwhich item succeeded and which ones failed.\nSo only the failed ones will need to be reprocessed.\nSo we have a whole set of articles around event processing,\nwhich we'll link in the show notes, and you can take a look there.\nSo those are our three different invocation modes.\nImportant to get those out of the way.\nMaybe now we could talk about best practices\nrelating to performance and cost.\n\n\nLuciano: Yeah, it's very important to understand, first of all, what is the formula to calculate cost when it comes to Lambda.\nAnd with many things in AWS,\nit's not always easy to predict cost with like extreme accuracy.\nBut in the case of Lambda, it's not that bad,\nmeaning that there is a simplified formula\nthat we can use to get a feeling for what the cost is going to look like.\nAnd this generally is a function of the memory\nthat you pick for your Lambda invocation\nand the execution time in milliseconds.\n\n\nSo that basically means that there is a price unit\nthat changes based on the amount of memory\nthat you allocate for your Lambda function.\nAnd then you have to multiply that unit price\nfor the number of milliseconds\nthat you have been invoking that particular function.\nAnd of course, if you have concurrent invocations,\nevery invocation amounts for its own cost independent.\nSo it's additional milliseconds that you need to multiply as well.\n\n\nNow, an interesting thing is that CPU is not part of this mix.\nNetwork is not part of this mix as well,\nbecause these other two units\nare automatically provisioned for you by AWS,\nand they are proportional by the amount of memory that you pick.\nAnd again, it's not always super streamlined\nto really understand how much CPU or what kind of networking speed\ndo you get.\nBut the general gist of it is that the more memory you provision,\neven if you don't necessarily need all of that memory,\nthe more CPU you get and the fastest the networking for your Lambda is.\n\n\nSo just be aware of these two things,\nbecause sometimes you might try to save cost\nby reducing the memory to the minimum,\nbut then you end up with very little CPU or very slow network.\nAnd that might end up making your Lambda spend so much time\ncompleting the computation,\nand it doesn't necessarily result in saving cost in any way.\nAnd you might even have degraded the user experience\nif you have a user on the other side waiting for things to happen.\n\n\nAnd this is where things get a little bit tricky.\nAnd thankfully, there is a tool that can help you out\nto figure out exactly what is the sweet spot\nbetween performance and cost.\nAnd this tool is called Lambda Power Tuning by Alex Casaboni.\nWe will have a link in the show notes.\nAnd what it does is basically going to simulate your specific Lambda.\nSo it's not going to do something very generic,\nbut it's actually using your Lambda code,\nand it's going to try different configurations in terms of memory\nand give you a nice chart where you can effectively see\nwith all the different configurations\nwhat is the time for computing an execution,\nand compared to that time and the amount of memory used,\nwhat is the cost for it.\n\n\nAnd at that point, you can decide for that particular Lambda function.\nMaybe you want to prioritize for cost,\nand it's fine to be maybe a little bit slower,\nbut you're going to save on cost.\nWhile with other Lambda functions,\nyou might want to optimize for performance,\nand it doesn't matter if you're going to be spending a lot more,\nbut you really want to have those milliseconds reduced to the maximum.\nSometimes there is a good way in the middle.\n\n\nSo the chart that you get at the end\nis definitely a very good way to figure out exactly\nwhat is the sweet spot for you for that particular use case.\nAnother thing worth mentioning, which is not necessarily a best practice,\nbecause again, it comes with lots of complexity,\nand it's not always a binary choice,\nis you might consider using a compiled runtime,\nas opposed to runtime such as Python, JavaScript, or Java.\n\n\nYou might want to use something like C++, Rust, or Go,\nbecause generally these runtimes can have very good performance\nin terms of cold start, in terms of execution times,\njust because those languages are more optimized for certain tasks.\nAnd this is especially true if you have CPU-intensive Lambdas.\nNow, where is a problem with that approach\nis that sometimes learning those languages\nis much more complicated than learning JavaScript or Node.js.\n\n\nSo the trade-off is a bit more on making an investment\nin terms of knowledge and maintenance costs,\nand then you might get benefits in the long term\nbecause your Lambdas might be more efficient.\nBut this is not always an easy choice to make.\nYou need to take care of making sure that in your team\nyou have people that can do that efficiently,\nthey have all the training available,\nand effectively you are introducing potentially new languages\nthat you might have to support long term.\n\n\nAnother key element to this conversation\nis that it also affects sustainability,\nbecause those combined languages\ngenerally have a much better carbon footprint.\nSo if it's something that you really care about,\nthis might be another element to bring into the equation\nwhen you decide to invest in these particular languages.\nAnd this is something that was mentioned by Werner Vogels\nin his keynote during the latest re-invent.\n\n\nSo again, something else worth considering,\nbut I wouldn't say it's an easy choice.\nEvery team generally might end up with different choices\ndepending on what they know already, what their skills are,\nhow much do they want to invest in learning new technologies.\nAnd sometimes you don't always have requirements\nof looking for extreme performance.\nSo yeah, up to teams to decide what to do,\nbut we would be really curious to know if this is something\nyou are considering for your team.\nI guess the next topic is how should people structure their Lambda code?\nThis is something we get asked a lot, to be honest.\nAnd again, maybe there isn't a one way of doing it,\nbut for sure there are some best practices\nthat I think we can recommend.\n\n\nEoin: Yeah, this is true.\nThere's definitely not one way to do it and one right answer, but it is a good idea\nto have some sort of layered architecture\nand make sure that the handler function itself\nideally only contains glue code.\nSo the layer that adapts the Lambda interface\nand the event source into the underlying implementation.\nSo you can pass it to a service function or a piece of domain logic,\nand then your handler is basically just serializing the input,\npassing it on and transforming the output\ninto the required response type.\n\n\nNow, this is going to make your code more testable\nbecause it allows you to test the service logic\nindependently from the Lambda function itself.\nThere's lots of ways of doing this. As we said, hexagonal architecture\nis one which you'll see mentioned quite a lot,\nespecially in the serverless space,\nbut there's other different ways of implementing,\nlike clean architecture, et cetera.\nThere's a blog post from Luca Metzelira on hexagonal architecture,\nwhich we'll link in the show notes,\nbut there's lots of resources out there on it.\n\n\nIf you want to test the whole Lambda function logic itself,\nyou can use integration tests then and end-to-end tests\nand we'd definitely recommend that because Lambda as a service itself\ncan introduce lots of interesting behavior,\nwhich you should cover in some sort of automated test\nand even performance and load tests as well.\nYou can also abstract dependencies like storage\nor interacting with other AWS services\nas part of an architecture like this,\nand then you can easily mock these abstractions\nor have secondary implementations you can use for local testing.\n\n\nNow, let's give a quick example.\nLet's say we've got an e-commerce shopping cart example\nand you want to add an item to a cart\nand you've got an API endpoint to achieve this.\nYou can define what the expected inputs and outputs are\nand the validation rules for this API endpoint.\nSo your input might be a cart ID, an item ID and a quantity.\nWith your validation, you can say,\nwell, the cart ID must exist in the database\nand be associated with an active user.\n\n\nThe item ID must exist in the database\nand it should be associated with a valid product\nand then a quantity should be a reasonable positive integer,\nmaybe below some arbitrary limit, depending on your context.\nIt's always good to have lower and upper bounds.\nAnd the output then would be an updated view of the cart\nwith the new items included perhaps.\nNow, this validation is all something\nthat you should be considering implementing in your Lambda Function\nor at some level in the stack, possibly multiple levels.\n\n\nThen also defining all the errors, right?\nNot just thinking about the happy path,\nbut what are the different errors\nand what is the information associated with those errors\nthat you want to leak outside your function.\nThen you can implement code to abstract the persistent layers,\nlike how you're storing the state of your user cart across requests.\nAnd this layer might have helper functions\nsuch as creating a cart, adding an item to a cart,\nremoving item from cart and maybe emptying the cart.\n\n\nSo that could be in a data access layer or repository.\nAnd then you implement the service that takes the inputs,\nlike the cart ID, the item ID, the quantity,\nand updates the given cart or returns the error.\nSo you would update the handler that takes the event,\nvalidates it, extracts the cart ID, item ID and quantity,\nand then pass it to the service.\nThen it would take the response from the service\nand convert it to the appropriate maybe HTTP response,\nif this is a HTTP API, and the handler would take care of working\nwith the Lambda proxy integration event and response format.\n\n\nNow, you don't necessarily always have to have all these layers of abstraction.\nSometimes it is okay to keep things simple,\nespecially if you've got something that doesn't do something very complex,\nas long as you've got some way of testing your code\nand you have confidence that it does exactly what it's supposed to do\nand that if you apply changes later, you can easily test it\nand you don't have to rework your tests completely.\n\n\nAnd you can also use a middleware library like Middy.js\nin Node.js, JavaScript, TypeScript, to abstract some of this logic,\nlike validation and serialization,\nand make it reusable and easily testable.\nAnd you can refer back to our previous episode on Middy\nto find out all about that.\nAnd again, this will be related to Power Tools,\nwhich maybe we should talk about next.\nThe Power Tools we mentioned already in the show\nin the context of idempotency and also middleware\nbecause it integrates with Middy, but even in Python,\nit will provide you that middleware support as well.\nWhat other things can Power Tools provide?\nI think we probably regard it as just a default best practice\nto just start off with Power Tools and functions these days.\n\n\nLuciano: Yes, I think before we get into the details, it's worth clarifying that Power Tools is effectively a library\nthat exists for different languages and specifically targets Lambda.\nAnd it tries to solve, I guess, some common problems\nthat you might have when writing Lambdas,\nand it tries to provide kind of a comprehensive,\nbetter-included solution that gives you tools to solve these problems.\nAnd the common things that Power Tools started with\nare logs, metrics, traces,\nbut we already mentioned also idempotency\nand different versions of the library,\nmeaning different languages might have more, I guess,\ncomplete support for different things than others.\n\n\nGenerally speaking, the Python one,\nbecause probably historically was the first one,\nis the most complete one,\nand the other ones tend to follow along\nafter a few months with the new features.\nSo definitely check out, depending on your language of choice,\nwhat are the available utilities,\nbut Power Tools is definitely something you should be using\nto the very least for logs, metrics, and traces.\nBut there is more, like idempotency that is absolutely useful,\nand if it's there for your runtime, you should definitely consider it.\n\n\nOther things that are there are, for instance,\nsupports for OpenAPI specification.\nFor instance, the Python one recently introduced\nlots of helper methods to make all of that process easier.\nNot really sure if that's already supported in Java\nor the Node.js equivalent,\nbut something that eventually is going to come up\nto those versions of the library as well.\nAnother thing that if you really care about metrics and alarms,\nyou might want to check out a tool we have already mentioned before\ncalled SlickWatch, which is an open source tool\nthat we created at Fortier to simplify,\nalmost automate the creation of sensible defaults\nwhen it comes to metrics and alarms and dashboards.\n\n\nSo worth checking it out because it can make your life easier,\nand it can effectively speed up the work around covering those areas\nfor at least 80% of the use cases.\nOther things that we already mentioned in terms of tooling\nis Lambda Power Tuning for performance.\nBut when it comes to all this topic,\nwe have a bunch of previous episodes\nwhere we cover details about how to do good logging,\nhow to use CloudWatch for logs, how to use CloudWatch alarms,\nhow to do metrics with CloudWatch.\n\n\nSo we'll have all the links for this episode in the show notes\nif this is a topic that you want to dive on\nand really understand the details.\nMoving on to another area,\nI'm just going to go through some quick suggestions\nand maybe going less in detail\ncompared to the previous areas we covered today,\nbut hopefully you still get some value\nin getting some quick suggestions on things\nto focus on when it comes to writing Lambdas.\n\n\nAnd one topic, for instance, is configuration,\nand can be a very big topic.\nThere are different ways to manage configuration when it comes to AWS.\nSo the quick suggestions we have on this one\nis when it comes to secrets,\ndon't store them in clear text as an environment variable,\nor even inside your code, that would be probably even worse.\nBut yeah, there are other ways to do that.\nFor instance, you can use Secrets Manager,\nyou can use SSM and have encrypted parameters.\n\n\nSo generally speaking, the recommendation there\nis when it comes to secrets,\nit can be very convenient to store them in plain text in your code\nor in environment variables,\nbut generally speaking, if you can avoid that, it's best to do so.\nAnd the other point is infrastructure as code.\nThis is a topic that also we cover extensively.\nSo the only recommendation we have here is use it.\nThere is no excuse not to use it.\n\n\nOf course, you can prefer different tools.\nThere is some, Terraform, CDK, Serverless Framework.\nWe have covered pretty much all of them in previous episodes,\nand we'll have the links in the show notes.\nBut the point is, regardless of which tool you prefer,\nyou should be doing infrastructure as code,\nbecause the advantages definitely outweigh the disadvantages,\nwhich might be maybe a little bit of learning curve,\nbecause you need to learn a new tool and become familiar with it.\n\n\nBut then there will be such great advantages\nthat definitely they're going to pay off big time,\nthat initial investment.\nSo if you haven't picked infrastructure as code yet,\ndefinitely put it at the top of your to-do list,\nbecause you're going to be grateful going forward.\nAnd for sure, the whole management of infrastructure\nis going to become so much easier and so much more reliable.\nThat is something that you will be thankful\nthat you have finally tackled.\n\n\nAnd finally, one last point is security.\nThis can be a massive topic on its own,\nbut the one quick tip that is particularly relevant for Lambda\nis to apply the principle of list knowledge.\nAnd the reason why I think this applies particularly for Lambda\nis because in Lambda, since you have such, generally speaking,\nat least such a small focus,\nlike generally Lambdas are like one purpose only,\nyou can really fine-tune the permissions for that one particular purpose.\n\n\nSo you might have, for instance, going back to our example\nof the adding item to the cart Lambda,\nyou might give that Lambda only the permission\nto write that particular item, maybe to a DynamoDB table.\nSo it can only add items to carts\nand not do anything else.\nSo if that Lambda gets compromised,\nit's not like an attacker can read passwords\nor maybe manipulate credit card detail.\nAn attacker will only be able to add items to a cart.\nSo, of course, not ideal anyway,\nbut the blast radius is very, very limited.\nSo this is why with Lambda, it's even more important\nto apply this principle because you can really fine-tune it to the maximum.\nAnd therefore, your application as a whole\nis going to become extremely secure,\nat least compared to more traditional and monolithic architecture\nwhere effectively your weakest spot\nbecomes the biggest vulnerability of the entire system.\n\n\nEoin: There are a lot of other areas you might pick when you're talking about best practices.\nWe didn't even cover things like deployment and dependency management\nor the hot topic of Lambdalith or monolithic Lambda functions\nversus single-purpose functions.\nLike maybe these are topics for a future episode,\nbut at the end of the day, a lot of these choices\njust come down to personal preference and context.\nSo for now, we'll just leave you with some extra resources to check out.\nSo look at the links in the description below\nfor AWS advice on Lambda best practices, which is worthwhile,\nbut also the great video series from Julian Wood,\nwho has 20 good videos on understanding Lambda,\nand I really recommend them for anyone\nwho's looking to fill any knowledge gaps there.\nSo thanks for listening, and until next time,\nwe'll see you in the next episode.\n"
    },
    {
      "title": "121. 5 Ways to extend CloudFormation",
      "url": "https://awsbites.com/121-5-ways-to-extend-cloudformation/",
      "publish_date": "2024-04-19T00:00:00.000Z",
      "abstract": "In this episode, we discuss 5 different ways to extend CloudFormation capabilities beyond what it natively supports. We started with a quick recap of what CloudFormation is and why we might need to extend it. We then covered using custom scripts and templating engines, which can be effective but require extra maintenance. We recommended relying instead on tools like Serverless Framework, SAM, and CDK which generate CloudFormation templates but provide abstractions and syntax improvements. When you need custom resources, CloudFormation macros allow pre-processing templates, while custom resources and the CloudFormation registry allow defining new resource types. We summarized recommendations for when to use each approach based on our experience. Overall, we covered multiple options for extending CloudFormation to support more complex infrastructure needs.\n\nAWS Bites is brought to you by fourTheorem, an AWS Partner that specialises in modern application architecture and migration. If you are curious to find out more and to work with us, check us out on fourtheorem.com!\n\nIn this episode, we mentioned the following resources.\n\nEp. 31 &quot;Cloudformation or Terraform&quot;\nServerless Framework\nSAM (Serverless Application Model)\nCDK (Cloud Development Kit)\nEp. 119 &quot;The state of AWS 2024 (AnsWeRS community survey commentary)&quot;\nEp. 93 &quot;CDK Patterns - The Good, The Bad and The Ugly&quot;\nSLIC Watch\nAWS SSO Utils by Ben Kehoe\nJavaScript library to safely create custom CloudFormaion resources\nClouformation CLI\nCloudformation CLI docs\nCloudonaut podcast &quot;3½ ways to workaround missing CloudFormation support&quot;\n\n",
      "transcript": "Luciano: CloudFormation allows us to define stacks,\nwhich is essentially a way to define collections of resources,\nand that's done using YAML or JSON in the forms of templates.\nThis is the AWS way of helping us to manage our infrastructure\nfrom the creation to updating and deleting resources as our application evolves.\nBut sometimes CloudFormation building capabilities just aren't enough for what we need to do.\n\n\nMaybe you find yourself wrestling with verbal syntax,\nor perhaps you need to provision resources that are not yet supported by AWS,\nor maybe resources that are outside the scope of AWS, maybe by third-party providers.\nOr you might need, for instance, to perform certain tasks before or after CloudFormation runs.\nMaybe you need to build some kind of application, maybe a front-end,\nand package it in such a way that then you can deploy to AWS as part of your own stack.\n\n\nSo today we will explore different ways to extend CloudFormation capabilities.\nWe will cover things like custom scripts, templating engines,\nhigher-level tools such as CDK, SAM, and the serverless framework,\nbut we will also cover CloudFormation macros and custom resources.\nWe will cover different use cases and the pros and cons of every approach.\nSo my name is Luciano, and together with Eoin,\nwe are here for another episode of AWS Bites podcast.\nAWS Bites is brought to you by fourTheorem, an AWS partner that specializes in modern\napplication architecture and migration. If you're curious to find out more and to work with us,\ncheck us out on fourtheorem.com. So let's start maybe by giving a quick recap of what\nCloudFormation is and why we might need to extend it. Eoin, do you want to cover that?\nSure. Yeah.\n\n\nEoin: Back in episode 31, we did a battle episode talking about CloudFormation versus Terraform. It's worth going back to a look at that one. It's one of the most popular\nepisodes even still. And in that, we talked a little bit about CloudFormation and how you\nmight extend it, but we're going to dive a little bit deeper today. So CloudFormation is the native\ninfrastructure as cloud solution from AWS. Like you said, Luciano, you get templates that allow\nyou to define stacks, which are just a collection of resources. You use YAML or JSON, and then you\ncan deploy them to AWS. AWS's responsibility then is to manage the state of those resources\nwhile they're provisioning the dependency between them and to detect failures, retry failures,\nand then roll back if anything goes wrong and try to keep your cloud in a consistent state. That's\nreally the goal. And it does its job pretty well. There are some cases, of course, where it might be\na bit limited or it can be inconvenient. So you do need to build from time to time alternative\nsolutions on top of it. I think we've been in this case quite a lot. We can let you know in a bit\nwhat we tend to go for. So CloudFormation does do its job pretty well, but there are some cases\nwhere you might need to customize it or extend it just because there are different situations you\nfind yourselves in. So one such thing is you just find that the syntax might be very verbose.\nSo you might want to write something a bit more high level, higher level component, if you will,\nCloudFormation that allows you to abstract that and have a modular reusable component.\nThen again, you might also just want to provision resources that aren't even supported yet by AWS.\nThis happens less these days, but it still does happen. One example I know you've come across\nrecently, Luciana, is Amazon Q Business. It's in preview, but there is no CloudFormation support\nyet. And then there are resources outside the realm of AWS, like you might be using a third\nparty vendor like Auth0 or SupaBase or something else. And you want to make sure that all your\nresources are part of the same stack for consistent updates and deployments and for making it easy to\nreference resources from one stack to another. Now you might need to do things before or after\nCloudFormation runs. So that's another use case for customizing it. So you might want to build\na front end application and copy all of the assets into an S3 bucket, or you might want to pre-process\nassets like optimizing pictures, building container images, something that normally\nfalls outside the realm of provisioning infrastructure, but it does become part of\nyour deployment. Or you might just want to fetch configuration that you might need as a parameter\nfor your stacks. So how do we get started? What's the first thing you came across as a solution to\nthis, Luciana?\n\n\nLuciano: Yeah, this is something that I came across a few years ago, I guess, when I started my Cloud journey, it was quite common for people to write their own wrapper scripts and sometimes even\nuse templating engines. And the idea is that you will not run the CloudFormation CLI directly for\ndeployments, but instead you will run your own custom script that will do many things and at\nsome point also run CloudFormation for you. And the idea is that you might or might not use a\npre-made template because sometimes you might create your own simplified templates, so to speak.\n\n\nAnd then as part of your script, you will use a templating engine, like something like Jinja,\nfor example, to generate additional pieces or generate entirely your own actual CloudFormation\ntemplate and then call the CloudFormation CLI to deploy that template. This was very common,\nfor instance, because for a long time, CloudFormation didn't even have a concept of\nfor loops. So for instance, if you had to provision, I don't know, 20 lambdas that were very,\nvery similar with each other, it would be very annoying to just do all of that copy paste.\n\n\nSo you would probably find yourself creating some kind of simpler higher level configuration and\nthen use something like Jinja to generate the actual underlying CloudFormation code for every\nsingle lambda. And you might also use that approach to run other kinds of commands, for instance, to,\nas we said, build containers maybe before you try to deploy the template that references that\ncontainer on a specific registry. So you can build and publish it as part of that script.\n\n\nAnd maybe you can also do things afterwards, maybe, I don't know, indicate somewhere, maybe\nin a chat message that a deployment was completed or the status of that deployment, all sorts of\nintegrations like that. It was something very common for people to do these kinds of things\nback in the days. Now, is this a good practice? It can be very effective, but I would personally\ndiscourage it because the problem is that you will end up with very custom code that you'll\nneed to maintain over time and that every new engineer in the team will need to get comfortable\nwith. And also it will be very different from company to company because everyone is effectively\ncreating their own solution from scratch. And writing this code in a reliable way can also get\nvery complicated very quick. For instance, you just need to try to imagine that the more steps you have,\nthe more opportunities for things to go wrong there are, and you have to think, okay, what do\nI do if something goes wrong at this step? How do I clean up? How do I make sure that my entire\ndeployment ends up in a consistent state? And again, this is not something very easy to do\nwell. So you might end up with lots of trial and errors before you have something stable.\nAnd in between, you might end up with a few broken deployments, which is not a great situation to be\nin. So there are today lots of better ways to achieve most of the same things we just discussed\nhere without having to struggle with creating your own custom scripts. So I think the main one\nis to use new tools that came in the industry in the last few years. So which ones come to mind\nfirst to you, Eoin?\n\n\nEoin: Yeah, there's a whole suite of tools that are essentially CloudFormation generators, and they all have their pros and cons. One of the first ones people might call to mind is the\nServerless Framework. It's a third party open source, primarily. I know that the latest version\ndoes have a commercial license if you're a company of a certain size, but the Serverless Framework\ngives you a simplified YAML syntax that will eventually be converted to a CloudFormation\ntemplate. Now, Serverless Framework supports other clouds and not just AWS, but AWS is,\nI would imagine, the biggest and most used cloud with Serverless Framework.\n\n\nThe Serverless Framework YAML essentially becomes a superset of CloudFormation where you can define\nnormal CloudFormation in there, but you can also provide this concise syntax for things like\nServerless functions and the triggers for those functions. That's really where it shines.\nOne of the notable things about it is that it has a really wide and rich plugin system for all sorts\nof use cases. So you can extend it with all sorts of plugins that can make the job of generating\nCloudFormation even easier. And more recently, CDK, and we talked about this recently in the\nstate of JavaScript, sorry, not state of JavaScript, state of Serverless survey results,\nCDK is getting really popular and it allows you to define infrastructure as code using the program\nlanguage of your choice, TypeScript, Java, Python, .NET, and Golang. And it makes it easy then to\ncreate higher level abstractions. There's a whole CDK episode we did where we talked about the\ndifferent levels of constructs. And at the basis of that, you have essentially types in the language\nof your choice that generate just the raw CloudFormation resources. Anything that's a\nhigher level construct is just built on top of that. So it generates CloudFormation templates,\nbut makes it easier for programmers who are skilled in these languages to make it reusable,\nmodular, and extensible. Now, if we go back to YAML land, we have the Serverless Application\nModel or SAM, and it is similar to Serverless Framework in principle and use cases. I would\nimagine it's inspired by it in some way, but it is an official AWS solution and competes with\nServerless Framework. It is slightly different in that it doesn't have that plugin ecosystem. In\nfact, it's much more strict when it comes to the syntax, but the way it works is by having its own\ntemplate language that looks like a superset of raw CloudFormation, but offering those interesting\nshortcuts. And its magic is done by using CloudFormation macros, which is one of the ways\nof extending CloudFormation that we're going to discuss in a moment. Now, I mentioned a few\nprevious episodes that are relevant. Don't worry, all the links to those will be in the description\nbelow. So we talked about CloudFormation macros, Luciano, do you want to take us through those?\n\n\nLuciano: So CloudFormation macros are effectively, you can imagine them as a function that can be executed\nand takes as an input your entire template and returns as an output an extended version of that\ntemplate. And the idea is that you can define macros somewhere, register it into your own\naccount, and then you need to reference them at the beginning of your own template. So you generally\nhave an annotation called transform with the name of a specific macro, and that tells CloudFormation\nthat that template needs to be transformed from using that macro before the deployment.\n\n\nAnd you can use this concept to do all sorts of different things, for instance, automate tasks,\nenforce policies, or even just create new resources, for instance, as part of expanding\nthat template. So what are they good for? We recently used the CloudFormation macros for\na project called SlickWatch. We will have the link in the show notes. And the idea of SlickWatch is\nthat, for instance, you might want to have best practice generated dashboards and alarms,\ndepending on the resources that you use on your template. So you can basically say,\nif I'm using an SQS queue, for instance, in my template, this macro will automatically generate\ndashboards and alarms for you to monitor the most common things that you would be worried about\nwhen using something like an SQS queue. And of course, you can extend that concept to different\nresources. And it solves lots of the boilerplate that you generally have to do when it comes to\nmonitoring for specific resources. So this is just an example of when a CloudFormation macro\ncan be very convenient. Take a simple template as an input, see what's inside, and do something\nuseful and produce maybe a slightly enhanced version of that template. And we already mentioned\nhow it generally works, but the idea is that you will need to write that code somewhere. And\ngenerally, you can write it as a Lambda function. Then you need to publish this Lambda function,\nregister it as a CloudFormation macro, give it a name. And from that moment on, you can reference\nthat transform into your own templates before deploying in this specific account where you\nprovision the macro. So you, of course, you need to make sure the macro is provisioned before you\nuse it. If you are, for instance, deploying a third-party macro from maybe an open source\nproject like SlickWatch. But then once you have done that thing one-off, you can use that transform\nin all sorts of projects that you are deploying in that particular account.\nNow, there might be some small problems with it. For instance, one, as I said, is that you\nneed to make sure things are provisioned before you can actually use it. And the other one is that\nif you have issues, for instance, as you write your transform, it's not always very straightforward to\ndebug exactly where the issue is because effectively you are taking a template that\nmight be very, very big and very involved in terms of properties. So you need to write generally very\ncomplex code that can parse the original template, make sure you really understand the semantic of\nthe template, and then you change things in that template so there is a lot that can go wrong in\ndoing all these different things. And we found ourselves that when building SlickWatch, there is\na lot of debugging that goes on. Of course, you can recreate locally different versions of the\ntemplates, run it locally, see what is the output of the template, but you still need sometimes to\ndeploy the template to make sure it is semantically correct and is doing exactly the thing that you\nwant it to do. So there might be lots of kind of development cycles before you get to a point where\nyou are happy with the outcome of that transform macro. There is actually another good example that\nwe bumped into multiple times and we might have mentioned it in previous episodes, is a macro\ncalled AWS SSO Util by Ben Kiyoi. And it's very convenient because it gives you higher level\nsyntax to do things that might get very, very verbose if you need to define the low-level\nresources that you generally need for SSO type of operations. So another one we will have a link in\nthe show notes, and it might be a good one to check out if you are just curious to see what it\ntakes to build a cloud formation macro. What else does come to mind, Eoin?\nWe should definitely talk about custom resources.\n\n\nEoin: I think this is the one we've ended up using the most, probably because they're pretty quick and relatively easy to implement compared to all the\nother methods. There can be a few foot guns with the custom resources method, so we should talk\nabout that. There's two ways of implementing them. You can do it with a Lambda function,\nor you can actually do it through SNS where you receive a notification and you can actually trigger\nlogic anywhere, like on an EC2 instance. And when you want to create a custom resource,\nit's fairly straightforward. You just basically in your template, you define a type and that could be\nanything like custom colon colon whatever name you specify, or you can just give us the static\nversion, which is like AWS CloudFormation custom resource. If you use the custom version,\njust be aware that you can't have multiple double colon separators for more complex spacing. You\ncan't have any more nesting. If you tried to do that, your stack will hang forever until you\ncancel the deployment and delete the generated change sets. So ask us how we know that one.\n\n\nYou can also specify custom properties. So you can pass custom properties into your\ncustom resource so that the logic that handles it can read them. And then you just need to specify\nwhat's handling the creation, updation, and deletion of the custom resource. So that's\ngoing to be a reference to a Lambda function ARN or an SNS topic, and you pass those in\na field called service token. So that's the only mandatory property for a custom resource. So\nthere's not a lot of code involved. Let's say you're using the Lambda methods then.\n\n\nWhen you deploy a stack with one of these custom resources in it, CloudFormation is going to\ninvoke that Lambda function and pass in a few properties. Important one would be the request\ntype, which tells whether the action you should perform is a create an update or delete. You just\nneed to essentially look at those, figure out what needs to be done. And like if you're creating\na new resource, return a physical resource ID. If it's an update, get the physical resource ID,\ncheck for old properties versus new properties, do the update, and then return the physical resource\nID. Now, when you're doing an update, it doesn't actually have to be the same resource ID you\noriginally received. If you provide a new one, then it's considered a resource replacement,\nwhich means that CloudFormation will call the delete action with the old one.\n\n\nIf you're really handling that proper life cycle of create, update, and delete,\nthat's some of the things you'll need to watch out for. I've seen quite a few naive and simple\nversions of custom resources, which essentially just have an idempotent action that happens when\na create or update occurs. And that I suppose simplifies the implementation. And you don't have\nto worry too much about detecting which properties have changed. When your function is invoked,\nit will also pass in a response URL. You can call that to tell CloudFormation that you're finished\nand your update or delete or whatever has succeeded or failed. It's a little bit more\ninvolved than just returning a value from your Lambda function handler. You need to actually\nmake a request to that specific URL. There are some libraries that can help you to do this\ncorrectly and safely. There's a few things you need to be aware of. First of all, your Lambda\nfunction might actually time out before it's able to send the response, which CloudFormation will\nnever get the response and it will take an hour to time out, which I've been through multiple times\nand it's not fun. Another thing is if an error happens in your import code, like in your module\ninitialization code, and you're not catching that and sending back a CloudFormation response in the\nhandler, then that can also cause this one hour hang. So you have to be really careful. Node.js\non NPM, there are a few modules we can link in the show notes that make it a bit easier to safely\nimplement CloudFormation customer sources and catch all of these errors and make sure that\nyou always send a response before any possible timeout. Now, the good thing about these customer\nsources is that they're probably the easiest ones to start with. We use them quite often.\nRecently, we were using them to run database schema migrations when you deploy an application\nso that you make sure you always deploy it with the application stack. And that makes a lot of\nsense for us and it was a good fit. That's very easy to use for self-contained resources\nthat you need just for one project. We mentioned the problems with timeouts. You do have to be\nreally careful. It's a good idea to deploy your code, test it outside of CloudFormation environment\nand really validate that it works before you try it inside CloudFormation. It's missing things like\ndrift detection, resource import. It's difficult to share it across multiple projects. And I suppose\nas well, you're using a Lambda function. These resources are then running in your own account.\nSo you need to make sure you've got the right networking and IAM permissions.\nIf you're running that Lambda function in private PC subnets, you need to make sure that you set up\nthe VPAC endpoints to interact back with CloudFormation with the response. Another gotcha\nthere is that the response URL actually comes through as an S3 pre-signed URL. So you need to\nmake sure then in that case, you've got a S3 gateway endpoint or interface endpoint.\nAnd if you don't do that, you're just going to hang and it was going to take a while for it to\nfail. And then sometimes try and roll it back and it'll roll back by trying to invoke the same\nfunction again, which is going to timeout again, which is going to take you another hour. So\nCloudFormation custom resources can be great, but when they don't work, they really induce rage.\nSo when it comes, is there anything we could do to mitigate the risk of rage here, Luciano?\nWhat else have we got?\n\n\nLuciano: Another alternative is CloudFormation Registry, which seems to be kind of the evolution of custom resources in many ways.\nIt's a recent development in AWS. And I have to be honest, before we cover this particular point,\nthis is the one we have used the least. So we will only cover it at high level and try to\nmention the differences with customer resources. But I don't think we have the level of experience\nto tell all the pain points and all the foot guns that I'm sure that there will be some of\nthem somewhere, even with the CloudFormation registry. So let's get into it. As I said,\nthere's a new way of doing effectively the same thing you do with custom resources,\naddresses the same use cases in many ways. But the idea is that rather than having just a lambda,\nthat it is part of your own stack, it's a little bit more like a CloudFormation macro,\nmeaning that you can register that custom resource at the account level, and then you can reference\nit in other templates that you are going to deploy in a specific account.\n\n\nAnd this is where the idea of the registry comes from. And you can even do resources that are\npublicly available. So that's something that can be very convenient, for instance, when you are\na provider of some sort, like a third party provider providing a service and you want to\nmake it easy for people to access those custom resources and install them in their own accounts\nwithout having to rewrite something themselves or having to download code and then run scripts\nto provision it inside their own AWS accounts. So this is one of the main advantages,\nhaving this kind of registry that allows you to easily make your custom resources available,\neither internally in your own company across multiple accounts, or even as a third party\nprovider to make it available to multiple accounts for your own customers. There is also a little bit\nmore. So we say that for custom resources, you have this concept of create, update, and delete.\n\n\nIn the registry, this has been extended to other additional operations. So there is, for instance,\na concept of read that allows you to see the state of a particular custom resource,\nbut also a concept of list that allows you to list all the resources of a given type\nand see exactly what is their own state. And that gives additional capabilities to\nCloudFormation. For instance, this is why this particular approach supports drift detection,\nbecause they can inspect at any point in time what is the state of a given custom resource,\nand then how does it compare with what you have in a specific template that you have provisioned\nbefore. So you might be wondering, how do you create your own first registry custom resource?\n\n\nThere is a CLI that you can use as an helper. It's called the CloudFormation CLI. We will have the\nlink in the show notes. And this CLI has a bunch of commands that you can run. And the first one\nyou probably want to run allows you to scaffold a new project. And you can pick different languages.\nFor instance, TypeScript is one of the supported languages. And when you do that, it's going to\ngenerate actually quite a bit of code for you that effectively is a skeleton that you can use\nto start building the logic. And the first thing that you will probably want to write\nis a JSON schema that defines all the properties that you want to accept as part of that custom\nresource. And because it's a JSON schema, it's not just a list of the properties,\nbut also the validation rules that you might want to enforce for every single property.\n\n\nAnd this is great because at this point, when CloudFormation deploys these resources,\nit can validate upfront whether as a user you are using the resource correctly. So compared to the\nother approach with custom resources, you will have to validate that at runtime. So for instance,\nin your own Lambda code and just fail the deployment of that particular resource if\nsomething doesn't look right from the user input. So this is kind of a way to catch errors a little\nbit early in the process and probably gives you a better user experience in terms of if something is\nwrong, it's going to fail very quickly. You can fix the problem and retry. So at this point,\nafter you define the schema, you can run other commands, if I remember correctly, the process,\nand those commands might generate a little bit more code like models that you can use\nin your code to effectively work with the properties provided by the user.\n\n\nBut then of course, at some point, you need to define exactly what is that creation,\nread, update, deletion, list, logic that you need to perform depending on the kind of resource you\nare working with. For instance, if you are trying to implement something that is like a third party\nprovider for some service, you probably need to use an API or an SDK provided by this third party\nto interact with the resources that live outside AWS and implement all these steps, create, read,\nupdate, delete, and list. So in the time that you will have placeholder where you can do all of\nthese things, and then when you feel that you are ready, you can call another command from the CLI\ncalled CFS submit, which is effectively going to package all this code together, ship it to AWS,\nand then make your resource available through the registry. At this point, that resource will have\na name and you can easily reference it inside your own templates. What are some of the advantages?\n\n\nWe already mentioned that there are extra features like diff detection. You can import resources as\nwell. Another interesting detail is that the code is executed by AWS for you. So you don't have to\nworry as much as you have to do with custom resources. Think about, okay, where is this\nLambda going to run? Did I provision enough memory or I don't know, timeout? Is that timeout\ncorrect? Is it going to be sufficient for what they need to do? Or think about, I don't know,\nnetworking restrictions, all that kind of stuff. You have less concerns because AWS is going to\nrun the code for you. Of course, you still need to provide permissions in some way. And the way\nyou do that is by providing a role that AWS is going to assume for you and that role constraints\nwhat can happen inside the custom logic. So you don't have to be worried in terms of,\nthis custom resource is going to create a massive EC2 instance that is going to cost me lots of\nmoney every month. You can restrict exactly what the role is and what can happen inside that\nparticular execution. There are other advanced features. For instance, there is a concept of\nhooks. And then another advantage is that AWS Config will automatically list all the custom\nresources that are created through the registry. So it's very convenient that if you just want to\nbe reassured at any point in time, if you have resources coming from the registry,\nyou want to see what those are, you can easily see a list of them, even across multiple stacks.\nWe will have a link in the show notes with all the documentation that you need to follow if you want\nto implement something like this. Of course, the disadvantage is that this process feels a little\nbit more involved. So something you need to keep in mind as opposed to just creating custom resources\nfor simple use cases probably is still simpler to use custom resources. For more advanced use cases,\nmaybe where you need to make those resources available in an easier way, probably going with\nthe registry is a better approach.\n\n\nEoin: Yeah, I was just thinking there. So I remember back a while, there was an announcement from AWS about this AWS Cloud Control API. And the idea was that they\nwould provide this new API that was like create, read, update, delete, and list for all the\nresources. And that CloudFormation was going to be linked to it, but that it would also allow other\nproviders like Terraform to quickly get access to new AWS resources without having to do all\nthis stuff. I think I haven't heard that much about it, but I know that HashiCorp released\nlike a new provider that was based on this Cloud Control API. So I was just wondering there as you\nwere speaking, is it possible if you publish your CloudFormation provider, the resource provider in\nthe registry, that it would be automatically supported then in Terraform if you use that\nprovider? I don't know the answer to it, but I'm just wondering if that's some neat side benefit\nyou might get by using this method. Yeah, I don't know the answer either.\n\n\nLuciano: So we'll bounce it back to our listeners. If you have done something like this, let us know in the comments, what was your\nexperience? But sounds reasonable to assume that it is either something you can do straight away,\nor maybe that it is easy enough to auto-generate a provider at the Terraform level to do something\nlike that from a custom reason in the registry.\n\n\nEoin: If you haven't seen the public registry, you can go into CloudFormation console and have a look at all the third-party extensions. AWS has some in\nthere for higher level components, but you have like MongoDB, Atlassian, Sneak, Okta, Snowflake\nresources in there. People assume, probably for the most case correctly, that if you're using\nCloudFormation, it's just for AWS resources. But with this method, it doesn't have to be that way.\nAnd it also means that if you've got a vendor and they don't support CloudFormation for\ninfrastructure as code, you might actually point them in the direction of this episode\nand the documentation and tell them, get on it. Yeah, that's absolutely a very good point.\n\n\nLuciano: So let's try to recap what are our final recommendation based on all the different\nmethods we suggested. I will remark my suggestion not to use custom scripts or templates unless you\nreally, really have to, maybe because we have a legacy application and you don't have time to\nrewrite that. But if you are building something new, probably there are better ways to do the\nthings that you need to do around CloudFormation. And you can definitely use CDK, SAM or Server\nFramework. They are great to have that kind of higher level experience, better tooling in general,\nbetter syntax, easier to extend and reuse code in many different ways.\n\n\nSo definitely rely on these tools rather than writing CloudFormation from scratch,\nwhich is probably great for simple use cases. But as soon as you start to build real applications,\nthose tools will really shine and give you lots of additional benefits that you don't get with\nraw CloudFormation. When you need to create custom resources or you need to somehow extend the code\ninside the template, there are a few different things you can do. We spoke about macros. Macros\nare great if you want to effectively pre-process a template that is about to be deployed. So you\ncan get the entire template as an input. You can produce an entirely new template as an output.\n\n\nAnd generally speaking, you might be adding a few things. Maybe, I don't know, you can automatically\ntag all of the resources based on some internal rules, or you can do more advanced things like\nthe ones we did with SlickWatch for simplifying the efforts of making applications very easily\nobservable and to have alarms. And the other two things that we mentioned are CloudFormation\ncustom resources and the CloudFormation registry. Those are great whenever you want to actually\ncreate the concept of a new resource, either something that doesn't exist yet in AWS, because\nmaybe it's something in preview. So you have maybe an SDK, but you don't necessarily have the\ncorresponding resources in CloudFormation. You might be doing your own custom resources to back\nfield that and still be able to do infrastructure as code. There are some limitations that we\nmentioned with custom resources and some gotchas, so be aware of those. But they are generally\nreally good when you have something self-contained. If you have something that instead you plan to\nreuse across multiple applications or even make available externally, then you should be looking\ninto the registry because that seems to be a much more complete solution and something that is easier\nto share even outside the boundaries of your own company. So all of that brings us to the end of\nthis episode. I hope you find it informative. One last thing that I want to mention, I want to give\ncredit to the Cloudonaut guys. There is a very good podcast that they did, I think a couple of years ago,\nbut I think it's still very relevant. They cover some of the topics we discussed today, and they\nalso mentioned some examples, some use cases that they had. So if you enjoyed this particular episode\nand you want to find out more, check out that episode. The link will be in the show notes. As always,\nif you found this useful, please share it with your friends, your colleagues, and leave us a comment.\nLet us know what you did like, if you have any question, and what else you would like us to cover\nnext. So that's all. Thank you very much for being with us, and we will see you in the next one.\n"
    },
    {
      "title": "122. Amazing Databases with Aurora",
      "url": "https://awsbites.com/122-amazing-databases-with-aurora/",
      "publish_date": "2024-05-03T00:00:00.000Z",
      "abstract": "In this episode, we provide an overview of Amazon Aurora, a relational database solution on AWS. We discuss its unique capabilities like distinct storage architecture for better performance and faster recovery. We cover concepts like Aurora clusters, reader and writer instances, endpoints, and global databases. We also compare the serverless versions V1 and V2, noting that V2 is more enterprise-ready while V1 scales to zero. We touch on billing and additional features like the data API, RDS query editor, and RDS proxy. Overall, Aurora is powerful and scalable but not trivial to use at global scale. It's best for serious enterprise use cases or variable traffic workloads.\n\nAWS Bites is brought to you by fourTheorem. If you need someone to work with you to build the best-designed, highly available database on AWS, give us a shout. Check us out on fourtheorem.com!\n\nIn this episode, we mentioned the following resources.\n\nAmazon Relational Database Service (RDS)\nAmazon RDS Aurora\nRDS Aurora Serverless\nComparison of Aurora cost (Cost calculator)\nCloudonaut podcast and blog post about Aurora Serverless V2\n\n",
      "transcript": "Eoin: Are you looking for an easy way to set up a relational database with best practices,\nresilience and disaster recovery in mind? Are you maybe looking for something reliable but also\ncheap and easy to maintain? Today we're going to try and answer the question, does such a thing\neven exist? The best hope for an AWS solution to this challenge is Amazon Aurora. We're going to\ndive into Aurora, talk about its unique capabilities, intricacies and of course, trade-offs.\nI'm Eoin, I'm joined by Luciano and this is AWS Bites.\nAWS Bites is brought to you by fourTheorem. If you need someone to work with you to build the\nbest designed, highly available database on AWS, give us a shout. Check out fourtheorem.com or contact\nus directly using the links in the show notes. Luciano, before we dive into Aurora,\nwhat's the use case? Maybe let's try and imagine a scenario where Aurora might be a good fit.\n\n\nLuciano: Yeah, I think it's always good to try to put a frame to the conversation by defining a good use case. Here we are talking about enterprise applications, so not really like hobbyist type\nof databases. So that's what it means. It means basically you might have hundreds or even thousands\nof data users connected to the database. Volume of transactions that can vary, they can be\nrelatively low, but they can grow as needed because as a business, you might have your own\nspikes and you need to figure out how to manage all of that. Generally speaking, we are talking\nabout really important business data, so something we can consider critical. So something you really\ndon't want to lose. And we are expecting low RPO and RTO. So RPO probably in terms of minutes,\nRTO in terms of maybe one hour. And of course, something that needs to be replicated across\navailability zones and possibly have regional failure. So this is a very common use case,\nbut it still considers something really, really hard to achieve. So somewhat of a pain point if\nyou find yourself having to set up this kind of databases. So we are wondering today if there is\na way that can make us happy and make all these things easy for you and give us low overhead\nmanaged database, high availability, multi-region, fast recovery, something that is relatively secure,\nalso as cheap as possible. That's always something nice to have and somewhat developer-friendly.\n\n\nSo is this something we can achieve on AWS? If yes, what kind of services we should be looking at\nand try to analyze and decide if they really fit our description. And the first service that comes\nto mind is Aurora. So what is Aurora? So if you're looking for a relational database on AWS,\nyou have a few options. You're probably aware of RDS, which is a service that allows you to\ncreate relational databases and supports a big number of databases, including SQL Server,\nOracle, but also open source ones like Postgres, MariaDB, MySQL. And the idea is that they give\nyou a service that is somewhat managed and it somehow removes some of the pain points of having\nto manage databases, but it's still running on EC2 instances. So it does abstract some things,\nbut there is still a bit of the pain point of having to manage servers.\nSo with RDS, you also have another category of databases, which is the one called Aurora.\nAnd Aurora promises to deliver MySQL and Postgres compatibility with performance three times faster\nthan the regular one that you find in the regular MySQL and Postgres RDS. On top of that, Aurora has\nsome different characteristics that are only possible because effectively AWS reinvented this\nkind of databases. It's a service that they somewhat recreated, provided the compatibility\nwith the protocols of MySQL and Postgres. So they figured out clever ways to optimize the service\nmore than what you would get with the open source alternatives. So let's try to dive into the\ndetails. Maybe we should talk first about the storage, because I think that's the first thing\nthat comes to mind when we try to think why Aurora is different from just a simple MySQL\nor Postgres.\n\n\nEoin: Yeah, definitely the most important thing to know about Aurora is that its storage is different to RDS and to most other databases out there. You mentioned three times faster\nperformance in some benchmarks. That ultimately comes down from the way they have designed Aurora\nstorage. I mean, the engines themselves are using the open source MySQL and Postgres engines on top\nof this new storage layer, but because of the way it's been architected, they've been able to reduce\nthe number of writes that those database engines have to do and achieve that better performance.\n\n\nNormally when you configure a database on a server, you configure the database running as\na process or a set of processes, and then you have the storage, which could be attached in some way.\nWith Aurora, it's a bit different in that the storage is completely separate from the compute\nlayer and it uses its own magic to give you that better performance, as well as more durability\nand faster recovery, which if you're an enterprise, all of those things sound like a dream.\n\n\nNow you can think of Aurora storage, I think, as an intelligent kind of EBS volume layer\nthat is database transaction aware. So all of the data you store is automatically stored in six\ncopies across three availability zones by default. So that automatically gives you great resilience.\nAnd then you have asynchronous replication processes that happen outside of the database\ncompute. So normally when you have other databases, you have to configure the engines\nthemselves to do the replication between the compute notes. With Aurora storage, it happens\nat the storage layer, so you don't have that compute level replication. So when your data\nis replicated, it doesn't actually affect the instance performance, which is a really good thing.\n\n\nAnd then because it's being written all the time, asynchronously, recovery time is really fast\nbecause the data is not affected by instances going down. The data is already replicated by\ndesign. And you can then add new instances to a database cluster that give you horizontal\nscalability using that existing storage layer. The other thing to know about Aurora storage is\nthat it scales automatically, so you don't have to provision storage capacity upfront.\nIt just grows automatically in increments of 10 gigabytes up to a maximum of 128 terabytes.\nSo let's try and understand all this a bit better. And we might take a look at, I suppose,\na few Aurora concepts and constructs and how you might architect a database based on Aurora.\nLuciano, would you like to take us through some of the terminology and concepts in Aurora?\nYeah, the first one that comes to mind is probably the concept of a cluster.\n\n\nLuciano: And this is something that is already different from a more traditional RDS database. So the first thing you need to create\nis this idea that your database exists in a cluster, you need to create this cluster.\nAnd this represents the storage layer. So even if you don't have any database, meaning the compute\npart of a database in this cluster, the storage exists as a kind of baseline. And of course,\nyou'll want at some point to add at least one instance to make it useful because just the\nstorage alone is not going to allow you to do any query or any actual operation. It's just there to\nkeep your data safe. So every cluster can have one writer instance that allows you to effectively\nunder-read and write requests, but you cannot add more than one writer. So you can have only one\nwriter per cluster. So that means that you can only scale writes vertically by using bigger\nwrite instance if you end up having necessity for a bigger write throughput. You can add though\nup to 15 reader instances. So you can definitely scale horizontally the number of reader instances.\n\n\nSo we can also call them read replica if that term is a little bit more familiar to you. And that's\nsomething that allows you to handle read scalability. So you can easily spin up more\ninstances if you see that you need more throughput in terms of read. There are ways to do auto\nscaling. So you can set up auto scaling policies, and this allows you to basically look for things\nlike CPU utilization or number of connections to provision new reader nodes as you see that\nyour traffic increases. And of course, you can also scale it down if you see that that traffic\ndecreases. So each instance in the cluster will have its own endpoint. So this is the idea that\nyou need to have a way to connect to the specific instances. So every instance has its own kind of\naddress that allows you to connect to it. But there is also a concept of a cluster\nread and write endpoints. And these endpoints are kind of the preferred way to connect to\na database because they will automatically do all the routing for you, figuring out which instance\nis the most appropriate to handle that particular request. And this is important because, for\ninstance, as you scale up and down, or maybe if there is some failover in your cluster, that layer,\nthat kind of global endpoint will know exactly what to do to make sure that your request gets\nanswered correctly. If you manage your own connection directly to the specific instances,\nthen doing all of that stuff is on you. And of course, that's not always fun to do. It can lead\nto all sorts of problems. So try to avoid it unless you really know what you're doing.\nSo on the topic of failover recovery, if your writer fails, because remember, you only have\none writer, so you might be wondering, okay, what happens if that writer fails? Of course,\nthere is some kind of failover mechanism. Aurora will automatically promote one of your\nreplicas to a writer. So this is all within a single region. We mentioned also that Aurora\nsupports the concept of multi-region, which is something that seems really, really cool\nand promising. And if you ever tried to do a multi-region database, you know that it's\nextremely complicated to do it correctly. So maybe we should talk a little bit more about\nthis particular characteristic of Aurora.\n\n\nEoin: When we get to talk about cost a little bit later in the episode, spoiler alert, Aurora is a little bit more expensive than the alternatives.\nAnd for that, you have to expect some extra value. And I think when it comes to replication,\nfailover, all of these disaster recovery scenarios and scalability, that's really where you see the\nvalue. And multi-region is one of those things where you just get something that you can't\nreally achieve easily with other databases. So let's talk about global databases. With Aurora,\na global database is something that connects together clusters across multiple regions.\n\n\nSo it's essentially a grouping of one or more regional Aurora clusters. And only one of those\nclusters can be the primary. And that's where the writer instance exists. Now, there is a thing\ncalled multi-master for MySQL only, but let's put that aside for the moment. If you're looking at a\nglobal database, you'll have one primary region and that's where the writer will exist. And a\nglobal database is really just an identifier that Aurora uses to replicate data from the primary\ncluster to read clusters in different regions. And because Aurora global databases are using\nAurora storage, replication is very fast, typically less than a second. And this is\nwith a database that can support up to 150,000 transactions per second. So when it comes to\nmulti-region disaster recovery, if you've got a very low recovery point objective (RPO), this is\na way to achieve it. It's really something that's very difficult to achieve without Aurora.\n\n\nNow, you can have up to six regions in a global database in total. And if we take the 15 read\nreplicas supported per region mentioned already, this allows you to go to a pretty big scale with\n90 read replicas in your database in total. So then if you've got your multi-region database\nsetup, you can use this for data locality. If you have some readers in some regions, which might be\nbetter to serve requests from users in a specific region, you can also use it for disaster recovery.\n\n\nAnd if you have your multi-region global database, you can trigger a failover from primary region to\na secondary region. And this is really useful for enterprise use cases where you need to seriously\nreduce the risk of data loss and lower downtime as well. Now, global databases don't have global\nendpoints. So we mentioned about cluster endpoints, read and write endpoints per region. So you still\nneed to use those regional cluster endpoints for your application. You need to decide which region\nyou're targeting. You can use DNS of course, to manage that or ensure that the application is\naware of the cluster typology and failover scenarios and can respond accordingly.\nBut there's no such thing as a global endpoint that automatically does that for you at the moment.\nInterestingly, there is a thing called write forwarding in Aurora. So you can actually\nconfigure regional read endpoints to take write requests, and they'll just forward them to the\nwrite node for you, which is something that might be useful to you, especially in disaster recovery\nscenarios. Now, I think that's pretty much all the terminology and some of the primary benefits of\nAurora, but something that makes the headlines quite frequently is Aurora serverless, sometimes\nfor the good reasons, sometimes not so good. Luciano, can you take us through Aurora serverless\nand what it can offer people?\n\n\nLuciano: Yeah, so what we've described so far is what we would call provision mode. So effectively you have to configure your instance sizes more or less similar to what you\ndo with standard RDS. And the only difference is that you don't have to provision the storage.\nBut as you say, there is also a serverless mode, and this is what is generally referred to as\nAurora serverless. It gets a little bit confusing because there was originally a V1, which was,\nof course, just called Aurora serverless. Then they did a big rebranding changes and they call\nit Aurora serverless V2. We'll talk a little bit later about the differences between these two\nversions. But the point is, what is this concept of Aurora serverless? Is it something that it is\nsomewhat easier to use because the terminology serverless generally is associated with something\nthat you don't have to manage almost anything. So it's kind of an easier user experience.\n\n\nOr maybe something else, is it something that maybe looks like if you ever use services like\nNeon or PlanerScale or Supabase, are we talking about something like that? And my personal answer\nis probably no. It is quite different from products like Neon, PlanerScale or Supabase.\nAnd I think before we go into the details, maybe it's worth remembering what a serverless database\nlook like, or at least trying to define it in some way. And of course, the first one that comes\nto mind when you try to think about a serverless database, of course, is DynamoDB in the context of\nAWS. And it's kind of the gold standard, if you want, for serverlessness in database world.\n\n\nAnd the idea is that it's a database that scales to zero by default. So you don't really have to\neven think about that. It can go up and down and you can also in terms of pricing, you can pick\nbetween provision and on demand. But when you pick the on demand approach, again, even the pricing\nmodel becomes more serverless. And you have almost a spontaneous creation of tables. You don't even\nhave to think in terms of clusters or databases. You just create tables and they almost immediately\nappear. But what is the problem with DynamoDB? Why aren't we just using DynamoDB then? Because\nDynamoDB is a NoSQL database. It's not an RDBMS type of database. So when you need relations,\nDynamoDB gets much trickier to use to do all the things that you can do with a relational database.\n\n\nSo services like Neon or PlanetScale are really cool because they try to give you that kind of\nexperience where almost everything is managed for you, almost like with DynamoDB, but you get a fully\nfledged relational database that you can use straight away. Just connect to it and use it.\nThe problem with those services is that generally they seem to be targeting more kind of software as\na services or smaller startups, not as much as the enterprise, at least for what we have seen so far.\n\n\nWhile Aurora on the other end seems to be positioning themselves as the relational database\nproduct for kind of the serious enterprise that needs a certain number of features and needs\nsomething to be really, really reliable. And maybe cost is not always the first trade off that they\nlook for. So what are we talking about here is more of a modern take on something like Oracle RAC\nand maybe a little bit cheaper, but the idea is that something runs natively in the cloud on AWS.\n\n\nWhat is Aurora Serverless at this point? Why did they give you all this appeal about DynamoDB and\nwhat we mean by serverless and why we think that Aurora Serverless is not really serverless as you\nmight think? The first thing, okay, scales up and down to some degree it does that. The problem is\nit doesn't really scale to zero. There is a concept of ACU, which stands for Aurora Capacity Unit.\nAnd the idea is that one ACU is equal to two gigabytes of RAM, more or less. And the idea is\nthat you cannot just scale to zero ACUs. There is a minimum and the minimum is 0.5, which means that\neven if you have a database that is totally idle, because maybe, I don't know, it's a dev deployment,\nyou maybe are on a break because it's the weekend, nobody's really using that database.\n\n\nYou are still paying for two days, 0.5 ACUs, at least for that particular deployment. And imagine\nif you have multiple development environments, maybe you try to segregate things by domain,\nyou might have dozens of database laying around just doing nothing and costing you money. So in\nthat sense, it's not really serverless as we might like it to be. So that means there is a minimum\ncost. And the other interesting thing is that there are still maintenance windows required.\n\n\nSo you need to plan around those. So depending on what you do, you might need... eight ACUs are\nrecommended for global cluster in the primary. So certain parts of your setup will require even more\nACUs if you want to follow the recommended setup. And two ACUs are for performance insight,\nwhich is a tool that gives you query metrics. So it can be even more expensive as a baseline\nif you actually apply all the suggestions that you get from the documentation. And the funny thing is\nthat we recently realized while we were using Aurora that there is the possibility to reboot\nthe server. So we were seeing the server rebooting. So it's kind of funny to see that\na serverless product, you actually see the server rebooting. So again, it doesn't really feel as\nserverless as you might like to think. So with all of that being said, there is a benefit in\nthis capacity of Aurora serverless being able to scale up and down. And it's something that can be\nuseful in some circumstances, especially for instance, when you have very variable traffic,\nit's something that can remove some of the headaches about planning, for instance,\nthe capacity of your database. So in those cases, even if it's not really a serverless database,\nthis product can be beneficial as opposed to just going for the traditional Aurora or\nRDS standard. But I mentioned that there are two versions, V1 and V2. Eoin, do you want to try to\nexplain what is it all about?\n\n\nEoin: Yeah, V1 was around for a good few years. I'm not sure exactly when it was announced, but V2 now has been, it was in beta for a while, and I think it's been at least\na year generally available. They announced it started this year that V1 is no longer going to\nbe supported from the end of 2024. And this is quite disappointing for some people because\nversion one actually did scale to zero. So that's something as you said, Luciano correctly, you\ncan't do it with version two. But V2, when they announced it, they did add some big advantages.\n\n\nSo scale up time is faster. With V1, this could be quite slow, could be like seconds or minutes.\nNow you can scale up in milliseconds, but the speed you scale up at actually depends on your\nbaseline capacity. So the number of ACUs you're scaling from. So the more ACUs you have, the faster\nyou'll scale up. In V1, it could take minutes and it only actually scaled in double increments.\nSo you could go from two ACUs to four, then to eight, and then to 16. Now you can just scale by\n0.5 ACUs all the time. Version two now supports global databases, where version one did not,\nand version two supports read replicas where version two did not. So version one was really\njust a single instance database, so you couldn't really regard it as a serious production ready\ndatabase for the enterprise. So I think now it's probably worthwhile thinking, OK,\nwhen should you consider Aurora Serverless version two instead of provisioned Aurora?\n\n\nAnd the primary difference is that scaling you mentioned. Aurora Serverless means you can scale\nvertically without having any failover. And that's really one of the sweet spots here. Because\nnormally, if you've got a provisioned instance and you decide it isn't big enough for your needs\nanymore, you would have to add a larger instance size as a reader and then promote it to the\nwriter and then deprovision the old one. And that takes time, and you might have outage time on the\nright instance while you do that. Otherwise, the management overhead between Aurora Serverless and\nprovisioned Aurora isn't really that different. As he said, you still have maintenance windows,\nyou can reboot the instance. It doesn't really seem like a serverless product. It has a serverless\nbadge, but I think it's a little bit of a mask it's wearing, to be honest. It might be a good\nfit for your pre-production or development databases where you might have lots of idle time\nand then you just want to scale them up as you deploy and run test workloads. So that might be\none of the cases where you can actually make use of the cost difference. Because generally,\nAurora Serverless will cost you more if you're comparing gigabyte for gigabyte. And it will only\nreally start becoming cheaper if you've got that really variable traffic pattern and you spend a\nlot of time scaling down from peak capacity. So we mentioned cost a few times. So let's go into\ncost a little bit. I've done some calculations on this. Maybe we can share the link to this cost\ncalculator, the AWS pricing calculator in the show notes. And if you look at the cheapest possible\nserverless V2 instance, so 0.5 ACUs, you're talking about around $50 a month. Whereas the\ncheapest Aurora standard that I could find at least was just a little bit more expensive,\ncloser to $60. But that was for a much bigger instance. So I think it was, I don't have it\ntwo hands right now, but I think it was eight or 16 gigabytes of RAM. So you're already getting\nfar more compute and memory than you would for the serverless version. Now, if you compare that to\nthe cheapest possible one you can get on RDS, I could pick one there that costs $15 a month.\nOf course, all of this, you have to try it and measure it yourself. There's no way of saying\nthere's absolute price comparisons between all of these options. It depends on your storage,\nyour traffic and everything. So you really just have to give it a go.\nIf you were to look at an R6G large, this is a Graviton memory optimized instance, I think 10\ngigabytes. So that's kind of entry level when it comes to Aurora standard. You're looking at\nprovisioned cost of over $200 a month. But if you wanted Aurora serverless as a primary in your\ncluster, you're going to need ACUs like you mentioned, that's going to cost you $400 a month.\nSo if you're like a startup bootstrapped solopreneur, you might be looking at some\nof these costs thinking this isn't going to work for me. And you might be better off starting with\nsomething like neon or one of the other services. Or you could just say, okay, well, look, the\ndatabase is a serious part of my infrastructure. I'm going to have to spend a serious amount on\nthis. I don't think that production grade, enterprise grade databases come for cheap,\nunfortunately. So that's our two cents on cost, but there's a lot of other features. We don't have\ntime to cover them all. In fact, I think we should probably do a few more episodes on Aurora\narchitecture, maybe setting up and managing Aurora, but what other topics should we run\nthrough this channel before we finish up?\n\n\nLuciano: Yeah, let's do a quick list of other things that might be interesting to know, but we'll leave it to you the task of like doing a deep down and\nreally trying to understand all of the details. So the first thing to mention is that there is\na data API, which right now exists only for Aurora Postgres. MySQL was interesting enough\nsupported in Aurora server as V1, but it's not there yet, at least in V2. So if you're planning\nto go from V1 to V2 and you were relying on this MySQL data API, just be aware that it's not there\nfor the V2, at least not yet. Hopefully it's going to come soon. But what is the point of this data\nAPI? So the idea is that with the database, you generally connect through kind of a raw TCP\nconnection that uses a protocol that is specific to the database system. While with this data API,\nthere is effectively an HTTP API that replaces all of that. And why is this convenient? Because\nit's a little bit easier, for instance, to call if you are in a Lambda, you don't necessarily need to\ninstall specific drivers for your own database. So it can make things a little bit easier in terms of\nconnectivity from different environments. Now, should you use it? Again, if you are in a server\nenvironment, serverless environment probably makes things easier. But if you are in other context,\nmaybe you have a more traditional application with maybe using an ORM, a standard framework,\nlike, I don't know, Spring Boot or something like that, it's going to be probably much easier to\njust use the tools that you are already familiar with and just do things in the classic way.\n\n\nAnother interesting thing is that there is an RDS query editor. So you can finally run queries\nin the AWS console. This is something that for traditional RDS, I was looking for for a long\ntime. It's good to know that with Aurora, you have that option. And it is based on the data API,\nso you need to enable the data API for it to work. So just be aware of that small detail.\nAnd there are some limitations that we have observed using it. But if you just need to run\na quick query just to try to validate some assumption, it can be a really useful tool,\nand it might save you a lot of time. Now, it's not necessarily the main tool you should be using\nfor doing all your data modeling with your database. But again, if you just need to use\nit for some debugging, it can be very beneficial. And the other thing is RDS proxy. It's probably\na better solution for Lambda compared to data API. It's an additional resource that you need to\nprovision for each regional cluster. And the idea is that because when you run Lambdas, you might\nend up having really, really quick spinning up thousands of Lambdas. And every single Lambda is\ngoing to try to establish its own connection. So if you try to do that in the traditional way,\nwhere you try to just establish a TCP connection directly against the database, you might very\neasily saturate the pool of connections available in the database. So this proxy tries to manage all\nof that, tries to provide some kind of share pool and manage all of this stuff so that you don't end\nup overloading your database in the case where you're spinning up thousands of Lambdas at the\nsame time. So it keeps your database a little bit leaner and manages all of these things,\nand also gives you faster failover because it's aware of the cluster topology. So you don't have\nto rely on DNS. And with DNS, oftentimes you might have problems because maybe you have a TTL,\nand then your DNS doesn't get refreshed fast enough. So you might be failing for a while\nbefore you figure out that there is a new DNS record, and then you can easily connect to the\ncorrect instance. Another interesting point is that allows you to enforce IAM authentication.\nSo this can be useful if you have, for instance, to share secrets with the application directly.\nThe small problem is that, of course, because it's an additional resource, there is an additional\ncost. So something that you have to consider is $25 a month, more or less. So it's not a massive\ncost, but depending what you are trying to do and what is your budget, it can be something\nsignificant and is worth considering that. So these three things I wanted to mention,\nData API, RDS query editor, and RDS proxy, somewhat interesting. So if you end up using our\nAurora or seriously considering Aurora, check them out a little bit better in the documentation.\nWe'll have links in the show notes if you really want to understand why those additional\nthree things can be really beneficial for you. I guess this is time now to jump to the conclusions.\n\n\nEoin: Yeah, while we were talking about v1 and v2, I was reminded that the Cloudanut podcast and blog, they had a good analysis of the differences between v1 and v2 because I think they were\nusing v1 and were a bit disappointed to realize that the migration path from v1 to v2 isn't great.\nEssentially, you have to take a snapshot and then copy it across, create a new v2 cluster from it.\nIt's worth checking out, but maybe we should finish up with some advice on when you should\nuse Aurora from our perspective. I think it's really for serious enterprise use cases.\n\n\nAnd I can see Amazon basically aiming at people who are paying a lot of money for commercial\ndatabases with high license costs and lower performance. And they're saying,\nthis is going to be cheaper and faster, even though it's still a significant cost for the\naverage user like us from our perspective. For enterprises, they can definitely make some savings.\nBut it's also good for single instance, low config, serverless type cases that need relational database\nrather than no SQL. So if you're somewhere in between those two extremes, you might just use\nRDS or another vendor, especially if you are cost conscious. So just to summarize everything we\ntalked about today, Aurora, its relational database solution on AWS, we talked about some of its unique\ncapabilities, intricacies and trade offs, and how it'll give you MySQL and Postgres compatibility\nwith faster performance. But the most important thing maybe to take away is its distinct storage\nlayer that gives you that better performance and durability and faster recovery. And then bear in\nmind as well the concepts of Aurora clusters, reader and writers, endpoints, instances, and\nglobal databases as well. When it comes to Aurora serverless, comparing its two versions and their\nfeatures, V2 is definitely more enterprise grade, but V1 is not going to be supported from the end\nof this year anyway. And while it definitely does not reach the gold standard of serverlessness\nset by DynamoDB, it does have its uses, particularly for variable traffic use cases,\nand maybe pre-production workloads as well. And we also touched on billing aspects and things like\nthe data API, RDS query editor, and RDS proxy as well. So overall, I think it is a really powerful\nand scalable solution. It's not trivial to use, especially when it comes to global scale, but\nstill way more trivial than the alternative because when you're dealing with clusters of\nrelational databases, at least as of yet, there is no silver bullet, ultimately simple solution.\nSo thanks very much for joining us and join us in the upcoming episode for more on Aurora and\na whole load of other AWS topics.\n"
    },
    {
      "title": "123. What do you need to know about DynamoDB?",
      "url": "https://awsbites.com/123-what-do-you-need-to-know-about-dynamodb/",
      "publish_date": "2024-05-17T00:00:00.000Z",
      "abstract": "In this episode, we provide a comprehensive overview of DynamoDB, including how it compares to relational databases, when to use it, how to get started, writing and querying data, secondary indexes, and single table design. We share our experiences using DynamoDB and discuss the pros and cons compared to traditional SQL databases.\n\nAWS Bites is brought to you by fourTheorem. If you need someone to work with you to build the best-designed, highly available database on AWS, give us a shout. Check us out on fourtheorem.com!\n\nIn this episode, we mentioned the following resources.\n\nOur previous episode &quot;122. Amazing Databases with Aurora&quot;\nConfigurable Maximum Throughput on On-Demand tables\nBest practices for designing and using partition keys effectively\nThe DynamoDB Book\nAlex DeBrie’s podcast (not about DynamoDB per se but still worth a shout!)\nOne of Rick Houlihan’s talks on DynamoDB\n\n",
      "transcript": "Luciano: Following on from our last episode on Aurora,\nwe are sticking with databases today.\nThis time we are discussing one of the most requested topics by our listener, DynamoDB.\nWe are going to give you our opinion on when and how to use DynamoDB,\nwhen you should avoid it,\nand whether the much talked about topic of single table design is actually worth the effort.\nBy the end of today's episode, we hope you will have a comprehensive understanding\nof the main DynamoDB concepts and how to get most of the value from DynamoDB.\n\n\nI'm Luciano and I'm joined by Eoin,\nand this is another episode of AWS Bites podcast.\nAWS Bites is brought to you by fourTheorem.\nIf you need someone to work with to build the best design,\nhighly available databases on AWS, give us a shout.\nYou can check us out at fourtheorem.com or contact us directly using the links you will find in the show notes.\nSo maybe to get started, what we can do is give a little bit of background\non what DynamoDB is and how does it compare with relational databases.\n\n\nSo DynamoDB is well known as one of the best in class NoSQL databases in the cloud.\nAnd because we talked about relational databases in the previous episode,\nagain, how does a NoSQL database compare with a SQL database?\nAnd it's not necessarily an easy description because NoSQL is a bit of a marketing term.\nSo it's not like there is a canonical definition,\nbut we'll try our best to try to describe the differences between those two classes of databases.\n\n\nSo let's start with relational databases first.\nRelational databases traditionally optimize for storage.\nAnd after all, we have to think that they were invented at a time where storage was very expensive.\nSo the goal at that point in history was to try to limit as much as possible\nthe duplication of data because storage was effectively a scarce resource.\nSo data is generally separated into normalized tables with defined relations between them.\n\n\nSo well-organized structure and well-defined schema.\nRelational databases normally use a language called SQL and highly optimized query engine\nthat basically allows to retrieve data across multiple tables in a very dynamic way.\nAllows you to combine data in different ways, filter data in different ways,\ndo updates across multiple records at the same time.\nAnd this has become over the years some kind of lingua franca for databases.\n\n\nAnd incredibly popular, lots of people in the industry know SQL as a language,\nwell understood, used in many products, even for reporting,\nnot just for actually interacting with databases.\nAnd if you think it's been around for 15 years, 50 years, it actually makes sense\nthat it's something so well-known and understood and adopted in the industry.\nSo SQL is kind of a way to do arbitrary requests or ask arbitrary questions to your database.\n\n\nAnd that's a great thing. It's actually a great feature, especially in comparison with\nNoSQL databases because you generally don't need to know in advance what you're going to\nbe using this database for. You just put all your data there, you give it some sensible structure,\nand then over time you can come up with new access patterns, with new questions to ask\nto your database and SQL is going to be flexible enough to allow you to express this kind of\nquestions to your database. But this is a bit of a double-edged sword because it's so flexible,\nthat also means that it cannot be optimized for performance for any use case or for any\nquestion that you might have. So sometimes you will find yourself, if you ever manage a SQL\ndatabase, trying to figure out why this query was particularly slow, how do I optimize it.\n\n\nSometimes that means maybe changing the data structure, maybe adding indices, maybe scaling\nhardware, maybe thinking how do I partition this data across maybe multiple instances of\nthat database. So these are kind of the pros and cons of relational databases. Let's talk now about\nNoSQL. And we already mentioned that NoSQL is a bit of a marketing thing, so let's try to figure\nout what is the simplest definition that we can give that probably most people would agree with.\n\n\nAnd one of the main points of most NoSQL products is that they are schema-less. So that means that\nwhen you store data, you store it in a set, for lack of a better word, let's just call it a set,\nlike one place where you put all your data and different records in that set can contain\na different structure of data. So you can have different fields. That's what we mean by schema-less.\nYou don't have to think in advance about a schema that will fulfill all the records that you want to\nput in that particular set, but every single item can have its own properties. And another interesting\npoint is that generally NoSQL products will be a little bit more relaxed when it comes to\nACID compliance. With ACID, we mean atomic consistent, isolated, and durable, which is\na property that most relational database will try to guarantee. In NoSQL databases, generally,\nthe producers are concerned about performance and making sure that the data can be easily\ndistributed across multiple nodes. So there are some trade-offs that are made where generally,\nfor instance, what NoSQL producers will do, they will give up on the idea of consistency and favor\neventual consistency so that it's easier for them to be able to distribute the data in a durable way\nacross multiple partitions. And the final point is that with NoSQL databases, you generally\nworry a lot less about storage cost. And this is probably because it's a much modern version of\ndatabases. So storage is a bit less of a problem since the 70s. So there is a lot more freedom to\nuse storage in ways where you might end up duplicating data. But once you do that,\nyou might be able to access that data much faster in some access patterns. If you want to think\nabout what is the simplest NoSQL database that you can imagine, you can just think about a key\nvalue storage. So imagine you have a map in any programming language where you can start key value\npairs. And you can imagine that the key is basically the thing that allows you to access\nrecords univocally. And then inside the value, you can store complex objects of any kinds with\nmultiple attributes. And they can all be different between every record. And it's also worth mentioning\nthat this is not necessarily the same thing when we mentioned document databases. You might have\nheard of MongoDB, which is generally classified as a document database because document-oriented\ndatabases are more of an extension of the key value concept. They tend to have a little bit\nof a more structured format and a more expressive query language. So when we talk about NoSQL,\nand especially in the context of DynamoDB, we are talking about something that maybe can be a little\nbit simpler than products like MongoDB. Hopefully that gives you a good introduction to the world\nof NoSQL and what we mean by NoSQL and how does it compare with relational databases. So let's now\ntalk about specifically DynamoDB. Eoin, where do we start?\n\n\nEoin: Yeah, let's start with some of the terminology and concepts around DynamoDB so that we can take the discussion from there. Just like SQL databases, you start with a table. So this is the primary unit.\nAnd you don't really create databases in DynamoDB, but you create tables. And that's your starting\npoint. Within tables, then you're going to be storing items. So an item is the term that is\nused to refer to essentially a row containing a key and attribute values. So then we talk about\na key and a key is less of a trivial concept really. But it is, as we mentioned, a type of\nkey value store, and every record is identified with a key. And there are two types that you can\nuse. You can either use a simple key or a composite key. And that would be the primary key uniquely\nidentifying an item in the table. A single key has a hash key only, which is also known as a partition\nkey. So it's probably a good idea to understand both the term hash key and also partition key.\n\n\nAnd then if you're using a composite key, you'll have that same hash key. But you can also have\nthen a range key, which is also known as a sort key. And when you're writing or reading DynamoDB\ndata, you'll always use the partition key. If you have a sort key as well, so if you are using\ncomposite keys, you'll need to specify it when writing data, but you don't necessarily have to\nspecify it when you're reading. So we'll talk a little bit more about all that later on.\n\n\nSo that's your key. And then your value is composed of attributes. You can have multiple\nattributes in each item, as you mentioned already, and an attribute has a name and a value. It's a\nlittle bit different to the document storage option or just a simple key value storage option,\nwhere you have a single value or you have a document with awareness of the structure.\nIn DynamoDB, you can have multiple attributes and each of those could be like a little document,\nbut in a very unstructured way. There's a number of different types supported. It's worthwhile\nunderstanding what types are supported, especially when it gets into the multi-value types. So the\nsimple values types, the literals you can store are string, number. You can also store binary\ndata as a type. And then you have lists, maps, and sets. If you just write kind of JSON type data\ninto an attribute, that's a map, a list is an array, and then you have sets where you don't\nget duplicate values. And you have three different types of sets supported. So you have string sets,\nnumber sets, and binary sets. And we'll talk a little bit more about how you use those.\nThere is also a null type that's quite rarely used. Now it is important to note that unlike\na lot of databases, the maximum item size in DynamoDB is 400 kilobytes. It's important to\nnote that this is per item, right? For the whole record, not just per attribute. You might think\nthis is a small comparison to like Cassandra or MongoDB, which lets you store gigabytes in\nrecords, but there's a lot of limitations like this in DynamoDB, which are there for a very good\nreason. And they're there because it helps them to deliver on the massive performance and\nscalability guarantees that they provide. So it seems sometimes working with DynamoDB that it's\nalmost like working with a low level database engine because they're strict about giving you\na limited set of features so that they can give you those guarantees in return. So if you want\nmore data than 400 KB per item, it's difficult to offload that into an S3 object. So maybe before\nwe start diving into more technical details, let's go up a level. Why and when would you use DynamoDB?\n\n\nLuciano: I will say that one of the main reasons to think about using DynamoDB is because it's so nice and easy to get started with. You can create a table in a matter of seconds. Either you click offset,\nor you just write a few lines of YAML in your CloudFormation file, and you have a table that\nyou can use to store data, read it, write it, and build an application on top of that. And this is\nsomething that I haven't seen in any other database, even when you can manage services.\n\n\nIf those are relational databases especially, it takes a longer time to get started with.\nSo for quick things, definitely a database to consider. It can also be very cost effective,\nespecially when you don't need a data intensive application, when you don't expect to be reading\nand writing all the time, or when you don't expect large volumes of data. It can be very,\nvery cost effective. And the main idea is that it's kind of a serverless database,\nat least from the pricing perspective. If you don't use it, you don't have to pay anything.\n\n\nIn reality, there are different building modes. But again, we can think about it as if it's a\nserverless service. So they try to provide you with an interface that the more you use it,\nthe more you're going to pay. If you don't use it, the cost is going to be very limited.\nAnd this can be a really big advantage, for instance, if you are a startup,\nwhere maybe at the very beginning, you are going to have very limited traffic as you try to build\nyour first MVP. Then eventually, if your product is very successful, it might grow. And of course,\nyour billing is going to grow with the success of your platform. It's very well integrated with\nother AWS services. That's another interesting point. For instance, we can talk about DynamoDB\nstreams, which is a nice way to basically get all the changes that happen in a DynamoDB table\nand stream them for real-time processing, for instance, to Lambda. And this is something that\nallows you to do change data capture, and you can do all sorts of interesting things with it.\n\n\nAlso, you can get very fine-grained access control because it's kind of a native AWS service,\nso to speak. You can use IAM policies to a very fine level of detail. You can really control\nwhat kind of, not just what kind of tables, but what kind of records different roles can actually\naccess. And this can allow you to do very cool things. For instance, if you're building a\nmulti-tenant application, you could limit, for example, a Lambda to be able to read only the\nrecords that are attributed to a specific tenant and not the other ones, which can be something\nreally, really beneficial if you're trying to get some kind of SOCK compliance, or if you just want\nto be sure that you're not going to be leaking data across tenants by accident. And this will\nbe something very, very difficult to achieve at this level with relational databases that are not\nso well integrated with the rest of the AWS ecosystem. And another few things that are very\nworth mentioning is that DynamoDB scales massively. And after all, we have to think that DynamoDB was\nbuilt for Amazon, so to solve all the problems that they were having with their own massive\ne-commerce as it was growing in popularity. And it is powering today the entirety of the Amazon\ninfrastructure. So you can imagine that if you can build something as complex as Amazon and as big as\nAmazon, there is a level of scale there that is not trivial to achieve with other technologies.\n\n\nIt can be very simple to use, of course, for simple use cases. If you have access patterns\nthat are pretty much key value based, then it is very simple to use. You just store your data by\nkey, you read your data by key, super easy to get started. If you need very low latency,\nDynamoDB is one of the best databases for that out there. It has very consistent load and latency\nresponses. For example, they promise you single digit milliseconds when you do a GET operation.\n\n\nSo when you can access a key exactly, you get single digit millisecond response, which is\namazing. And that's very consistent, regardless, for instance, of the size of your dataset.\nAnd this is something that can be a great candidate, for instance, when you're building\nweb applications and you want to make sure your users have very snappy responses, there is a\nfeeling that the application is very responsive, or other use cases where you need to guarantee\nthat the access to the data is as fast as possible. But now let's talk very quickly on\nwhen you might not want to use DynamoDB, because of course it's not a silver bullet that is going\nto solve all your problems. So some cases that come to mind is the main one is probably when\nyou need flexible querying capabilities. And this is actually very common for startups.\n\n\nConversely, we say that DynamoDB is really good for startup environments because the pricing\ndynamics will scale with the growth of your company. But on the other end, you have to\nconsider that DynamoDB requires you to understand really, really well the way you'd need to access\nthe data. And when you're building a startup, sometimes you need to pivot multiple times before\nyou figure out exactly what's your product market fit, what's the product that is really solving a\nproblem for your customers. So you're going to go through significant different iterations of your\nproduct. And as such, you're probably going to change the way you use the data in your database\nmultiple times. DynamoDB might not be the best database for that. It will require you a lot of\nhard work to always adjust changes to the structure of your database, change it to the way you query\nthe data. Something like a relational database might be much more suitable for that kind of\nthing because with the flexibility of SQL, as long as you're keeping your data normalized,\nthen you can easily adjust for different access patterns. So definitely worth considering if you\ndon't really think you understand well your current and future access patterns, DynamoDB\nmight create a little bit of friction for the evolution of your product. Other reasons why you\nmight not want to use DynamoDB is, for instance, when you need to integrate with other systems that\nexpect a SQL interface. DynamoDB is not going to give you a SQL interface or at least not a\ntraditional one. So definitely it will make your life much harder if you need to integrate with\nsomething that is expecting SQL as a language. Another case is when you might have lots of\nrelational data by nature. So you really need to use features like join and join data across\nmultiple tables. That's not even something that is supported in DynamoDB natively. So you will need\nto do your own joins with code inside your application. And that's something that is not\ngoing to be very efficient and it's going to be very tricky to do well and to scale it.\nAnd finally, if you, for whatever reason, need to manage a database by yourself, like you need to\nhost it yourself and run it yourself in your data center or even inside your own cloud account,\nDynamoDB doesn't offer that option. DynamoDB is only a managed service. Amazon will give you a\nsingle node local version that you can use and run it yourself, but that's meant to be used only for\nlocal testing and development, not to be run in production. Now, I think at this point, it might\nbe very beneficial to try to explain a little bit more how DynamoDB works, because I think that\nis going to demystify why there are so many constraints, but at the same time also why\nDynamoDB can be so effective and performant in certain use cases.\n\n\nEoin: DynamoDB data is stored in partitions. You might've guessed this already, since we mentioned that\ndata needs a partition key, that hash key we referred to earlier. So when you provide your\nvalue for your primary key, the partition key part at least, that key is going to be hashed\nby DynamoDB. And the hashed value is going to be used by DynamoDB to route it to the server nodes\nwhere the partition or that shard of data is stored. And it's the scalability of the partition\nmodel that gives DynamoDB its infinite scalability. Then each partition has a primary node that will\nhandle writes just like many other databases, but it will also have two secondary nodes. And for\ndata to be written, it has to be written by the primary and at least one other secondary node.\n\n\nThe third node then can be updated asynchronously. So that will give you better performance on\nwrites. But what this means is that because any of these nodes can handle reads and only one of\nthe secondaries is updated synchronously, you might end up reading from a node that doesn't have\nthe latest data. And this is part of DynamoDB's default mode of eventual consistency. And if this\ntradeoff is a problem for you, there is a way around it, you can explicitly request strong\nconsistency when reading. And that may take a little longer because it has to wait for the third\nnode to acknowledge. But you will pay an increased price for this mode, essentially double based on\nthe billing model, which we'll explain a little bit later. And that billing model, the pricing\nmodel of DynamoDB is very tied into its performance and scalability. Because when you write, you\nconsume a write capacity unit, when you read, you consume a read capacity unit. So WCUs and RCUs.\n\n\nOne RCU will allow you a strongly consistent or two eventually consistent reads per second,\nand they can be up to four kilobytes. And a write capacity unit allows you to write one item up to\none kilobyte. You have two pricing options. You've got provisioned mode where you can say, okay,\nI'm going to need 500 RCUs and 500 WCUs. And then you pay a fixed amount per hour as long as the\ntable exists. The newer mode, which is more serverless is the on demand capacity. And that\nwill scale the WCUs and RCUs up and down for you. If you don't use them, you don't pay. But if you\ndo use them, the cost is generally higher than provisioned capacity. So you need to measure your\nown workload and decide which one works. Generally, we'd say start with on demand capacity, measure\nhow much you're using, look at your bill and optimize accordingly. And the good news there\nactually is that I think just last week, AWS released a new feature, which will allow you to\ncap the maximum on demand capacity. So you can manage that maximum cost and don't have to lie\nawake at night worrying about it. Now, when we talk about partitions, you might have heard the\nconcept of hot partitions, especially if you're reading older blog posts or content where your\nthroughput could suffer if you didn't actually evenly distribute the partition keys across your\nwhole data set. And if you do read anything like that, don't worry, because Amazon has since added\nan adaptive capacity feature a few years ago that automatically solves that for you. So they'll\nmanage capacity according to the size of the partition keys on different nodes. But it is\nstill important to note that each partition does have a maximum throughput. So it's 3000\nRCUs or 1000 WCUs. So if you are going to have a lot of traffic, you should make sure that you're\nnot just using a small number of partition keys. And that will allow you to ensure that you get\nconsistent performance across all of your data. So I think partitions, they're basically the\nfundamental concept to understand. We've talked about strong consistency and eventual consistency.\nLet's talk more practically, how do you get started and what you do to start using DynamoDB?\n\n\nLuciano: If you're used to more traditional relational databases, one thing that might be surprising\nabout DynamoDB is that it doesn't use something like an ODBC or JDBC type of connector. Instead,\nyou just do HTTP requests. So in a way, it's like you have a web API to interact with when you use\nDynamoDB. In reality, you rarely want to use the web API directly. You will be using the AWS SDK,\nwhich of course abstract all of that communication in a much nicer way.\n\n\nWhen it comes to the SDK, there are actually two different types of client. And this is something\nthat sometimes can be a little bit confusing, but the idea is that you have clients at two\ndifferent levels. You have the main DynamoDB client where you still need to understand a\nlittle bit what is the protocol when it comes to specifying the different types of values that you\nneed to read and write. Instead, when you use the Document client, that type of client is generally\na little bit more integrated with your programming language of choice. It can understand the types\ndirectly from the type that you express in your programming language, and it's going to do an\nimplicit conversion behind the scenes for you. So if you are trying to put a string, for instance,\nin an attribute, it's going to automatically create the correct shape of the object that the\nunderlying client expects to say that that value is going to be persisted in DynamoDB as a string\nand not, for instance, as another data type. So generally speaking, I would recommend to use the\nDocument client because it will make your life a little bit easier, and it will abstract some\ndetails that you don't necessarily have to worry about when it comes to the underlying protocol of\nDynamoDB. Let's talk a little bit more about how do you write data, what kind of options do you have\nthere, and all the right actions that you need to do, as Eoin, you mentioned before, force you to\nprovide the full primary key. So you need to explicitly say, this is the primary key that I'm\ngoing to be used to store this particular record or to update a particular record.\n\n\nAnd one interesting thing, and this is something of a pain point that I had a few times in the past,\nis that you have no way to do a query such as update all the records where this particular\nclause is true. You need to read and write based on a primary key that you need to know in advance.\nSo you cannot just use arbitrary attributes to do that stuff. Now, when it comes to writing, you have\na few different operations that you can do. The first one is put item, where basically you are\neither creating or overriding an existing item, so a single item. Then you have update item, which is,\nagain, either write or update. You can specify a subset of attributes in this case,\nand you can also use this particular operation, for instance, if you want to delete existing\nattributes from an existing record. And you can also use it in interesting ways. For instance,\nif you have a record or an item that contains a set or a map, you can just insert or remove\ndata from the underlying set and maps that exist in the attributes of your item.\n\n\nAnd finally, and this is something that can be actually very common, I've seen it a few times\nwhen using DynamoDB, if you have counters, you can use the update item to just say increase by\none. And if you consider that this is a distributed database, you don't want to read the data first,\nthen in your code increase by one and then start the data again, because you might have that\noperation happening simultaneously, potentially in a number of different concurrent executions,\nand therefore your counting might be overriding each other. So it's better to just let the\ndatabase do the increment for you, because that way can be done consistently. Then, of course,\nwe have delete item operations, and it's important to know that you can also do batch writing. So,\nfor instance, if you need to insert lots of data into DynamoDB, maybe you are loading data from,\nI don't know, fixture data that you need to use in your application, you can do that,\nbut there are limits. For instance, you can write up to 25 items if they're not wrong in a single\nbatch. So you need to create multiple batches according to how many items you need to write.\n\n\nAnd finally, you can also use the transact write item, which allows you to write data as part of a\ntransaction. And the other thing is that when you write something into DynamoDB, so when you do\nan update operation, you might be interested in receiving a response from DynamoDB, and you can\nactually specify what kind of response you want to get back from DynamoDB. So different options\nare, for instance, if you don't really care about anything, you can just say, no, just write the\ndata, I don't care about the result of that operation. Then, for instance, when you're doing\nupdates, it can be very interesting to know what was updated. So you have options like all old,\nor updated old, or all new, or updated new, that will allow you to select a subset of the data that\nwas actually updated and compare it with the previous data. And going back to the case of the\ncounter, if you want to say, for instance, increase this particular attribute by one,\nyou don't necessarily know what's going to be the final value, because maybe you didn't read the\nvalue in the first place, or maybe the value that you have right now in memory in your program is\noutdated, because meanwhile, there have been other increases from other concurrent executions,\nyou can get the new value as a response from your update operation when you select one of\nthese attributes. So that can be a convenient use case. For instance, if you are building some kind\nof counter, increase the value, and you want to know what's the most recent count in your program.\n\n\nAnother thing is that you can add condition expression. So when you write, you can say,\nwrite this record only if certain conditions are happening, and don't write it if those conditions\nare not satisfied. And this can be useful, for instance, if you want to guarantee data integrity,\nfor instance, you might want to create a new user in a table, and maybe you want to make sure that\nthere is only one user with a given email. Again, thinking that you might have concurrent execution\nof your program in different environments, maybe different lambdas, it's not unlikely that you can\nhave a very similar request from two different lambdas in a very short amount of time. So for\ninstance, if a user is submitting a form twice by mistake, you might end up creating two users with\nthe same email. By using a condition expression, you can say, don't create a user a second time if\nthis email is already existing in the primary key, for instance, of another record. Going into\nqueries, you have different ways to query your data. The simplest one is probably get item.\n\n\nAnother use case is scans, which basically allows you to iterate over the entire table.\nThis is generally a very niche use case. You rarely need to do that. Or if you find yourself\ndoing that, probably you should think twice because it's not always a good idea to do this.\nSo unless you really know what you're doing, try to avoid scans as much as possible. And the main\nreason is that, especially if you have a large data set, a scan might take a very long time to\ncomplete, but also it's going to be very expensive for you. So just be sure that you are aware of\nthat. If you find yourself using a scan, make sure you know what you're doing. And if you have other\noptions, probably go with the other options. Of course, we have a concept of query as well,\nwhere you might want to retrieve multiple records together, but of course it is still somewhat\nlimited to the partition key. So you can query only for a given partition key and then filter\nthe subset of records from your sort key when you have a composite key. And you can have expressions\nsuch as equality, begins with, between, but you cannot do more generic expressions that you might\nfind in SQLite, for instance, when you use the like operator and you cannot even do ends with.\n\n\nSo you need to be very careful depending on the type of queries that you expect to do\nin structuring your keys so that the query operation allows you to do the queries that\nyou need to do. And you can also use filter expressions, which are a little bit more flexible\nand they can be applied to any attribute, not just the primary key and the secondary key.\nAnd these filters are a little bit funny. They work in a way that you might not expect\nthe first time you use them, because if you're using again to SQL, the filtering happens at the\ndatabase level where the database is just going to give you the data that matches the conditions\nthat you are looking for and ignore everything else. While here with DynamoDB, when you use\nfilter expressions, you are actually still getting all the data, effectively discarding the records\nthat don't match that particular filter expression. Finally, each query has one megabyte read limit.\nIf you need to read more, you need to use pagination. Thankfully, the SDK these days\nmakes that much easier than it used to be, especially in dynamic programming languages\nlike JavaScript, you can use async-atorators and that's a relatively easier experience to go\nthrough all the different pages. But of course, you need to be aware that you are making multiple\nrequests to DynamoDB. You're sending multiple HTTP requests. So the more data you read, the more\ntime is going to be required to read the entire dataset. Now we should probably talk a little bit\nabout indices because that's such another interesting topic in DynamoDB and it's something\nthat can allow for other access patterns.\n\n\nEoin: And we've talked about one type of index, kind of, so far because we mentioned primary keys, but you can actually add additional keys to\nsupport querying by fields that are not in the primary key. As we've said, and you've talked\nthrough the query semantics, you need to specify the partition key. If you want to do more granular\nfiltering, you need to use a key expression, but what about different access patterns?\n\n\nWhat if you need to query by something else entirely? Well, that's where indexes come in and they're called\nsecondary indexes in this case. There's two types of secondary index. There's the local secondary\nindex, which is stored together on the same partition as your database. And because of that,\nthe partition key is always the same as your primary key's partition key. And only the sort\nkey is different. Then you have global secondary indexes, which are stored separately, and they can\nhave a different partition key and sort key. For that reason, they're a lot more common.\n\n\nLocal secondary indexes have a few more limitations and they also share the table's capacity,\nwhereas global secondary indexes have their own capacity. And when you hear people talking about\nglobal secondary indexes and local secondary indexes, because they're a bit of a mouthful,\nthey'll normally say GSI and any kind of secondary index allows you to retrieve atom attributes from\na set of related records by different keys. You can imagine having a DynamoDB table that stores\ncustomer orders. And normally you retrieve it by customer ID and maybe date for the sort key so\nthat you can filter by date. But you might also want to retrieve by product ID and amount.\n\n\nSo you could put product ID and amount in as a separate global secondary index. One of the cool things\nabout indexes is that they can actually be sparse. So what does that mean? Well, if some of the\nattributes in your index aren't present in any item that you're inserting into a table, your\nindex doesn't actually need to store that record at all. So the volume of data in an index could be\nmuch less than in the table itself. And because of that, indexes can actually be used as like a\nmaterialized view or a filter on data because it's already pre-filtered based on whether those\nattributes are present or not. And that's quite a common pattern for GSIs. You can also use indexes\nto store different but related entities together in the one table. So we talked about storing\ncustomer orders, but what if you wanted to store customers' orders and products and query them\ntogether? You can actually do that in DynamoDB. And you do that by overloading the partition keys\nand sort key values so that you can query them individually and using more indexes as well to be\nable to support more and more query patterns. And this approach is called single table design.\n\n\nAnd it typically means having a naming convention in your keys, like having a partition key,\nwhich has a syntax like customer hash, and then a customer ID. And then maybe in your sort key,\nyou'll have a order hash order ID. And then you might have a separate product ID column,\nwhich is used in a secondary index to query the product. It's a total shift from the simplicity\nof the default DynamoDB approach. And it's incorporating relational modeling from relational\ndatabases, but it allows you to get the best of both worlds with some trade-offs, but you can\nactually implement relational design in this way. And this all came about, well, it's been around\nfor a while, I guess, but it was popularized when Rick Houlihan, who used to work at AWS advising\nall of their amazon.com teams on how to do this. He gave a series of very famous re-invent talks\ndescribing advanced DynamoDB modeling. And this really gave a lot of momentum to the idea of single table design.\n\n\nI remember seeing this talk and thinking, wow, this is amazing, but\nI had to watch it a few times to really understand it because it's kind of mind-meltingly high speed\nand deep dive, like the most level 400 talk I've seen. And then Alex DeBrie gave a much more\naccessible guide on it in his great DynamoDB book. Yeah, he has got a lot of great content\naround DynamoDB, so much so that I'm surprised they haven't renamed it to DynamoDeBrie at this point!\n\n\nSo the fundamental idea with single table design is if you know your access patterns ahead\nof time, you can design your DynamoDB table indexes and keys to store all of this data,\nrelated data together. So that could be created together and it can allow you to do all this\nrelational modeling, but still gain from the performance and scalability of DynamoDB.\nUnfortunately, it's not really very easy to grasp and do well. I'm still afraid of it,\nto be honest. Even if you do do it, it can be difficult for others on your team to understand\nand troubleshoot when they join the team. Even I've seen single table designs, which I've\nimplemented and understood, and then gone back to it a few months later and thought,\nwhat is this schema? I can't remember how this is modeled. And people have tried to provide\ntooling around that to make it easier. And that has helped to design it, but I still don't see\na great solution to ultimately making it accessible and understandable for everybody. Of course,\nwe mentioned that you need your access patterns well-documented and understood ahead of time. So\nif they change, you need to be able to plan and execute a schema change and a migration later.\nSo it's not a silver bullet. And while it looks really cool and it's very appealing,\nI would tend to say, don't get caught up in it and don't worry about it too much. What do you\nthink, Luciano? I mostly agree with what you said there.\n\n\nLuciano: The only thing I can add is that I found that it might help a little bit if you try to abstract all of that stuff in your code,\nmeaning that you are going to use something like the repository pattern to say,\nwell, I have a code layer where I can just say, give me all the, I don't know, products,\nor give me what's in the cart for this customer. And behind the scenes, you have abstracted all the\nnecessary logic to integrate with DynamoDB from a team perspective that may make things a little\nbit easier because you are not necessarily required to go and look under the wood to exactly\nsee what's happening with DynamoDB. But of course, as you say, then if you eventually find yourself\nin the position where you need to change the data structure to accommodate for different access\npatterns, then somebody will need to be able to touch that layer and make the necessary\nadjustments. So this is not necessarily a silver bullet. It's just, I guess, good code practices\nthat might create abstraction layers that can be more accessible to a larger group of people in the\nteam. So that's maybe something else to consider if you do find yourself using the single table\ndesign pattern, if you see value in it, and there is definitely value, that can be one of the\npractices you can use to make your life as a team a little bit easier.\n\n\nAnd I think at this point, we've covered enough ground when it comes to DynamoDB. This was a longer episode that we\ngenerally do, and hopefully you enjoyed it anyway. We tried to share as much as we could about the\nbasics of DynamoDB, what it is, how does it compare with relational databases, how do you use it,\neven up to talking about the single table design pattern. And of course, don't forget that if you\ndecide to use DynamoDB, don't forget that relational databases are still pretty ubiquitous.\n\n\nIn a way, if you're using AWS, it makes sense to adopt DynamoDB, but you always need to look at\nyour requirements and make sure you make it a conscious decision. Definitely, there are many\nadvantages in using DynamoDB, but also we can say the same when it comes to traditional relational\ndatabases and SQL. So don't be feeling like you are missing out if you prefer to use a relational\ndatabase rather than DynamoDB. I think there are still many ways to use relational databases and\nmake them scale in the cloud even at very high scale. So I think we will love to hear more from\nyou if you're using DynamoDB, if you totally ditch relational databases, or if you are still\nfeeling more attached to relational databases than to DynamoDB. And maybe hear about the stories that\nyou might have, if you have any scar from DynamoDB or any scar from relational databases. It would be\nnice to put these ideas into context because I think the context is really the key here.\nIt's not really like one technology is better than the other. Different use cases might be more suitable\nfor different types of technologies. With that, we will leave you some additional resources in\nthe show notes. We will link the DynamoDB book that we mentioned by Alex DeBrie, but we will also\nlink Alex's podcast and YouTube channel where you can find additional content and we will share some\nof the talks we mentioned about Rick Houlihan. So thank you very much for being with us and we look\nforward to seeing you in the next episodes.\n"
    },
    {
      "title": "124. S3 Performance",
      "url": "https://awsbites.com/124-s3-performance/",
      "publish_date": "2024-05-31T00:00:00.000Z",
      "abstract": "In this episode, we discuss some tips and tricks for optimizing performance when working with Amazon S3 at scale. We start by giving an overview of how S3 works, highlighting the distributed nature of the service and how data is stored redundantly across multiple availability zones for durability. We then dive into specific tips like using multipart uploads and downloads, spreading the load across key namespaces, enabling transfer acceleration, and using S3 byte-range fetches. Overall, we aim to provide developers building S3-intensive applications with practical guidance to squeeze the most performance out of the service.\n\nAWS Bites is brought to you by fourTheorem an AWS consulting partner with tons of experience with S3. If you need someone to work with to optimize your S3-based workloads, check out at fourtheorem.com!\n\nIn this episode, we mentioned the following resources:\n\nOur previous episode on S3 best practices\n“Deep dive on Amazon S3” (re:Invent talk from 2021)\nMore recent re:Invent talk on Amazon S3 (with updated data and more best practices) - We discovered this one just after the recording\nMulti-part upload user guide\nCode examples from the SDK (high-level and low-level APIs)\nNode.js official helper library (@aws-sdk/lib-storage)\nExample on how you can implement byte-range fetches\ns3-getobject-accelerator library\naws-c-s3 library\nS3 storage lens\nDocs on S3 Transfer Accelerator\nPerformance Guidelines for Amazon S3\nPerformance Design Patterns for Amazon S3\n\n",
      "transcript": "Eoin: S3 must be the most loved of all AWS services.\nIt's a storage service that allows you to store files with a simple API\nand takes care of scalability, durability, security,\nand a whole bunch of other things with very little effort on the developer side.\nS3 is becoming the ubiquitous cloud storage platform\nand powers a large variety of use cases.\nAnd for some of these use cases, performance really matters.\n\n\nSo if you're building a product that relies heavily on S3,\nthere are a few interesting optimizations that you might want to leverage.\nIn today's episode, we're going to talk about some of the lessons\nwe've learned and some of the tips and tricks\nthat we've discovered along the way working with S3 at scale.\nMy name is Eoin, I'm joined by Luciano\nand this is another episode of the AWS Bites podcast.\n\n\nAWS Bites is brought to you by fourTheorem,\nan AWS consulting partner with tons of experience with S3.\nIf you need someone to work with you to optimize your S3-based workloads,\ncheck out fourtheorem.com or contact us directly using the links in the show notes.\nWe already spoke about S3 best practices back in episode 33.\nNow that was more of a generic episode on a variety of best practices\nthat are relevant to using S3, but we did give a quick intro\non what S3 is, the related terminology.\nSo if you haven't checked it out, it might be a good one to go back to.\nToday though, we're going to assume you already have a little bit of basic knowledge\nabout the service and how it works, and we're going to focus mostly on performance.\nBut let's give a brief intro. Luciano, where would you like to start?\n\n\nLuciano: I think it's a good idea to still review how S3 works under the hood, because I think understanding, at least at the high level, what's the machinery behind it,\nit's important to really understand why certain performance or activities actually work.\nSo if we want to just start with some stats,\nthis is something that we can just observe to understand the scale of the service.\nAnd this is coming from a presentation that's maybe a little bit obsolete at this point,\nbecause it's a presentation from reInvent that was delivered in 2021,\ncalled Deep Dive on Amazon S3.\n\n\nIt's a really good one, so we'll leave the link in the show notes.\nBut the data that they share there is that S3 stores exabytes of data.\nThis is 1 billion gigabytes, I had to look that up, across millions of drives.\nSo you can imagine that AWS somehow has to manage this huge amount of physical drives\nwhere all your data is going to be stored in a way or another.\nSo this is the level of complexity that AWS is taking care of for you,\nso you don't have to worry about the kind of management of physical devices.\n\n\nNow, there are a list that's, what they say, trillions of objects stored in various S3 markets.\nSo all these drives are effectively a distributed system that shares all these trillions of objects.\nAnd the service can handle millions of requests per second.\nSo I hope that all these numbers give you an idea of the volume and the scale of the service.\nThere is another one, they even say that they can reach a peak of 60 terabytes per second of data processed.\n\n\nSo again, how is that magic happening?\nWe don't necessarily know all the implementation details.\nBut the interesting thing to know is that AWS does all of this at scale and still guarantees data durability.\nAnd the way they do that is by storing your data in multiple copies in different places.\nSo we are obviously talking about the distributed system here,\nbecause it wouldn't be possible to reach this level of scalability with just one big machine, of course.\n\n\nNow, if we remember the networking basics, you know that there are regions,\nand inside regions there are availability zones.\nAnd you can imagine an availability zone as a separate data center with independent connectivity, power, and so on.\nSo in most cases, and I say in most cases because there are certain configurations that you can tweak,\nbut by default S3 stores your data across multiple availability zones.\n\n\nThat basically means that as soon as you send an object to S3,\nAWS is automatically copying that object across independent availability zones.\nAnd then you get an acknowledge.\nThat means that at that point your file is saved securely across different locations.\nNow, in all that process, at some point the data is being stored in a physical disk.\nAnd you can also imagine that it's stored in many of them,\nbecause of course if the data is living in independent locations,\nthere are independent disks that are keeping different copies of your data.\n\n\nSo you can imagine that managing all these disks is tricky,\nand AWS needs to really have a solid process to check for physical device failure.\nAnd they actually can predict when the devices fail,\nand they can actually replace them before they actually break.\nAnd they can do all of that without basically losing access to your data.\nSo they can still do all this swapping of disks and making sure that your data is always available and durable,\nwithout you having any interruption of service.\n\n\nThere is another cool feature that you can enable, which is called cross-region replication.\nSo by default a bucket lives in one region, and the data is shared across multiple availability zones.\nBut if you want extra guarantees, or maybe you want lower latency because you might have\nthe necessity to access to that data from different locations around the world,\nwhat you can do is you can enable this cross-region replication.\nAnd what happens is basically for every object you create in a bucket,\nyou can replicate that object in other regions as well.\nA bucket exists in other regions.\nAnd you can even make the data available to any location through something called\nAWS Global Accelerator. And we'll mention some around that a little bit later in this episode.\nSo hopefully that gives you an understanding of the scale and the things that AWS takes care of\nfor us when we use this service. So probably this is a good point now to jump to the first\nperformance tip.\n\n\nEoin: ... 10,000 parts and you don't even need to upload them in order. So every part is, I think,\nbetween the limits. It has to be between five megabytes and five gigabytes per part.\nSo if you've got a three megabyte file, you wouldn't use a multi-part upload for it.\nIt has to be at least five megs. And AWS generally recommend you use something like eight or 16\nmegabytes for your part size. When you upload a single part, S3 will return to an entity tag,\nalso known as an ETag for the part. And you record that with the part number.\n\n\nAnd when you do the third step in the process, which is complete multi-part upload,\nthen you essentially provide a manifest of all of the part numbers and ETags with that request.\nYou can even send AWS a checksum of the original file to make sure everything was transferred\ncorrectly. And it's not a checksum of the entire object, but rather each individual part.\nThere's a link in the show notes to a user guide that will help you to understand that process.\n\n\nYou generally don't have to do this yourself since most of the SDKs include some higher level\nabstraction in the API, or in the SDK for uploads and downloads, actually. But the upload part will\ngenerally automatically use multi-part uploads when it makes sense. And we'll provide links to\ncode samples, the SDKs, including one example is the Node.js helper library, which is the lib\nstorage in the AWS SDK version three. You can also do some cool esoteric things with this as well.\n\n\nI remember having a case before when we needed to essentially merge a lot of CSV files. And those\nCSV files didn't have headers in them. So we were able to do that just using S3 features. Because\nwhen you specify a part for a multi-part upload, it doesn't have to be something that's on your\nclient machine, it can also be an existing object on S3. So you can use it just to concatenate a\nbunch of files on S3 without any of that data, leaving S3 and being transferred to your machine.\n\n\nNow, let's get on to multi-part downloads, or as it's better known, byte range fetches. So when\nyou're doing a get object command, you can specify the start and end range for bytes. And if you want\nto download the entire file, it's generally not built into the SDKs. But there are examples of\ndoing of implementing this yourself, we'll provide a link to that in the show notes.\nThere is a very interesting podcast episode and a library associated with it from our friends at\nCloudonaut. And they had a very specific need for one of their products to download large,\nlarge objects from S3 in Node.js and implemented a highly optimized library for it. So you can check\nthat link out in the show notes as well. So that's tip one. Basically, use concurrency,\ndo multi-part uploads and byte range fetches for downloads. What else should we suggest, Luciano?\n\n\nLuciano: Another common thing is to try to spread the load across different key namespaces. And I\nthink to really understand this one, we need to explain a little bit how some of the details of\nhow S3 stores the object and what are some of the limits. Because if you look at the documentation,\nwhat the documentation says is that you can do 3500 put, copy, post, or delete operations,\nand 5500 get and head operations per prefix. And this is where things get a little bit confusing,\nbecause what does it mean per prefix? And if you look at other parts of the documentation,\nthere is an official definition that says a prefix is a string of characters at the beginning of the\nobject key name. A prefix can be of any length, subject to the maximum length of the object key\nname, which is 1204 bytes. You can think of prefixes as a way to organize your data in a\nsimilar way to directories. However, prefixes are not directories. So you can kind of make the\nparallel that a prefix is like saying, I don't know, \"/home,\" \"/luciano,\" \"/documents,\" and then\nthe name of your object. But behind the scenes, AWS is not really maintaining a file system. It's\njust a way for you to organize your data. What is interesting, though, is that somehow\nAWS is using this information to distribute the data across multiple partitions. And this is\nprobably where the limit conversation comes from. You can do a certain amount of operations per\nprefix, but that probably really means per partition. And this is something that is not\nalways entirely clear. What is the logic that AWS uses there to define how prefix maps to actual\nphysical partitions? So it's something that AWS tries to determine automatically, depending on\nyour usage patterns. But what we have seen in the wild is that if you really do lots of requests,\neven if you have different prefixes, you can still get throttled and see 503 errors.\nSo it is really important if you're running at such scale to monitor the number of 503s,\nbecause if you're using the SDK, there are retries. So eventually you might be able to\nget your operation successfully performed. But that operation might take a long time,\nbecause there is a loop of retries that is happening behind the scenes. So you need to\nbe aware if you're trying to get the best performance when retries are happening.\nAnother interesting thing that we bumped into working with one of our customers\nis that we were still getting lots of 503s and at some point we decided to talk with support.\nAnd it was a long conversation. We got lots of help from AWS, but it seems to be possible to get\nAWS to tweak whatever is the internal mechanism for your specific use case. So if you're really\nhitting all these limits and you don't know what else can you do, I think the best course\nto action right now is to just open a ticket, try to talk with AWS, explain your use case.\nAnd I think they might be able to discuss with you very custom options that are the best solution for\nyour particular use case. I think this is still very rare in the industry. We only had one use\ncase, at least that I can remember on in my career. But again, if you happen to do thousands and\nthousands of requests to AWS per second, it's not unlikely that you're going to bump in this\nparticular limit action. So just be aware that there are solutions, even though the solution is\nnot necessarily well documented, but you can talk with AWS and they will help you to figure out the\nsolution. Overall, the idea is to try to think about namespaces that make sense and then distribute\nyour access, your operations to different namespaces if you want to leverage as much\nrequests per second as possible. What's the next one you have, Eoin?\n\n\nEoin: The next one is going down to the network\nlevel. So it's a fairly common design\npattern in networking\nand files storage to horizontally scale\nperformance using multiple\nconnections. So if you're making\nrequests from one network device to\nanother, you might bump into some\nbandwidth limits of that\ndevice and devices in between. So\ndistributing the requests\nacross multiple devices, multiple\nend-to-end connections can definitely\nhelp you to achieve higher\nthroughput. So if an example of\nthat is born out, again, going back to\nthe Cloudonaut example, they realized that\nconnecting to S3 from\nan EC2 instance, there's a limit of five\ngigabits per single VPC flow.\n\n\nAnd a VPC flow is defined as\ncombination of source IP, source port,\nand then a destination IP\nand destination port. And if\nyou're just doing a fairly simple HTTP\nrequest to an S3 endpoint, you're going\nto get a DNS lookup.\nIt's going to give you back an IP or a\nset of IP addresses. Your\nclient is going to pick one\nand make the connection to that IP\naddress. But if you're a\nlittle bit smarter about it,\nyou can take all of the IP addresses for\nS3 back and use that to\nget multiple connections\nfrom your source to the destination. And\nthat's exactly what that\nclever library from Cloudonaut\ndid. It's something this load balancing\non the client is\nsomething that the AWS CRT,\nthe Common Runtime does as well. So the\nAWS CRT library, which is\nused in the Java SDK and Boto3\nas well, has the capability to do that\nand do all this download performance\noptimization too. So\nit's worth checking out. And then as\nwell, on the topic of network\nconnections, different\nenvironments vary, different EC2\ninstances have different bandwidth\ncharacteristics on the network\ndevices. And then you have enhanced\nnetworking and elastic fabric\nadapter to really squeeze more\nperformance out of it. Also bear in mind\nthat when you're running\nan AWS Lambda, your network\nconnection size depends on your memory\nconfiguration because it's linearly\nproportional. So if you're\nfinding that bandwidth is a constraint,\nyou might think about,\n\"Okay, well, can I do multiple\ndownloads and multiple functions or do I\njust need to up the\nmemory so that I get maximum IO\nthroughput on that as well?\" So that's\nthe lower level performance\ntips. What else do we have, Luciano?\n\n\nLuciano: Another interesting one is the\nusage of the edge. Let's\ncall it like that. The idea is\nthat you can enable something like Amazon\nS3 transfer acceleration.\nThis is more when you have\nuse cases where you might, for instance,\nbe building a web\napplication and you might have users\nthat are connecting from all around the\nglobe. And of course, if you\nstore your data in a bucket\nthat exists only one region, you might\nhave good latency for all\nthe users that are close to the\nregion and very poor latency for all the\nother users that may be\nvery far away from the region.\n\n\nSo one way that you can solve this\nparticular problem and give\nmore or less similar performance\nto all the users, regardless of where\nthey are around the globe, is\nto enable this feature called\ntransfer acceleration. And there are some\ndata that AWS shares in\ntheir marketing page where\nthey say that this can improve as much as\nbetween 50 and 500% performance for\nlong-distance transfer\nof large objects. So that means, imagine\nthat you have a bucket somewhere in\nEurope and a user from\nAustralia is connected to that bucket.\n\n\nYou can imagine that there\nis by default significant\nlatency, but enabling this kind of\nfeature will reduce that latency\nsignificantly. And this is\na feature that you need to enable\nbecause, of course, there is a\nsignificant complexity to\nmake all of this happen for you. Data is\nreplicated effectively\naround the globe for you.\nSo it's something that you enable it and\nyou need to be aware that you pay a\npremium price for it.\n\n\nSo it's not a free feature. So of course,\nit makes sense to use it\nonly when you really have that\nparticular type of use case, not just\nenable it because it might\nseem like a convenient thing to\ndo. And if you know CloudFront, this\nfeature is effectively leveraging\nCloudFront under the hood\nand is just distributing the data across\ndifferent edge locations and then using\nthe AWS Packable Networks to make sure\nthat the connection between\nyour actual region and the edge\nlocation is as fast as it could be. This\nis a feature that you need\nto enable at the bucket level.\n\n\nSo you just go to the bucket setting and\nyou can enable it there\neither from the UI or with the CLI.\nAnd effectively, then when you want to\ndownload a file from S3\nusing this particular feature,\nyou have to specify a special endpoint\nthat is called\nhttps://s3-accelerate.amazonaws.com\nSo that basically, rather than going\ndirectly through the bucket endpoint,\nis going to go through this special\nendpoint that uses the Edge Network.\n\n\nNow we can give you a lot more details,\nbut it's probably going to be more useful\nfor you to redirect to the documentation\npage. We have a link in the\nshow notes if we're curious\nto really find out how do you really\nenable this and kind of how\nto with all the steps that you\nneed to follow if you really want to\nimplement this particular feature.\nAnd the other option is if you are just\nbuilding a website, for\ninstance, and you want to make sure\nthat all the static assets of that\nwebsite are available in the edge\nlocations, you can use\nCloudFront directly. So you just enable a\nCloudFront distribution.\nCloudFront is effectively a CDN.\nSo that will make your object available\nin different locations\naround the world. AWS\nclaims that they have about 400 edge\nlocations. So this is\nprobably going to have a good\ncoverage for all around the globe. And\nyeah, if you're doing all\nof that, there is another\nextra advantage because at that point you\nare serving an entire\nwebsite from an S3 bucket.\nBut if you just enable the S3 website\nfeature, that by default is\nonly HTTP, which is not really\nideal these days. You probably want to\nhave HTTPS. When you use CloudFront, you\nalso get support for\nHTTPS. So that's one more reason to use\nCloudFront when you are\nserving just static assets for a\nwebsite. That I think concludes this tip.\nSo what do we have next?\n\n\nEoin: The next one and final one is a bit neat\nmaybe, but if your\napplication relates to tabular data,\nlike analytics or data science, you can\nleverage some of the\ngreat tools that optimize data\nretrieval from S3 for you to avoid\nreading data that you don't need. And\nthis goes back to our\nbyte range fetch really, but it's also\nsaying some of the tools are already\ndoing this for you on\nthe hood and you don't even really need\nto think about how it works. And the\nsimplest one of all is\nS3 Select. This is an S3 API. It's\navailable in all the SDKs\nand in the console. And it's\npretty straightforward. It allows you to\nretrieve specific rows and\ncolumns of data from S3 using\na simple SQL-like syntax. So you could do\nselect columns from table,\nwhere, and put some simple\nwhere clause. There's no joins or\nanything complicated like\nthat in it. It's just for\na single table. And that avoids you\nhaving to retrieve large\nvolumes of data over the network\nand you push the heavy lifting onto S3.\n\n\nNow, if you're doing\nsomething a bit more complicated\nand you're in this space, you might be\nfamiliar with the arrow,\nwhich is core to a lot of these\ndata science tools, the arrow library and\nformat tools built on top\nof it, like pandas and polars\nand DuckDB. These all ensure you don't\nhave to read all of the\ndata if you don't need it,\nparticularly if you're using optimized\ncolumn number formats like\nParquet. All of these tools\ncan intelligently like with Parquet file,\nfor example, the\nmetadata describing what data is\nin it is at the bottom of it. So those\ntools will go read the\nfooter from the Parquet file,\nthen they can figure out where the\ncolumns are stored in the\nfile and where the row groups are\nthat's split into groups. And then they\ncan retrieve only the\ndata you need. Polars and\nDuckDB are particularly fast when it\ncomes to this kind of use\ncase. They'll leverage those\nbyte range queries automatically for you\nand are surprisingly fast\nin how they can run and are\nalready putting a lot of engineering\neffort into optimizing things\nlike object retrieval from S3.\nSo you don't even have to think about it.\nSo in terms of additional\nresources, we're going to\nthrow a bunch of links in the show notes,\nwhich we hope are valuable,\nincluding some performance\nguidelines on S3 and design patterns.\nApart from that, let us know if you have\nany more S3 performance\ntips. I'm sure there's more out there.\nJust let us know in the\ncomments. Thanks very much for\njoining us this time, and we'll see you\nin the next episode.\n"
    },
    {
      "title": "125. A first look at CloudFront Hosting Toolkit",
      "url": "https://awsbites.com/125-a-first-look-at-cloudfront-hosting-toolkit/",
      "publish_date": "2024-06-14T00:00:00.000Z",
      "abstract": "In this episode, we discuss the newly announced CloudFront Hosting Toolkit from AWS. We provide an overview of the tool, which aims to simplify deploying modern front-end applications to AWS while retaining infrastructure control. We discuss the current capabilities and limitations and share our hands-on experiences trying out the tool. We also talk about alternatives like Vercel and Amplify, and the tradeoffs between convenience VS control. Overall, the toolkit shows promise but is still early-stage. We are excited to see it evolve to support more frameworks and use cases.\n\nAWS Bites is brought to you by fourTheorem an AWS consulting partner with tons of experience with AWS. If you need someone to help you with your ambitious AWS projects, check out fourtheorem.com!\n\nIn this episode, we mentioned the following resources:\n\nCloudFront Hosting Toolkit official announcement\nPrevious episode &quot;80. Can you do private static websites on AWS?&quot;\nPrevious episode &quot;3. How do you deploy a static website on AWS?&quot;\nCloudFront functions\nCloudFront Key-Value Store\nSandro Volpicella's article on CloudFront Hosting Toolkit\nOpen Next\nCoolify\n\n",
      "transcript": "Luciano: A couple of weeks ago, AWS announced a new open source tool,\nCloudFront Hosting Toolkit, a new tool that promises to streamline the experience\nof deploying front-end applications to AWS while retaining full control of the underlying infrastructure.\nWe are particularly excited about this announcement because it was recognized by some people in the community\nas the AWS response to something like Vercel.\n\n\nWe took CloudFront Hosting Toolkit for a spin and we are eager to tell you what we found out.\nIn this episode, we'll discuss all about CloudFront and Toolkit,\nbut we will also talk about modern front-ends and what's the expectation for a modern web application these days,\nbrought from a user perspective, so visiting an application,\nbut also from a developer perspective, so building that kind of application.\n\n\nWe will then discuss CloudFront Hosting Toolkit and see how it stands against this way of seeing modern front-ends.\nMy name is Luciano and as always, I'm joined by Eoin for another episode of AWS Bites podcast.\nAWS Bites is brought to you by fourTheorem, an AWS consulting partner with tons of experience with AWS.\nIf you need someone to help you with your ambitious AWS projects, check out fourTheorem.com.\n\n\nYou can contact us directly using the links in the show notes.\nSo maybe we can start this episode by describing what a modern front-end application looks like\nbecause for many years, the golden standard of front-end application has been single page applications.\nAnd if you're not familiar with that definition, a single page application is effectively,\nyou can imagine like a single HTML shell that is loaded and that shell only loads some JavaScript.\n\n\nAnd inside that JavaScript, you have all the logic that your application needs.\nSo that JavaScript will do client-side routing, which basically means that once the application is loaded,\nthere is JavaScript logic that will look at the current URL and decide which components need to be mounted\nor unmounted to effectively refresh the page dynamically.\nIt will also do client-side HTTP requests, something that has been called originally Ajax requests.\n\n\nIn the past, now it's more called fetch requests.\nBut the idea is that once the application is loaded, it doesn't really have any dynamic data,\nsomething that you might keep, for instance, in a database.\nSo whenever you need to load dynamic data, you need to do HTTP requests to a backend API,\nand that API will give you the dynamic data that then can be displayed in the page.\nSo this basically means that you can also the entire front-end as a static website\nin something like CloudFront, for instance.\n\n\nAnd the great part of all of this is that you don't need a backend at all.\nIt's just static files. The browser can load them, and all the business logic needed for your front-end\nexists in those JavaScript files loaded by the browser.\nAnd this keeps things very simple, and it can be also very cost efficient,\nbecause once you host in S3 and CloudFront, that can scale to a massive amount,\nstill with very little cost.\n\n\nThe main frameworks that emerged in the last decade that try to cover this kind of use cases\nare probably React, Vue, and Angular.\nThere are so many others, and your framework comes out every week.\nBut these are probably the three main ones that you will find in most projects.\nNow, there are some problems with this approach, this SPA's approach.\nOne problem is that you have perceived loading times that can be very long.\n\n\nAnd this is because until all the JavaScript is loaded in the page, you basically see an empty page.\nYou see just a white screen, and it might look like the website is broken if it takes too long.\nAnd with some framework, it's actually very easy to forget to do some optimization,\nand you might end up with your JavaScript bundle that can be multiple megabytes.\nSo really, lots of people might have this risk if they underestimate the effort and\noptimize the JavaScript code that you just deploy your application,\nyou expect everything works perfectly, but then the user has a very bad experience.\n\n\nWe also mentioned that all dynamic data needs to be loaded externally.\nSo that means that for all the kind of stuff, you need to build your own API,\nyou need to deploy that API independently, and then your UI still needs to fetch all of that\ndata, which might add to the delay before the user can see something useful.\nAnd that also creates some SEO problems because search engines will access your website,\nbut then everything is happening after JavaScript is loaded, and not every search engine will\nactually wait for that amount of time or even try to run JavaScript.\n\n\nSo your indexing against search engines might be not necessarily the best because it's going to be\nvery unpredictable what different engines will be able to see from your website.\nSo to address all these problems in the last few years, this trend of building UIs has evolved a\nlittle bit, and it seems like we are going back to server-side rendering. So what we used to do\nin the times of PHP, Ruby or Rails, Django, basically you always have a server that's\ngenerating all the HTML, all the JavaScript, CSS for your websites, and you can generate all of\nthat stuff dynamically. So whenever you need to read data from a database, your server will do\nthat, compile the final page for the user, and then send all the assets to that.\n\n\nSo in the JavaScript space, there are some new tools that have emerged to try to satisfy this\nneed, and probably one of the main ones is Next.js. And Next.js basically allows you to do\neverything still with JavaScript. So you will be able to use, for instance, if you like React,\nyou will be able to use React components both on the client-side and the server-side. And Next.js\ngives you an experience, a development experience, where it's easy to perform actions on the server\nand pre-generate content, ship that content directly to the user, so it's going to be loaded\nvery quickly. And then whenever you need to do something client-side, you can still use all the\ntools that you generally use with React and do client-side stuff. So it's kind of an hybrid,\nwhere in the same application, you can define all the backend logic and the frontend logic,\nand wherever you need to share components, Next.js makes it very easy. Now, Next.js is an open source\nproject that is built and maintained by Vercel. And of course, Vercel has an interest in that\nproject because they have a commercial offering where they make it very, very easy to host Next.js\nprojects and more. And we think that their experience is actually very interesting.\nSo maybe we should talk a little bit more about that, just to see how this tool announced by AWS\ncompares to that kind of experience.\n\n\nEoin: The number one thing that you'd think about when you think about Vercel is probably the developer experience. And that involves how easy is it to get up and\nrunning and deploy your first application and keep it running. And the zero-config nature of Vercel\nis probably one of its best selling points because it's just very easy to connect your GitHub repo\nand have something deployed in a couple of minutes. It doesn't just limit itself to Next.js\nas well. It supports many other frameworks out of the box. And when you deploy it, you get the\nautomated deployment from your repo, but you also get custom domains and your HTTPS certificates\nare going to be provisioned for you. And then it'll also scale on demand for you. So it's\nusing other cloud providers under the hood, including AWS, and it's managing all of the\nscalability. But as well as just scaling within regions on cloud providers, it's also edge\noptimized. So it's doing edge up deployments as well to various CDNs and running a lot of those\nserver actions and server component rendering on the edge for low latency. One of the really\ncool things is that if you're working with branches, it will give you preview deployments\nfrom your branches. So this means that right away, when you create a feature branch or a fixed branch,\nand you're working on something, you immediately get a preview URL that you can share with\nstakeholders. And one of the cool features of that is that your participants can annotate\nand make comments in line in your preview environment. So you can iterate really quickly\nwith feedback. It also has a whole set of other integrations with other products like databases,\nyou can get a Postgres database up and running pretty quickly. You get logs,\nthey've been putting a lot of investment into AI support recently, CMS integration, and also other\nthings like analytics and important security aspects as well. So there's a whole lot to love\nabout Vercel, but they're not a sponsor of this episode. So while we like the developer experience\nand the feature set, it's still a software as a service, and you have a build or buy decision,\nand you have to evaluate the vendor on their own merits. If you want some more control autonomy\naround the infrastructure, you might want to deploy it to your own AWS account. And Vercel\nisn't something that will allow you to do that. So if you use Vercel, you don't have a lot of\nvisibility into how your code is deployed and where it's going under the hood. And especially\nin the context of working with enterprise organizations, there are a lot of good reasons\nto avoid Vercel and to host front end applications yourself directly on AWS. So where do we start?\nIf we want to host a front end on AWS, this is something that we've talked about a lot. It's a\nbit of a bugbear of ours, I suppose, just trying to find the right most optimal way to host static\nwebsites. We covered it in one of the very first episodes, episode three, how do you deploy a static\nwebsite on AWS? And then back in episode 80, we were talking specifically about private static\nwebsites for enterprises. And there was always, it's always a trade off, no clear winner there.\nBut now AWS has announced a new open source tool to help you tackle this problem. And this is the\nCloudFront hosting toolkit. So this certainly got a lot of people's attention, a lot of people\ntalking because just you only have to mention Vercel competitor to get people's ears pricked\nup. And Luciano, you've been good enough to take a good look at this for us. So maybe your best\nplace to give us an introduction, what is this CloudFront hosting toolkit and what can we do\nwith it?\n\n\nLuciano: Yeah, if you look at the announcement blog post, we will have a link in the show notes, they describe this tool as an open source command line interface to help you deploy fast and secure\nfront ends in the cloud. So that kind of tells you what is the ambition of the tool. And the idea is\nto try to remove as much as possible the complexity of setting up everything you need\nto run your own front end on AWS. So you have a CLI, you run the CLI, it can, once you install it\nand run it against your particular repository where you are building your own front end,\nit can detect the framework of choice. For instance, Next.js is one of the supported ones.\n\n\nAnd at that point, it can start to generate configuration for you, for instance, recognizing\nhow do you build for this particular front end. It integrates with your repository. Right now,\nit supports GitHub. And basically what it does, the CLI will allow you to connect your GitHub\nwith a build pipeline hosted in AWS. And also at that point, it can create all the necessary\nresources. So for instance, what do you need to host a front end? You would need an S3 bucket,\nyou would need the CloudFront distribution, DNS record, TLS certificates, and more. It will create\nall of that stuff for you using infrastructure as code. So you don't really need to worry about\nbasically coding all these things or figuring out where to find all of the examples to put all these\nthings together. So that removes a lot of the friction of self-hosting your own front ends.\n\n\nAnd everything that it does, as I said, is done through infrastructure as code, and it's made very\naccessible. It's still deploying through CloudFormation so you can see exactly what's\nbeing deployed. You have configuration files that gets generated for you. So you have also\nopportunities to still sticking with the same process that is given to you by the CLI. You can\ntweak and configure certain things to your liking. And then even if at the end of the day, you still\nwant to change more, all the infrastructure is there, all the CloudFormation is there.\n\n\nSo you can take that and do further customization if you need to. And somewhere else, I think in the\nrepository, they say something that to me represents the ambition of this project. And it reads us,\ngives you the convenience of a managed service while putting you in full control of the hosting\ninfrastructure and deployment pipelines to make it your own. So that's a good marketing phrase to\nsay, yes, we want to make it easy for you, but at the end of the day, this is your own infrastructure.\n\n\nYou'll have access to it and you'll be able to change everything you need.\nYou can have your cake and eat it.\nSomething like that, yes. But now we get to the more painful notes, I guess, because yeah,\njust to be real, we need to say that the current state is still a version 1.0 and it's still very,\nvery early. Well, it was announced only a few weeks ago and it's fair to say that it's still\nvery early days. So if our feedback is going to look a little bit harsh in some parts,\nat the same time, you have to think that this is an evolving project. It's only early stages,\nso it can only get better. So let's be fair in that sense. So if you look at the supported\nfeature, right now it supports everything that you need to host an SPA that is statically generated.\n\n\nAnd that is a very important note because it means that right now you cannot do any server\nside rendering. By default, it doesn't support any of that. So in a way, it's still a little\nbit behind the current trends. And if you use, for instance, Next.js, Next.js allows you to do\nwebsites that are only static generated with some tweaking in the configuration and just by not\nusing certain specific features, you can do that. But you need to be aware that this is an existing\nlimitation. If you want to use this tool with Next.js, you can only use the features that are\nsupported with the pre-rendered static version of the website. The other feature is that, as we said,\nintegrates really well with GitHub for version control. It's going to do for you atomic and\nimmutable atomic deployments on CloudFront. We'll talk a little bit more about that later. It's a\npretty cool implementation. So I think it's worth spending some time on how all of that works.\n\n\nIt's a CLI tool, so very easy to install with NPM, very easy to run it in any new project. It takes\nliterally a few seconds just to get started. And also it does security and caching best practices.\nSo all of these things are already incorporated in the generated infrastructure. So one less\nthing to worry about because if you were doing all of this yourself, maybe you get something working\nafter a few hours of investigation and trainings, but then at the end of the day, you might still be\nunsure on whether that's something that can be production ready or not. This tool removes a\nlittle bit of that concern because it's already following tons of best practices that have been\nestablished in the AWS landscape for the last few years. And then it allows you to use custom\ndomains. So if you want to host your own front end in your own custom domain, all of that process\nis streamlined. It's going to integrate with Route 53 and generate the necessary records,\ngenerate the TLS certificates for you, and make sure everything is integrated properly. If you\ndon't have a custom domain, it's going to use a CloudFront domain for you. And finally, there is\nalso a CDK level three construct for the toolkit that can be used as an alternative for the CLI.\nSo that's maybe another option if you prefer to go more with something that is integrated with your\nown CDK rather than using the CLI. But I mentioned atomic and immutable instant deploys, which is a\nlittle bit of a mouthful. So do we want to try to explain what that really is and how it is\nimplemented? Yeah, it's pretty neat.\n\n\nEoin: It uses CloudFront functions and CloudFront key value store, which you may not have come across. Let's go through these fundamental pieces of CloudFront\nfirst. So CloudFront functions are a little bit like CloudFlare workers, I guess, or other edge\nfunctions that you might've come across. They're like a much simpler version of Lambda because you\nalso have Lambda at the edge, but Lambda at the edge is more like a fully fledged Lambda,\nalthough not quite. CloudFront functions are just more like lightweight JavaScript isolated\ncontexts that are much more efficient, but also restricted in terms of what packages you can use\nand that sort of thing. So you could use CloudFront functions for writing lightweight functions for\nhigh-scale latency-sensitive CDN customizations like redirects or request response modification,\nURL rewrites, that sort of thing. And then you have CloudFront key value store. And this is a\nbit like a simplified version of DynamoDB that runs on the edge. And we've seen this kind of\nservice from CloudFront, sorry, from CloudFlare as well. It's a global low latency key value store,\nand it allows you to read from CloudFront functions. And by having that state in your\nkey value store, you can do more customizable logic at the edge functions. So when you have\nCloudFront key value store, you can make updates to function code and updates to the data associated\nwith the function independently of each other. This kind of separation will simplify your\nfunctions code and makes it easy to update data without the need to deploy code changes.\n\n\nNow, when it comes back to this CloudFront hosting toolkit, the basic idea is that every time you\nwant to deploy a new version of your front end, rather than replacing the entire content of the\nS3 bucket hosting your assets or overwriting, you'll create a new unique deployment ID and\nhost all the files for a deployment version in a dedicated prefix, like a subfolder. So let's say\nyour first deployment ID is version one, then it'll create a prefix like slash version one.\n\n\nAnd then within that, you'll have your index.html and your bundled JavaScript. And then with your\nsecond deployment, it would be version two under a slash version two prefix. So when a user will\ntry to access a page by doing get index.html, this request goes to CloudFront first and the\nCloudFront function is the first bit of logic that gets evaluated. It can go to the CloudFront\nkey value store to read the ID of the latest deployed version. And the function will then\nrewrite the URL to point to the specific assets in the S3 origin for that current version.\n\n\nSo if you're going, making a request to get slash about that would become get slash version two\nslash about dot html, for example. So then it'll load that specific asset from S3 or cache and\nreturn it to the user. And then when you have a new deployment, it'll just create a new prefix\nin S3 to host all those assets. When all is done, the new version ID is stored in that key value\nstore. And the next time somebody is accessing the website, they'll get the latest version. Now,\nthere's a few nice things about this approach. It's an immutable deployment approach, which means\nthat every deployment is clean and new, and you can troubleshoot it in its own space. But you also\nretain older versions, which would make it easy to do rollbacks if you have to do that. There is\nlittle risk then that a user will receive assets from different versions, which is a common problem\nif you're just kind of overwriting into a single prefix in S3 or if they're accessing the website\nduring an update. The other nice thing about this is that it could easily support preview\nenvironments. But we haven't seen anything showing that this is supported yet in CloudFront hosting\ntoolkit, but it seems like this kind of architecture would certainly enable it. So\nmaybe let's talk about your real world practice. You've taken it for a good test drive CloudFront\nhosting toolkit, and you've had some successes and some bumps in the road Luciano.\n\n\nLuciano: So I'm going to try to describe the steps that I performed to try to test this tool and the different things that I found out.\nAnd hopefully that can give you an idea of how the tool works, the kind of developer experience\nyou get and what gets created for you behind the scenes. So I started with a plain vanilla Next.js\nproject. There is one CLI command that you can find in the Next.js documentation that basically\nuses a starter kit. And that starter kit is just a simple repo where you have a one Next.js page\nthat points you to the documentation, to the about page and a few other simple things. So effectively\nit's like one page application, and it's like a static, statically renderable application because\nit doesn't really do any server side action or anything like that. So you run this first command,\nthis command will do all the scaffolding for you. At that point, you can do npm install to install\nthe dependencies. And if you want to run your frontend locally, just to see it running and play\naround with it, you can just say npm run dev, and that will give you locally an environment that you\ncan use to tweak things around. Then at some point you are maybe happy with it and you want to try to\ndeploy it using this CloudFront hosting toolkit. The first thing you need to do is install the\ntoolkit itself. We said it's a CLI application, it's built with Node.js. So of course Node.js\nbecomes a requirement at that point. But once you do that, you can easily install with NPX or just\nby doing an npm install global of the @aws/cloudfront-hosting-toolkit. And\nthat will basically pull the latest version of the toolkit for you in your local development\nenvironment. Now the idea is that you need to use it against an existing repository. So this is why\nI created an Next.js project first. Once I did all of that and I was happy with the outcome, then you\ncan start the tool. And the way you start the tool, you go with your CLI into the specific folder where\nyou have your frontend application and you just run cloudfront-hosting-toolkit init\nThat's literally the one command you need to run. And at that point, you have a guided process\nthat will effectively ask you some questions to try to understand how to deploy this project for\nyou. And the first question that it's going to ask is what is your repository? And of course,\nit's going to be able to auto detect it from the current working folder. If you have already\ninitialized that as a Git repository and it's hosted on GitHub, it's going to automatically\ncomplete that particular question. But you can change it if you have different requirements,\nor maybe if you haven't set up the repo yet locally, you can still point it to a repo that\nyou have set up already remotely on GitHub. The next step is going to ask you the name of the\nbranch for deployments. It defaults to main, but I think you can use this if you want to change that\nto something else. Maybe you want to have a branch called deployment or deploy that you specifically,\nwhen you want to trigger a deploy, you can decide which branch is actually going to trigger the\ndeployments that way. The next question is what is your framework of choice? And here, you have a few\nframeworks that are supported. AngularJS, Next.js, React, Vue.js. There is another one which is\nno build required, which basically means everything that is in the main folder is going to\nbe deployed. And I think this is for static websites that don't require a build process. Maybe\nyou just created a few simple HTML, CSS, and JavaScript files. You have them there and you just\nwant to deploy them. The cool thing here is that it was able to automatically detect that I was\nusing Next.js. So that option was already selected for me. I just needed to hit enter and proceed.\n\n\nBut of course, you can change that if maybe it's not able to detect your framework, or maybe you\nare actually using something else and it didn't really detect it correctly. The next step is\nasking you if you want to use a specific domain name, or if you want to use the default CloudFront\ndomains. In this particular case, I didn't have a domain, so I just went with the CloudFront one,\nwhich maybe is a little bit of a simpler path. I'd be curious to try it again with a custom domain,\njust to see all the extra resources that it needs to create to support that. But I haven't done that\nyet. Once you've done all of this, this is basically the initial setup. So it's basically\nmaking sure that it understands your project and the configuration of your project. But at that\npoint, it can start to generate things for you. And it's interesting to see that it generates a\nfolder called the ./cloudFront-hosting-toolkit. And in this folder, it will generate three files for\nyou. The first file is a code build job configuration. So it's basically the YAML file\nthat contains all of that configuration. And this is awesome because that means that if you want to\nchange anything in the pre-generated build step, because it's kind of assuming, okay, this is a\nNext.js project. I know how to build a Next.js project, but maybe you have additional requirements.\n\n\nMaybe you want to do additional things that are not the standard way of building an Next.js website.\nYou can easily change all the steps and customize it to your liking without having to change the\ntoolkit or having to change anything else in the infrastructure as code. So it's just this one file\nallows you to change things. It also generates in this folder the CloudFront function that is used\nfor URL rewriting, the one you described before, Eoin. And you can see all the JavaScript code that\nis being generated. So also there, you have an opportunity to change things around. Maybe you can,\nI don't know, add some extra logic to check if there is a query string parameter that says\nversion equals something. Maybe you can actually enable that feature preview that way and by\nyourself, assuming that it's not supported yet. But basically, this is just to tell you that you\nhave one file with all the logic that are visible. It's easy to change. And if you change it, this is\nwhat's going to get deployed for you. And then finally, there is a generic JSON file that contains\nall the options that were requested to you in the initial init phase. So if you change your mind,\nmaybe you don't want to deploy from main anymore, you want to deploy from another branch, you can\neasily change that JSON file to get your base configuration updated. At that point, what you\ncan do is that you can effectively deploy. At this point, you haven't deployed anything. You just\ngenerated these three configuration files, but those three configuration files are enough for\nyou to start the deployment. So you can run another command called cloudfront-osting-toolkit deploy\nAnd what it's going to do is basically going to take all these three generated\nfiles as an input, and it's going to create all the necessary cloud formation templates to deploy\neverything else that we described before. So it's S3 buckets, CloudFront distributions,\nthe CloudFront function, the CloudFront key-value storage, and there is a lot more. You can\ncheck out, there is a diagram in the announcement page and in the repository showing exactly all the\nresources that get created and how they are related to each other. And this is also going\nto do a few other things. So initially it's going to bootstrap your AWS account, which I think it\nmeans it's going to create a dedicated street bucket for the deployments. It's going to create\nthe repository, the CodeStar integration. And actually the CodeStar integration is really\ninteresting because what it does the first time you need to manually confirm that connection\nbetween your GitHub account and AWS. And it's something that you have to do in a browser where\nyou are effectively authenticated on both sides. So what it's going to do is going to generate\nthe request for you. Then it's going to redirect you to the AWS console and in the AWS console,\nyou can click to basically connect your own GitHub account. To be honest, this was amazing to see on\none side, but on the other side, in my case, it was a little bit annoying because I use,\nwhen I access to AWS, I generally use anonymous sessions because I might have sessions for\ndifferent AWS accounts. So when I was trying to connect to GitHub, it didn't have my own\ncredentials. And it took me a while to connect into that anonymous environment with GitHub,\nasking you MFA and confirmation by email, just because they see that it's a different session\nthat you haven't used before. But eventually I managed to get it working. And then all the\nlocal setup was done. The remote setup was done on AWS. And you just need to click enter to say\nthat you successfully connected GitHub with AWS. And then at that point it's going to proceed with\nthe cloud formation deployment. Now, at this point, I had a problem because once everything\nis deployed for you, it's also going to try to run the code build for the first time and try to do a\nfirst deployment of your front-end application. Like if you were doing your first commit,\nbasically. And that one actually failed because there is an issue with the version of Next.js\nthat I'm using. I'm using the very latest version and that latest version doesn't support Next\nexport, which is what the default build pipeline is using to create the static version of the\nwebsite. Now, this is something you can easily fix. You just need to create a Next config.js file,\ndo the modern way of saying that you want to produce a static website, which is just like\none option, I think it's export static or something like that. Commit that into your own\nrepository. And of course you also need to update the code build pipeline, not to use Next export\nanymore, because at that point, when you do Next build, it's already creating the final version of\nyour website for deployment. And this is basically where I stopped, but I could see that everything\nwas generated correctly. I could see that if I was doing another commit, the build pipeline would\ntrigger automatically and do all of the deployment we described. So I think even though there are\nsome steps that might be a little bit annoying, overall, that experience was quite pleasant.\nI don't know what you think so far, and if you have any final consideration on your end.\nYou know, we talked about this kind of stuff when we were talking about CDK in previous episodes.\n\n\nEoin: I personally have a bias against code, sorry, against tools that generate code that introduce\nanother level of abstraction, mainly because it's just another, you have to understand kind of\nwhat's going on under the hood, and then also what's happening in this abstraction layer. And\nif, especially with a tool like this that might not be perfectly stable yet, you might wonder,\nwill it suddenly generate a completely different set of infrastructure that might make it difficult\nto do seamless upgrades? And I'd much prefer to spend the time to provision all of the\ninfrastructure myself and really understand it more often than not. But I can certainly see\nthe value in it, because I have been in a case recently where I've been trying to deploy a static\nversion, like a SPA version of an Next.js app, with only client-side rendering, with static export\nto CloudFront, and it's definitely not a trivial thing. So I think there's, I definitely commend\nthe effort here to get all of this stuff working. It's not an easy thing to do, and it does provide\nlike a good blueprint for people to get started with it. So while my bias isn't necessarily drawn\nto a tool like this, I think it is pretty promising. I think there's some really good\nideas in here, like the immutable deployments, and it seems to be simple enough to use. There's\nobviously some rough edges that need to be polished. The fact that it doesn't support\nserver-side rendering may be a bit of a blocker for a lot of people at this point. Let's hope\nwe'll see a bit of evolution there. I mean, I kind of think back to one of the previous\nreleases from AWS we covered, which was the integration testing toolkit, the IITK,\nand it was also a similar kind of review, like a lot of promising stuff, but needs more work.\nAnd when I went back to look at that recently, because I did use it in a side project,\nit doesn't seem to have actually evolved that much. So I know, we know that AWS likes to\nrelease things early and get feedback from the user base, but let's hope that this one\nsees the love and attention that we need. What's your take?\n\n\nLuciano: Yeah, I absolutely agree with everything you said.\nA few things that I can mention in addition to that is that I was able to fix an initial issue that I had with the CLI not\ninstalling properly. So I just submitted a PR and I was impressed to see how quickly I got a review.\nThe PR was properly commented and released and very quickly all happened in the span of an hour.\nAnd the problem was fixed immediately. The maintainer was extremely welcoming. So that's\ndefinitely a good thing to see. And I think that creates a big opportunity for everyone that wants\nto contribute to this kind of project to be able to do that. It seems really like a very welcoming\nenvironment for people to contribute. So maybe if the tool doesn't have all the things you need\nand you are willing to invest a little bit on it, definitely think about contributing and\ngiving back and working with the maintainer to get to the point where this tool is actually\ngoing to be helpful for you and for other people. So that's absolutely a plus one for the tool and\nthe current maintaining team. Another thing that I think is worth mentioning is that there is another\nwalkthrough by our friend Sandro Volpicella. We will have the link in the show notes. He has also\ndone a similar kind of test ride of the tool and wrote a blog post detailing all the things that\nhe found out. I think there is even a few more things that he explains. Like for instance,\nthere is a step function that is used for updating the versions and he also goes through on\nhow that works and why it needs to be there and maybe how it can be improved. So if you want to\ndo an additional deep dive on the technology and get another perspective on the tool,\ndefinitely recommend to read that blog post. And then finally, I think it's worth mentioning a\nfew potential alternatives because we are effectively working with a customer that might\nneed this kind of stuff. So we are doing lots of research on what are the possible options there.\nAnd it's also a topic we are really passionate about. As you can see, we keep talking about this.\nSo other things that we think are going to be suitable, for instance, you can more or less\nrelatively easy, you should be able to create a container and then run that container with a Next.js\napplication with full server-side rendering either on AppRunner or Fargate. But then of course,\nyou have something that is running 24/7. So maybe not necessarily what you want to do if you\nwant a more serverless approach. And in that front, there is another project that seems very\npromising that is called OpenNext, which we haven't tried yet, but in the docs, it says that it also\nsupports server-side rendering on Lambda. So that seems maybe a closer re-implementation of what\nVercel does for you, but that you can deploy on your own on AWS. It's definitely worth a look.\nAnd hopefully we will have time to try it out and maybe give you an overview of what that looks like.\nSimilarly, there is another project called Coolify, which is a little bit more generic.\nIt's probably a modern take on Heroku. So it allows you to host your own PaaS and it does that\nin a serverless way. But as such, you should be able to host a container that you can run with\neverything you need to run an Next.js project. So another thing that we really like to try,\nwe haven't had the time yet, but if we do try it, we'll let you know our findings. And finally,\nthere is Amplify, which always gets mentioned every time that people talk about Vercel versus\nAWS. I don't personally have an opinion on that one because I haven't really used Amplify much.\nSo, oh, and I'll leave it to you to mention something about Amplify if you want.\nI've looked at, you know, I don't have a huge amount of experience either.\n\n\nEoin: I've used the Amplify CLI and the client SDK a lot. The managed side of Amplify is a whole other set of features.\nLike they allow you to easily provision storage and hosting and all sorts of other stuff. I know\nthat they've introduced a new generation two, and I'd love to do an episode on that in the future.\nBut from what I can see with the new generation two Amplify, it's also solving the same sort of\nproblem in a much more managed service kind of way. So it seems a little bit more like an AWS\nmanaged Vercel alternative. And I think from what I've seen so far, a lot of work has gone into\nthat. I haven't tried it, so I don't know exactly what the experience is like. Let us know if you\nhave, because I'd love to know. I have seen some online discussion. Actually Yan Cui was\nmentioning the release of the CloudFront hosting toolkit. And one of the principal SAs at AWS\nwas answering the question, well, why did you actually create this tool when you already have\nAmplify? And it seems like they explained pretty clearly that strategically they're basically\nsaying, if you want to manage service, use Amplify. But if you want a little bit more\ncontrol over the infrastructure and you're really just looking for something to generate that\nblueprint infrastructure, then that's what the CloudFront hosting toolkit is aimed to solve for\nyou. So at least that gives you the right kind of context for deciding which approach you might like\nto take. Awesome. And I think that covers everything we wanted to cover for today.\n\n\nLuciano: Let us know if you had a chance to try the tool yourself and what is your opinion, or if you've\nused any other alternative tool, what do you use and whether you like it or not. I think this is\nstill a topic that deserves a lot of attention. I don't think there is a final solution that is\ngoing to fit all the use cases. So I'm really good to see... Well, unless you use PHP and become a\nmillionaire that way. But yeah, let us know what are you using today. And I'm curious to see how\nthis space is going to evolve in the next few years, because I think there is still a lot of\ninnovation that needs to happen in this space to fit all the different use cases that people\nhave today. So thank you very much, and we'll see you in the next episode.\n"
    },
    {
      "title": "126. Bastion Containers",
      "url": "https://awsbites.com/126-bastion-containers/",
      "publish_date": "2024-06-28T00:00:00.000Z",
      "abstract": "This episode discusses solutions for securely accessing private VPC resources for debugging and troubleshooting. We cover traditional approaches like bastion hosts and VPNs and newer solutions using containers and AWS services like Fargate, ECS, and SSM. We explain how to set up a Fargate task with a container image with the necessary tools, enable ECS integration with SSM, and use SSM to start remote shells and port forwarding tunnels into the container. This provides on-demand access without exposing resources on the public internet. We share a Python script to simplify the process. We suggest ideas for improvements like auto-scaling the container down when idle. Overall, this lightweight containerized approach can provide easy access for debugging compared to managing EC2 instances.\n\nAWS Bites is brought to you by fourTheorem an AWS consulting partner with tons of experience with AWS. If you need someone to help you with your ambitious AWS projects, check out fourtheorem.com!\n\nIn this episode, we mentioned the following resources:\n\nOur previous episode 78. When do you need a bastion host?\nBasti: Securely connect to RDS, Elasticache, and other AWS resources in VPCs with no idle cost\nOur GitHub gist with a Python script you can adjust to your needs\n\n",
      "transcript": "Eoin: Hello and welcome to AWS Bites, episode 126.\nI'm Eoin and I'm here with Luciano.\nToday we're going to revisit the topic of secure access to private VPC resources.\nA while back we talked about SSH, SSM and bastion hosts in episode 78.\nRecently though we've been using a bit of a simpler approach that avoids EC2 instances\naltogether and uses containers instead. By the end of this episode you'll hopefully have all\nthe knowledge you need to use Fargate and SSM as a lightweight on-demand access mechanism for private\nresources in a VPC. AWS Bites is brought to you by fourTheorem. If you're looking for a partner\nto architect, develop and modernize on AWS give Fourth Theorem a call and you can check out\nfourtheorem.com. Luciano, when we're looking at VPC access, private VPC access, what problem are\nyou actually trying to address here?\n\n\nLuciano: Yes, so the problem is commonly when you have some resources that exist in a private VPC so they are not publicly accessible on the internet and at some\npoint you maybe you have a bug, maybe you need to do some manual intervention on a resource, you\neffectively need to access that resource whether that's a database or a server or a container but\nrunning on a private perimeter. Another example that we mentioned a lot is for instance if you\nhave a Redis cluster sometimes you just want to run some Redis commands just to see like maybe\nthe state was corrupted, maybe you want to validate why a certain bug happened that maybe\ninvolves Redis. So yeah, the problem is how do you connect to these kind of resources to do all these\nkinds of investigations or actions that you might need to perform for a resource that exists in a\nprivate VPC subnet. And generally speaking there are some traditional solutions.\n\n\nThe most common one and the one that has been existing for the longest probably is setting up a jump box or\nbastion host which is basically an EC2 instance that has a public IP. Maybe you are protecting that\ninstance with some security group that limits the IP ranges that can access it and then of\ncourse you have SSH keys but it's still an instance with a public IP so it's publicly\nreachable on the internet and that instance also has access to the private VPC so effectively what\nyou do you establish a connection to this machine using the public internet and then use that\nmachine as a tunnel to reach out into the private VPC and connect to the specific resource that you\nwant to access. Other approaches are using a VPN and another thing that we've been discussing in\na previous episode is still relying on the concept of a jump box but it makes it a little bit more\nautomated and on demand and it's a CLI tool called Basti that effectively makes it easy to create\nthat instance, create the connection for you, create the tunnel for you only when you need it\nso it's going to create it and destroy it depending on when you need that session and also when you\nwant to use it to access an RDS database it does a bunch of stuff to make that access to RDS even\neasier for you. So you can see that as an improvement on the classic idea of the jump box or\nbastion host. So all of these approaches are something we discussed in detail in episode 78.\nSo go and check out that one if you missed it we will have the link in the show notes but these are\napproaches that work but hopefully today we're going to be able to present a better solution\nbut maybe let's start by discussing what are the challenges with this particular type of approaches.\n\n\nEoin: Well we've definitely seen challenges where some companies actually have restrictions around\ncreating EC2 instances and it just might mean that there's more governance and procedure around\ncreating instances and operating system security and running agents on them so it just might mean\nthat there's a lead time in actually getting EC2 instances up and running and it's not just\npossible to fire one up because of all the security stuff and sometimes it's just really\nhard they just don't let you do it just because there's more attack servers area so there's a\nlittle bit more of a process around it. As well as that EC2 instances do require quite a bit of\nmanagement and maintenance to keep them up and running and up to date. Sure you can get one up\nand running reasonably quickly in general but then you have to think about what happens when\nyou need to upgrade to the newest version of Ubuntu and various other pieces of the stack become\ndeprecated or suffer from some sort of bit rot and for that reason a lot of people we've spoken\nto about this kind of challenge just prefer if they could just run a container. It's generally\neasier to do there's fewer guardrails in place so we've been thinking about this and a while back\nstarted actually figuring out ways of using containers as a bastion instead of having to\nworry about virtual machines at all.\n\n\nLuciano: Yeah I'll try to describe how that works so let's say that you have your private VPC all set up you have an existing or a new subnet and you want to set up\na ECS cluster of course assuming that you don't have one already you can just set up a Fargate\nservice with a task definition and this is going to be your Bastion container. You basically need\nthose two things you need to set up the cluster and then the service and you have the that Bastion\ncontainer running as a container but of course you need to make sure that certain conditions are met\nso you need to do a little bit of extra configuration. The first thing is that you need\nto enable ECS for your Fargate service this basically will connect your container to SSM\nallowing authorized principles to effectively use SSM as a tunnel to reach into the container and\nthis is kind of the key here because the idea is that you don't need to expose anything on the\npublic network through the AWS control plane and SSM you'll be able to establish a connection\ndirectly into your container so your container is not directly exposed to the open internet.\n\n\nThe cool thing about SSM is that it can allow you to run commands for instance to create a shell\nenvironment within the container and again this is without exposing anything on the public internet so\nSSM will basically route those commands through AWS into the container. We have a little bit of\na better explanation in episode 78 so again please check out that one if anything we are saying\ndoesn't make too much sense hopefully episode 78 will provide more details that can explain better\nwhat we are about to say today. So you'll need to decide which container image to use at this point.\n\n\nWe have the opportunity to run a container but of course what kind of image do we need because\ndepending on the image we pick we will have different tools that we can use. So what do we\nwant to do with this container? Do we want to access a database? Maybe we need a specific\ndatabase client. We want to access Redis. Maybe we need the Redis CLI installed. We want to just run\nAWS commands. We need the AWS CLI there. Maybe we need to do some network troubleshooting. We need\nspecific network utilities installed in our Dockerfile. So that's also an important step.\n\n\nMake sure that you understand exactly what kind of use cases do you want to cover and prepare your\nDockerfile accordingly so that you have all the tools that you need already available as soon as\nyou create a session. Of course you don't want this service to be always up and running because\nyou are going to be paying for a running container that maybe you only use occasionally when you want\nto debug something and the cool thing with Fargate is that you can create the service\nbut where the desired count is zero. So that basically means that you have an instance of\nthe server already pre-configured but effectively no container is running so you are not using any\ncompute in practice. And when you need it you can just bump that count to one. At that point Fargate\nis going to spin up one instance of that container and then you can start to create whatever. You can\nrun the commands through SSM to log in or create a shell and I think this is where Eoin you're going\nto explain how do you actually use this container.\n\n\nEoin: If you have an EC2 instance and you want to connect to it with SSM it's reasonably straightforward because you can just\nclick connect in the EC2 console and get a shell open in your browser or you can use AWS CLI with\nthe SSM extension installed. Then you just run `aws ssm start-session` and you pass in the instance\nID for the EC2 instance. With ECS or Fargate it's a little bit different. When you want to create a\nremote shell in a container you can run `aws ecs execute-command`, pass in your cluster task container\nand it will start up an interactive session for you on your container.\n\n\nSo that's very handy if you've just got containers you want to debug even if you're not using it as a bastion it's just handy\nif you're trying to debug something running in the container or figure out some problem. Now once you\nhave this remote shell you have access to the container that has access to your private VPC\nresources without having to make that publicly exposed on the network. If you need to connect to\nyour database you can just run your database client in this shell but what if you don't want\nto run a shell but instead you want to connect like a graphical database client from your own\ncomputer instead. In that case you're going to need a tunnel that presents a local socket\nsecurely forwarding all the traffic to and from the database server on the private network. And\nwith EC2 it's again a similar method but with ECS AWS actually provides something called an SSM document.\n\n\nNow SSM documents are useful for lots of different types of automation on remote\nservers but there's a specific one called start port forwarding session to remote host that AWS\nprovides as an SSM document that anyone can use. So if you run this document with SSM it's actually\ngoing to set up the tunnel for you and all of a sudden you've got the magic happening that allows\nyou to securely tunnel through to your local machine. The command syntax itself it's a bit\nof a mouthful but, because anyone might be interested, it goes a bit like this so you're\ndoing `start-session` and then you pass in a target which is composed of your ECS cluster, task ID and\ncontainer. But you also pass in a document name which is your \"StartPortForwardingSessionToRemoteHost\"\ndocument and then you give that document some parameters as a stringified JSON\nand those parameters have your remote host, the remote port number and the local port number. And\nthat's going to set up one or more port mappings for you between the remote environment, the remote\nnetwork and your local network. And then you can simply use your database client to connect to\nPostgres remotely and securely over this local port. And you can set up loads of different\nport mappings at the same time so you might have one tunnel for your database, one for a private\nAPI gateway endpoint, one for your ElastiCache. Once you've done the solution once it's quite\neasy to repeat and set up as part of your troubleshooting environment. Now there's probably\na few steps there, is there any way we can kind of codify this all together, make it easier for\npeople?\n\n\nLuciano: Yeah probably there is room here for a new open source tool, something similar to what Basti does that tries to remove all the complexity of running multiple commands, passing the right\nparameters. We can probably apply that same idea to this new approach using containers. But for now\nwe haven't done all of that, we have a simpler version of this which is effectively a Python\nscript that we will make available in a gist, the link will be in the show notes and this should\nsimplify this process making it easier for you to just run the right commands with just minimal\nconfiguration. It's not a lot of code so you can probably read it in 10 minutes and really\nunderstand what's going on. You should find more or less everything we explained today,\njust follow the order, see the different commands and how the parameters are wired together.\nBut yeah generally what we will do is when we set a CDK project we will have the script to be part\nof our deployment and available when we used to use this particular pattern. And you can automate\nthe entire process by providing this script in a development environment and for troubleshooting.\nFor example what you do is you start a container in Fargate, you create the tunnel, you generate\nIAM credentials for your database users assuming you want to access the database, then you launch\nsomething like psql if your database is a Postgres. If you have another database server you need to\nuse the correct client to access that server and the specific command that is required for your\nclient to connect to that particular machine. So this is all we can do with this particular script\nbut is there any room for improvement?\n\n\nEoin: Yeah one thing I thought that would be nice to add this would be just some automation to shut down the container automatically when the tunnel hasn't\nbeen used for a period of time. We've discussed different ways of doing this. One way might be if\nyou capture CloudTrail events for SSM start session to your Fargate service by using CloudWatch\nlogs for your CloudTrail. And then you could do a CloudWatch logs metrics filter which is going to\ncount up the number of start sessions over time. You could use those metrics then to auto scale the\ncontainer down when no session has been started for a certain period. Now you don't necessarily\nknow that it's not being used just because it was started a long time ago so maybe it's also\npossible to use SSM session logs because you can configure SSM to also log all of the commands for\ncompliance that are executed over a remote session. So it might be useful to do that as well to gain\nmore detailed activity information and determine whether sessions are actually idle. We'd love to\nget ideas from people but generally I think this approach is a simpler option. It allows you to\nget those connections up for private resources because it's containerized. It might just be\neasier to manage, keep up to date and switch on and off on demand. I'd love to know are people\nusing something like this from all our listeners and if you have any suggestions for improvements\nor other tools we can use let us know in the comments. But until next time thanks a lot for\nlistening and we'll catch you in the next episode.\n"
    },
    {
      "title": "127. Load Balancers",
      "url": "https://awsbites.com/127-load-balancers/",
      "publish_date": "2024-07-12T00:00:00.000Z",
      "abstract": "An overview of load balancers, explaining how they distribute traffic across multiple servers and provide high availability. We discuss layer 4 and layer 7 load balancers, detailing their pros and cons. We then focus on AWS load balancers, covering network load balancers and application load balancers in depth, including their features, use cases, and pricing models. We conclude by mentioning some alternatives to AWS load balancers.\n\nAWS Bites is brought to you by fourTheorem an AWS consulting partner with tons of experience with AWS. If you need someone to help you with your ambitious AWS projects, check out fourtheorem.com!\n\nIn this episode, we mentioned the following resource:\n\nOur previous episode &quot;88. What is VPC Lattice?&quot;\n\n",
      "transcript": "Luciano: Hello there and welcome to episode 127 of AWS Bites.\nMy name is Luciano and I'm here with Eoin.\nSo cloud computing is all about elastic scalability,\nhigh availability and security.\nFor most architecture to get all of that,\nyou'll need a load balancer.\nBut what is a load balancer really?\nThis is the topic of our podcast today.\nSo we are going to explore how does it work, a load balancer,\nwhat type of load balancer should you choose\nwhen it comes to performance, cost and flexibility.\n\n\nAnd by the end of this episode,\nhopefully you should have a clear idea\non which load balancer to choose when you need one on AWS.\nSo let's start with an example.\nYou have a client application that could be a web browser\nor maybe another kind of application,\nmaybe running on a mobile phone.\nIt could be an IoT device.\nThis client is going to send a request to a server.\nWhat kind of server is probably the first question\nyou should ask yourself?\n\n\nWell, even if your application is running on a physical host,\nmaybe that can be a virtual machine running on EC2\nor maybe a container running on Fargate,\nit's unlikely that your traffic is going to be served directly\nto that application server.\nAnd because there will be a few problems with that approach.\nFirst of all, it would expose your server directly\nto the public internet,\nwhich can cause some kind of security problems.\n\n\nYou generally don't want to put your web server\njust in front on the public internet.\nThat would also mean that you have only one host.\nSo that means that if you start to get a lot of traffic,\nyour single instance machine is going to be overloaded very quickly.\nAnd then you have limited options on how to scale it\nin response to an increase of traffic, for example.\nAnd you can also think that this is a single point of failure,\nnot just in terms of scalability,\nbut maybe if something else goes wrong,\nmaybe, I don't know, you install an update\nand that update goes wrong,\nor maybe your host fails because of an hardware failure.\n\n\nAt that point, your application just becomes unavailable\nbecause your only server just died\nand no traffic can be served anymore.\nAnd again, if you want to deploy a new version of the software\nor maybe upgrade something in the configuration of your server,\nthen you need to decide,\nwhat am I going to do during the downtime?\nI will have some kind of downtime.\nHow do I manage all of that?\nAnd that's going to degrade the experience of your users.\n\n\nSo ideally, you don't want to have just one instance,\nwhich brings us to the challenge that we need to do something\nin our architecture that allows us to support multiple instances of an application.\nAnd actually, this problem existed even before cloud computing.\nSo back in the days around the 1990s,\nwhen people started to ship all kinds of server applications,\nthey started to have these problems even in their own data centers.\n\n\nSo how do we scale and make things highly available for our users?\nAnd that's where softwares like NGINX,\nSquid, HAProxy, Trafic, and many others,\nwere started to come up to help solve this problem.\nSo the idea is that you use one of this software\nto effectively become the entry point into your application,\nand they can distribute and manage multiple connections\nand distribute those connections across a variety of servers\nthat exist behind the scenes of this specific application.\nSo this is where we can say that the idea of a load balancer was invented.\nAnd we will discuss a little bit today about more of the details,\nwhat is really the responsibility of a load balancer,\nwhat different kinds of load balancers exist,\nand then we'll talk a little bit more specifically about AWS.\nWhat kind of load balancer do you have there,\nand which ones should you choose depending on your specific use cases?\nSo maybe make sense to start with the features of a load balancer.\nEoin, do you want to talk a little bit about that?\n\n\nEoin: Yeah, feature number one is the ability to distribute the traffic across multiple hosts and solve a lot of those problems you were talking about there.\nAnd then load balancers will generally have different algorithms\nfor distributing the traffic, like simple round robin, weighted distribution, etc.\nAnd often there's a lot more and a lot different edge cases\nthat load balancers will have to handle for you.\n\n\nThen they'll need health checks because if you've got multiple hosts\nand they need to be able to redirect traffic,\nthey want to direct it to healthy hosts,\nso ones that are proven to be able to serve responses to requests effectively.\nSo health checks are an important feature.\nTLS and SSL termination.\nNormally TLS termination these days for HTTPS\nor even any other protocol supporting securing encryption,\nit's a really important feature.\n\n\nAnd load balancers can offload the computational workload of TLS\nfrom your server by handling it at the entry point into your network.\nSo if the route between your load balancer and the host\nis secured via another mechanism,\nlike in the case with AWS VPCs,\nwhich are already secured and encrypted traffic by default,\nyour hosts will then have a reduced workload.\nSince they don't have to do that TLS handshaking,\nencryption and decryption.\n\n\nOther important features are things like DDoS protection\nto protect against distributed denial of service attacks.\nAnd then a firewall, right?\nA web application firewall or other firewall rules\nor software or hardware to detect intrusions and prevent intrusions.\nNow there's a lot more features you can get with load balancers,\nbut it'll ultimately depend on the type of load balancer you have.\nAnd it's probably a good idea now to start talking\nabout the different types of load balancers.\n\n\nIf we're going to do that, we should talk about the OSI networking model.\nYou might be very familiar with this.\nMaybe you've heard about it, or you might have a vague memory\nor flashback to your college days or somewhere\nwhere you learned about it before.\nSo let's quickly revisit it, quick refresher,\njust to make sure we're all on the same page.\nThe OSI model, it's a theoretical model.\nIt doesn't necessarily translate directly to physical implementations\nor software implementations, but it's a good model\nfor talking about networking.\n\n\nBecause when we're talking about networking protocols,\nwe usually start defining about which layer they belong to\nwithin this OSI model.\nSo it's useful for understanding what happens when traffic\nis flowing between a source and a destination.\nFor load balancers, we're going to focus on layers four and seven,\nand there are seven layers in the OSI model.\nBut let's quickly run through the layers\nso we can understand where they fit.\n\n\nAnd if we start from the bottom up, your first one is layer one,\nthat's the physical layer.\nThat's where your hardware devices are.\nAbove that, you'll have layer two, which is called the data link layer.\nAnd this is like low-level communication protocols\nlike Ethernet or Wi-Fi.\nAnd above that, you'll have your network layer,\nwhich is routing and addressing.\nAnd this is in TCP IP.\nThis is where IP sits, Internet Protocol.\n\n\nSo those are kind of low-level things.\nMost people will be familiar with IP.\nIf you're dealing with things like load balancers,\nyou probably have some familiarity with that,\nand you'll understand what an IP address is.\nBut above that, then you talk about your transport layer.\nThat's layer four.\nAnd this is where things like TCP, UDP, and TLS more or less goes there.\nTLS, I suppose you can almost put it anywhere\nbecause it's like an encryption thing on top.\n\n\nBut transport layer is like TCP traffic or UDP traffic\nbetween source and destination, usually.\nNow, above that, you get into higher-level protocols.\nLayer five, typically called the session layer.\nThis is for coordination of sessions, really, between two hosts.\nAnd protocols like NFS will operate at that level.\nDepending on the system, you don't necessarily have all the layers.\nSo a lot of systems don't have session layer,\nso you don't have to worry about it too much.\n\n\nLayer six also is something you don't typically have to think about too much.\nIt's called a presentation layer.\nAnd that deals with the structure of the data itself,\nlike whether it's HTML or JSON or JPEG or text.\nAnd also compression can fit in there too.\nBut then at layer seven, this starts to get relevant for most of us again,\nbecause this is the application layer, and this is the highest level.\nAnd it deals with the high-level application protocol.\nThis is where HTTP and HTTPS live.\nAnd for other things like email, SMTP is also a protocol that fits into layer seven.\nSo now that that slightly torturous theory lesson is over,\nlet's get back to load balancers, knowing roughly where layer four,\nthe transport layer, and layer seven, application layer, sit.\nAnd we can talk about layer four and layer seven load balancers.\nSo what's the deal with layer four load balancers, Luciano?\n\n\nLuciano: So since layer four load balancers operate at the transport layer, they don't really understand anything about what we might call the protocol.\nLike, is it an HTTP request or is it maybe an SMTP?\nOr they don't even understand whether you are trying to transmit,\nI don't know, a JPG or an HTML page.\nThey are only dealing with transport.\nSo that generally means TCP or UDP.\nAnd of course, this is a trade-off.\n\n\nIt comes with some advantages and some disadvantages.\nAnd the main advantage is that because it focuses only at the TCP or UDP level,\nthere is very low latency.\nLike it doesn't have to really unpack or pack any data\nor trying to really understand the content of the packets,\nbut it can literally move packets from one place to another,\nwhich means that it can move like millions of billions of packets\nin the order of second with very little hardware costs, so to speak.\n\n\nSo that's why layer four load balancers generally are considered most performant.\nAnd they can move massive amounts of traffic.\nSo that's definitely the main advantage.\nBut the disadvantage is that because they don't understand the actual traffic,\nthat the amount of features you can have at the protocol level, of course, is very limited.\nAnd for instance, you cannot do things like, I don't know,\nif you are doing an HTTP request to a specific path,\nforward that request to maybe an endpoint, but not to another.\n\n\nOf course, because you cannot even understand the concept of a path\nbecause that exists at the HTTP level,\nthat's definitely a feature that cannot exist in this kind of load balancers.\nMaybe the things that you can do, because you can understand IP,\nso you can understand source IP and destination IP,\nyou can probably create rules on how to forward traffic based on IP information.\nThis is something that exists at the TCP level,\nso you can use that and use it to do routing or maybe understand,\ndepending on the distribution of traffic that you want to use,\nthere can be an information you can take and apply to the specific protocol.\n\n\nAnd there are some more modern load balancers that have this concept of a proxy protocol,\nwhich is something that can add additional information into the packets themselves.\nFor instance, I know that some of them will allow you to see what was the originating IP address,\njust because they embed that information into this particular space inside the TCP packet.\nBecause the idea is that since they will be normally rewriting TCP packets themselves,\nfrom your server perspective, it looks like every request is being originated by the load balancer.\nSo if you actually want to know which one was the actual client IP,\nyou need to try to read this additional information from somewhere else,\nand the proxy protocol helps with that.\nSo hopefully that gives you an idea of what layer 4 is good for, what are the pros and cons,\nbut let's talk about layer 7 now.\n\n\nEoin: Yeah, if you're looking for some more control at the layer 7 layer,\nthen you'll need a layer 7 load balancer, because they can see the traffic at that HTTP or HTTPS\nlevel. That means you can see the headers, the path, query parameters, and even the body.\nDoing this, as you said, Luciano, it's going to introduce some latency, but with that,\nyou'll get a lot more useful features. And the list is a bit longer, like you can get\nrouting based on header, path, and query. You can even modify all of those things before you pass\nthe request to the backend. You can do HTTP compression in your load balancer, so you could do\ngzip encoding at the load balancer level. You can do much finer-grained specific HTTP health checks,\nlike checking the health of a specific URL and inspecting the response in detail,\nlike the status code and the body or response headers. You can even do caching in the load\nbalancer. Another thing that is quite common in load balancers is to do authentication\nand authorization at the load balancer level. Like layer 7 load balancers can also almost be used as\na very effective API gateway. When we're talking about security, we mentioned things like DDoS\nprotection, but when you're operating at layer 7, you can also have a web application firewall\nlinked to your load balancer so that you can do lots of more specific HTTP, like kind of OASP top\n10 type attack detection. Because you can see headers, you can see cookies, and therefore you\ncan also do cookie-based stickiness in layer 7 load balancers. And you don't need that proxy\nprotocol thing that was added for network load balancers or layer 4 load balancers, Luciano,\nbecause the X-forwarded for header is very typical with load balancers and reverse proxies.\nWhereas layer 4 load balancers normally have one connection inbound associated with another\nconnection to the backend, layer 7 typically have a different model where they terminate a connection\nand then forward traffic to like a pool of connections to the backend server. So it can be\nmore efficient in terms of connection utilization as well. So that's layer 4, layer 7 load balancers.\nWhere does this all fit into the world of AWS? We mentioned that there are four types. We're going\nto talk about two, the network load balancer and the application load balancer in AWS.\nYes.\n\n\nLuciano: So in AWS, you can of course integrate any kind of third-party load balancer if you're willing to spend the time to set it up correctly on EC2 instances, for example. And of course, that can be\ncomplicated. It can take a lot of time and you'll need to know how to do it correctly.\nSo there are alternatives that AWS provides in the form of managed services. And this is probably\nthe most used load balancer you will see in AWS. There are actually four different types,\nand they are the classic load balancer, which is now deprecated, then network load balancers,\nwhich are the layer 4 that we described before. Then we have application load balancers,\nwhich is going to be layer 7. And then we have another kind of load balancers, which are called\ngateway load balancers, which are generally used when you have a third-party security\nprevention detection device, and you need all the traffic to go through that particular device.\n\n\nThis is a little bit of a more niche use case that only makes sense when you have specific\nsecurity requirements and you are setting up this kind of third-party tools that need to\nlive at the load balancer level. So we can use the short name NLB for network load balancer and ALB\nfor application load balancer. They can be either public or internal phasing. So that's a great\noption to have because that means that if you are creating an application that needs to be internet\nphasing, you can just put a load balancer there and it's going to handle all your incoming traffic.\n\n\nIf you're building something internally, maybe this is an internal API or internal microservices,\nif you want, you can also use all the same features of both NLB and ALBs just for internal\ntraffic. So traffic that is only existing inside your own private VPC. So let's talk a little bit\nmore about network load balancers. As we said, they support layer 4 and these are great candidates\nif you don't need all the features of layer 7. And these are generally the way to go because the\nlatency is so much lower. So if you need to provide as low a latency as possible, you probably\nwant to use a network load balancer. They have some support for TLS termination and you can use\nAWS certificate manager, but still that termination is already done at the TLS level. It doesn't\nreally understand, for instance, HTTPS. So it only unwraps the TLS traffic. Another thing that is\ninteresting is that because you are using certificate manager and it can use SNI, it can\nsupport multiple domains. So that's another great feature to have for NLBs. So one more reason to\nconsider a network load balancer is that a network load balancer will give you a fixed public IP\naddress. And either it will provision one for you or you can use an existing Elastic IP that you have\npre-allocated before. And this is great because sometimes you might need that for specific reasons\ndepending on your network needs. Instead, if you compare that with an application load balancer,\nan application load balancer will only give you a CNAME and that you can then configure,\nfor instance, if you need to forward traffic from somewhere else, from like a DNS, you can point it\nto that particular CNAME. But then behind the scene, the IP associated might change over time.\nSo you can not rely on that IP being fixed. So if you really need a fixed IP, don't use\napplication load balancer, but use network load balancers. Now, what are some of the features of\napplication load balancer and why it might be worth considering those?\n\n\nEoin: Application load balancers are going to support everything we talked about in terms of layer 7 load balancer features are\nsupported really in application load balancers. So you got the path-based routing, etc. You can do\nweb sockets, you can do HTTP version 2, not version 3 yet, and you could do gRPC over HTTP 2 as well.\nYou can also, again, like network load balancers, you can do your TLS termination,\nand it's integrated with certificate manager and also supports server name identification. One of\nthe unique features of application load balancers is that they support lambda functions as targets.\n\n\nYou can't do that with a network load balancer. And they have authorizers built in. So you can\nuse an OADC provider, use an OADC authorizer, and you can also use Cognito user pool authorization.\nThey also integrate pretty well with ECS, like through ECS deployments. Containers are\nautomatically added as targets to your load balancer and ECS will watch your health checks\nand make sure that the deployment is successful. And then you can integrate into AWS web application\nfirewall or WAF to get your security rules applied, like IP address blocking or preventing\nSQL injection or cross site scripting attacks. Now, if we look at NLDs in terms of how you\ncreate them at ALBs, it's actually very similar between the two. There's just a few subtle\ndifferences, even though the feature set is quite different. So let's just talk through the concepts\nand how you kind of construct an architecture around load balancers, whether you're using an\nNLD or an ALB. So you create your load balancer first. And once you have a load balancer,\nit doesn't do anything on its own. You have to create a listener. And a listener is where you'll\nspecify the port and the protocol. And if you want to use TLS termination on your load balancer,\nyou can specify your certificate as well, linking it into ECM. Now within the listener,\nyou can specify your default action and an action can either provide a fixed response like a 404\nor forward traffic to a target group. And target groups are really one of the most important\nconcepts when it comes to load balancers, because this is where you route traffic from the load\nbalancer to the backend. Target groups represent your fleet of backend things. Now it differs a\nlittle bit at this stage if you're using an application load balancer, because you don't\njust specify the default action, you can also specify additional listener rules. Each rule\nwill have its own target group. And the listener rules can say, based on the path being slash\nlogin, go to this target group. Or if I've got a HTTP header, which matches this value,\nthen go to a different target group. And all of these rules are prioritized, like you give each\none a unique priority number value, and they're evaluated in that order. If no rules match at all,\nthe default action will apply. So your default action might be to serve index.html, or it might\nbe serving a 404. And you can have up to 100 rules in these application load balancer listeners as\nwell by default, but it's only a soft limit. So you can get more if you ask nicely. And go back\nto the target groups, whether you're using an ALB or an NLB, these are groups of targets,\nas the name suggests, but the targets can be IP addresses. And that IP address doesn't even have\nto be on AWS, it could also be on premises. And a lot of people use that as a gateway drug,\nif you'll excuse the pun, to cloud migration. It's like using a load balancer with an on-premises\nserver as the backend, and you can shift the traffic over to cloud instances over time.\nYou can also have application load balancers that are targets of load balancer listener rules.\nSo that means that you can have a hierarchy of load balancers. And it's not uncommon to do that.\nA lot of people will have that kind of architecture where they find out traffic\nacross multiple load balancers. You might have like a network load balancer in front of an ALB,\nbecause you want a fixed IP address, for example. And that's not something that's uncommon at all.\nIt's pretty common. For application load balancers, we mentioned that you can have\nLambda functions as targets. At scale, actually, application load balancers can be a much more\ncost-effective alternative to API gateway in front of Lambda, albeit with a reduced feature set,\nbecause you don't have things like caching, rate limiting, and API keys, and various other features.\nBut the target group is also the place where you configure health checks,\nwhich will make a request to the targets to determine whether they should be added to\nor removed from the target group based on their health. And you can specify the frequency,\nthe number of target group health checks that have to succeed in order for a target to be added\nto your pool. So Luciano, I think we covered quite a lot. Although there's probably quite a lot else\nwe could cover. Is there anything else we need to know?\n\n\nLuciano: Maybe it's worth spending a few minutes on discussing pricing, because pricing can be a little bit difficult when it comes to, well,\nprobably when it comes to everything in AWS. But yeah, that doesn't change for load balancers.\nWhat is the model for load balancer? There is a minimal unit of time, which is the hour. So\nyou pay for... Actually, you can pay also for partial hours, I believe. Is that correct?\n\n\nBut yeah, you basically pay time and then the capacity unit. So this is not too much different\nfrom other pricing models that you can find in AWS, but there is this concept that you are using\na service for a certain amount of time. And then depending on how much you use that service, you\nhave these units that try to represent the amount of, I guess, capacity that you are consuming per\ntime. So we have different kinds of capacity units. For NLBs, we have NLCU, so Network Load\nBalancer Capacity Units. And for ALBs, you have LCUs. Now, there is quite a bit of complexity\nin these LCUs to be able to fully cover it here, but it's basically a formula that is based on new\nconnections, active connection, process bytes, and dual evaluations. So if you're really curious,\nI recommend checking out the documentation and trying to figure it out. You probably end up with\na big spreadsheet if you really want to do a solid model to predict the cost. And keep in mind that\nyou also have to pay for data transfer out per gigabyte beyond 100 gigabytes to the internet.\n\n\nBut data transfer between ADs in the same AWS PPC is free. And by the way, you can enable or disable\ncross zone traffic in load balancers for LPs, but cross zone is always available, always enabled for\nALBs. Now, another thing that might be worth covering very quickly is what are your alternatives?\nWhat if you don't want to use AWS load balancers? What are your options? And we already mentioned\nat the beginning that you could effectively host anything on EC2. For instance, if you like Nginx,\nof course, you can use Nginx as a load balancer. You just need to make sure you provision it\ncorrectly on AWS infrastructure. Similarly, you can use HAProxy or whatever else you like.\n\n\nMy caveat there would be that I think it's significantly complex to do that correctly\nand to make it scalable and secure. So do that only if you really have the skills and you are\nreally knowledgeable about these tools and all the relevant networking configuration.\nThe pricing is simpler though.\nYeah, that's actually one reason maybe to consider that if you don't account for the amount of hours\nthat you will be spending configuring all of that. But yeah, another option could be using\nDNS load balancing, which has interesting trade-offs. For instance, one thing you could\ndo, you could do geographical distribution of traffic. Maybe you can distribute traffic across\nregions using DNS load balancing, but also you know that DNS is always the reason for problems\nin the web. So there is significant complexity there to do it right, to do DNS caching and\nvalidation and all that kind of stuff. So again, it's an option, but comes with its own trade-offs.\nAnd another option that we have been discussing before is VPC lattice. And we have an entire\nepisode and a blog post. So if you haven't seen that, we'll leave the link in the show notes so\nyou can check those out. So hopefully this was a complete enough coverage of load balancers and\nhow do you use load balancers on AWS. I'm sure that there is a lot we have missed. So let us know\nif you have any particular experience that might be worth sharing. What kind of load balancers do\nyou like to use? Or maybe any other comments of question that you think is relevant here.\nSo that's all for today. Thank you for being with us. And as always, if you liked this episode,\nconsider sharing it and giving us a like. See you in the next one.\n"
    },
    {
      "title": "128. Writing a book about Rust & Lambda",
      "url": "https://awsbites.com/128-writing-a-book-about-rust-and-lambda/",
      "publish_date": "2024-07-25T00:00:00.000Z",
      "abstract": "In this episode, we discuss Luciano's new book project on using Rust to write\nAWS Lambda functions. We start with a recap on why Rust is a good fit for\nLambda, including performance, efficiency, safety, and low cold start times.\nLuciano provides details on the book's progress so far, the intended audience,\nand the current published chapters covering Lambda internals, getting started\nwith Rust Lambda, and building a URL shortener app with DynamoDB. We also\nexplore the differences between traditional publishing and self-publishing, and\nwhy Luciano chose the self-publishing route for this book. Luciano shares\ninsights into the writing process with AsciiDoc, code samples, SVG image\ngeneration, and using Gumroad for distribution. He invites feedback from\nlisteners who have experience with Rust and Lambda.\n\nAWS Bites is brought to you by fourTheorem. If you are looking for a partner\nto architect, develop and modernise on AWS, give fourTheorem a call. We have\nalso been working with some of our customers to rewrite some of their most\nused Lambda functions in Rust, greatly reducing cost and improving\nperformance. If all of this sounds interesting, check us out at\nfourtheorem.com!\n\nIn this episode, we mentioned the following resource:\n\nOur previous episode\n&quot;64. How do you write Lambda Functions in Rust?&quot;\nCrafting Lambda Functions in Rust book's website\nThe official Rust book (available for free)\nJames Eastham awesome YouTube channel\nAI as a Service book\nNode.js Design Patterns book\nLiran Tal's awesome AsciiDoc book starter template\n\n",
      "transcript": "Luciano: Hello and welcome to another episode of AWS Bites podcast.\nMy name is Luciano and I'm joined by Eoin to discuss a new side project.\nI'm currently writing a book about Rust and Lambda.\nSo today I want to share with you some of the details about this project.\nWhy do I think that using the Rust programming language\nto write Lambda functions is a great idea?\nWhat you can expect from this book and a little bit about the publishing process.\n\n\nI think that Rust is innovating many areas of the software engineering world\nand my bet is that in the coming years we will see it innovating\nand improving the world of AWS Lambda as well.\nSo I hope that this is going to be an interesting episode and let's get into it.\nAWS Bites is brought to you by fourTheorem.\nIf you are looking for a partner to architect, develop and modernize on AWS,\ngive fourTheorem a call.\nBy the way, we have also been working with some of our customers,\nrewriting some of their most used Lambda functions in Rust,\nwhich is something that greatly reduces cost and improves performance.\nSo if all of this sounds interesting, check us out on fourTheorem.com.\n\n\nEoin: Okay, I remember episode 64 where we talked about writing Lambda functions in Rust.\nSo that's not the goal today, I think.\nWe want to talk a little bit more about the book,\nbut at the same time, maybe it's worth a quick recap\nbecause that was about a year and a half ago.\nI think a lot happens in a year and a half in the Rust and in the Lambda ecosystems.\nSo do you want to remind us why it might be a good fit for Lambda functions?\n\n\nLuciano: Yes, I'll try to do a quick recap about Rust first, and then we'll talk about Rust in the context of Lambda.\nMaybe people are not familiar with Rust.\nSo Rust is a relatively new programming language,\nwas born to compete with C and C++, mostly in terms of performance,\nbut the idea was to try to provide a better experience\nin terms of a language that is memory safe by default.\nAnd that kind of memory safety is built into the language and the compiler.\n\n\nIt does have a very good and modern toolchain,\nand the language itself is very modern as well,\nand it takes inspiration by many other languages.\nAnd because it also provides a few different high-level abstractions,\nthese days Rust has become more of a general-purpose programming language.\nSo it's not being considered as a system programming language anymore\nbecause you can write from firmware and an operative system and video games,\nbut also you can write more high-level applications\nsuch as web backends and even web frontends using WebAssembly.\n\n\nSo it's kind of a language that has become general purpose\nand you can use it for a variety of projects.\nAnd one of the main things is that software written Rust\ncan be very memory efficient and very performant\nand also based on some research that has been done,\nif we look at the consumption in terms of electricity consumption\nand CO2 emissions, Rust can also be environmentally friendly,\nand it's considered to be one of the most environmentally friendly languages out there.\n\n\nNow, in the context of Lambda, why Rust is an interesting choice?\nWe said already that it can be very memory efficient and fast,\nso that means that you can lower down the cost,\nthe execution cost of Lambda function if you write them in Rust.\nAnd just to remind you what's the execution cost,\nyou basically have to take a unit cost based on the amount of memory\nthat you allocate for your Lambda function\nand multiply that by the number of milliseconds that your execution is lasting.\n\n\nSo with Rust, you generally get Lambda functions\nthat don't require a lot of memory,\ndepending of course on the problem that you're solving,\nbut if you compare an equivalent implementation between,\nfor instance, JavaScript or Python and Rust,\ngenerally the Rust implementation doesn't require nearly as much memory.\nSo you can save a lot in terms of allocated memory,\nwhich reduces one unit of cost.\nThen, generally speaking, Rust can execute very fast,\nespecially when you have CPU-bound computation.\n\n\nSo in that case, you are also reducing the number of milliseconds,\nmost likely, that your execution is going to last.\nSo basically, you are optimizing on both dimensions\nfor the Lambda execution cost,\nwhich generally means that if you have a Lambda\nthat you are executing thousands or even millions of times per day,\nif you rewrite that one in Rust,\nyou are going to probably save a lot of money.\nNow, this is of course not the only reason why you should consider Rust.\n\n\nThere are other few reasons that I think are interesting.\nAnother one is that if you have, let's say, user-facing Lambdas\nor Lambdas where you expect to have a very low latency in the response,\nyou might have occasional cold starts,\nand that might be a problem if there is a user waiting for that cold start.\nImagine you are writing like an API\nand you want this API to be consistently very responsive and very low latency.\n\n\nWith Rust, you don't avoid the cold start,\nbut the cold starts are so low,\nI've seen in the order of 20 to 40 milliseconds,\nthat they are becoming basically negligible.\nSo that's another good use case for considering Rust.\nAnd then there are another few interesting reasons.\nOne is that, as I mentioned, the language is very modern\nand imporals some constructs in the type systems\nthat will help you to prevent a certain classes of bugs.\n\n\nSo, for instance, there is the result type,\nwhich basically helps you to manage exceptions,\nor the option type,\nwhich avoids the classic null pointer exception\nthat you have in other languages.\nSo, generally speaking, the programming language itself\nhelps you to prevent a large class of bugs\nand write code that can be generally more correct.\nSo if you have business-critical applications\nwhere you want to have, I guess, extra peace of mind\nthat the code that you are writing is probably not going to be bug-free,\nbut as close to bug-free as possible,\nI think Rust can help as well in that direction.\nAnd we also mentioned reducing CO2 emissions.\nLambda is already a great platform for that.\nBut if you combine that with Rust,\nI think you get an even better, I guess, stance\non being environmentally friendly when it comes to compute in the cloud.\nNow, again, I want to remind you that we have been speaking a lot\nin that previous episode about the whole experience,\nhow do you get started, the tooling, the pros and cons of using Rust.\nSo if you're curious about, like, actually, how do you get started\nand how do you write your first Lambda in Rust,\nI would recommend people to check out that previous episode.\n\n\nEoin: All right, let's talk about your book then.\nSo what is the title and who's it for?\nWhat's your intended audience?\nOr have you thought that much about that so far?\nBecause a lot of people don't.\n\n\nLuciano: Yeah, I would probably say a little bit.\nIt's probably something subject to change,\nbut we'll get to the details of that.\nThe title of the book is called Crafting Lambda Functions in Rust.\nThere is a website that provides all kinds of information,\nand the website is reachable at rust-lambda.com.\nThe link will be in the show notes as always.\nSo what is the expected audience?\nAnd this is a bit of a guess at this stage of the book,\nbut we are imagining the book to be something that is designed\nto be accessible to readers that have a very basic knowledge of Rust\nand AWS, so not meant to be consumed only by experts,\nbut also by people that are a little bit entry level\non both Rust and AWS.\n\n\nWe want to try to provide a gentle introduction\nto the serverless ecosystem in AWS,\nwhich also means that you don't need to be proficient\nwith serverless in general, Lambda, or DynamoDB,\nand all the other classic serverless services in AWS.\nThe idea is that, of course, you need to have your own AWS account\nand have some very basic knowledge about what you can do with AWS.\nSo you need to be able to access the dashboard,\nmaybe know a little bit about IAM,\nbut you are not expected to be an expert in all things AWS.\n\n\nAnd in fact, throughout the book,\nwe try to introduce all the necessary concepts,\nso not just explain what Lambda is,\nhow does it work internally and how do you start using it,\nbut then as soon as we start to use Lambda with API Gateway\nand DynamoDB and other services like EventBridge,\nmaybe we introduce those services,\ngiving, I guess, the fundamental knowledge\nthat you need to have to understand\nhow to build something practical with those services.\n\n\nAnd we also touch a little bit on infrastructure as code\nand we use AWS SAM for that.\nAs I mentioned, you don't need to be an expert in Rust either,\nbut of course, we cannot explain everything about Rust\nbecause that would make the book probably thousands of pages.\nSo just to be practical and pragmatic,\nwhat we suggest is that people should understand\na little bit of the syntax.\nAnd if that's something that you don't know yet,\nprobably the best thing you can do\nis to look at the official Rust book, which is available for free.\n\n\nAnd we will have the link at the show notes as a starting point\njust to get familiar with the syntax.\nSo just to summarize, this book is for you\nif you are a cloud developer with a passion for serverless,\nyou have a very basic understanding of the Rust programming language,\nat least from a syntax understanding perspective,\nyou have a basic knowledge of AWS,\nand you basically are trying to figure out a new way to build greener,\nmore efficient and cost-effective serverless solutions.\nBy the way, I mentioned the we a few times,\nso it's definitely worth stating that it's not my own solo side-project.\nI've actually been working with a co-author for this,\nand the co-author is James Eastham.\nAnd James is an amazing author with lots of experience\nwith serverless and event-driven systems.\nHe, by the way, used to work for AWS and recently moved to Datadog,\nand he has an amazing YouTube channel that talks a lot about serverless,\nAWS, event-driven architecture.\nSo if all these topics interest you, you should check it out,\nand we will have the link in the show notes as well.\n\n\nEoin: So what's the current status of the book?\nIs it a work in progress?\nHow far have you progressed so far?\n\n\nLuciano: Yeah, the book is heavily work in progress at this stage.\nWe started, I think, about two months ago now,\nso it's still relatively, it's a good progress project.\nAnd another thing worth mentioning is that it is self-published.\nProbably we'll get more into that later.\nAnd the current version, what we have so far is four chapters,\nwhich is about 110 pages or something like that,\naround 100 pages, more or less.\n\n\nAnd we have the book already available online.\nSo you can buy it today as an early preview,\nand you get about 50% of discount on the expected final price.\nAnd that means that you can download what we have written so far,\nand then you will receive all the future updates of the book for free.\nAnd since it's an e-book, you get a few different formats.\nSo there is a PDF, which also comes in dark mode, if you prefer that.\n\n\nThere is an e-book format, and there is even an HTML format.\nSo you can read it in whichever format you prefer.\nRight now we have four chapters, and those chapters cover,\nIn the first chapter, we do an introduction to what Lambda is,\nwhat is Rust, why the two together make sense,\nwhich is more or less what we described at the beginning of this episode,\nplus a detailed introduction about serverless AWS Lambda,\nand how AWS Lambda works under the hood,\nbecause we believe that it's important for you to understand\na few important details about Lambda,\nlike the internals of Lambda, like what is a runtime,\nhow different environments are created and reused and destroyed.\n\n\nSo we try to explain all of that just to give you a good context\nto understand the computation model.\nOn chapter two, we start to build a first Lambda function.\nIt's kind of an L award, and it's just to get familiar with the toolchain,\nincluding Cargo Lambda, which is a CLI tool that you can use to do\nscaffolding of new Lambdas, local testing,\nbuilding the Lambda for different architectures,\nand even publishing the Lambda to AWS.\n\n\nThen in chapter three, we start to build a more realistic application,\na URL shortener, and we use this opportunity\nto start to introduce infrastructure as code and explain what SAM is\nand how to use SAM together with Cargo Lambda.\nAnd then in chapter four, which is currently the last chapter we have,\nwe continue developing that application,\nand we bring it to a state where it's more or less working\nbecause we start to persist the data in DynamoDB,\nso you can actually create a short URL\nand use it across different implications of Lambdas.\n\n\nAnd in the process, of course, we need to learn what DynamoDB is,\nwhat are the trade-offs of DynamoDB,\nhow to use the SDK for Rust to interact with DynamoDB,\nand we also learn how to make arbitrary HTTP requests from Lambda\nbecause we want to effectively crawl the URL\nfor which we are creating the short URL\nto establish a title and a description from the page metadata.\nSo this is just a nice example to see that in a Lambda,\nyou generally will need to do some kind of HTTP request.\nSo how do you do that?\nWhat are the different pieces of configuration and libraries\nthat you can use to do all of that?\nSo I think this is what we have so far,\nand by the end of chapter four,\nyou have a working implementation of a URL shortener.\nIt's not perfect because it's still very bare bone\nand there are lots of corners that we've cut to get there,\nbut at least it's something working\nthat you can deploy in your own account and use it.\n\n\nEoin: That sounds great. You've made a lot of progress in two months,\nI think, to have those four chapters out there,\nand people can have a look already.\nThat's really one of the benefits of self-publishing, I guess.\nBut what about the whole scope?\nDo you have an idea of what the whole shape is going to be\nby the time you've finished?\nWhat else do you expect to cover?\n\n\nLuciano: Yeah, I think ideally what we want to cover\nthroughout the entire book is not just an introduction\nto serverless and Rust in the context of AWS Lambda,\nbut more making sure we touch on all the things\nthat you need to be aware if you want to do something\nthat we can consider production-ready.\nSo that means that now that we have kind of a skeleton\nof an application,\nwhich is definitely not production-ready at this stage,\nwhat is missing to make that production-ready?\n\n\nAnd what we see that we should be covering\nis things like best practices\nin terms of structuring the code.\nFor instance, if you have multiple Lambda functions\nin the same project, which is very common,\nhow do you organize those in the context of Rust?\nAnd then if you have shared libraries,\nwhich is also very common,\nthat you need to reuse across these Lambdas,\nwhat is the best way to do that?\nAnd similarly, how do you make your code testable,\nespecially in an environment like Lambda,\nwhere you generally interact with a lot of external systems,\nlike for instance, even DynamoDB,\nwe can imagine it's an external system.\n\n\nHow can you write, for instance, unit tests to cover,\nbasically, to make sure that your code is working correctly\nand run this test and have the kind of extra confidence\nthat you are doing the right thing?\nAnd also touch about integration tests\nand end-to-end tests as well.\nThen right now we are only covering HTTP events.\nSo you are building an API.\nSo your URL shortener is just effectively an API.\nYou can use it as an API and that's it.\n\n\nBut what if you want to use other event sources,\nfor instance, DynamoDB Streams or EventBridge,\nor maybe even custom events,\nmaybe you are building a step function and you have a step,\nwhich is a Lambda written in Rust.\nSo we want to cover some examples\nabout these different kinds of events that you can get\nand different kinds of integrations\nwith different AWS services.\nAnd then there is the topic of observability.\n\n\nSo we need to cover how to do good logging, tracing,\nintegrated with CloudWatch,\nmaybe explore open telemetry as well,\nwhich seems to be a topic that is more and more discussed\nin the context of Lambda.\nAnd it's definitely well supported\nin general in the Rust ecosystem.\nSo trying to see how you can use it together with Lambda\ncould be interesting.\nAnd then finally, configuration management\nand handling secrets.\n\n\nThere are some other topics that we are considering\nmaybe as an appendix or maybe as advanced chapters.\nWe'll probably see what happens as we get there.\nBut there is an interesting middleware system\nthat is built in in the Rust runtime.\nSo it could be interesting to show that,\nlike the capabilities that you have,\nto show how you can use existing middlewares,\nbut even creating your own middleware.\nAnd you can also write Lambda extensions in Rust.\n\n\nAnd this is very interesting\nbecause you can use those extensions\neven in Lambda functions written in other languages,\nlike Python or Node.js.\nAnd this is something that, for instance,\nif you are somebody that is creating something like Datadog\nor other system where you want to collect telemetry\nor maybe send data\nas part of your Lambda execution life cycle,\nyou could create a Lambda extension written in Rust,\nwhich will be very efficient and have a minimal impact\non whoever is installing that extension.\n\n\nSo that's something that could be interesting\nto explore as well.\nAnd then finally, there are other topics such as\nif you want to create like a CI-CD pipeline,\nmaybe using GitHub action, providing some example\nof how you can tie everything together.\nAnd finally, maybe considering alternatives to SAM,\nlike using CDK or Terraform\nbecause everyone has its own preference.\nOf course, we have to pick one\nand SAM seems to be the easiest to explain.\nBut if you are already familiar with CDK and Terraform,\nit might be worth explaining\nor at least showing some examples\nof how you can achieve the same things\nby using your favorite infrastructure as code tool.\nSo it's important to mention that this schedule is not final.\nWe are trying to get as much feedback as possible\nand use that feedback to understand,\nokay, what should we be covering next?\nMaybe we should focus more on one topic rather than others.\nYeah, look, that sounds fair enough.\n\n\nEoin: A long time ago, well, maybe not that long ago,\nbut a while back, I wrote a book with Peter Elger\nand it was kind of published with Manning.\nSo there was also an early access program,\nbut I guess it wasn't self-published.\nSo we had editorial support along the way.\nYou've also written a book,\nNode.js Design Patterns with Packt.\nSo what do you think the trade-offs are then\nbetween self-publishing versus going with a publisher?\nAnd why in this instance, with your experience,\ndid you go for this model?\n\n\nLuciano: Yeah, you're definitely saying that there are trade-offs.\nI totally agree with that.\nThere's not like one model is necessarily\nbetter than the other.\nI think you need to evaluate the pros and cons\nand decide which one is the best.\nSo I definitely don't regret going with a publisher\nfor Node.js Design Patterns,\nbut I was also curious to see what it would look like\nto try to do a self-published book.\nAnd I think in general, when you go with a publisher,\nthe publisher will take care of many aspects of the process.\n\n\nSo you can focus most of your time\non basically the content itself.\nSo writing down the content and making sure the content\nis up to the standards that you want to provide.\nSo in that case, when you have a publisher,\nthe publisher will help you a lot,\nfirst of all, to define the structure of the book,\nmore or less what is in terms of like broad strokes,\nthat the topic that should be covered\nand how to distill down the topic\nbased on a target audience.\n\n\nSo the publisher will also help you a lot\nto make sure that you understand\nwhat is your target audience\nand to tune that content to that target audience.\nThen one of the things that the publisher does best\nis establishing a timeline\nand making sure that you stick to that timeline.\nThey also help you to find reviewers and editors.\nThey will help you with graphics, layouts, publishing,\nall that kind of stuff.\n\n\nAnd of course, the big element is also marketing\nand branding because of course, with a publisher,\nthere is a little bit of extra trust that you get\nbecause if people know Manning or Packt,\nthey will trust that brand probably more\nthan they will trust an author that they've never seen before.\nSo as I said, they also make sure that you don't get stuck.\nSo sometimes when you write a book,\nyou realize that maybe there is something you don't like\nand you want to take a very big change.\n\n\nI think the publisher will make sure that you evaluate\nthose kinds of changes really, really well\nand you don't end up spinning up in circles\nand not really focusing on delivering something\nbecause maybe you keep changing ideas.\nSo that's also very good for focus\nand making sure you actually deliver a complete project\nrather than, I don't know,\nmaybe trying a few different things and then giving up.\nAnd I think all these things are great\nif you are a first time book author\nbecause you will need to understand that kind of process.\n\n\nYou'll need to have the diligence.\nYou'll need to have somebody to support you\nand also push you when you need to be pushed.\nBut on the other end,\nthere are some other cons to having a publisher.\nOne that I think is very important\nis that you don't really have a lot of flexibility.\nSo you have to stick to that particular process\nthat they have in place.\nAnd sometimes that process can be a little bit annoying.\n\n\nFor instance, with Packt,\nit means that you need to use Microsoft Word\nand all the templates that they provide\nwithout the styling that they have already established,\nwhich of course is good for them\nbecause they want to be consistent with their branding\nand they want to be sure the final product\nis going to look as they expect.\nBut if you don't like the kind of tooling\nand the kind of way of writing,\nit can become very annoying,\nespecially when you have to write hundreds of pages.\n\n\nSo that's one thing that is worth considering.\nThen there is also the financial perspective.\nSo the publisher takes the bigger share of the revenue.\nAnd even if you are an established author,\nmaybe you can negotiate a little bit of a better deal,\nbut it's not going to change the equation significantly.\nSo that's something to keep into account.\nI still think that writing a book\nis not something that you do for getting rich.\n\n\nRealistically, you are not going to pay,\nyou're not going to get paid\nas much as you would get paid\nif you were spending the same amount of time\njust doing your software engineering job.\nSo the value of writing a book is not, I guess,\nthe financial return that you might get from it.\nBut if you do self-publish,\nI think that equation changed significantly\nwhere at least I think you can get your time paid off\nwell enough if the book is successful.\n\n\nSo that's something else worth considering.\nLike if you're looking for a slightly better return,\nself-publishing can give you all of that.\nBut that also means that you have to do all the extra work\nthat the publisher is doing for you.\nSo the process itself is more complicated\nbecause you need to spend all that extra time\nto do all the things that the publisher\nwill be doing for you.\nIn terms of our process,\nI'm really happy that we are not using Word\nfor this particular project,\nbut we figured out a different tool chain.\n\n\nIn particular, we are using AsciiDoc,\nwhich if people are not familiar with it,\nis a language that is somewhat similar to Markdown,\nbut much more powerful\nbecause it was built effectively to write books.\nSo there are so many more features\nthat you don't have in Markdown.\nAnd we found a very good starter template\nprovided by Liran Tal.\nWe will have the link in the show notes.\nIf anyone is curious to see what that looks like,\nmaybe I think you need to write a book as well.\nI think that could be something that saves you\na lot of time because it's already a structure\nfor you to take care of most of the things,\nincluding for instance, the publishing in different formats\nand supporting dark mode for PDFs.\nCan I just ask about the build process Luciano?\n\n\nEoin: Actually, I remember that with Manning,\nthey did, while they had a word option for doing editing,\nthey also had an AsciiDoc option\nwhere they give you templates\nand it was easy enough then to, you know,\nbuild, create a source code repository,\nbuild your book with a GitHub action or whatever\nthat worked pretty well.\nBut one of the things I remember\nthat's really quite time consuming\nwhen you're doing this kind of process\nis taking, merging your code samples\nand your book content and images as well.\n\n\nSo we were doing an AWS book, so we had code samples,\nwe had then the source code for the book text itself,\nand then we had screenshots,\nwhich were manually screen grabbing,\nyou know, high DPI, copying in,\nhaving to like manipulate them in various different ways\nto make them look right.\nAnd that's actually quite a slow process.\nI remember hearing Dave Thomas\nfrom the Pragmatic Programmer\nand he has his own publishing company.\nAnd I remember him saying that they set up a build process\nthat would actually build and test the code in the book\nas you're building the book.\nSo I'm just wondering if you've got any neat tricks\nor if you're also just taking a manual approach\nwhen it comes to incorporating those snippets.\n\n\nLuciano: Yeah, I know that in AsciiDoc,\nyou can include snippets from source code directly.\nSo there is a way to do that.\nWe are not leveraging that yet\nbecause since we are building like a real application\nand we show like a few steps to get to something working,\nthen if you go on the book repo where you see all the code,\nthe code is more or less like the final state\nof that particular chapter.\nSo it is a little bit tricky.\n\n\nLike we cannot just include the final state.\nWe need to explain all the different steps.\nSo it is a little bit tricky to reconcile\nthe actual working code with a step-by-step explanation\nof how do you get to write that code.\nSo right now we are still doing mostly manual things\nin that sense.\nAnd yeah, also the other thing is that you might want\nto highlight, for instance,\nthe differences between step one and step two.\n\n\nAnd again, that's not something that you implicitly have\nin a working code base.\nSo you need to apply all the formatting in that sense.\nAnd thankfully, AsciiDoc makes that relatively easy,\nbut there is still a significant amount of manual work\nto do all of that.\nAnd it can be painful if at some point you decide\nto do a big refactoring.\nThat means that you need to go back in all the steps\nand figure out how do I apply.\n\n\nDo it all again.\nExactly.\nIn terms of images, so far we've been using only SVGs\nbecause we don't really have screenshots.\nAnd we created a little tool that allows us to convert\nan SVG to the equivalent dark mode.\nSo basically what it does, inverts the colors,\nand then you can specify in the SVG markup\nwhen you don't want the color to be inverted,\nfor instance, I don't know.\nIf it's the orange of the lambda icon,\nwe probably don't want that to be inverted in the dark mode.\nSo we have a way to do that.\nBut if we regenerate the base image,\nit's just a command line that we need to run\nto regenerate the dark mode version.\nSo that helps a little bit to speed things up.\n\n\nEoin: Nice, so where is the shop front and how are you processing the money side of things\nfor the book?\n\n\nLuciano: Yeah, that's something that we had to research\na little bit.\nWe ended up choosing Gumroad,\nwhich I don't know if it's the best in terms of pricing\nbecause I think they take a flat 10% of revenue.\nAnd I know that there are other shops\nwhere you can get a better deal.\n10% is not necessarily a lot,\ndepending on the price that you have.\n\n\nEoin: Compared to Apple!\n\n\nLuciano: Exactly, but I know that there is, for instance,\nLemon Squeezey, it gives you a better deal in that sense.\nBut what we were looking for is to have something\nthat is simple enough to use,\nand at the same time that could automatically\nsplit the revenue between the authors.\nAnd apparently Gumroad is the only one we found\nwhere this feature exists.\nAnd this is amazing because from a tax perspective,\neveryone receives their own share of the money,\neveryone is liable to do their own tax report\nif we don't have to create like an intermediate company\nor do, I don't know, invoicing\nor moving money between the authors.\n\n\nSo all of that management,\nwhich honestly it's something I was a little bit scared about,\nis taken care by Gumroad.\nSo I'm really happy about that aspect.\nSo the other thing is that in Gumroad\nit's very easy to generate discounts if you want,\nfor instance, maybe you are attending a conference\nand you want to create a discount code just for that day\nto incentivize people to buy the book\nafter they hear your talk.\n\n\nGumroad makes all of that extremely easy.\nSo that's another thing that I wasn't missing\nfrom the relationship with the publisher\nwhere you can get discount codes,\nbut you need to be very careful to ask way in advance.\nAnd sometimes it just doesn't work out\nand you lose opportunities.\nSo yeah, generally speaking right now,\nI'm very happy with the whole self-published experience.\nI think that the only tricky thing is to try to stick\nwith some kind of timeline or at least to have a good cadence\neven if you don't really have a strict deadline,\nbut at least make sure that you are progressing steadily\non the book.\nAnd I think being two authors helps a lot\nbecause we can kind of push each other\nand try to keep each other accountable.\nAnd that makes sure that we keep progressing\nwith some kind of a reliable pace.\nSo I don't know if you have other questions,\notherwise this is, I think,\neverything I wanted to share so far.\n\n\nEoin: I'm curious, maybe in the future,\nwhen you're closer to the finish, we can talk about it\nand we can go through some of the contents\nin a bit more detail and maybe have James on as well.\nBut that was really interesting.\nAll I can say is best of luck with it.\nThank you.\nYeah, I'm going to get reading.\n\n\nLuciano: Nice, yeah.\nWe'll definitely have all the links in the show notes again.\nAnd if you end up getting the book,\nI'd be really curious to get the feedback,\nbecause again, we are trying to use as much early feedback\nas possible to try to drive basically the evolution\nof the content itself.\nSo also I'm curious in general,\nif people have considered writing Lambdas and Rust\nand why, if they actually tried it,\nwhat the experience was like for them.\nSo if that's something that you have done\nor even just consider, leave us a comment\nbecause I'd be really curious to read about that experience.\nSo thanks again for being with us today for another episode.\nAnd we look forward to seeing you in the next one.\nBye.\n"
    },
    {
      "title": "129. Lambda Provisioned Concurrency",
      "url": "https://awsbites.com/129-lambda-provisioned-concurrency/",
      "publish_date": "2024-08-23T00:00:00.000Z",
      "abstract": "In this episode, we discuss AWS Lambda provisioned concurrency. We start with a\nrecap of Lambda cold starts and the different concurrency control options. We\nthen explain how provisioned concurrency works to initialize execution\nenvironments in advance to avoid cold starts. We cover how to enable it, pricing\ndetails, common issues like over/under-provisioning, and alternatives like\nself-warming functions or using other services like ECS and Fargate.\n\nThis episode of AWS Bites is powered by fourTheorem. Whether you're looking to\narchitect, develop, or modernize on AWS, fourTheorem has you covered. Ready to\ntake your cloud game to the next level? Head to\n⁠⁠⁠fourtheorem.com⁠ to check out our in-depth articles,\nand case studies, and see how we can help transform your AWS journey.\n\nIn this episode, we mentioned the following resource:\n\nEpisode 60: &quot;What is AWS Lambda&quot;\nEpisode 104:\n&quot;Explaining AWS Lambda Runtimes&quot;\nEpisode 108:\n&quot;Solving Lambda Cold Starts in Python&quot;\nEpisode 120: &quot;Lambda Best Practices&quot;\nAWS Lambda Concurrency Explained\nby James Eastham\nProvisioned Concurrency pricing\nLess than 1% of invocations are cold-starts\n(statement)\nMiddy Warmup Middleware\nLambda speculative warm-up init\n(mention in the Docs)\nEpisode 64:\n&quot;How do you write Lambda Functions in Rust&quot;\nEpisode 128:\n&quot;Writing a book about Rust and Lambda&quot;\n\n",
      "transcript": "Eoin: Ever had one of those days when a cloud deployment just refuses to play nice?\nWe sure did, thanks to some quirky issues with Lambda's provisioned concurrency.\nEvery issue is an opportunity to learn something new.\nAnd after some deep digging, we uncovered some insights about Lambda provisioned concurrency,\nand we just thought we'd share them with you today.\nSo we're going to talk about the joy of cold starts, Lambda concurrency,\nand the different concurrency control features available, how provisioned concurrency works\nitself, some of its limitations, common problems, and of course, those pesky pricing details.\nI'm Eoin, I'm here with Luciano, and this is another exciting episode of the AWS Bites podcast.\nThis episode of AWS Bites is powered by fourTheorem.\nWhether you're looking to architect, develop, or modernize on AWS, fourTheorem has you covered.\nIf you want to take your cloud game to the next level, then head over to fourtheorem.com\nand check out our articles, case studies, and see how we can help transform your AWS journey.\nLuciano, take it away. What have we got to say about Lambda and provisioned concurrency?\n\n\nLuciano: Yeah, let's start with a little bit of an introduction.\nOf course, we spoke before about how Lambda works in general and what is a cold start.\nAnd there are a few episodes that you can check out if you want to review these topics.\nOne is number 60, what is AWS Lambda.\n104, explaining how Lambda runtimes work.\n108, how to solve cold starts, specifically in Python.\nAnd then we also have an entire episode dedicated to Lambda best practices, which is episode 120.\n\n\nSo definitely review those if you're interested in really going deep down into the rabbit hole\nof all things at AWS Lambda.\nBut of course, it's probably worth doing a super quick recap of the things that are important today.\nAnd I think it's important to mention what happens when a Lambda function starts.\nAnd a Lambda function basically needs an environment that is created on demand\nwhen a specific event occur.\n\n\nAnd if you have multiple concurrent events, more Lambda environments are created as needed\njust to try to keep up with the load.\nRemember that AWS Lambda will create environments\nand each environment will process only one event at a time.\nSo if you have two events, a new environment needs to be created.\nAnd of course, these environments are totally dynamic.\nIf there isn't really lots going on, maybe the throughput of event decreases\nor at some point you have a period of total inactivity,\nAWS will start to reclaim resources and it will destroy those environments.\n\n\nAnd you have to think that of course, creating one of those environments\nis not a trivial operation.\nSo it kind of requires some work on the AWS side.\nAnd just to simplify, you can imagine that this execution environment\nneeds to be created somewhere.\nAnd specifically, these are micro VM running on Firecracker,\nwhich behind the scenes is deployed on EC2 instances.\nAnd when all of that is created, of course, the code that you want to provision\ninto your Lambda function needs to be pulled from somewhere.\n\n\nSo that can be either S3 or a container registry.\nThen at that point, the instance is ready to be initialized\nand the initialization phase has multiple steps.\nFor instance, if you have Lambda extensions enabled,\nthose need to be initialized first.\nThen depending on the specific runtime you're using,\nfor instance, if you're using Node.js, the interpreter itself needs to start\nand maybe it's going to do things like loading libraries,\ndoing JIT compilation and whatever makes sense\nfor that particular runtime that you're using.\n\n\nAnd then finally, your code starts to be initialized.\nAnd you know that in your code, you generally have two parts.\nThere is an init code, which is where you can do all the things\nthat you want to do the first time that the environment is initialized.\nAnd this is what's going to happen.\nAnd then you have the under code, that's the code that gets executed for every event.\nSo the init of your code is what happens when the environment is created.\n\n\nAnd of course, all of this stuff can take some time.\nAnd when this happens and you are trying to process an event,\nall this extra delay is called a cold start.\nNow, you might be wondering, is this something bad?\nAnd the answer is that it really depends the way you look at it,\nbecause on one side, cold starts are actually cool\nbecause they are the necessary trade off\nthat allows Lambda as a service to scale to zero.\n\n\nSo if we didn't have cold starts, we probably had something\nthat was running all the time.\nAnd the pricing behind Lambda would be very different from what it is today.\nSo in a way, they are kind of a necessary evil.\nAnd the other thing is that sometimes they are very negligible,\nbecause if you're doing some kind of background processing\nand it might be not very particularly time sensitive,\nif you have to wait a few milliseconds or a few seconds even extra,\nit's probably not going to be the end of the world.\n\n\nImagine you are just sending an email in the background\nor maybe resizing a picture.\nIt's probably fine if that particular process happens\na few milliseconds later rather than happening immediately.\nBut of course, if you have a use case, maybe, for instance,\nlike an API request with a user, like maybe using a browser\nthat triggers that API request, and that request is handled by Lambda,\nif there is a cold start, the user might actually perceive that slowness\nand it might affect the user experience.\nIn that case, you need to be a little bit careful with cold starts.\nAnd if you are in one of those cases, you probably want to know,\nokay, what are my options for reducing the cold starts?\nAnd one of such options is provision concurrency,\nwhich is what we are going to talk about today.\nYeah.\n\n\nEoin: And maybe before we go further, I mean, I always feel a little bit reluctant to talk about topics like this,\nbecause I think cold start problems are generally overstated,\nespecially by people who don't use Lambda really in anger.\nSo it's really an advanced topic. It's something that's useful to know about,\nbut I wouldn't fret about knowing all the different options\nand hyper-optimizing functions that probably don't need to be optimized\nin a lot of cases. Simpler is generally the better approach\nif you can get away without all of these fine-tuning options.\n\n\nLuciano: I actually remember that there was a case studied by AWS\nwhere they looked at all the Lambda invocations\nthat they have across all their customers,\nand they came up with a percentage that I don't remember exactly how it is,\nbut I think it was in the order of like 1% of all the function invocations is a cold start.\nSo generally speaking, this is a problem across all the customers.\nIt doesn't happen so often. Then of course, if you have very sparse workloads,\nmaybe you might be more affected than customers\nwith lots of events coming in all the time.\nYeah. Yeah.\n\n\nEoin: But look, these are useful things to know so that you've got these configuration options in your back pocket\nif you ever really do need to take advantage of them.\nSo let's first clarify that there is a quota on the number of Lambda environments\nyou can have running in a given AWS account in a region.\nThe documented default is 1000 concurrent executions\nacross all functions in an account in a region,\nbut that's a soft limit and can be increased if needed.\n\n\nNow, a lot of people have been seeing for a while now\nthat new accounts have a limit of 10.\nWe suspect that this is for abuse prevention\nto prevent people spinning up new accounts\nand managing to mine Bitcoin before they pay the bill.\nBut if that is your case, it can be raised.\nYou just need to request a support quota change.\nNow, let's talk about concurrency then.\nSo if the number of in...\nThis is the number of in-flight requests\nthat your function is currently handling,\ngenerally matches the number of active execution environments for Lambda.\n\n\nNow, there are two types of concurrency controls available.\nYou've got reserved concurrency and provisioned concurrency.\nThey can be confusing.\nReserved concurrency is the maximum number of concurrent instances\nallocated to your function.\nAnd when a function has reserved concurrency,\nit is reserved for that function.\nSo no other function can use that concurrency.\nIf you've got lots of traffic,\nor maybe you've got both the triggers,\ntons of unnecessary invocations,\nyou might end up in a scenario\nwhere you spin up enough Lambda environments\nto reach the account level concurrent execution limit.\n\n\nAnd that means no more Lambda environments can be created.\nAnd if you consider that environments are created\nfor specific Lambda functions,\nyou might end up in a scenario where you can't even process events\nbecause you can't spin up new Lambda function environments\nto handle new events.\nSo reserved concurrency is just useful to ensure that\nyou've both got a cap on the number of concurrent executions for a function,\nbut you also ensure that other functions can't steal an allocation\nfor a specific function.\n\n\nAnd of course, that has the impact of other Lambda functions\nhaving less capacity available.\nSo it's a trade-off.\nNow, reserved concurrency is just something\nyou can configure for a function.\nIt doesn't have any additional charge.\nIt's just a question of putting that cap on a function.\nIt's also used in some cases,\nif you've got an errant function,\nsomething that's causing a lot of problems.\nMaybe you've got a recursive loop\nor something that's triggering a lot of errors or a cost issue.\n\n\nYou can just set your reserved concurrency to zero\nand that will stop your function from being invoked altogether.\nThat's a useful tip.\nNow, this one doesn't really help with cold starts.\nIt just really helps you to make sure you can clean up,\nyou can keep scaling up specific functions up to a certain point.\nEnvironments are still created on demand\nand cold starts are still part of the picture in that case.\n\n\nNow, provisioned concurrency is something that AWS added a good bit later.\nAnd this is something that a lot of people welcomed.\nI'm not sure, to be honest,\nbut it essentially means that you've got a number\nof pre-initialized execution environments for your function.\nAnd these ones are ready, once you've deployed them\nand they're in an active state,\nthey're ready to respond immediately to incoming events.\nSo this is something that can be useful\nfor reducing cold start latencies for a function.\nAnd of course it does because you've got these environments\nrunning essentially ready.\nThey've started warm.\nThere is a cost impact on that.\nSo there are additional charges.\nSo let's talk about how provisioned concurrency works then.\n\n\nLuciano: Yeah, provisioned concurrency, as you said, keeps a certain number of Lambda execution environments warm for you.\nSo this basically means that as soon as you have enabled provisioned concurrency\nand set a specific amount for a function,\nAWS will need to spin up that number of execution environments for you\nso that they are ready and warm for whenever new events come in.\nSo basically if you receive a request,\nyou will have this Lambda environment already available.\n\n\nAnd also this environment not going to be eventually disposed by AWS,\neven though you might have a period of time\nwhere you don't receive enough events,\nor maybe you have even zero traffic.\nIf you have provisioned concurrency,\nyour instances will still be there and available,\neven if nothing happening in your account.\nSo in a way, this is going to help you to fight cold starts,\nbut it doesn't necessarily mean that you won't have cold starts anymore.\n\n\nIn fact, if you think about that,\nyou are just setting a number of instances that are ready for you.\nBut then if you start to have more events than you anticipated,\nthen Lambda still needs to scale up even more.\nAnd that means that even beyond the amount of provision instances,\nAWS will start to create new instances.\nAnd that means that those new instances will incur in a cold start.\nSo you might still see cold starts\nif you didn't really predict exactly the number of warm instances\nthat you needed in the first place.\nSo just be aware that there's not like a universal solution\nthat's going to totally eliminate cold starts,\nbut it's something that might help you to reduce the amount of cold starts\nthat you will see for specific Lambda functions.\nAnd another thing is that you can also set the provision concurrency to zero,\nand this is going to have the same effect that you described before.\nSo it's something you can use\nif you want to basically stop a function from running altogether.\nNow, how do you enable provision concurrency?\nIt's probably something that we should discuss,\nand I'll let you all talk about that.\n\n\nEoin: Enabling provision concurrency theoretically is quite simple.\nIt's just a number, an integer property\nthat you're associating with a Lambda function\nthrough the web console or through APIs.\nYou can configure up to the unreserved concurrency\nin your account minus 100.\nSo this is a reservation of 100 units of concurrency\nfor functions that aren't using reserved concurrency.\nFor example, if your account has a limit of 1,000\nand you haven't assigned any reserved or provision concurrency\nto any of your other functions,\nyou can configure a maximum of 900 provision concurrency units\nto a single function.\n\n\nNow, with Lambda functions, you have different versions and aliases.\nSo generally, you can get away with using the $LATEST default alias\nfor version for a function.\nBut when you're using provisioned concurrency,\nyou need to create an explicit function version with an alias.\nAnd it's on this alias where you set the provision concurrency value,\nnot on the function itself.\nSo this is something that can introduce a little bit more complexity.\n\n\nAnd this is a reason why you shouldn't just jump\nfor these optimizations by default.\nFor example, if your function has an event source mapping,\nyou have to make sure that the event source mapping\npoints to the correct function alias.\nOtherwise, your function won't use provisioned concurrency environments.\nAgain, it's worth remembering that configuring provisioned concurrency\nfor a function has an impact on the reserved concurrency pool\nfor other functions.\n\n\nSo if you've got function A and function B,\nyou configure 100 units of provisioned currency for function A,\nother functions in your account must share the remaining 900 units of concurrency.\nSo this is true even if function A isn't being invoked,\nand you're not making use of those 100 units.\nAnd this is very similar with reserved concurrency,\nbecause when you reserve concurrency,\nyou're also not making it available for other functions.\n\n\nThe difference is that with provisioned concurrency,\nyou have warm Lambdas running all the time.\nWith reserved concurrency, you don't.\nNow, it's possible to allocate both reserved concurrency\nand provisioned concurrency for the same function.\nAnd if you do that, the provisioned concurrency can't be greater\nthan the reserved concurrency.\nNow, if you're using all of this stuff,\nyou probably want to monitor your metrics.\n\n\nAnd with Cloud Web Metrics, you have a concurrent executions metric\nthat will show you the number of concurrent executions for your account.\nAnd you should look at that and tweak your settings accordingly.\nAnd it's something you could use once you're looking at concurrent executions\nfor any function.\nYou can use that to figure out what the optimal provisioned concurrency might be.\nThen you're more likely to reduce cold starts\nand balance that with the cost impact.\nThere's a good video actually by James Eastham\nwith a good walkthrough and some code examples.\nAnd we'll definitely have that link in the show notes.\nSo that's configuration. Let's talk about money.\nYes.\n\n\nLuciano: So provision concurrency cost is calculated from the time you enable it for a specific function until you disable it,\nif you, of course, ever disable it.\nAnd it's rounded up to the nearest five minutes.\nSo imagine that you, I don't know, enable it for seven minutes\nbefore disabling it, you will be paying for 10 minutes.\nThe price depends on the amount of memory you allocate.\nSo similar to the invocation cost of a Lambda.\n\n\nAnd of course, the amount of concurrency you configure on it.\nDuration is calculated from the time your code begins executing\nuntil it returns, otherwise terminates,\nrounded up to the nearest one millisecond.\nSo basically you are paying an extra cost on top of the usual invocation cost\nthat you would have to pay if you were not using provision concurrency.\nAnd that in a way makes sense because, of course,\nAWS is keeping those instances for you reserved\nand nobody else can use those instances,\neven if you are not processing any event.\nSo of course, there is a cost associated\nto have all this infrastructure reserved for you.\nWe'll be linking the full pricing documentation in the show notes\nif you want to review exactly what is the fee for your specific region\nand also changes depending on the architecture that you use and the memory.\nSo if you really want to do some simulation\nor have a better understanding of how this might impact your cost,\ndefinitely check out the official documentation for all the official numbers.\nNow, what are some common issues and maybe suggestions\nfor troubleshooting based on our experience?\nYeah, there are definitely things to look out for.\n\n\nEoin: One is over provisioning or under provisioning.\nIf you over provision, you're going to end up paying for compute you won't use.\nIt seems like you're getting away from the goal of using Lambda in the first place.\nAnd if you're under provision, you may still see cold starts.\nSo you really have to think about whether you want to get into this or not.\nScaling limitations as well.\nSo if you abuse reserve concurrency, you might end up in a situation\nwhere you can just erode the total Lambda concurrency pool\navailable to a given account or region.\n\n\nSame goes for provision concurrency.\nThis can make it very hard for you and your team to keep using Lambda functions\nand it can affect the capacity you have for Lambda functions\nthat don't have provision concurrency.\nNow, when we mentioned an issue we encountered recently,\nessentially it was a deployment error when we were deploying with provision concurrency\nwith an alias and the error, I think we got it surfaced through cloud formation,\njust said handler error code not stabilized.\n\n\nAnd this is AWS telling you that it was trying to warm up an execution environment\nfor a Lambda with provision concurrency, but it failed to do so.\nThe error is pretty vague, but there are actually a number of reasons why this can happen.\nSo it can happen because the specific version can't be deployed.\nMaybe your Lambda zip size is bigger than the 50 megabyte limit\nor the total 250 megabyte limit.\n\n\nYour Lambda is deployed correctly, but the initialization code fails.\nSo maybe you've got a bug or a typo in your code, it fails to import a dependency\nor to initialize a client, permissions error, that kind of thing.\nSo it makes sense, of course, that this can fail your entire deployment\nbecause AWS cannot fulfill its contract of warming up these functions\nas you have requested and creating this provisioned concurrency.\nBut it's something that you mightn't think of\nif you're just moving from a non-provisioned concurrency setup\nwhere you don't have to worry about failures in your code\nuntil the function is actually invoked.\nSo you just have to be a little bit more careful about that.\nRight. So I think we've given a good overview of provision concurrency,\ntalked about pros and cons.\nIt's not as simple as you might like. It's just the nature of it.\nWhat are some alternatives if we've put people off?\n\n\nLuciano: Yeah, I think it's definitely worth mentioning that it's not a silver bullet\nand there are lots of trade-offs that you need to carefully analyze\nand take a decision on whether this is the solution\nthat is going to solve your problems or maybe you want to look at other solutions.\nSo let's just try to give you some alternative ideas\nto try to fight cold starts because that's our premise today.\nWe are trying to think if I am annoyed by cold starts\nbecause they are affecting my applications in a way or another,\nwhat can I do to reduce or totally eliminate cold starts?\n\n\nAnd the first thing that comes to mind is that you can do your own warm-up as needed.\nAnd this is actually something that people used to do\nbefore this provision concurrency feature was enabled in Lambda.\nAnd actually, when I was working in the very first version of Middy,\nthis is something that didn't exist.\nAnd in Middy itself, one of the very first middleware we created\nwas a middleware that would help you to basically use event bridge as a scheduler\nto effectively send a ping every sometime, maybe every five minutes, every 10 minutes,\nwhatever made the most sense for you,\nto effectively wake up a Lambda environment for you,\nmake sure that there was at least one Lambda environment.\n\n\nAnd then the middleware would basically check,\nokay, if this is an event coming from event bridge,\nI'm just going to ignore it because I know that was only used to wake me up.\nBut if another kind of event comes in, maybe an API request,\nthen of course your own handle project is going to run.\nAnd of course, you don't have to use Middy to do this.\nYou can do this on your own.\nThe amount of code you need to write, it's relatively simple and small.\n\n\nBut yeah, and you can even do that without using event bridge.\nSo whatever is going to trigger your Lambda,\nof course, is going to create potentially a new environment.\nSo if you can do that recuringly,\nyou are going to create instances that will be around for a little bit\nand they will be warm to handle real events.\nSo that's just an idea.\nOf course, it's also very tricky that this particular approach\nwill avoid cold starts entirely.\n\n\nIt's just a way to try to reduce cold starts.\nThen depending on how well you can predict traffic coming in,\nyou might have different type of results.\nYou will see more or less cold starts.\nAnother interesting approach is Lambda's napstart.\nThis is more relevant if you're using Java.\nAnd again, it doesn't really solve cold starts per se,\nbut it can greatly reduce the duration of a cold start,\nespecially for languages like Java,\nwhere the cold starts can be more significant than with other runtimes.\n\n\nSo if you're using Java and you want to reduce the cold start duration,\ndefinitely check out snapstart.\nAnd then the other approach is that you might want to consider other AWS services,\nbecause of course, if you really are in a situation\nwhere you cannot tolerate cold starts,\nmaybe Lambda is not the solution for your problem.\nMaybe you need to use something like a container,\nmaybe running on Fargate if you still want to have\nkind of a serverless deployment experience.\n\n\nAnd in that case, you will have an instance that is running all the time,\nand therefore you are not going to have that particular problem of seeing cold starts.\nOf course, in that case, you might have the problem of how do I scale up?\nAnd then you need to see what that service is going to offer you\nto being able to scale in the case that you start to get more and more traffic.\nAnd then another final suggestion, which maybe can feel a little bit funny,\nbut it's actually serious, is that you could consider using Rust with Lambda.\nAnd the reason is that with Rust, we have seen really amazing performances\nespecially when it comes to cold start.\nThey can be 10 or 20 milliseconds for the majority of times\nif your Lambdas are still relatively small.\nSo that's maybe an amount of time that is basically making the cold start negligible.\nSo if you're interested in this approach, we have actually two podcast episodes,\nnumber 64 and 128, where we talk about creating Lambda function Rust\nand what all of that entails.\nSo you might check out those.\nSo that's all I have for suggestions.\nWhat else do we want to share?\n\n\nEoin: I'd like to remind before we finish up that AWS keeps introducing optimizations under the hood, even things so that you don't necessarily have to change to get performance improvements.\nAnd one thing we didn't talk about, maybe we can find a link for the show notes,\nbut I think people discovered about a year or so ago that AWS was doing some pre-warming\nof functions that didn't have provision concurrency turned on as well.\n\n\nSo they're doing things to try and make sure that your cold starts are as low as possible.\nAnd I think that's going to continue.\nWe've seen it with Snap Start, and I think we can expect even with that optimization\nof Python functions episode, we talked about how container images are optimized\nfor lower cold starts now.\nSo I would say again, just think a little bit before you invest too much time in all\nthe complexity of configuring provision concurrency, if you really don't need it.\n\n\nBut I think that wraps up our deep dive.\nAnd hopefully now you've got a clear understanding of how it works, its benefits\nand potential pitfalls.\nI think it's provision concurrency is definitely a useful tool in your AWS arsenal.\nNow, if you enjoyed the episode, do like, subscribe and share it with your fellow cloud\nenthusiasts.\nDon't forget, we really love hearing from you.\nAnd thanks very much to everyone who does reach out to us and gives us comments, questions,\nand just lets us know that they like the podcast.\nSo do drop us a comment or question.\nYour feedback does help shape our future episodes.\nSo thanks for tuning in and we'll catch you in the next episode of AWS Bites.\n"
    },
    {
      "title": "130. Growing in Tech with Farrah Campbell",
      "url": "https://awsbites.com/130-growing-in-tech-with-farrah-campbell/",
      "publish_date": "2024-09-06T00:00:00.000Z",
      "abstract": "In this episode, we had the pleasure to interview Farah Campbell, head of modern\ncompute community at AWS, prolific speaker, and former AWS Hero. We discussed\nFarah's career journey from healthcare into tech, tips on public speaking,\ndealing with imposter syndrome, the pace of innovation in the cloud, and\npredictions for the future. Farah shared personal stories and advice for getting\nstarted in tech and being an active member of the community. It was inspiring to\nhear from someone so passionate about helping others learn and grow.\n\nAWS Bites is brought to you by fourTheorem. If you are looking for a partner\nto architect, develop and modernise on AWS, give fourTheorem a call. Check out\n⁠⁠⁠fourtheorem.com⁠.\n\nIn this episode, we mentioned the following resources:\n\nFarrah's favourite\nAWS Bites episode with Jeremy Daly\nFarrah on X (Twitter)\nFarrah on Linkedin\n\n",
      "transcript": "Luciano: Hello, everyone, and welcome to another episode of AWS Bites podcast.\nMy name is Luciano, and as always, I'm joined by Eoin.\nBut today is a bit special because we have with us a very special guest, Farrah Campbell.\nAnd I'm sure that if you have done anything with AWS, you are familiar with Farrah.\nBut in case you're not, Farrah works for AWS, and she is Head of Modern Compute Community.\nShe's also a prolific international speaker, and she has been working very closely with AWS Heroes.\n\n\nSo today we want to get Farrah's opinion on a bunch of AWS related topics,\nand we're sure it's going to be an awesome and interesting episode.\nSo without further ado, let's get into it.\nAWS Bites is brought to you by fourTheorem.\nIf you're looking for a partner to architect, develop and modernize on AWS,\ngive fourTheorem a call.\nYou can check out fourtheorem.com to find more information about what we do,\nsome of our case studies, and how to reach out to us.\nOkay, let's get started.\nSo thanks, first of all, Farrah, for taking the time.\nWe know that you are always super busy and going around off to different conferences and events.\nSo it's amazing that you found some time for us,\nand we're really happy to have you here on the podcast.\nShould we start maybe with a quick introduction on your side?\n\n\nFarrah: Yeah, thank you for having me.\nSorry it took so long to make this happen.\nI'm super excited to be here.\nBut yeah, no, my name is Farrah Campbell.\nI work at AWS, and I get to work with our amazing developers around the world,\nspecifically the Heroes and our Community Builders.\nBut yeah, I'm super excited to be here.\n\n\nEoin: Farrah, you mentioned the AWS Heroes program, and that's one thing I really wanted to ask about,\nbecause we have, I think, a former Hero and a current Hero on the call.\nSo tell us about this program.\nI think a lot of people are really interested in understanding how you get into it, right?\nWhat are the features of an AWS Hero?\nAnd maybe do you have any recommendations for people who might like to be a Hero one day?\n\n\nFarrah: Yeah, sure.\nNo, I mean, like the AWS Heroes program really recognizes\npeople that go out of their way to help others through knowledge sharing.\nThey go above and beyond really trying to make sure that they help to unblock people\nwith things that they've been able to figure out.\nThis isn't a program that you can apply for.\nIt's not something you can get accepted or rejected from.\nIt really is a program that looks for people that are going out of their way consistently.\nAnd it's basically, it's something that happens because of other things that you are doing.\nAnd I guess my recommendation would be that for any aspiring Heroes,\nthat really shouldn't be the goal.\nThe goal really should be to help others and to continue to go above and beyond for them,\nto share knowledge, trying to help others within the communities that you're a part of.\nMaybe you maintain open source projects, but really the goal should be to help others\nbecause there really is no set way to become one.\nI still don't even know how I got in.\n\n\nEoin: It sounds like the whole idea of trying to become an AWS Hero is against the very principle really that the people who aren't really trying,\nyou're just helping people by their nature will get selected.\nI should probably clarify as well.\nI mentioned we have a former Hero and that's yourself.\nAnd just in case anybody thinks you can get ejected from the Heroes program,\nyou probably should explain why you're not allowed to be a Hero anymore.\n\n\nFarrah: Yeah, you cannot be a Hero and work at AWS.\nThe point is to recognize people outside of the company that are working to help others.\nIt's not about the people internally that are working to help others.\nBut yeah, I actually thought about that when I was deciding if I was going to actually apply for the job.\nSo I'm like, did I want to lose my Hero status?\nBecause I just loved being part of that group so much that it was hard to let that go.\n\n\nLuciano: You are still a hero for us, don't worry!\n\n\nFarrah: I appreciate that.\nOnce a part of the group, I think you're always part of the group.\nAnd that's one of the beauties about the program.\nAnd actually, I think just the AWS community in general is just such a welcoming community.\nAnd I feel like there's so many people that work to help each other.\nAnd we actually really enjoy working together and spending time together.\n\n\nEoin: Yeah, I mean, it's a shame that you had to hand in the Hero badge.\nBut as in your current role, I guess you get to be part of that in a very impactful way.\nThere's also the AWS Community Builders idea.\nAnd since Heroes sounds like it's very much about community, and getting content out there,\nhelping people to learn in various different ways, it might be a bit confusing for people.\nWhat is the distinguishing thing about Community Builders compared to Heroes?\n\n\nFarrah: Yeah, sure.\nI mean, because the Heroes program is a bit more exclusive, and there's no real way to put your hand up and say,\nhey, I want to be involved.\nThe Community Builders program was created, it was a few years ago, and that one you can actually apply for.\nSo we have applications that you can put your name in the pool.\nI believe it's once a year, we used to do it twice a year, but now it's just once a year.\nAnd so that the waitlist is open, and we'll review those applications and hopefully accept as many people as possible.\nBut that really is the defining differences that you can apply to be an AWS Community Builder and put your hand up to be considered for the program,\nor the Hero program, somebody internally has to make that call and to basically reach out to you.\n\n\nEoin: Okay, and so what would you say to people who are saying, okay, that's fine, I just have to wait to be called on as a Hero, but what do I need to do to get involved in Community Builders?\nIs there somewhere they should go?\n\n\nFarrah: Yeah, I mean, you go to the AWS Developer Center page, and it's listed right there.\nEven if you search AWS Community Builders, the application will come right up.\nIt's a pretty highly utilized site.\nBut that's really different.\nAnd get involved in the user groups in your area.\nOr maybe if you're involved in DevOps, and you want to get involved in the DevOps community,\nand then starting to use AWS, but I would really say, just joining some of the local communities that are in your area,\nthere's so many of them all over the world, it's actually incredible, really.\nSo there's many opportunities just to kind of start getting involved.\nAnd we also have a lot of people listed on the websites.\nAnd so you can reach out to others that are actually in the community,\nwhen you join an AWS user group or attend an event, and kind of ask somebody, what did they do to join the program.\nBut I think really starting to kind of network with the like-minded individuals is really helpful for people that are looking to be part of the program.\n\n\nLuciano: That's definitely a great suggestion.\nI actually want to change subject just a little bit.\nI was in Copenhagen, I think it was AWS Community Day, some time ago, we met there, and I attended your talk.\nI think it was an amazing talk where you basically share your career and some advices for people that are not necessarily just in cloud, but like more in technology.\nSo maybe one thing that we can talk about today is just a quick summary of your career, what you think were the right choices, the mistakes and the things you learn.\nAnd then in a way, what can we share with people so that we can give them suggestions if they want to get started in tech?\n\n\nFarrah: Yeah, no. So actually, mine is really interesting.\nYou know, I spent over a decade in healthcare management, did not enjoy that really at all.\nIt just was not, it wasn't fulfilling in any way that I wanted it to be.\nAnd I was always infatuated with the tech industry.\nMy stepdad had a company that I got to work at when I was 18.\nAnd it was just so fun to be around people that were like creating something from nothing, you know, come up with these random ideas.\n\n\nAnd then, you know, they were writing it in that language that I didn't even understand.\nSo I was just so curious about everything that they were doing.\nAnd it just seemed kind of like magic to me.\nAnd then I moved to Portland, Oregon, and there was, I mean, there seemed like there's a kind of a vibrant tech community there coming from like Klamath Falls and Eugene, but there really wasn't at the time.\n\n\nAnd so I decided that I wanted to start trying to get involved in tech or get a job and I applied for like a year, nothing happened.\nAnd then I started volunteering at tech events.\nThe first one was TechFest Northwest.\nI ended up meeting Kara Swisher.\nAnd she was basically like, why are you not working in tech?\nYou seem like you love it.\nAnd I, you know, basically explained I hadn't, nobody was taking my applications, I wasn't getting accepted.\n\n\nBut I had this offer for this part time job.\nAnd she's like, you know, you should really just kind of go for it.\nAnd she sent me Steve Jobs commencement address from Stanford.\nAnd so I did, I, you know, quit my full time job, which was really scary, because I was a single, you know, I was a single mom, I've always been a single mom with two boys.\nAnd so, you know, making sure that I could pay the bills was really important.\n\n\nBut I ended up turning that job into, you know, full time within a month, I was an office manager.\nAnd from there, my, you know, I met somebody at that office.\nAnd so then my next role was a director of operations at another small startup, and then another director of operations role that got expanded to customer and people operations.\nAnd then I ended up at Stackery, you know, where it was like, basically my first time, you know, I guess, like out in the field or in the wild.\n\n\nAnd, you know, really, my goal working there was to build out the AWS relationship and, you know, to help to get, you know, developers interested in the work, you know, the work that we were doing.\nAnd that's kind of really where it all started, you know, it was kind of my first intro working in the cloud at Stackery and into serverless.\nAnd it was just so many exciting things happen.\nAnd so I really think that job was really pivotal, because it allowed me to, you know, put my, you know, to get started with the AWS community and then serverless and things kind of really just took off from there.\nIt's really crazy, because every single one of my jobs, I was not qualified for at all.\nBut I somehow made it work.\n\n\nLuciano: Is that like an implicit suggestion to people to try to go off their comfort zone and try new things?\n\n\nFarrah: That's actually what was my talk was about yesterday is like, you know, stepping outside of your comfort zone.\nYou know, I talked about, you know, as children, we believe we can do anything, you know, like the world is our oyster, you know, I mean, like, jump, I thought I could run as fast as my parents car was going and I obviously couldn't, you know, but, you know, talking about like, instead of thinking that you can't, you know, start telling ourselves that, you know, we can, and really kind of adopt this like growth mindset, you know,\nstop questioning our ability, you know, embrace, you know, that that actually failing is an opportunity to grow, and you're only really failing if you give up.\nAnd so I think, yeah, I think really stepping outside of your comfort zone and trying things that, you know, really kind of push you outside of, you know, what you're comfortable doing is kind of key really here, I think, and growing specifically, I think, in this industry, too, because things are just changing all the time.\nAnd so I feel like most people are always feeling like they don't know everything and are feeling like they're out of their comfort zone all sometimes, no matter what they're doing.\n\n\nEoin: Is that characteristic that kind of perseverance to keep pushing yourself outside your comfort zone until you got that turning point and you were able to get into that role at Stackery?\nIs that something that's part of your personality?\nOr was that something that you learned through the process?\nWas there like a sliding door moment where you could have just called it quits and didn't?\nI definitely could have called it quits.\n\n\nFarrah: I mean, I was in tears basically and having panic attacks on most every single one of the roles that I was doing just because I felt so out of my element.\nAnd I just felt like everybody knew so much more than I did.\nIn fact, I mean, like I as an office manager, I mean, I stopped the meeting because I thought they were talking about people when they were talking about Jason and Cassandra.\nYou know, and so like I was constantly, you know, asking, you know, what I felt were silly questions, but they really weren't, you know, I mean, I remember my first time actually was talking about my first time using the computer.\n\n\nAnd it's like when I was 18, I was really kind of doing something.\nAnd then like the error popped up and it said you performed on a legal operation, you know, is going to close down.\nAnd I was so worried the cops were going to get called and I was going to be in trouble with my stepdad.\nI mean, it just kind of, you know, I feel like I've constantly, you know, felt out of my element.\nBut again, I kind of gone back that like that was not me as a child.\n\n\nI was definitely a go getter.\nAnd I don't know what happened to me over the years that where I came into this questioning everything I can do or sizing myself up.\nAnd so I really when I made the decision to make the move to tech, I really decided I was going to be brave, you know, to push myself.\nI was going to do whatever I could to be positive because there was no way I was going to keep showing up every day when I was feeling like a failure or without, you know, continuing to kind of focus on like the potential that was there.\n\n\nAnd just, you know, trying to have some like resilience to, you know, what I was going to be going through.\nI think that's really a huge part of, you know, actually kind of growing your career.\nAnd then I don't think anybody exceed, you know, cheap success in isolation.\nAnd so I think, you know, another really critical aspect of my career growth has really been the connections that I've made along the way.\n\n\nVery early on, you know, I was very focused on the relationships, not the transactions that, you know, were at hand and, you know, work to make sure that, yeah, just work to make sure that I was developing\na relationship that would end up helping me.\nI mean, some of those early relationships are still helping me, you know, today.\nThe people that I can lean on, the people that, you know, want to see me succeed.\nAnd I think that's really, really important when you're getting started in tech is, you know, to kind of have somebody there to help support, kind of keep you, help keep you moving forward.\nYou know, somebody else that is, you know, can maybe offer some advice when things seem tough or just to give you some like encouragement when you're ready to throw your keyboard or your computer at the wall because something still isn't working.\n\n\nLuciano: Another similar question I have is that you moved from kind of a very serverless focused community to a broader role where you are advocating for pretty much the broader world of AWS.\nIs there any anything interesting there that you want to share?\nMaybe any anecdote, any story or I don't know,\nyou liked it more when it was serverless and more focused or like the whole AWS gives you more opportunities?\n\n\nFarrah: I mean, serverless will always be, have like a very special place in like my heart, just because it was such a different feeling getting started with this community.\nYou know, everything was so new.\nEverybody was learning from one another, you know, there was really no wrong answer because you know, people were working to, you know, figure out what they had to solve problems for their own business objectives and those were different, but there was so many different ways to do things,\nright.\n\n\nAnd again, it was just so exciting.\nYou know, the community really wanted to learn from one another, so it definitely will always hold a special place in my heart.\nI think the big difference is just understanding like the scale and the amount of things that are happening all over the world and, you know, being able to kind of see that from the inside.\nIt's just incredible.\nAnd I think another thing is like being able to kind of help bring some of the containers and serverless together.\n\n\nLike I will never forget the very first time I had a call with the Heroes and we had, you know, I was bringing the serverless Heroes and the container Heroes together.\nAnd they're like, I think it was Mike Fielder.\nHe's like, are we supposed to be on the same call together?\nLike, are we allowed?\nLike, are we all really going to...\nWe're bringing this all together.\nAnd I think that's really important because it allows the conversations and, you know, for people to learn from things that are working for other teams, you know, that might, you know, different technologies that other people are using.\nBut yeah, I think the scale really, and, you know, coming from small startups, you know, kind of very focused on, you know, very specific, you know, workloads and things that we were at Stackery and now at AWS, it's, you know, it's really incredible.\nAnd it also feels like more pressure.\nI'll say that too, because like the impact, anything that you do, there's more impact.\nYou can have.\nAnd so there's, I think it's, for me, it feels like a lot more pressure to, you know, to do the right thing.\n\n\nEoin: I can only imagine, to be honest, because of the scale of everything you're dealing with at AWS.\nBut one thing you still do quite a lot of, and you mentioned that, you know, you're just, you're still speaking quite a lot.\nYou've done a lot of talks around the world, even several keynotes.\nIs this part of what you were saying about just building your network or what is speaking specifically for you?\nWhat's the goal there?\nWhy do you do it?\nI don't really know why I do it.\n\n\nFarrah: I actually am surprised people ask me.\nI mean, even the first time when I was asked, I didn't know, I actually was really scared to go talk in front of a bunch of developers.\nBecause I didn't know, I didn't think that they were, I would have anything relevant to say.\nBut I think it kind of started after I was involved in this documentary called The Chasing Grace.\nAnd the film was really about, you know, different aspects of, you know, working in tech specifically, you know, from an, I guess,\nfrom a female's point of view, I'll say that.\n\n\nBut the film I was in is about, you know, closing the pay gap and different ways, you know, women have approached, you know, handling that.\nBut from there, I had a little bit of exposure.\nAnd I got to meet a few other people.\nAnd I was talking to Andrew Clay Schafer at Reinvent.\nThis is, I don't know, seven years ago.\nAnd he actually said, I should get up on stage.\nHe's like, I should start doing talks to kind of help, you know, show people that you can, you know, you can change your career and end up in a good place, right?\n\n\nAnd so I put my hat into the ring one time at a local event in Portland, Oregon called TAO.\nAnd it was a lightning talk about opening your doors and changing your narrative.\nAnd again, it's, you know, kind of shifting your mindset into, you know, I can and, you know, focusing on being positive, you know, choosing to reframe things that are happening on a day to day basis that are negative, that might impact, you know, how I'm feeling to something more positive, you know, so it doesn't affect me and bring me down.\n\n\nBut then after that, I was offered like to DevOps days, Dallas reached out and offered me offered me a slot.\nAnd then from there, serverless days, Sydney, that was my very first international talk, and I will never forget it.\nBut I think I, you know, I keep doing it because I keep getting asked and I feel like there must be something there.\nAnd people must be learning something from me.\nSo if I if I can help anybody else to, you know, show that that you're making a jump in your career or that, you know, I don't I didn't get a college degree.\nYou know, I was a very, you know, I was married, divorced with two kids at 24.\nAnd that is not that is not how I envisioned my life to be when I was a teenager.\nAnd so I think it's, you know, I'll keep doing this as long as I feel like other or as long as others feel like I have something useful to say.\n\n\nEoin: I've seen you speak in the past, and I think it's clear why you became a Community Hero.\nLike for anybody, we all suffer from imposter syndrome, but for anybody who has like a harder route into tech, then to be honest, this relatively straightforward route that I had, you know, it could be so challenging for people.\nBut even even people like I definitely got something out of it from the point of view of thinking about imposter syndrome, which is, I think, universal.\nYeah. And I'm sorry. How do we get over that?\n\n\nFarrah: Like, what is I mean, like I even I work with some of the smartest people from all over the world, even I mean, like community members internally at AWS.\nAnd I feel like everybody has imposter syndrome.\nI'm like, why is this when like, everybody is so incredible?\nIt just I mean, it makes me feel better sometimes, like selfishly, which I feel bad even saying that when I hear others do.\nBut I even talk to some distinguished engineers here at AWS that, you know, suffer from it.\nI'm like, how is that even possible?\nLike, you have so many people that look up to you, you know, how do you why aren't you feeling that, you know, and kind of like, I got this, like, I'm good at what I do.\nBut maybe maybe someday we'll get there.\nBut I just feel like I hear so many people talking about imposter syndrome, and I really wish that we could move past that.\nBut it's tough.\nYeah, let's let's see what we can do.\n\n\nEoin: I mean, I think it's great that we're talking about it, because at least then, you know, it helps to build empathy when you realize that everybody's feeling the same way to some degree.\nAnd, you know, to treat people a bit better and avoid a lot of the problems we've had, I guess, traditionally.\nSo I think any more people we can get out there speaking who aren't speaking today, that that certainly helps.\nJust just to broaden the voices we have out there, and I'd love to know, do you have any advice for people who are struggling to get out there also nervous, like 99% of people are reluctant to apply and commit to doing public speaking?\nDo you have any advice?\nYeah, I mean, I would say start small.\n\n\nFarrah: Like, I mean, I got to I and I definitely did that, you know, you want to start with something a little bit more low pressure engagement, you know, maybe like a local meetup, you know, there's web, you know, webinars, you know, there's there's so many different opportunities to kind of throw your hat\ninto the ring. And then I think too, like, it's really important to know your why, you know, so like, why do you want what do you want to speak about? Is it you know, to inspire? Is it to educate to share your experiences, you know, so I think really, that will really help to kind of guide content to make sure that you know, that the information is valuable.\n\n\nAnd I think, you know, people really connect with stories, you know, personal ones. And I that, you know, that's really how I mean, all my talks have been, you know, focused on that. And they'll start with, like, you know, my story, the journey, the challenges, the triumphs.\nI don't think a lot of people even like to talk about the challenges are where like the things that really were hard, or, you know, kind of discussing all the trials and just the inadequacies that you know, you know, you might have had along the way, but I think, you know, really being authentic is really key, you know, to making lasting impact. And, you know, if this is something that you want to do, I think, you know, making sure that it's like, whatever you're doing is authentic is really, really important if you want to continue to do it. And then believing in yourself, like confidence comes from within, you know, remind yourself that, you know, you have valuable insights,\nthat your voice matters, that your opinion matters. And I have reminders that I sent, that are sent to my, that I set up, and I have multiple different ones that help me with, you know, being brave, or just kind of telling myself that I'm like, I'm a badass, like, you got this, you know, and just like, and those are really, really helpful for me to kind of keep going. Because, you know, when I get, sometimes you get into that negative self talk. And so when a reminder kind of pops up, or remind you to step outside, or you got this, it's okay to fail, you know, don't give up, that's actually failing, you know, things like that.\nI think the reminders have really, really helped me. And I do those a lot when I'm getting ready to do a talk, because it's, I still am so nervous. And just, yeah.\n\n\nLuciano: That's definitely a great strategy. I like it. Maybe I'm going to copy it. But yeah, I wanted to remark a little bit the imposter syndrome with a slightly different question. My opinion, at least the way I feel it is more like an anxiety that this world is moving so fast, and you'll never be able to even catch up.\nBecause as you learn something new, there are already five new things that just came out this morning. And you like, really don't even have the time to try to learn all of them. So I think, personally, I feel that this weight of this, this world is going so fast, and I can just try to chase it, but I'll never reach a point where I can say, Oh, yeah, I know everything that there is to know in this craft, because changes so fast, right? Which in a way is great, because our work is so varied, and everything is always new. So every day is a different adventure, but can create that anxiety and that feeling of\nnot good enough for what the world needs right now. So I guess if we try to translate that into the cloud, we have seen lots of changes, of course, since the beginning of the cloud is one of the most rapidly evolving fields in tech. But I think especially in the last few years, there has been a lot of innovation. So maybe my question to you is like, yeah, what do you think about that? And maybe what's your bold prediction for the future of the cloud?\n\n\nFarrah: I mean, I think innovation is going to grow and is going to continue at a rapid pace. I don't see it stopping anytime soon. It doesn't feel like it's going to stop. I also agree that things have just seemed like incredibly fast, like changing on I mean, even if you look at like announcements from AWS, there's I mean, there's something coming out, we're getting close to reinvent, there's going to be so many new things that are now said it's really, really hard to keep up with. And I feel like it does, like kind of it makes you feel like inadequate. But I think yeah, like the innovation and growth will continue. I honestly think there's going to be more adoption and trust.\n\n\nYou know, with AI, ML, the Gen AI, I think it's going to become part of just the tooling conversation. I don't think it's going to be the focus on Gen AI, I think it's going to be this is kind of going to be like your assistant that people are using. I think that's probably my big thing is that. I think that we've over indexed talking about it so much. But I really, I mean, I see how it can be helpful. I use a lot of these tools to, you know, kind of get some of the grunt work out. And so it allows me to work more on strategic things instead of the actual, you know, the building this random, you know,\ngoing through survey data and trying to, you know, make some assumptions like and I can easily create an app that does things like that for you know, for me. But I think that it's soon it will just become part of like the tooling that people are using. And there won't be such a focus on like, let's make sure we say this as a word in part of the you know, the the blog post just so we can get attention. It kind of reminds me of like when well serverless was that way very early on, like blockchain was that way, you know, early on, I mean, the same is like, it's Yeah, it'll it'll move on.\nAnd it just kind of become part of the way we work. And I don't think that I think we're all going to be learning, we're all going to have to learn just as much as we do today. In five years, 10 years, I don't think the pace ever stops. Because it just the more we learn, the more we want to do more of and it just it just keeps just keeps going.\n\n\nLuciano: Yeah, absolutely. I agree that there is a little bit too much of a hype on GenAI. My theory is that because it's something that has an effect that is like tangible on everyone, even people that are not necessarily techie, I think that that's maybe kind of inflating the conversation a little bit more than it should. But yeah, I agree with your prediction that it's going to become more of a tool that we will be using naturally in our day to day life, whether it's tech or not, maybe is even irrelevant. But yeah, it's something that we are going to just be be used to it and not even think so much about it.\n\n\nFarrah: Yeah, or like, just, you know, I think even to now, like people are, you know, kind of, I don't know, you kind of want to step away or like, maybe I don't want to even read that because I'm sick of hearing about it. You know, people won't feel like that anymore. But yeah, it's, it's a it's a crazy time. And I don't see it ever ending. But I mean, was it different earlier on? Like, when do you I'm actually very curious. I have a question for you guys that have been in this a lot longer than I have, you know, how like, how you really think in the last three to four years, it's been a lot different.\nOr have you seen this this this kind of pace of innovation start earlier than you know, what I've experienced?\n\n\nLuciano: I'll let you go first, Eoin.\n\n\nEoin: Yeah. You know, it does feel like it repeats itself for sure. And I'm sure you've seen that as well for like, that these these hype words come and come into this cycle for two or three years, and it was blockchain and crypto and web three. And there's always something right. Gen AI is definitely different, because it seems every time there's a new one, it gets amplified a little bit. So I think part of that is like the social media growth effect as well.\n\n\nIt just, it amplifies every message so much until you can barely tolerate the amount of noise around it. But unfortunately, that means that it's, there's very little space for 'sane discussion', if I could call it that, or just meaningful discussion, because everything is drowned out. So I think you just it feels I don't know what you think, but it feels like you just kind of have to wait until the noise dissipates to find out exactly what's left. That's that's going to be sustainable and meaningful into the future. I'm definitely I definitely have Gen AI fatigue just\nfrom listening to the term. But I still it's still exciting, right? But it's very hard to actually keep that excitement going when there's so much hype.\n\n\nFarrah: I definitely agree with that. I definitely agree with that.\nYeah, I agree as well. I think that the bit that maybe we are not talking nearly as much is not Gen AI in itself. I think as a topic, it can be interesting. There's definitely lots of interesting stuff in there.\n\n\nLuciano: But I think we are failing to bring to the conversation table, the pros and cons, like the why and the when and the how and it's not a solution for everything. So I think if we get to a conversation where we focus more on the technicalities of it, where we say, okay, for this use case, maybe yes, maybe this way, for this other use case, it might look like it makes sense. But then in reality, there are other solutions that might be better. So it's not like it's a universal tool that is going to solve all the problems. And I think that's what the media's are actually portraying. And that's a little bit misleading and annoying from somebody that comes with a little bit of a technical background.\nSo hopefully when we get there, I think the conversations are going to get a little bit more useful and maybe less annoying.\n\n\nFarrah: I definitely agree. I feel like we've over indexed on it so hard and make it like promise it's going to do all these things. But I really see it as an assistance. It can be like somebody could do some helpful things, but you're still going to check on it. You're not going to just use whatever somebody gave you.\nBut anyhow, I also remember you brought up the buzzwords. So when I was at Stackery, I made a shirt called, it just said, 'Continuously DevOpsing microserverless', because I just was like, I'm going to put all the buzzwords on the shirt. It was so funny. I made socks and shirts actually. And they were actually a pretty big hit.\n\n\nEoin: You need to do an updated version with Gen AI now.\n\n\nFarrah: Gen AI, NFT, Web3. Yeah, no, I'm just kidding.\n\n\nLuciano: That can be an outfit for re:Invent.\n\n\nFarrah: Well, actually, Matt Bonig told me he wanted to wear an outfit. He wants to be a mascot at re-invent. And so maybe I'll tell him he has to pick something that works with that.\n\n\nLuciano: We'll take some of the credits when that happens.\nOkay, let's change the subject a little bit. We actually have just one last question, which is a little bit of a fun one. If you have a favorite cover art of AWS Bites podcast, just because we spend more time, I think, creating the cover art than everything else. I think this is an important question for us.\n\n\nFarrah: So I think my mind has to be good amped with Jeremy Daly because I like the reference to Sound of Music.\n\n\nLuciano: Nice. Yeah, that's a good choice.\n\n\nFarrah: Are they all, it looks like they are all based on some movies and things.\n\n\nLuciano: I don't know. I think they're pretty random. Whatever is the inspiration of the day, but definitely there have been a few inspired by movies. So yeah.\nSo do you have any final remark or anything else that you want to share? If people maybe want to connect with you or shout to you, where can they find you?\n\n\nFarrah: They can find me on Twitter. I still call it Twitter. I'm not gonna not call it Twitter. But I'm with FarrahC32 on Twitter. I'm probably more active there than LinkedIn. But I also, you know, I try to, I try to be as active as I can on as many of these social sites as you know, that I have time for.\nBut yeah, LinkedIn, Twitter. I love connecting with others. So you know, please do reach out. Definitely always happy to help. And if people have questions about how to kind of get started in, you know, within the community, they want to ping me directly.\nI'm, you know, that please do. I try to, you know, work with as many people and offer help and support wherever I can.\n\n\nLuciano: And to make that a little bit easier, we'll make sure to have all these links in the show notes. So you don't have to remember or try to do the spelling. Just just click on the link and you can connect with Farrah.\nSo again, thank you so much for being with us. It's been an honor and a pleasure. So yeah, again, amazing stuff. And thank you, everybody, for listening in today.\nAnd hopefully this was interesting to you. Leave us a comment. Let us know if there was any question that we might have missed, because maybe we'll do another episode in the future with Farrah and we can ask more questions.\nSo, all right. Thank you so much, everyone. And we'll see you in the next one.\n"
    },
    {
      "title": "131. What do you do about CloudFormation Drift?",
      "url": "https://awsbites.com/131-what-do-you-do-about-cloudformation-drift/",
      "publish_date": "2024-09-20T00:00:00.000Z",
      "abstract": "In this episode, we discuss the concept of CloudFormation drift, what causes it,\nhow to detect it, and strategies for resolving it. We explain that drift happens\nwhen the actual state of resources diverges from what is defined in the\nCloudFormation templates. Common causes include manual changes, third party\ntools, mixing IaC solutions, and automation. We then cover built-in drift\ndetection in CloudFormation and integrating it with alarms. Finally, we suggest\napproaches for reconciling drift like change sets, deletion protection, and\nbringing up parallel stacks.\n\nThis episode of AWS Bites is brought to you by fourTheorem. Need to modernize\nyour infrastructure or build scalable cloud solutions? fourTheorem brings the\nexperience to build high-quality, maintainable, and scalable cloud\napplications that evolve with your business needs. Visit\n⁠⁠⁠fourtheorem.com⁠ to see how we can help take your\ncloud journey to the next level.\n\nIn this episode, we mentioned the following resources:\n\nEp 31 -\nCloudFormation or Terraform\nEp. 121 -\n5 Ways to extend CloudFormation\nAutomatic Drift detection\n(AWS tutorial)\nEp. 11 -\nHow do you move away from the management console\n\n",
      "transcript": "Eoin: Have you ever deployed infrastructure with CloudFormation,\nonly to notice later that things weren't quite lining up as they should?\nWell, you might be experiencing CloudFormation Drift.\nWe've all been there. Deployments look fine initially,\nbut gradually drift away from their original configuration over time.\nThis can lead to very unpredictable results\nwhen you try and update your stack later on,\nand it can really end in disaster.\n\n\nSo today, we're diving deep into CloudFormation Drift,\nwhat causes it, how to detect it and fix it,\nand most importantly, how to prevent it in the future.\nAnd we're going to cover what it is,\nhow it relates to CloudFormation and other infrastructure as code tools,\nwhy it happens, how to resolve it,\nsome best practices on detecting drift as soon as possible,\nand to try and avoid it entirely.\nI'm Eoin, I'm here as usual with Luciano,\nand this is another episode of AWS Bites.\n\n\nThis episode of AWS Bites is brought to you by fourTheorem.\nNeed to modernize your infrastructure or build scalable cloud solutions?\nfourTheorem brings the experience to build high quality,\nmaintainable and scalable cloud applications\nthat evolve with your business needs.\nVisit fourTheorem to see how we can help take your cloud journey to the next level.\nWe might just start with a quick CloudFormation summary.\n\n\nWe've talked about it before, but let's rapidly go through it again.\nCloudFormation is an infrastructure as code or IaC service from AWS\nthat allows you to define and manage all of your AWS resources and configuration\nusing templates that are written in either JSON or YAML.\nSo you can use it to provision, model and manage all of the resources\nfor your application in a fairly safe and repeatable way.\n\n\nSo instead of creating things ad hoc manually,\nyou define them in code using a template\nand CloudFormation is the service that handles provisioning and updating it for you.\nSo it's perfect when you need repeatability\nbecause it allows you to define a consistent repeatable infrastructure setup.\nAnd also if you've got complex environments.\nSo if you've got projects involving multiple resources\nlike EC2, RDS, VPCs and Lambda,\nCloudFormation can handle all the dependencies between them fairly seamlessly.\n\n\nIt's also really good if you want to automate infrastructure management.\nAnd this is almost table stakes these days,\nbut you can automate the entire lifecycle of your infrastructure,\ncreating, updating and deleting resources without manual intervention.\nAnd it allows you to provide support for best practices\nlike versioning enrolled back as well.\nSo it has automatic rollbacks if something goes wrong during deployment.\nBy the way, if you want to dive a bit deeper on CloudFormation,\nwe have spoken about it before.\nSo in the show notes in the description below,\nyou'll see links to episode 31,\nthe battle between CloudFormation or Terraform\nand episode 121,\nwhere we talked about five ways to extend CloudFormation.\nAnd that's the overview of CloudFormation and infrastructure as code.\nBut we're here to talk about Drift.\nSo what is the concept of Drift?\n\n\nLuciano: The concept of Drift is not unique to CloudFormation.\nIn fact, it's something that can occur even with other infrastructure as code tools.\nFor instance, you mentioned Terraform, even with Terraform.\nIf you don't do things correctly or by the book, as they say,\nyou can have Drift as well.\nSo it's not necessarily an issue only with CloudFormation,\nbut something that you need to keep into account every time you do infrastructure as code.\n\n\nAnd today, of course, we are going to focus a little bit more on CloudFormation\nand the tooling around it\nand some of the tips that are maybe a little bit more specific to CloudFormation.\nBut I think the general concepts and the advice that we are going to give\ncan be applied also to another IaC tool.\nSo if you prefer to use Terraform,\nI think you're still going to find value in this episode.\nSo back to Drift,\nit happens when the current state of your infrastructure\ndiverge from the defined configuration in your templates.\n\n\nSo in other words, if you have defined something using infrastructure as code\nand at some point what you have defined there\ndoesn't exactly match what's in your AWS environment.\nSo you have, if you just look at your templates,\nyou think the reality should look in a certain way,\nbut then if you actually look at your deployed stack,\nit doesn't really look exactly like you defined in your template.\nSo you might be wondering how is this even possible\nbecause we just say that CloudFormation infrastructure as code in general\nis a way to get reproducible and deterministic deployments.\n\n\nSo how is it possible that all of a sudden it doesn't really match anymore\nwhat was defined in a template?\nAnd this is something that can happen as we said\nand generally happens when you start to misuse infrastructure as code.\nSo again, let's maybe try to do a little bit of a step back\ntrying to describe what's the idea of infrastructure as code.\nIn general, when you use tools such as CloudFormation or Terraform,\nthose are declarative tools we can say.\n\n\nSo tools where in a way they provide you with a language\nthat you can use to define or describe\nwhat's the desired state of a given stack.\nSo you never tell the infrastructure as code tool\nwhat is the sequence of steps that it needs to do to get to a certain state.\nYou just define what is that state\nand you let the tool figure it out by itself\nhow to get from the current state to the new desired state.\n\n\nAnd the problem is that most of these infrastructure as code tools,\nthey don't necessarily query upfront your current environment.\nFor instance, your current AWS account.\nThey will just store the state of the last deployment somewhere.\nIn the case of CloudFormation, this is entirely managed\nby the CloudFormation service itself.\nIf you use something like Terraform, Terraform is actually quite flexible.\n\n\nIt gives you visibility of the state file.\nYou can decide to store it in a simple file in the file system.\nYou can store it in DynamoDB, you can store it in S3.\nActually, Terraform, it's quite open about that.\nCloudFormation is a little bit more opaque, but the same principle applies.\nSo what happens the next time that you do a deployment\nis that rather than reassessing what's the current state,\nthe tool is just going to take the latest state recorded\nduring the previous deployment as the starting point.\n\n\nSo it's going to try to understand, okay, what was that starting point?\nWhat is the new state that you want to go to\nand make a number of assumptions\nand figure out what is a good plan to go from A to B.\nBut the problem is that if you did something wrong,\nand we'll talk a little bit more about that in a second,\nwhat the infrastructure tool thinks it's A\nis not exactly matching the current reality.\nSo it might actually be something else\nand therefore the plan to go from A to B is not necessarily a good plan\nfor what concerns the reality of your stack.\nSo maybe we should mention some more practical examples\nof what can actually introduce this kind of situation\nthat we are calling effective drift.\n\n\nEoin: I think the most common case for drift\nis just when you're making manual changes.\nSo someone might manually update a resource\nwithout updating that CloudFormation template.\nThat could be like tweaking an EC2 instance setting,\nchanging some security groups\nwhile trying to debug a connection issue\nor changing an IAM policy to try and resolve some permissions issues.\nYou can also have things that are a little bit more subtle\nlike changing the number of desired tasks in a Fargate service.\n\n\nAnd that's something that is quite common to happen\nactually outside your infrastructure's code tooling.\nSo manual changes, probably very common.\nThen you've got third-party tools.\nSo sometimes external automation tools may modify resources\nwithout CloudFormation being aware of it.\nThese can be tools that try to assess compliance\nand that might apply resources on your behalf\nto apply specific best practices.\n\n\nAnd it can also be internal, not necessarily third-party tools,\nbut just external automation.\nYou might have set up specific automation\nthat can alter resources.\nLike you might have a script that helps saving cost\nby turning or scaling down EC2\nor turning it on and off just to match your expected load.\nOr you might just have some internal tooling\nthat applies security best practices like turning encryption on.\nUsing a mixture of IaC tools as well can cause drift.\nSo if you use a mix of different tools\nand stacks managed by different tools\nhappen to share the same resources, this might cause drift.\nThis is a dangerous area, I would say.\nThe shared resources state might look very different to every tool\ndepending on the order of deployments\nand when it took its perception of state, if you like.\nSo effectively, each tool won't be able to know\nwhat the changes are applied by the other two.\nSo that's how it can happen, but how do we avoid it?\n\n\nLuciano: Yeah, I think at this point, it should be very clear\nwhat can actually introduce drift, what drift is.\nSo in a way, we can start to guess and working backwards\nand try to think, okay, how can we avoid it, right?\nAnd one thing is that we should try to avoid manual changes\nas much as possible, unless you know what you're doing.\nLike, I think there are some cases where you might still need to do manual changes\nbecause maybe you are trying to do certain things that\nin that particular moment, it's just easier to do it manually.\n\n\nWe mentioned, for example, trying to resolve an issue with a security group.\nIt is, I think, totally legit to try to figure out exactly\nwhat is the correct configuration.\nAnd then, of course, you need to remember to apply that correct configuration\nto your infrastructure as code to make sure that maybe you had a little bit of a drift\nfor a few minutes, but then that drift is immediately reconciled\nand your confirmation stack is effectively in line with the reality\nof your stack deployed on AWS.\n\n\nSo again, manual changes generally to be avoided.\nYou can still do it, and in some cases, it can be useful to do manual changes,\nbut always remember that manual changes will introduce drift\nunless you propagate them back into your stack.\nThen the other advice is always use IaC.\nSo try to avoid hybrid mode deployments, we would call them,\nwhich is sometimes you have this situation where different teams have different practices.\n\n\nSo you might have teams that still do things manually,\nother teams that use infrastructure as code,\nand then somehow you end up with this kind of mixed stack\nthat has some resources that are managed manually,\nother resources that are fully managed by IaC.\nAnd yeah, things can get messy really quickly\nand it's going to be very hard then to tell when a drift is going to happen.\nIt's eventually going to happen, but then how it's going to happen\nand then what kind of results you can have as a consequence of that.\n\n\nAnd we also mentioned not to mix different IaC tools.\nSo you can use different IaC tools together.\nThere are ways to do that safely.\nI think the risk is when different IaC tools are actually sharing the same resources,\nbut you can have a mix of IaC tools when they manage different stacks.\nAnd other things are if you're using external automations,\nideally you want this automation to either update your templates\nand not the resource directly,\nor if it's not easy for you to let the automation update the templates,\nmaybe you can have some kind of reporting\nthat then you can manually take and apply into your infrastructure as code\nrather than letting the automation change resources and then ending up with drift.\nSo it might be of course more complex to set up automation this way,\nbut I think you always need to prefer the safety rather than maximum automation\nand then end up with something that can be inconsistent.\nSo now that we know some ways that we can avoid drift,\nhow can we detect it?\nHow do we really know if we have drift in one of our stacks?\n\n\nEoin: Yeah, it's very important to have some sort of ability to detect that you've got drift\nand then obviously take remediation steps.\nI mean, you might also have a degree of acceptable drift.\nI think when you're talking about automation tools,\nyou know, updating security configuration or applying tags,\nsometimes organizations just take the view that the infrastructure as code tools\nupdate the resources and the automation continually aligns them to comply.\n\n\nAnd that's acceptable drift, I guess to a degree.\nBut if you have other kinds of drift,\nthen you might just realize too late because the deployment fails in a weird way,\nlike tries to modify a resource that doesn't exist anymore.\nSo luckily, CloudFormation for the past few years has a feature to detect drift\nand you can use it from the management console by just going to a stack.\nYou click on stack actions and select detect drift.\n\n\nAnd I think this is supported for most configurations now.\nWhen it launched, it didn't support everything, didn't detect everything.\nIt's going to start a background scan and CloudFormation will compare the known state\nwith the actual state of all of the resources in the stack.\nAnd then once it's finished, you can visualize the results of the operation\nby clicking on view drift results under stack actions.\n\n\nSo in that page then after a few minutes, you'll see if your stack has drifted\nand if it has, it'll also give you a diff of the current stack state\nand what CloudFormation expected.\nNow, luckily, you can automate this process to some degree\nand trigger an alarm if one of your stacks has drifted and that's a good idea.\nI think if you're concerned about drift and going to take it seriously,\nthere is a tutorial by AWS that explains you how to do that\nand we'll have a link in the notes below.\nSo we've got some drift, we've seen it, we've detected it.\nWhat do we do?\n\n\nLuciano: Yeah, I don't think there is a universal solution\nand to be fair, it's always a little bit of a pain.\nThere are situations where maybe you just created that drift\nlike the example we mentioned about the security groups\nand therefore in that case, it's very easy to reconcile your stack\nand fix the drift because you know exactly what did you change\nand you can easily reapply the same changes into your infrastructure as code.\n\n\nAnd in the case of a security group, that's probably going to be a few properties.\nMaybe you're going to open different ports,\nbut it's very limited to a specific area of your stack.\nBut in some cases, it might be much more complex\nand you probably need to be a little bit creative in trying to figure out exactly\nfirst of all, why the drift happened,\nwhat is exactly the desired state versus the current state\nand how to reconcile the two.\n\n\nAnd the general problem is that you might think about a very simple solution.\nThe simplest solution is probably, okay, I'm just going to destroy the stack\nand recreate it entirely with a well-known, well-defined end state,\nwhich is something that works.\nBut of course, if you have an application that is running in production,\nyou probably cannot afford to do that\nbecause of course, you are going to create downtime.\n\n\nSo it gets really tricky when you want to try to reconcile drift\nwhile not creating downtimes.\nAnd that's why sometimes you need to be a little bit creative.\nAnd recently, for instance, we had a use case at work\nwhere we had some drift related to a load balancer.\nSo we needed to do some changes to this load balancer,\nbut doing the changes will recreate the load balancer entirely.\nAnd because in that particular stack,\nthe DNS entries were not managed by that stack,\nbut were managed externally,\ndoing the changes will basically recreate a new load balancer\nwith a different IP address or a different DNS record,\nand therefore we will have downtimes.\n\n\nAnd to solve that particular use case,\nwe basically needed to keep the existing load balancer as it was,\nspin up a new load balancer with basically exactly the same targets,\nthen change the DNS in the other stack,\nand effectively we kind of switch load balancers that way.\nSo we had a moment of time where we had two different load balancers\nthat led time to the DNS to propagate correctly,\nand then at that point we could delete the old load balancer.\n\n\nSo in that sense, this is just an example that shows you\nthat sometimes you need to think about\na little bit more involved and complex solutions\njust to make sure you slowly converge to the desired state\nby trying to avoid to destroy resources that might be used in production,\nand therefore if you just destroy them, you might end up with downtimes.\nSo yeah, I guess in general, what you want to do is try to apply,\nto figure out, first of all, what happened,\nfigure out how do I get to the specific target state,\nand then try to create a plan where step by step\nyou understand what's going to happen,\nwhat is going to be the new state,\nand then slowly make sure that that state converges to the desired one.\nNow at that point, you hopefully are going to end up with a new version of your stack\nwhere your actual state described in the stack\nand the state present in AWS matches.\nSo from that moment on, if you keep doing changes correctly,\nonly using infrastructure as code, that situation shouldn't happen again.\nSo yeah, this is, I guess, one of the approaches.\nDo you have other ideas, Eoin, that you want to share?\n\n\nEoin: I would suggest maybe using change sets in CloudFormation\nas a way to help with that, because with change sets,\nit's a bit like a Terraform plan.\nIt's not going to apply changes.\nIt'll just create a change set for you with the diff,\nand it'll show you what changes are going to happen beforehand.\nNow you can also enable deletion protection on resources that you want to protect.\nOf course, if you can't afford downtime or some data loss,\nsometimes the simplest solution when you have drift\nis just to destroy the stack and recreate it.\n\n\nSo in the development environment or pre-production environment,\nthat might be the way you go.\nOtherwise, you'll have to come up with a plan with multiple incremental steps\nthat can help you to minimize damage\nas you convert your infrastructure as code state to the actual state of the stack.\nAnd that can seem like a lot of work,\nbut at the same time, if you don't do it very frequently, it is an awful lot of work.\n\n\nBut if it's something you get used to,\nit's a good practice to just get into the habit of.\nOther times, it might just be safer to bring up an entirely new stack in parallel.\nDo all the necessary data migration, if any,\nand then shift the traffic to the new stack\nand then finally remove the old drifted stack.\nSo yeah, I mean, resolving drift might be tedious and costly.\nThat's why you want to avoid it as much as possible in the first place.\n\n\nMaybe another worthy mention is that if drift includes new resources,\nif you were to consider that other resources might have been added as well,\nthat should have been in that stack.\nYou can also use CloudFormation import.\nAnd that's another way to manage drift.\nWe mentioned this in our way back in episode 11.\nHow do you move away from the management console\nand how to get stuff that isn't managed by infrastructure as code into it?\nSo that's one dimension as well.\nJust on drift detection as well, as you know,\nwe should probably add that drift detection itself\ndoesn't have any cost or charge associated with it.\nBut depending on the resources involved, correcting it might, of course.\nSo changing resource types or scaling can trigger charges.\nOf course, factor in the cost of downtime\nor potential security risks from unmanaged changes as well.\n\n\nLuciano: Absolutely.\nI think this covers more or less everything we wanted to share for today.\nTo summarize, CloudFormation drift is an issue that can be very tricky\nand can be unexpected sometimes.\nSo even if you do an effort to maintain your stacks well\nor using infrastructure as code,\nthere are always so many different factors that can sneak in and cause drift.\nSo I think it's just a good practice to try to stay vigilant\nand maybe come up with some automation like the one we mentioned\nfrom the tutorial by AWS that we have in the link in the show notes\nto try to give you some kind of alarm as soon as possible\nwhen drift is detected so that you can action sooner rather than later.\nBecause of course, the more drift compounds,\nthe more challenging it's going to get to resolve that drift.\nSo that's everything we have.\nAnd we actually are curious to know if you had any interesting story\nof stack drifting and maybe you had to come up\nwith some very creative resolution strategy.\nIf that's the case, please share it with us,\neither by reach out to us individually on our social channels\nor maybe by leaving a comment on YouTube\nor rather in your podcast player of choice.\nSo with that, thank you very much for staying with us\nand we'll see you in the next one.\n"
    },
    {
      "title": "132. GitHub Action Runners on AWS",
      "url": "https://awsbites.com/132-github-action-runners-on-aws/",
      "publish_date": "2024-10-04T00:00:00.000Z",
      "abstract": "In this episode, we provided an overview of GitHub Action Runners and discussed the benefits of using self-hosted runners on AWS. We covered options including EC2 and CodeBuild for running GitHub Actions, compared pricing across solutions, and shared our hands-on experience setting things up. Overall, using AWS services can provide more control, lower latency, and cost optimization compared to GitHub hosted runners.\n\nAWS Bites is sponsored by fourTheorem, an Advanced AWS partner that works collaboratively with you and sets you up for long-term success on AWS. Find out more at fourtheorem.com.\n\nThe source code for the project we discussed is available on GitHub: fourTheorem/codebuild-gha-runners!\nIn this episode, we mentioned the following resources.\n\nCloudonaut - Self-Hosted GitHub Runners on AWS\nAWS: Best Practices for Working with Self-Hosted GitHub Action Runners at Scale on AWS\nGitHub - philips-labs/terraform-aws-github-runner\nGitHub - garysassano/cdktf-aws-codebuild-github-runners-organization\nGitHub - machulav/ec2-github-runner\nAWS CodeBuild Managed Self-Hosted GitHub Action Runners\nHyperEnv - Self-hosted GitHub runners on AWS\nRunsOn - Self-hosted runners on AWS\nActions Runner Controller for Kubernetes\nBiome\nSLIC Watch\n\n",
      "transcript": "Luciano: Hello and welcome to episode 132 of AWS Bites.\nIn today's episode, we're diving into GitHub Action Runners on AWS.\nGitHub Actions is a fantastic tool for automating CICD workflows\nand its runners handles all the heavy lifting.\nWhile GitHub offers hosted runners,\nusing self-hosted runners on AWS gives you more control,\nlower latency and cost benefits.\nWe will explore today how AWS services like EC2 and CodeBuild\ncan be used to power your GitHub Actions runners\nand how to set up self-hosted runners to optimize your workflows.\nI'm Luciano and today I'm joined by Eoin for another episode of AWS Bites.\nAWS Bites is sponsored by fourTheorem,\nan advanced AWS partner that works collaboratively with you\nand sets you up for long-term success on AWS.\nFind out more at fourtheorem.com.\nSo Eoin, let's maybe start by explaining what are GitHub Actions runners.\n\n\nEoin: Well, a lot of people will know GitHub Actions as the fairly powerful automation tool integrated into GitHub\nthat allows you to automate tasks like testing, building,\ndeploying applications directly from their repositories.\nIt's become, I think, the leader.\nEverybody's rushing to it in some form or another,\nbut it allows for custom workflows triggered by events like code commits,\npushes, pull requests and releases,\nmaking CICD processes more streamlined and manageable.\nThere's a lot you can do with GitHub Actions,\nbut GitHub Action Runners are the underlying compute instances\nthat execute the jobs that are defined in GitHub Actions workflows.\nNow, GitHub itself provides hosted runners\nand you will see them used on open source projects\nand anything on GitHub out there.\nBut users can actually also set up self-hosted runners,\ngiving them more control over the environment\nand the software and the hardware.\nSo there are people out there using GitHub-hosted runners\nand using self-hosted runners.\nBut why would you want to go to the trouble of running runners on AWS?\n\n\nLuciano: Yeah, before I answer that, one thing I want to mention as well is that I really like that\nwhen you said events, that's really broad.\nYou can capture all sorts of events inside GitHub.\nSo if you use GitHub extensively,\nmaybe you even create, I don't know, software releases\nor even track comments on PRs.\nAll of these things are events\nand you can run workflows as a response to that event.\nAnd I've seen lots of very interesting use cases.\n\n\nLike recently, we use Release Please,\nwhich is a tool that helps you to automate releases.\nAnd it kind of uses all of these different events\nwhen you create a PR, when you merge domain,\nwhen people comment on your PRs\nto kind of help you to automate most of the release workflow.\nSo this is just to mention that one more reason,\nI think, why people are moving to GitHub Action\nif they use GitHub as a repo.\n\n\nAnd yeah, integrates well with all parts of the software,\nlike lifecycle management.\nSo yeah, to go back to your question,\nwhy should you use runners on AWS?\nWe just mentioned that running self-hosted runners\nmeans getting more control.\nAnd some specific reasons why we should do that on AWS\nis that if you're using AWS anyway,\nyou can leverage IAM permissions model.\nSo for instance, if you are maybe deploying stuff on AWS\nfrom your workflows,\nyou can have all of these credentials set up in your runners.\n\n\nAnd therefore, when your workflows are running,\nthey already are out and together\nrather than using other mechanisms\nto propagate credentials to the GitHub-hosted runners.\nAnother approach is that you...\nSorry, another reason is that your runners\nare co-located within the infrastructure.\nSo at that point, if you have everything running on AWS,\nmost likely you're going to have lower latency.\nAnd yeah, you can effectively use the same regions\nthat you use for your workloads.\n\n\nAnd therefore, any action that you perform there,\ndeploying resources, connecting to specific resources\nyou might have available should be much faster\nthan just using the self-hosted one by GitHub,\nwhich I believe run on Azure.\nSo it might be a totally different network\non a totally different region.\nAnd therefore, you might see increased latency.\nAnd another point,\nwhich is probably the main one for people is cost.\n\n\nBecause of course, when you are hosting your own runners,\nyou can find lots of different ways to optimize for cost.\nAnd effectively, when you use the hosted ones by GitHub,\nyou get a certain amount of free capacity.\nBut then at some point, you need to start paying.\nAnd so when you host your own runners on AWS,\nof course, you can pick specific instances\nthat can be cheaper.\nYou can switch instances on and off depending on capacity.\nAnd therefore, you have more opportunities\nto optimize for cost.\nAnd one final reason is that sometimes\nyou might have special requirements\nin terms of hardware or operative system.\nMaybe you need a GPU to perform certain tasks.\nSo if you have control on the type of machines\nwhere you are running your workflows,\nof course, you can pick the more specialized hardware\nthat you might need.\nSo the next question is,\nif all of that sounds appealing,\nhow do you do self-hosted runners on AWS?\nWell, in general, right?\n\n\nEoin: If you want to add self-hosted runners,\nyou can do it at an organization level\nor for specific repositories.\nAnd if you have like a GitHub enterprise level setup,\nyou can actually also add them\nat the enterprise level for all organizations.\nNow, to add a self-hosted runner in your GitHub setup,\nyou need to run the runner application\nwith which GitHub will provide.\nAnd that can run on Linux, Windows, and macOS.\n\n\nAnd it can run on x86-64 or ARM architectures.\nNow, if you have existing machines,\nyou can just follow the guided process\nthrough the repository organization runners option\nin your GitHub console.\nAnd there'll be a link to the documentation\nthere in the show notes.\nOnce you have that runner application running,\nGitHub will tell you how to set it up with credentials\nso that it could talk to GitHub.\n\n\nIt basically long poles GitHub for build events,\nall those different event types you talked about,\nand just starts builds as required,\nsending the status and log information back to GitHub,\nas well as like pulling and pushing cache data to GitHub.\nYou don't actually have to keep all of those runners\nrunning all the time.\nSo you have options there.\nYou can have a small pool running\nor even no runners running all the time at all.\n\n\nGitHub allows you to then scale up by using webhooks\nto let you know when a workflow job has been queued.\nIn the GitHub docs, you'll see that referred to\nas ephemeral runners.\nNow, ephemeral runners have the advantage\nof just running one job generally at a time.\nSo you have build isolation and increased security\nsince a short-lived runner has less risk\nof intrusion and data leakage.\nSo you can run the runner application on EC2,\non premises, in containers,\nor even like on a fleet of Raspberry Pis under your bed.\nThe limit is just your imagination.\nNow, a fairly common approach is to run it on Kubernetes.\nThere is an Actions Runner Controller\nthat helps with scaling and orchestrating runners as needed.\nThat's quite common.\nIf you are a Kubernetes shop\nand you have a team to run and support that infrastructure,\nthis is a good option to follow.\nBut another common approach is just simply running on EC2.\nSo maybe we could talk, Luciano,\nwhat are some of the options there?\nYou're thinking about running on EC2,\nyou just have to run this application.\nBut as we know, running a fleet of EC2 instances\nand scaling up and down, not necessarily trivial.\nWhat are our options?\n\n\nLuciano: Yeah, absolutely.\nI agree that it's maybe something tempting\nto start to set up everything from scratch.\nYou set up your own VPC,\nyou set up EC2 instances with auto-scaling rules,\nbut then over time, this is something you'll need to maintain.\nAnd there is a lot of moving parts.\nSo it might become something that requires\na significant amount of maintenance.\nAnd maybe at that point,\nmaybe you have many advantages,\nbut on the other side,\nyou need to consider the total cost of ownership\nthat is going to start to build up.\n\n\nSo be aware of all of that.\nBut thankfully, there are some solutions\nthat try to make all of this easier.\nOne is called HyperEnv for GitHub Actions Runner,\nand it's produced by Andreas and Michael Wittig\nfrom the Cloudonaut podcast as well.\nWe mentioned them a few times in some other episodes.\nSo definitely worth checking it out.\nThis is a product that you can find in the AWS marketplace,\nand it manages all the scaling in and out of instances for you,\nbut lets you use your own AWS account\nand runs EC2 instances on demand.\n\n\nAnd you pay per vCPU minute.\nThere is a little fee, I mean,\nthat you pay on top per virtual CPU minute usage.\nAnother option that we discovered quite recently\nis called runs-on.com or runs-on.\nAnd it's quite similar to HyperEnv,\nbut it basically lets you run everything yourself.\nYou are using it commercially,\nbut basically you buy it\nand then configure a license for a reasonable small fee.\nAnd effectively, this can be a nice option\nfor non-commercial projects,\nespecially if you are looking for something\nthat allows you to drastically save cost.\n\n\nOne of these options may be something\nthat can help you to basically get the best of both worlds.\nYou are running and managing your own EC2 instances,\nbut with a limited amount of management effort on your side.\nAnd at the same time,\nit should help you to save cost\ncompared to the hosted option directly by GitHub.\nThe other question is like,\nare you willing to pay another vendor?\nOf course, this is something you need to commercially decide\nif it's worth doing for you.\nBut yeah, on the other side,\nyou are buying more control and flexibility.\nAnd there is another option,\nof course, we mentioned already in the intro\nthat is using CodeBuild.\nSo what about CodeBuild?\nIs it a good option, a bad option?\nIs it easy or difficult to set up?\n\n\nEoin: Well, that's a good question.\nI mean, it's one that really intrigued me\nwhen they announced that you could do this last year.\nFirst, CodeBuild, for anyone who isn't really familiar with it,\nbecause I think a lot of people have given CodeBuild\na bit of a pass,\nit's AWS's continuous integration service.\nAnd it lets you run, build, and test workloads\non lots of different compute types.\nIt's been around a good while and is fairly mature.\n\n\nIt doesn't have the workflow and orchestration support\nthat you get with things like GitHub Actions,\nGitLab CI, or say CircleCI.\nBut I think it's quite underappreciated, actually, CodeBuild,\nbecause it's quite simple in what it can do,\nbut it's really powerful.\nIt doesn't let you orchestrate workflows\nlike those other tools do, as I mentioned,\nbut AWS has a separate service for that,\nwhich isn't as good, I would say,\nwhich is CodePipeline.\n\n\nNow, you can check out episode 44 on that\nwhen we did kind of a CodePipeline\nversus GitHub Actions episode.\nBut CodeBuild itself is quite powerful, right?\nIt supports Linux, Windows.\nYou can do Android builds.\nYou can do macOS builds.\nAnd you can run really large instances\nlike with 72 vCPUs and 145 gigs of RAM.\nYou can do ARM and x86 on it.\nAnd recently, it has added the ability\nto run on AWS Lambda under the hood,\nwhich can make scale up much faster\nif your build is suitable for Lambda.\n\n\nAnd it even has GPU support.\nSo it's useful for training and inference\nin your builds as well.\nSo it doesn't have to be a choice of CodeBuild\nor GitHub Actions these days.\nBecause CodeBuild now has support for GitHub Actions,\nthat means you can actually use CodeBuild\nas your runners and get all the orchestration\nand all the integration\nand all of the action support\nfrom the whole GitHub Actions ecosystem.\n\n\nBut surprisingly, it's not very complex.\nIn fact, it's a very small amount of work\nto set this up.\nAnd I think people should really try it\nif they're looking at one of these options\nand thinking, how can I improve cost,\nget better integration with AWS\nin my GitHub Actions?\nI'd definitely say, give this a try.\nAnd we have some help,\nlots of resources for you to check out.\nThere are plenty of subtle things\nthat can go wrong in the setup.\n\n\nSo we will share a code example\nthat will help here,\nas well as some repositories\nthat other people have created\nthat will help you to get it right first time,\nnot like us.\nSo the first thing you need to do\nis to configure authorization for CodeBuild\nto connect to your GitHub account,\nlike be it a repo, an organization,\nor your whole enterprise.\nNow there's three ways of doing that.\nYou can use OAuth in the console,\nyou can use a personal access token,\nor you can use CodeConnections.\n\n\nI prefer the CodeConnections approach,\nkeeping personal access tokens up to date\nand securing them and sharing them\nis something I'd rather avoid.\nSo CodeConnections is a method\nthat uses GitHub apps,\nand AWS basically automates the process\nof installing a GitHub app\ninto your GitHub account\nthat has the right permissions for the repos.\nAnd you can create this\nwith infrastructure as code.\nSo in our code example,\nthat will link in the show notes,\nit'll create your CodeConnection.\n\n\nThis used to be known as CodeStar Connections.\nSo you will see in the documentation\nboth terms being used.\nAnd that basically creates\nthis connection resource in a pending state.\nYou then need to go into the console\nand activate it and link it\ninto your GitHub account\nwith an OAuth flow.\nAnd you just need to do this once.\nIf you've already done it,\nyou can just reuse an existing one.\nSo once you have that CodeConnection set up,\nthen you can actually create\na code build project.\n\n\nAnd the code build project creation\nis pretty simple.\nThere are a few gotchas\nwith the documentation.\nSo again, look at our code examples.\nHopefully it'll make it easier for you.\nYou just need to set up your source.\nNormally with code build projects,\nyou're linking it to a repo,\nbut you can also make it a generic one\nthat isn't linked to a specific repo,\nbut instead is linked to your organization.\n\n\nAnd then you just set your authorizer\nto the CodeConnection ARN\nyou've just created.\nYou need to enable webhook triggers\nso that it can create GitHub webhooks\nand link AWS and GitHub together.\nAnd then you'll need your IAM role as normal.\nAnd that'll need permissions\nto access CodeConnection,\nCloudWatch logs,\nand whatever else you might need\nfor your scenario.\nSo this is one of the advantages\nis that rather than having to do\nan OIDC integration\nor some vault shenanigans,\nyou can just use IAM\nfor your permissions\nin your code build job.\n\n\nAnd you can still do\nall those other methods as well,\nof course.\nIn your build project,\nyou can also set the default compute type,\nlike the operating system,\ncontainer image,\nthe CPU architecture,\nand whether you're using\nstandard Linux or AWS Lambda.\nThe nice thing here, however,\nis that you can actually\noverride those values\nin your GitHub workflow YAML files as well.\nAnd you can switch the instance size\nand the operating system as needed.\n\n\nAnd you can even do matrix builds\nwith all the different CPU architectures\nand operating systems too.\nAnd then you can set a build concurrency\nwithin your project.\nIt is worth mentioning\nthat the default code build\nconcurrency limits are quite low,\nlike sometimes one.\nSo that's a soft limit.\nYou just need to go into the service quote\nas part of the AWS console\nand request one.\nThey're just trying to save you\nfrom running up a big build.\n\n\nNow, the code build project name\nyou give is important\nas this is what links it\nto your GitHub workflow YAML.\nSo in your workflow,\nyou just set your runs on.\nIf you've used self-hosted runners,\nyou'll be used to setting runs\non something like self-hosted.\nThat's a property\nin your workflow YAML.\nAnd you just need to use\na special syntax,\nwhich is like code build\ndash project name dash\na couple of variables\nthat come from your GitHub run.\n\n\nAnd then you just use\nyour GitHub actions as normal, right?\nYou'll commit this workflow.\nIt should start running\ndirectly on code build.\nYour actions should look like\nany other GitHub actions running\nand your pull requests\nand deployments\nare all running in GitHub actions.\nBut the actual build\nis running on code build.\nAnd your GitHub action caches,\nas we mentioned,\nare still there.\nMight be worthwhile\ntalking about why you'd use\ncode build for GitHub actions.\n\n\nI'd say like we talked about\nsome of the reasons\nfor running on AWS,\nbut for code build specifically,\nI was generally\npretty impressed by it.\nIt's quite simple to set up\nand gives you a lot of flexibility.\nSetting up all the stuff\non EC2,\nmanaging a third party vendor,\nanother one\nmight be a bit of friction\nfor you and your organization\nto do that.\nSo code build is definitely\nworth checking out.\nSo if you have like a VPC\nin AWS\nand you want your build environment\nto access this network\nand keep everything\nwithin that AWS network boundary,\ncode build is a good way\nto do that.\nIf you want to use\nIAM roles for permissions\nand don't want to use\nthe OIDC credentials flow\nor something else,\nthat's a good reason\nto use code build.\nOr maybe you've already\ngot GitHub runners\nand you're just struggling\nto cope with capacity.\nSo you'd want some extra\non-demand paper use capacity\nand you can just add\nsome code build\ninto the mix as well.\nOr you might actually have\nreserved code build capacity\nbecause you can use,\nyou can create\ncode build reserved capacity\nfleets that are cheaper\nand always available for you\nand you could utilize\nthose in your actions as well.\nShould we describe\nour experience in general?\nWe have our code repository.\nWhat do you think, Luciano?\nHow would you assess it?\n\n\nLuciano: Yeah, I think we spent some time experimenting\nwith this\nand getting to understand\nthe limitations.\nWe, as you said,\nwe have a link\nto our repo\nin the show notes.\nSo definitely worth\nchecking that out\nto see exactly\nwhat did we try.\nAnd this repo\ncontains a CDK stack\nthat sets up\neverything for you,\nwhich means\nit's going to create\nthe CodeConnection,\nit's going to create\ntwo different code build projects.\n\n\nOne is using\na small size code build runner\nand another one\nis using Lambda runners\nwith four gigabytes of RAM.\nSo you can see effectively\nthat the difference\nbetween the two\nwhen you run\nthis particular stack\nor maybe you can use it\nas a reference\nand just pick one\nor the other\nand copy paste\nthat particular piece\nof configuration\ninto your own\nspecific project.\nTo be fair,\ngetting the configuration\nright was a little bit\nchallenging.\n\n\nThe documentation\nisn't too bad,\nbut it doesn't cover\nall the configuration options,\nparticularly if you want\nto set it up\nfor an organization\nand not just\nfor a single repository.\nI think there are\nsome blind spots there\nand we were left\nto figure it out\nexactly how to do it.\nNot the end of the world.\nI think eventually\nwe figure it out,\nbut just something\nthat can be maybe\nimproved on.\nSo if someone from AWS\nis listening,\nplease make that\na little bit better\nfor future users.\n\n\nSo the examples\nalso don't provide\na good CloudFormation,\nCDK or Terraform example.\nSo it's,\nas always,\na little bit more\nyou go in the web UI\nand click stuff around,\nwhich I mean,\nit's a good way\nof starting,\nbut in general,\nwe all know\nthat it's better\nto do things\nin infrastructure as code.\nSo it would be nice\nto start to see\nmore examples\nin that direction\ncoming directly\nfrom the official documentation.\n\n\nOf course,\nthe community\nis always great.\nThere are other people\nthat have tried\nthings like this already\nand they have shared\ntheir own solutions\nand you can find\nexamples using Terraform\nor even CDK-TF.\nSo it was easy\nto just look at those\nand try to figure out\nexactly what we were doing wrong\nand what we should change\nin our setup.\nWe will have some links\nfor other repos\nas well in the show notes.\n\n\nIn terms of performance,\nthe performance\nthat we saw\nwas actually pretty good,\nbut not as fast\nas you will get\nwith runners\nthat are always running\ndirectly on GitHub.\nThat's especially\nthe latency\nto start\na new workflow.\nIn general,\nwe saw about\n30 seconds\nof overhead\njust to start\na new run\nof a specific workflow.\nAnd we tested\nwith a very simple workflow\nthat just did\n60 seconds of sleep\nand the minimum\nend-to-end time\nthat we observed\nwas 90 seconds.\n\n\nSo this is how\nwe noticed\nthat there was\nsome delay\njust outside\nour business logic\nfor the workflow itself.\nOf course,\nin that delay,\nwe need to assume\nthat there is\nlots of stuff going on,\nfor instance,\nprovisioning the ephemeral runner,\nwhich of course\nis going to add\nthat overhead,\nbut I don't know,\n30 seconds feels\nmaybe a little bit too much.\nHopefully,\nsomething that\nyou can somehow\nfine-tune\nand reduce that latency\nbecause sometimes\nif you have\nvery short-lived\nworkflows,\nit might be nice\nto see them\nstarting straight away\nand get a result\nstraight away.\n\n\nVery commonly,\none thing that I do a lot,\nespecially for\nopen source projects\nis to have,\nI don't know,\na workflow that just\ndoes linting\nand some other\nvery small checks\nfor code quality\nand those tests\ncan run in like\nseconds.\nSo it would be nice\nto just see\na green tick\nfor every commit\nin a matter\nof a few seconds.\nSo hopefully\nsomething worth\nlooking at\nand maybe\nsomething that\ncan be improved.\nIn our case,\nwe tested 100\nconcurrent jobs\non each runner.\nMost of them\ntook around 90 seconds,\nbut we had\nhigh degrees\nof concurrency,\nso 60 of them\nmore or less\nwere running\nsimultaneously\nand some of them\ntook over 3 minutes.\nNow,\nprobably it's because\nthey were left\nin a queue\nfor a while,\nbut yeah,\nI don't know\nif you have\nmore details on that,\nor if we should\njust talk about\npricing.\n\n\nEoin: Very difficult to know,\nI think,\nwhere all that time\nis going.\nIf you look at,\nbecause you can look\nat the code build\njob execution timings\nas well,\nand in general\nit shows like\nthree seconds\nto provision a runner.\nI think there's also\nsome just communication\ndelay between\ncode build\nand GitHub\nand I don't know\nwho's responsible\nor if it's just like\nthere's a queue\nand the queue\nhas to drain\nor what,\nsome of it\nyou don't see,\nbut basically\nthe times we're reporting\nthat 90 seconds\nis everything\nin your GitHub\nactions workflow\nand code build\nwill often be finished\nbut it still seems\nto take some time\nfor GitHub to say,\nokay,\nyour job workflow\nis finished.\n\n\nSo your job\nmight actually be done,\nit's just not reporting\nit as done.\nI don't have all the details.\nNow,\nwe should talk about pricing.\nIt's interesting to compare.\nNow,\nit's also quite difficult\nbecause you're not always\ncomparing apples with apples\nbut if you exclude\nthe AWS free tier\nwhich has like\na good code build junk\nand the free minutes\nyou get with your\nGitHub subscription,\nwe can just do\na fairly simple\npricing comparison.\n\n\nIf we take a fairly\nmodest 2vCPU instance\nand try to pick something\nthat's relatively similar\nacross the different options,\nmaybe looking at the cheapest\nwhich is just\non-demand EC2.\nNow,\nyou could also do\nspot EC2,\nof course.\nThat's probably\nyour best cheapest option\nis to use spot\nand I think\nthe runs-on tool\nwe mentioned\nallows you to do spot\nas far as I remember.\nIf we take\nan EC2 medium\nand then compare it\nwith a code build\nsmall,\nthe price for code build\nis about seven times EC2.\n\n\nNow,\nbut if you compare that\nto GitHub Actions\non a standard 2-core,\nGitHub Actions\nis about 11 times\nthe cost of EC2.\nNow,\nif you go to\nreserved code build instances,\nit's only about 4x the price\nand lambda is not too bad\nactually.\nIt's even slightly cheaper\nthan code build standard.\nIt's about 6.4 times\nthe price\nfor a 4 gigabyte lambda\nwhich is roughly similar.\nSo you can choose\ndifferent size lambdas.\n\n\nNow,\nof course,\nyou're not really comparing\nthe same thing\nbecause with EC2\nyou're going to have\nmore of a rectangular\nutilization graph\nwhich often has\na lot of waste in it,\nright?\nSo you might end up\nrunning your EC2\nfor five minutes\nin order to run\na two-minute job.\nSo there's always going\nto be a bit of extra waste\nwhen you use EC2\ncompared to using something\nthat's very on-demand\nand per second\nlike code build\nor lambda\nor GitHub Actions.\n\n\nAnd of course,\nyou have the extra overhead\nwith EC2\njust in terms of setup\nand maintenance.\nIt doesn't really make sense\nto always just compare\non a cost basis like this\nbecause when you have\nvery bursty demands\nfor your builds\nand sometimes you've got\nnothing running,\nmost of the time\nyou've got nothing running\nbut then you've got\na hive of activity\nwhen people need\nlots of concurrency\nand maybe you don't want\nto wait for lots of EC2s\nto spin up and react.\n\n\nYou want things that are\nreally very suitable\nfor the specific workflows\nyou need,\nlots of different compute types,\nsome on Lambda,\nsome on Linux,\nsome on Windows,\nwhatever.\nSo it might just make more sense\nto use something like code build\nor something else.\nOf course,\nI'm talking about pricing there.\nIf you're using ARM,\nit's half the price on code build.\nWith EC2,\nit depends on the instance type\nbut if you can get away with ARM,\ndefinitely go for that.\n\n\nYou get 50% saving\nout of the box.\nNow, we talked about\nrunning on Lambda\nand it's quite interesting\nwhen you actually see\nyour GitHub Actions\nrunning on Lambda.\nIt's almost surprising\nto see it just succeed\nwhen it's like cloning your repo,\ndoing NPM install.\nEven though these are\njust node processes,\nof course they should run on Lambda\nbut in the testing we did,\nwe didn't come across\nany major limitations.\n\n\nEverything we did,\nin fact,\nwe just took an existing\nopen source project we had,\nthe SLIC Watch project\nwe've talked about before.\nIt's got quite a few build steps in it,\ndoing code coverage,\ntests,\nunit tests,\nrunning lots of different NPM builds,\nTypeScript transpilation\nand everything.\nIt just worked.\nThere are some limitations\nwith Lambda.\nYou can use custom images\nbut you can't do things\nlike root,\nuse root.\n\n\nSo you can't use\npackage manners\nlike yum and RPM.\nYou can't do Docker in Docker\nbecause you don't have\nprivileged mode\nand it's not a proper\nDocker runtime.\nYou don't have any\nfile system access\noutside slash TMP.\nYou don't have the GPU option.\nYou don't have caching\nand you don't have a VPC.\nSo if you want to use a VPC,\nyou have to use\na standard runtime\nin CodeBuild.\nBut still,\nI would say,\nlike you just mentioned,\nlinting Luciano.\nThat's a perfect example\nof a workflow\nthat can run very easily\nin Lambda.\nAnd the chances\nof you having to wait\nfor CodeBuild\nto provision a container\nis much less\nif you're just using Lambda.\nSo it should be\nup and running\nin a few seconds\nand you should have\nyour results\na few seconds later.\nActually,\nI was using the Biome\nJavaScript TypeScript linter\nin one of these tests\nand the whole thing\njust ran in a couple of seconds.\n\n\nLuciano: Yeah, Biome is amazing,\nespecially for large projects.\nIt can do\nall the linting\nand formatting\nin literally seconds\nor less.\nAnyway,\nI think we have covered\neverything we wanted\nto cover for today.\nHopefully,\nwe gave you\na good overview\nof what is\nGitHub Action Runners,\nwhy should you\nconsider running\nyour own hosted runners\non AWS.\nAnd yeah,\nhopefully,\nwe gave you an idea\nof the different options,\npricing comparisons.\n\n\nSo let us know\nif you found\nall of that interesting\nor if you have\nother questions\nabout things\nmaybe we didn't cover\nor if you tried yourself,\nmaybe you know\nother tools,\nmaybe you have\nyour other resources\nthat you came across\nand use them\nto set things up.\nPlease definitely\nshare them with us\nbecause, of course,\nwe are happy\nto compare other options\nand, of course,\nmake all of these resources\navailable to other people.\nSpeaking of which,\nagain,\nall the things\nwe mentioned today,\nwe will have links\nin the show notes.\nWe will also have\na few additional links\nabout other resources\nthat we used\nduring our experiments.\nSo hopefully,\nall of that together\nwill give you\na very good starting point\nfor whichever option\nyou are going to choose\nif you end up playing\nwith the idea\nof running your own\nGitHub Runners\non EC2\nor Lambda\nor maybe using CodeBuild.\nSo thank you very much\nand we'll see you\nin the next episode.\n"
    },
    {
      "title": "133. Building Businesses in the Cloud with Fiona McKenna",
      "url": "https://awsbites.com/133-building-businesses-in-the-cloud-with-fiona-mckenna/",
      "publish_date": "2024-10-18T00:00:00.000Z",
      "abstract": "In this special episode of AWS Bites, Eoin is joined by Fiona McKenna, co-founder and CFO of fourTheorem, to discuss startup advice, hiring and growing teams, creating an environment for success, and managing cloud costs. They cover important themes around people, culture, leadership, and finance from Fiona's extensive experience in the tech industry.\n\nAWS Bites is sponsored by fourTheorem, an Advanced AWS partner that works collaboratively with you and sets you up for long-term success on AWS. Find out more at fourtheorem.com.\n\nFind Fiona on LinkedIn: https://www.linkedin.com/in/fiona-mc-kenna-174172a2\n",
      "transcript": "Eoin: Hello and welcome to another special episode of AWS Bites.\nI'm Eoin and today Luciano is taking a break from AWS Bites duties.\nThat's because we're joined by a special guest, Fiona McKenna,\nco-founder, CFO and head of HR at fourTheorem.\nFiona has been working in tech for most of her career,\nhelping startups and larger organizations grow and be successful.\nI've been lucky enough to work with Fiona at fourTheorem for more than five years now.\n\n\nAnd in that time, I've benefited from Fiona's expertise in running startups,\nmaking and raising money, as well as building successful teams of great people.\nWe're going to talk about startup advice, hiring amazing people,\ncreating an environment for success and, of course, managing cloud cost.\nAWS Bites is brought to you by fourTheorem.\nIf you're looking for a partner to architect, develop and modernize on AWS,\ngive fourTheorem a call.\nCheck out fourTheorem.com and the link will be in the show notes.\nSo first of all, welcome to AWS Bites, Fiona.\nI know you're a long-term fan and listener.\nCan you start by telling us a little bit about yourself?\n\n\nFiona: I am a long-term fan and I'm absolutely thrilled to be here today, Eoin.\nThank you so much for inviting me.\nAnd I'm really looking forward to seeing what artwork you guys come up with for this week's podcast.\nSo yeah, about me.\nI'm a startup-aholic.\nI've had a career of two parts.\nI'm almost 40 years working.\nI'm going to discount the first 15 because that was in banking and insurance\nand foreign direct investment manufacturing type businesses.\n\n\nAnd I forayed into the world of cloud before there was an internet, really a proper cloud for them.\nSo in 1999, when I was part of a business that was called Virtual Financial Management,\nit was using a frame relay solution to create the illusion of global access to financial ERPs on the cloud.\nAnd it didn't last.\nIt lasted about four months before we didn't get our funding.\nBut it brought me, it opened the door for me to join telecoms.\nAnd from telecoms, I've gone into really businesses that have been in all sorts of different industries,\nbut always centred around using technology or harnessing the next evolution of technology\nto drive their business growth and to stay relevant.\n\n\nEoin: Maybe we can start with talking about the people side of the business, because I think that's something that really interests people and hiring and growing teams.\nIt seems like something of an unsolved problem.\nWe had an episode (87) where we talked about interviewing at fourTheorem.\nA lot of people really seemed to like that one because it gave a bit of an insight into the inner workings.\nMaybe from your perspective, what do you look for as we're growing the team?\n\n\nFiona: So it's always about the people.\nI mean, it's really amazing.\nAnd this has been true for my whole career.\nYou might get excited about the technology aspect of it,\nbut the technology is really just the tool that enables people to demonstrate, showcase their skill sets.\nAnd we're looking for people.\nI'm looking for curious, interested people that sort of want to push the boundaries a bit,\ntake themselves out of their comfort zone,\nmaybe don't know exactly what they want to do next,\nbut know that they want to do something different.\nAnd I've listened maybe to our podcasts or seen us at conferences and events and things\nand know that we could give them an opportunity to grow and to take their career to the next level.\nMaybe it's worthwhile taking the opportunity then to shout out to everyone who's listening.\n\n\nEoin: If you do feel like you've got something that is a fit for us to get in touch,\nbecause we're always happy to look at new people, right?\n\n\nFiona: Absolutely, always.\nAnd I'll have a chat with anybody that's interested, that has something to offer us, or something that we can offer them.\nI'd love to have a chat.\nAnd that's part of our process.\nWe call them chats for a reason.\nWe try to keep the process as informal as possible.\nWe're not the traditional organization that has 18 different types of interviews,\nand they're trying to catch you out.\nWe're truly looking to see, is there a good fit between you as an individual and us as a company,\nand how we can grow together, and what you can bring to our culture,\nand how you can help us to develop and grow, and vice versa.\nSo that's always better done if you're actually just chatting to somebody.\nYou can really get to know them, as opposed to asking them where they see themselves in five years' time,\nor drilling down and trying to catch them out on something on their CV.\nThat's really not what we're interested in at all.\n\n\nEoin: Yeah, and we always try to get people, I guess, to understand that it's a two-way process.\nYou know, they can interview us as well.\nIt's difficult, of course, for people to really accept that,\nand understand that they're not being evaluated and critically analyzed.\nBut it's definitely something we value, isn't it?\nThe opportunity for people to really see if we're a fit for them as well.\n\n\nFiona: Yeah, I mean, I really can't stress that enough.\nYou know, it really is.\nAnd we've spoken to some great people, right?\nWhere it's been clear that there might be some gap in their CV to date,\nthat they need to bridge that first.\nSo, you know, that's the other reason for keeping the chat stay informal.\nTalk to us now.\nWe can give you a steer as to what gaps you should be filling,\nwhat other experiences should we be looking for.\n\n\nAnd come and talk to us in a year's time, two years' time, three years' time.\nIf there's one thing we know is how fast time flies,\nand that we meet the same people.\nYou know, the tech ecosystem is such a small world\nthat you're going to meet each other at different points in time\nin our career.\nAnd if it isn't a good fit today, it might be a good fit in the future.\nBut everybody, I like to think that everybody gets something\nfrom going through the interview process with us.\nAnd we really do.\nWe set that side of time.\nAt the end of it, you know, we always make sure\nit isn't all questions going our way.\nThere's always time at the end of each interview\nfor them to come back and ask us those type of questions.\n\n\nEoin: We often hear from other people saying, oh, hiring is the biggest challenge we have.\nWould you agree?\nDo you think, what are your biggest challenges\nwhen recruiting the right people?\nHow do you feel we go about attracting and retaining\nas well the right people in what is, I suppose, a competitive field?\n\n\nFiona: It is.\nIt is a very competitive space out there.\nLook, we're very, very lucky because of our profile\nas individuals as much as as a company.\nWe get a huge amount of inbound people coming in to talk to us\nwho feel they have a really good sense of what fourTheorem is about.\nI give credit to you, Luciano, I think the podcast,\nthe consistency of that over over 100 episodes.\nYou really get to see what what life is like\nwithin the fourth era of ecosystem over a continued pace of time.\n\n\nBut we're not acting in how we do things.\nWe're really honest about the good and the bad.\nWe're very transparent.\nTransparency and openness, it's it's part of our DNA\nand it's there from the very first interviews the whole way through.\nSo we have we've a great inbound pipeline, if you like, of potential people.\nAnd then I think it's important for us to explain that, you know what, we're not, you know,\nso we're not that big global multinational\nand we don't necessarily have the same packages that they can offer.\n\n\nBut I always ask people to focus on effective hourly rate, for example,\nin terms of how you work for us.\nWe don't ask people to do 90 hours a week.\nIn fact, we actively work to ensure that they do not do 90 hours a week.\nYou know, we don't schedule any calls with people after 530 in the evening.\nWe try to highly hire people that are kind of living within plus or minus two hours of our time zone\nbecause we just don't want to be that business.\n\n\nYou know, as people starting at 5 a.m. and still working at 11 p.m. at night,\nthat's not really what we're about.\nI think that's a big part of our retention.\nI think I think the next generation that's coming through, Gen Z and their ilk,\nthey have much more understanding about the importance of a rounded life.\nAnd, you know, I know in my own case, my family told me that they looked at us as parents\nand they didn't want to repeat the kind of career that they saw when they were, you know,\n10 and 12 growing up and the hours that myself and my husband were doing.\n\n\nThey don't want to work like that.\nThey want to they want to do a good day's work and then they want to have a life outside of work.\nAnd I think a company like fourTheorem has a great way of balancing those needs\nand making sure that they get all of their growth potential within a career perspective,\nbut they also get to live their life as well.\nSo I think that's really kind of played to our advantage.\n\n\nRetention also is part of, you know, acknowledging that they won't be with us forever.\nSo we're not trying to lock people in to stay with us for the next 10 or 15 years.\nMaybe this is where they spend five years, seven.\nMaybe they do spend 10 years with us,\nbut only if they're continuing to get the growth opportunities and it's working for them.\nWe're not we're not looking to sort of lock people in.\nWe want people to be with us if it's the right thing for them,\nbecause ultimately that means we end up giving the right result to our clients.\nWe've got happy, motivated people who are enjoying their work.\nThey're doing interesting things.\nThey're solving gnarly problems.\nAnd if they get to the point where they're not feeling that they're able to do that,\nthey can move on to something different and we wish them well.\nHopefully they'll bring us into their other businesses and help us solve problems there with them.\n\n\nEoin: So you've been part of companies that have grown larger than fourTheorem is today.\nWhat are the challenges that you think people should watch out for as they go from a small team?\nIf you're growing to 50, 100 people and beyond, what have you seen?\nWhat do you recommend there?\n\n\nFiona: Oh, you have to accept that there's growing pains.\nYou know, that's the biggest the biggest challenge of it is.\nAnd as your team grows to, you know, from the from the 20 to the 50 people, your focus.\nI'm talking now in the context of leaders and founders.\nYour focus is going to have to narrow.\nYou know, you can't be everything to everybody or you'll become the blocker.\nYou will be the one that's getting in the way of growth and progress.\n\n\nSo I think from from the very early days, you need to be thinking about where do you make the most impact in the organization?\nWhich we know. So where do you stay?\nWhere do you when your focus has to narrow or where is your your focus going to narrow into so that you're delivering the most for the rest of the team?\nAnd then you need to be making sure that you're bringing in people behind you.\nAnd you're always thinking about that succession plan.\n\n\nYou know, your business is going to be different.\nThe culture is going to be different.\nAnd you have to accept to that.\nAnd this is the hard part of it is that sometimes you're going to have to bring in people that are very different to you because the leadership team of a business of well over 100 people will have different qualities.\nAnd afterwards to the crazy mad entrepreneur chaos monkeys that are able to adapt fast and make decisions fast.\n\n\nThey'll become more process driven, more more, you know, numbers oriented.\nAnd that might suit you. So, you know, you have to think about your own step down date, your own best before date.\nYou know, when do I when do I stop becoming a force for good in the organization?\nAnd when do I start becoming part of the problem or something that's blocking it?\nAnd you need to let go of control.\nFantastic advice.\nI could dish it out all day.\nI'm not 100 percent sure how well I can live to it because of my own personal experience.\nI've voted my way out whenever the company has gotten to that sort of size and I can't have my finger in Santa Pots.\nI've moved on to the next job.\nSo I'm going to have to learn, live my own advice in our growth journey here and forth here as a founder.\nI'll have to make sure that I don't overstay my best before date.\n\n\nEoin: I think we all struggle with that, right?\nYou want to feel responsible, I suppose, for the success of the company.\nTherefore, it seems like a contradiction to accept that the continued success will mean taking a step back.\nAnd that it's OK to make yourself dispensable.\nIt's so easy to say.\n\n\nFiona: But then actually doing it is that living it is is the really hard.\nIt's the really hard thing.\nNow, I have made myself redundant in the past.\nSo I have some experience of of having done it.\nAnd I did have I can remember a point of time.\nI'm speaking with a CEO and basically saying, you know, that the role that I was doing, I was not I was not the most effective person in doing that role.\nAnd we needed to sort of split things out.\n\n\nAnd it was but it's very hard.\nIt's a very hard thing to do, because mostly, certainly if you set up a business or if you're if you're off that sort of entrepreneurial mindset, you like to be stuck into everything.\nAnd and you're successful in part because you believe you're actually really good at everything.\nAnd that, you know, that confidence, that belief sometimes drives results, too.\nSo it's I think having having good advisors in the background that are that you kind of you're checking in with, not not within the company necessarily, but maybe your own mentors, that they're good people sort of to help hold you to account, you know, to check in with regularly and go, well, this is where we're at now.\nAnd I'm thinking about this and that and then take take their advice.\nIf you ask people's advice, listen to it.\nYeah, I think that piece of advice about knowing your limits and knowing when to step back, it's not just true for startup founders, but probably everybody in any position.\n\n\nEoin: So I think a lot of people are going to get benefit from that.\nBut you're known very well for advising startups about hiring, running the company, financing and funding.\nA lot of our listeners will be involved in startups or might even be thinking of taking that leap.\nSo what advice would you offer them apart from the stuff we've already mentioned about, you know, growth, etc?\n\n\nFiona: Well, do it.\nFirst of all, if you have this voice in your head that's telling you you need to do it, you should do it.\nI don't look back in 40 years time.\nAnd so there was that window when I really had this great idea and I didn't do it for all the reasons.\nYou can make the list of reasons on both sides.\nBut if it's really if it's inside you, you should follow your dream.\nYou should really do it.\nThen you have to get sensible.\n\n\nOK, so that's the that's the emotional part.\nYou've made the decision.\nYou're going to do it.\nIt's always about cash, right?\nIt always starts about cash.\nIt's about cash.\nIf you're raising money, you need to make sure you get the right money.\nSo if you're taking it in from investors, it can be so tempting when you get these term sheets put in front of you.\nSomebody believes in your dream to go.\nI'm just going to sign this the first offer and I'm going to take it.\n\n\nMake sure that that you're taking money from the right partner, that somebody that is investing truly in your business plan and what you want to do with the business.\nAnd isn't just seeing an opportunity for a quick return or won't change the goalposts if things get hard, as they inevitably do.\nI've been in so many startups.\nNothing has ever gone to plan.\nSomething has always come out of the woodwork to change you or shock you and catch you along the way.\n\n\nSo getting the right partner and the right money is sort of the first part of it.\nAnd then minding that money.\nYou know, again, it's you know, there's something around.\nI think it's revenues, vanity, profit is sanity and cash is king.\nYou know, I work it, always work it back from cash.\nIf you're stressed about cash, if you're stressed about making payroll, you've got people that have left jobs to come and work with you.\n\n\nAnd now it's coming to the end of the month and you don't have enough cash in the bank to pay the wages.\nYou will not be able to run your business effectively.\nYou will make the wrong decisions.\nYou you'll become revenue focused to get cash in the bank first thing and first out of the way.\nLike it distracts from everything.\nSo get your nest egg.\nThree months is kind of my rule of thumb.\nThree months runway, three months of paying the rent in the building and all of the wages.\n\n\nSo that cash flow isn't isn't making you make bad decisions.\nSo mind your cash.\nNever ignore a letter with a harp on it.\nSo get all your other stuff done right, too.\nIf you can afford a bookkeeper, an accountant, hire one day one.\nIf you can't and you're doing it yourself, you know, do it properly.\nPay your VAT, pay your PRSI.\nYou know, these things I've seen so many people to get tempted.\nWe'll pay this. We'll pay the VAT man later.\n\n\nRevenue don't do interest free loans.\nThey don't care. You know, they say they care, but they don't care.\nThey want their money.\nSo, you know, pay pay all your bills.\nKeep on top of things.\nIf you're not going to be able to pay things, get out in front of it as early as you possibly can and take the cash distraction off the table.\nAnd then the other big piece of advice is it's much easier to co-found with others.\n\n\nYou know, it's a lonely journey if you're doing it on your own.\nAnd if you can't find other people to come on that journey with you, that's a much you're minimizing the risk.\nYou're giving yourself confidants, people to talk to, to share the whole, all of the hard decisions to sense check things with.\nBut when you're doing that, too, get the paperwork out of the way to begin with.\nGet your shareholders agreement.\n\n\nI call it like the prenup, you know, figure out how how will you start company if you hate each other's guts.\nWrite all that down and then hopefully put it in a folder somewhere and it never has to be looked at again.\nBut at least you then have no matter how much you love each other right now.\nExactly. Exactly.\nAnd it's a hard thing to do because you think like we're all we're all on the same page and I never I'd never mess you over.\n\n\nI'd never do this. But then life gets in the way and things change.\nAnd maybe the direction of the business changes and one person isn't happy that you're doing this and you're not doing that.\nAnd if you don't have that, if you didn't, if you didn't sit down and work all this stuff out in the beginning, it can be really, really damaging.\nIt can be damaging to you as an individual, the stress it brings on.\n\n\nIt can be damaging to the business, to the brand that you've all worked so hard to build.\nSo get your prenup, sign it and then, as I said, hopefully put it in the drawer and never have to look at it again.\nAnd I think the last key piece of advice is, and I referred to this earlier, too, is find yourself some mentors.\nWhat I've always been amazed by is how willing people are, particularly Irish people, to give you advice.\n\n\nIt's a big it's a big compliment to be asked to be a mentor to somebody.\nAnd most people take that compliment very seriously and they take the ask and they commit to it in a way that is, you know, so generous and so overwhelming.\nAnd then you get to pay it back in future, too.\nYou know, and that's why I try not to say no if people ask me for help, because when I was asking others for help, they were always there for it.\nAnd if you have those those mentors, again, because they're not in the day in the day to day fray and the decisions that kind of wear you down sometimes, they can help bring you back to your core values, what your mission is, you know, help you sort of do a reset so that you bring your best self into the business.\n\n\nEoin: Great advice.\nAnd maybe we should clarify for our international audience, a letter with a harp on it.\nIt's not a letter from Guinness, right?\nIt's from the government!\n\n\nFiona: Yeah, for Americans,\nit's the IRS.\n\n\nEoin: OK, so staying on the topic of money, then let's cover cloud economics, because look, when you mention cloud and money, the whole business model around cloud can't be far off.\nAnd you've worked in tech before and after the advent of cloud computing as have I, and a lot of people at Fortheirum.\nNow, this usually means shifting from upfront investment to pay-per-use pricing.\nAnd we often see it as a big win, especially for startups who might be low on capital in the beginning.\nIt seems like a fair deal.\nIs that benefit recognized widely, do you think, by CFOs, other financial leaders in the broader tech industry and in other industries?\n\n\nFiona: Sadly, no.\nI don't think so.\nYou know, it's really interesting.\nWe've had this conversation a lot internally, and when we're doing these sales proposals, we think about, you know, this is great.\nYou know, it's all going to be OPEX now.\nBut in businesses, particularly in bigger businesses, their financial modeling tends to be in two parts.\nYou've got your OPEX plan for next year, and then you've got your capital plan, that might be multi-year.\n\n\nAnd it can often be a lot easier, right, especially in the bigger organizations, to get that capital plan over the line.\nBecause the very nature of us as accountants is, you know, we like to see assets.\nWe like to put, you've been there, I'm sure, Eoin, you're putting the sticky label on each individual piece of kit.\nYou can bring people in and bring them down, show them your server room with all these lights flashing and multi-million investment.\n\n\nEverybody can really get their arms around it and feel comfortable.\nAnd you've got that predictability.\nI'm writing that cost over five years.\nThat depreciation figure is never going to change.\nI know what my tax-free allowances are going to be like on it.\nFrom a point of view of certainty, it's the old world, the on-prem world, was a much more certain environment for the CFOs.\nAnd because we tended to be less involved with really understanding how tech works,\nwe never really considered too much about that wasted capacity, you know.\n\n\nYou spent to the most bursty, busiest day of the year, you know, be it Black Friday or whatever it happened to be,\nyou had enough compute power for that.\nYou never really thought about how much that was wasted then for the other 365, four days of the year.\nIt just didn't really come into the frame of reference.\nSo I think the whole benefits of cloud from a cost point of view are not really fully understood yet by the wider accountancy profession.\n\n\nAnd I think there's a long way to go on that.\nAnd that's only to get them to understand lifting and shifting, right?\nYou know, from my data center to another data center.\nAn understanding about modern architecture and truly software that, you know,\nyou're only paying to be used when you need to use it.\nYou know, those principles aren't fully understood.\nAnd I think as a profession, we're not very good at trying to make ourselves, inform ourselves,\nbe better educated on how cloud comes.\nYou know, in previous businesses, there was always that kind of chasm between the engineering people,\nbe it, you know, physical kit engineering or software engineers and the accountants on the other side.\nAnd I've spent my career trying to kind of bridge that gap in any organization I've worked in.\nAnd there's still a lot more bridges to be built.\n\n\nEoin: Maybe kind of a related question is around migration, right?\nBecause cost is obviously a big component of that.\nBut you've seen kind of firsthand what it looks like when we work with clients migrating to AWS.\nAnd this is a big step.\nThere's a lot of fear, I think, there.\nDo you have any advice from your experience for larger companies thinking about moving to the cloud?\nAgain, like so many of the rest of our conversations on, the common thread comes through again.\n\n\nFiona: It's start with your people.\nYou know, you really need to engage as many people in the organization as you possibly can.\nOnce this migration conversation, modernization, even if we call it, you know, it's become part of it.\nYou know, it's very easy to assume that your IT team, that they have all the skills necessary to make this happen.\nThey probably don't.\nIf they've been, you know, armed deep in keeping your monolith alive for the last 20 years,\nit's not leaving them a huge amount of time to educate themselves on cloud technologies.\n\n\nAnd they are not the same thing.\nYou know, it's not as simple as if I've got a software engineer that that same software engineer can build this big pathway to migrate me off that monolith into this microservices environment.\nThere's a massive learning curve there, and it can be terrifying.\nYou know, and you load on top of that the fact that for the last 12 months, every time you opened anything,\nall you were reading about was Gen AI coming to take over everybody's jobs.\n\n\nSo now you've got people maybe using old tech, wondering, is the management going to, you know, bring in some Gen AI solution that's going to walk them out of their jobs?\nSo you've got all of those uncertainties going there, too.\nSo I think the more consultation is involved, the more you get the whole team looking at this and then identify this.\nThis is, you know, you're going to eat this elephant one bite at a time.\n\n\nFind the easiest bite to take.\nThe first, most, you know, most easily digestible broken down piece that you can migrate so that people can see the value.\nYou know, do that small project.\nInvolve as many people looking at the results of this as possibly can.\nSo you build up their confidence and pick the right partner.\nOf course, I'm going to sit here and say pick the right partner, but it really does make sense, whether it's fourTheorem or some other partner.\n\n\nFind somebody who does this as their bread and butter.\nThese are not trivial things to do.\nThey're hard projects.\nBut when you talk to somebody like ourselves that we've been doing this now in this particular environment for seven or eight years, you know, we see a lot of repeat problems.\nThe industries might change, but the gnarly part stays very similar across the individual things.\nWe can help them identify those quick wins.\n\n\nAnd then also we can go in there and work with the team and do that valuable upskilling, leaving them at the end, of course, with still with our jobs intact, but much more interesting jobs now.\nYou know, you remember that project where we deleted 70 percent of their code base.\nYou know, when you're not maintaining when you're maintaining a code base, it's only 30 percent of its original size.\nThat leaves you a lot of time to help innovate.\n\n\nTo be more impacting on the business and to have a more interesting job.\nSo they'll retain their people.\nThey'll upskill their people and they'll be they'll be ready for the next disruptor that comes along.\nThe ones that put their head in the sand and go, do you know what?\nI bought that big on prem upgrade two years ago.\nI'll wait till it dies.\nI have another five or six years to run about it.\nThey're going to wake up one day and find that there's another revolution or something has come along that's completely going to eat their lunch and leave them in a panic mode.\nAnd that's not the way to do it.\n\n\nEoin: OK, since we're talking about AWS partnering, obviously, fourTheorem is known as a top AWS partner.\nThere's room for plenty more.\nThere's a lot of work out there to be done.\nSo a lot of people out there, especially our listeners, right, might be thinking of either freelancing or starting off their own cloud consultancy.\nWhat advice can we impart, specifically you, Fiona?\nDo you have any tips, tricks, warnings to share?\n\n\nFiona: Well, all the earlier warnings about, you know, setting up your business and cash flow, right, and investment.\nTake that as, you know, the first stuff to do.\nBut if you're thinking, let's imagine you're already out there, you know, and you think, oh, I'm going to become an AWS partner.\nThis will be great.\nAnd I give a bit of advice around that.\nAnd I think, you know, first and foremostly, and to be fair, AWS tell you this out the gate, you know, they're not going to fill your sales pipeline.\n\n\nWell, of course, you do think when you become a partner that they will fill your sales pipeline.\nAnd they do.\nThey have because you look, they have loads of these account management teams.\nBut they are never going to fully get you in the way that you get your own business.\nSo if you become a partner, understand that you have to, you have to, first of all, you've got to be responsible for your own sales process.\n\n\nSo becoming a partner from AWS or Azure or anybody else isn't going to solve your new business pipeline piece.\nYou have to solve that for yourself.\nOnce you accept that, then you can really lean in and you can leverage these partnerships to the best of their ability.\nBearing in mind that you will almost always be smaller than these huge, big organizations.\nSo there's going to be a level of bureaucracy.\n\n\nYou'll deal with some wonderful people, but the system and the process will be very frustrating.\nThey'll have rules, you know, AWS funding rules can be like looking at health care plans in Ireland where you're going, I don't know how to compare one with the next.\nYou can get very caught up with, you know, trying to keep up with it.\nAnd they do a great job of selling their programs.\nBut make sure that if you are going to attach yourself with a program, if you keep coming back to first principles and saying, is this the right thing for me?\n\n\nBecause they will require a lot of investment on your part in accreditations and upskilling your teams and getting pieces of paper.\nAnd if that's right for your business, do it for sure.\nBut make sure you've validated that up first.\nAnd you think, OK, these are very good skill sets.\nI want all my team to have them.\nDon't do them because it ticks a box in a program.\nDo them because they're right for you and make sure that throughout everything you do, that you keep coming back to your own customer obsession.\n\n\nYou know, AWS is customer obsessed, but you need to be even more focused on your own customer obsession.\nYou are not AWS or Azure or any of these other people.\nYou are fundamentally your own independent consultancy.\nAnd that's what your clients are relying on, right?\nIf they want to hear the sales pitch, they can go direct to the providers themselves.\nIf they want to hear the unvarnished truth and the warts and all version of it,\nthey're going to come to somebody like fourTheorem.\nMake sure you treasure that value and you respect that relationship between yourself and your customer.\nAnd that things don't become confused and you're not effectively just a sales agent for somebody else.\nMaintain your own integrity.\n\n\nEoin: So much great advice in there, Fiona.\nThanks a lot for joining us.\nFor me, it was great to shift away from the trivial technical topics towards the more important themes of people and finance.\nIt's always great to hear your insight.\nSo for people out there who would like to hear more, where should they find you?\nHow can they connect?\n\n\nFiona: Come find me on LinkedIn.\nI'd love to connect.\nWe'll chat.\n\n\nEoin: Okay.\nBrilliant.\nWell, listen, thanks again, Fiona.\nAnd thanks everyone for listening.\nWe're going to be back actually with more special guests in the near future,\nas well as some of our usual technical ramblings.\nIf you haven't already subscribed to the podcast or our YouTube channel,\nmake sure you do now so you'll get notified when new episodes are released.\nAnd let all your friends know that we're broadening out our subject matter\nand it's really well worthwhile subscribing.\nTake care and we'll see you in the next one.\n"
    },
    {
      "title": "134. Eliminate the IAM User",
      "url": "https://awsbites.com/134-eliminate-the-iam-user/",
      "publish_date": "2024-11-01T00:00:00.000Z",
      "abstract": "In this episode, we discuss why IAM users and long-lived credentials are\ndangerous and should be avoided. We share war stories of compromised credentials\nand overprivileged access. We then explore solutions like centralizing IAM\nusers, using tools like AWS Vault for temporary credentials, integrating with\nAWS SSO, and fully eliminating IAM users when possible.\n\nAWS Bites is sponsored by fourTheorem, an Advanced AWS partner that works\ncollaboratively with you and sets you up for long-term success on AWS. Find\nout more at fourtheorem.com.\n\nIn this episode, we mentioned the following resources:\n\nEpisode 118: &quot;The landing zone: Managing multiple AWS accounts&quot;\nEpisode 96: &quot;AWS Governance and Landing Zone with Control Tower, Org Formation, and Terraform&quot;\nDatadog Security Report (IAM stats)\nCredentials provider chain in the JavaScript SDK\nCredentials provider chain in the AWS CLI\nEpisode 45: &quot;What’s the magic of OIDC identity providers?&quot;\nEpisode 112: &quot;What is a Service Control Policy (SCP)?&quot;\nEpisode 115: &quot;What can you do with Permissions Boundaries?&quot;\n\n",
      "transcript": "Luciano: Hello, friends of AWS Bites podcast, the show where we share stories and hard-learned lessons\nfrom the cloud trenches.\nToday, we are here to sound the alarm on something very serious.\nYou need to eliminate IAM users from your AWS accounts and fast.\nSeriously, if you are still using them, it's like an incident just waiting to happen.\nIt's like handing your credit card details to a stranger online and hoping they will\nsurprise you with a gift.\n\n\nBut I'll give you a spoiler.\nYou won't like the gift that you will get.\nSo just be very careful.\nDon't do that.\nAnd we have talked already about governance and lending zone in the past.\nBut today, we want to focus specifically on why IAM users and long-lived credentials\nare one of the biggest foot guns that exists today in AWS.\nAnd hopefully, we'll also share some strategies on how you can start to get rid of them if\nyou are still using them.\n\n\nMy name is Luciano, and today I'm joined by Conor, our new co-host, who brings tons of\nexperience in managing AWS accounts.\nSo I'm really excited.\nLet's get into it.\nAWS Bites is brought to you by fourTheorem.\nIf you are looking for a partner to architect, develop, and modernize on AWS, give fourTheorem\na call.\nCheck us out at fourtheorem.com.\nSo as I said, we already covered similar topics.\nBut today, we have Conor, who brings lots of expertise and fresh perspective.\nSo maybe it's a good idea to start with just Conor introducing yourself so people can\nknow all the amazing things that you have done.\n\n\nConor: Sure.\nLong-time listener, first-time caller, I guess, to AWS Bites.\nSo I joined fourTheorem about 18 months ago.\nI'm a senior infrastructure engineer.\nAnd I guess I've worked with a lot of startups over my career.\nVery interested in the security space in AWS and infrastructure as code and all things safety\nand security in the cloud, I guess.\nSo I've had the privilege of joining a lot of excellent teams over the years.\nBut a lot of times, I may have been the first security nerd or AWS practitioner to join the\nteam.\nSo I have often encountered very mature AWS accounts with lots of IAM users.\nAnd what we want to chat about today, I guess, is there's a lot of modern techniques and tooling\nthat help us totally eliminate those IAM users, which have become a bit of a security\nnightmare or certainly a potential rake in the grass for a lot of organizations.\nSo yeah, that's me.\n\n\nLuciano: I can only add that you are one of the people I know with the most Terraform expertise.\nSo I'm really lucky to have you as a colleague every time I have a question about Terraform.\nSo that's just to send a little bit more context.\nBut yeah, going back to IAM users, I guess one main question that people might have is\nlike, what is really the problem, right?\nIt seems like something that has been in AWS forever.\n\n\nSo why should it be a problem in the first place, right?\nAnd we said it is one of the biggest foot guns that you have today.\nAnd the main reason to that is that when you use IAM users, generally, you are also using\nlong-lived credentials.\nSo you go into that specific user and maybe through the web UI and you generate credentials\nfor that user.\nAnd then God knows where you're going to copy-paste those credentials, store them somewhere forever,\nmaybe forget about them.\n\n\nAnd I don't know, they can end up in development machines.\nThey can end up in servers.\nThey can end up inside applications that somehow need to interact with AWS.\nAnd the problem with that is that they are generally clear text.\nSo easy to exploit or exfiltrate or copy-paste.\nAnd the other problem is that generally they are very broadly scoped.\nSo you can have permissions to do lots of things.\nEven if originally you didn't intend to do all these different things, maybe it was convenient\nto just give this particular user lots of open permissions.\n\n\nAnd then whoever has access to the credentials inherits all of these permissions.\nSo imagine, yeah, you might end up with somebody just spinning up the most expensive EC2 instance\njust because that particular user has permissions to do that.\nAnd the other problem is that those credentials get rarely rotated.\nThere are ways that you can rotate the credentials, but there's generally something that people\ndon't bother doing.\n\n\nAnd I personally have seen, it's funny that you can go to the web UI and see how long\nthose credentials existed.\nLike, it's not rare to see like these credentials existed for like 2000 days.\nAnd yeah, you can think of how many things could have gone wrong in so much time.\nLike how many people could have had opportunities to take these credentials and use them to do\nsomething.\nSo the other interesting use case that we have seen a lot is that if you create IAM users\nfor, I don't know, developers in your company, people will come and go in the company.\nAnd sometimes you don't have a very strict process for offboarding people from your AWS\naccounts.\nSo I know of people that were able to access accounts in AWS from not their first company,\nbut like two previous companies that they were working before, like years before.\nAnd still their credentials were totally valid and they could see everything in the account\nand do all kinds of actions that they could do as developers, even though they were not\nemployed anymore in that company.\nSo just be aware that these are just some of the risks.\nBut I don't know, Conor, if you have anything else you would like to add.\n\n\nConor: Yeah, I guess it is one of those difficult challenges.\nYou know, a lot of my history was in IT and onboarding and offboarding people from teams.\nSo you tend to bias towards single sign on and role based access control and credentials\nthat you can expire or at least reason about their status.\nAnd like you mentioned, IAM users are incredibly challenging to keep on top of, especially in larger\norganizations.\n\n\nSo just to add to your war stories there, like I've seen, I've seen it all, I guess, you\nknow, that the worst is when you find the root account that has an access key and secret\naccess key generated and it's being used in a pipeline.\nBut I've always seen, you know, front end engineer Bob has a large team and they're great\nand they're tenacious about getting work done.\nBut eventually, you know, Bob moves on and suddenly all of the other front end engineers\ndevelopment environments stop working.\n\n\nIt's typically because Bob has been really helpful and, you know, shared his IAM access\nkeys with the team, maybe check them into that repo for that app.\nSo very difficult to manage that as a, you know, a central cloud team or a practitioner that's\ntrying to keep on top of the AWS environment.\nSo I guess it's not usually somebody's fault.\nYou know, it's up to the platform team or the security team to try and put the correct\nguardrails in place so that it's easy to do the right thing and very difficult to do the\nwrong thing.\n\n\nAnd I guess later on in the episode, we're going to get into a lot of the modern techniques\nfor doing things the right way that just make it much more difficult to create these\nkind of edge cases in the first place.\nBut yeah, IAM users will end up on developer machines.\nThey'll end up in Dropbox, Google Drive, Slack messages.\nYou know, you'll have users that were intended for human access that end up in pipelines or\nproduction systems.\n\n\nSo I guess our goal today is to tell everybody that there is a better way and you can get\nto the point that you have no IAM users at all.\nDepending on your environment, that's going to be a simple or very arduous journey.\nBut I guess we want to help you to get there.\nSo I guess the reason you end up with these in the first place is because somebody wants to\ndo programmatic access to AWS, right?\nWhether it's from a pipeline that's running on an EC2 instance or ECS Fargate, or more\ncommonly, the development flow where you want to interact with S3 or CloudFormation or you\nwant to run CDK deploy from your laptop.\nYou need programmatic access to the AWS account.\nAll requests to AWS have to be signed with the SIG V4 algorithm.\nSo I think what we want to focus on initially is how does it work?\nIf I run AWS S3 LS from the CLI, what goes on under the hood there in the SDK for it to\ntry and find credentials and authenticate itself against AWS APIs?\nDo you want to chat about your experience with that, Luciano?\n\n\nLuciano: Yeah, that's a very interesting topic.\nI actually really like that kind of stuff.\nLike you mentioned, the SIG V4 algorithm.\nWe're not going to go into detail on that one today, but we'll post a link in the show\nnotes if you're curious.\nWhat we want to focus a bit more on is that there needs to be a process for if you're using\nthe SDK or the CLI to locate credentials in that execution environment.\nAnd there are specific pages that you can find in the AWS docs, depending if you're using a\nspecific SDK, like for instance, the JavaScript one, there is a page that details exactly all the\nsteps that are performed to try to find a viable set of credentials when you create a client\nusing the SDK.\n\n\nAnd there is a similar page for the CLI, so we'll make sure to share the links as well.\nBut just to summarize, what are the different steps?\nSo you can have an idea of all the different things that can provide credentials to a given\nenvironment.\nSo the first thing is that as you create a client, you are effectively instantiating a\nclass.\nLet's imagine in JavaScript, you say, I don't know, new S3 client, something like that.\n\n\nYou can provide options there as the constructor of that function call.\nAnd some of the options there are specifically put in place so that you can provide inline\ncredentials.\nSo if you do that, this is the first thing that the client is going to look for.\nAnd if you provide the credentials there, those credentials will be used.\nBut those credentials there are not mandatory.\nSo what happens if you don't provide them?\n\n\nAnd the next step is that if credentials are not there in the constructor options, the client\nis going to look for specific environment variables.\nAnd I'm sure you have seen the AWS secret key and that kind of environment variables.\nSo this is the next thing that the client is going to look for.\nSo if in the current execution environment, you have those variables set, those will be\nused for your client to interact with AWS.\n\n\nThen the next step is that if you don't have those environment variables, then it's going\nto look for shared credentials files.\nYou can imagine similar to when you configure your CLI that you might have credentials files that\nway.\nAnd then you can have in specific environments, for instance, if you're running your code in\nECS, ECS has mechanisms to provide credentials through, for instance, roles.\n\n\nYou can have short-lived credentials.\nSo if you have done all of that, configure correctly, the SDK can actually load these credentials.\nSo that would be another step.\nAnd we can keep going.\nThere are other steps.\nYou can use credential processes, which is just a mechanism where you can have custom ways\nto say, I can call a specific process.\nAnd that process is responsible for somehow fetching credentials that I can use.\n\n\nAnd this is generally when you want to integrate with, I don't know, third-party tools.\nMaybe you have some kind of vault or secret manager and you store configuration there.\nThat configuration can contain your AWS credentials and you can create that mechanism to load credentials\nfrom a custom place.\nAnd then finally, another example is if you're using EC2 as a concept of instance metadata.\nSo that's basically a mechanism where if you provide a role to an EC2 instance, that role\ncan have effectively permissions attached to it.\n\n\nAnd the way that your client inside that EC2 inherits those permissions is by accessing this\nmetadata server, let's call it, it's kind of a local server that if you call it through\nan HTTP endpoint, it's going to give you temporary credentials that are scoped specifically with\nthe role attached to the instance.\nAnd I guess some of these ideas are better than others because they will give you shorter\nterm credentials.\n\n\nBut because the most common or at least historically the most used option is just copy-paste some\ncredentials into the environment, people tend to do IAM users, copy-paste credentials and\nuse them even for programmatic access on a kind of server process or something that should\nhave been managed differently.\nSo that's, I guess, probably trying to shed some light on the fact that, yeah, there are\ndifferent ways to provide credentials and it's kind of a step-by-step.\nIf the first step fails, it's going to look for the second step and so on.\nSo it's important to also understand what is the priority of what the SDK or the CLI is\nlooking for.\nSo again, link to the links in the description if you're curious to find out exactly from the\ndocs what are the different steps.\nSo the question now could be, okay, I kind of get the point that I shouldn't use IAM users,\nbut if I am, what are some of the mitigation strategies that I can start to use today?\nYeah.\n\n\nConor: So I guess the community kind of arrived at a lot of interesting patterns.\nI was trying to find the article this morning, but it seems to be removed probably because\nthere's better strategies available now.\nBut I think it was Coinbase back in 2017.\nI had a great article on their engineering blog about the Bastion IAM account.\nSo that might be a more typical pattern for an SSH jump host or something.\n\n\nPeople would usually mention Bastion in that context, I guess.\nBut what Coinbase had established, and it was a pattern that I saw used a lot across the industry,\nwas they would elect an account.\nLet's call it the Bastion account.\nMaybe it was the management account of the organization.\nIt didn't really matter.\nAnd what you could do is you would create your concrete IAM users in that account.\nSo we'd create an account for Conor and an account for Luciano.\n\n\nStraight away, that was a big win, right?\nBecause you were at least centralizing the IAM user.\nAnd when you had access to maybe dozens of accounts, we still had one Conor and one Luciano\ninstead of dozens of each.\nAnd so the pattern that was established there was to try and enable role-based access control.\nSo typically then you'd create maybe an admin or a power user or a view-only role in each\nof the accounts that we operate in.\n\n\nAnd then what they would do is they would set strict conditions in the trust relationships\nof those roles.\nAnd that might say, the only person who can assume me is Luciano or Conor from this Bastion\naccount.\nAnd you could attach other STS conditions like a multi-factor token must be present and must\nbe valid.\nSo that was one of the ways that practitioners tried to create this kind of role-based access\ncontrol, centralize it through a single IAM user.\n\n\nAnd at least then we had our sane access pattern, easy to onboard users, easy revocation.\nSo that was a pattern that was kind of a battle-tested pattern.\nAnd then on the client side, I guess, or on your developer machine, you've still got that\nplain text long-lived credential.\nOkay, so how do we protect that?\nSo another excellent tool that people might recognize was by 99designs, and that was a\ntool called AWS Vault.\n\n\nAnd I guess the innovation there was they allowed you to use whatever keychain you had on your\nsystem, whether it was Linux or even the macOS keychain.\nIt would essentially escrow the access key and secret access key in the keychain.\nAnd then instead of directly exposing those credentials to the SDK or the CLI, like we just\nspoke about, it would either make some sort of STS get temporary credentials API call, or\nyou would perform the STS assume role operation to assume that concrete role in the target account,\nright?\n\n\nLet's say the admin role in my development account.\nAnd so now you had a scenario where your access keys were protected in the keychain.\nYou were only ever retrieving temporary credentials and exposing them to the runtime, whether it\nwas the SDK or the CLI, where a lot of these tools actually injected the environment variables\ninto the shell.\nMultiple reasons for that.\nOne reason is because all of the SDKs understand that, as we just covered.\n\n\nBut also it has one of the highest precedences in the credential chain.\nSo more often than not, that's the behavior you wanted when you were running locally.\nSo that was kind of the mitigation strategy that the community arrived at.\nIt was a very common pattern.\nAnd I guess I'm showing my bias here where I've always worked at smaller startups and organizations.\nObviously, at larger enterprises, you'd have your SAML identity provider, which would provide\nsimilar entry point to the system.\nYou would still assume roles in target accounts.\nSo that's one way it worked.\nAnd you can still operate that methodology.\nIt has a good few moving parts.\nYou need to really understand trust relationships and multi-account strategy.\nAnd you've got to have all your infrastructure's code tooling in place to make that a feasible\npattern.\nBut there are modern alternatives today.\nFinally, spoiler alert.\n\n\nLuciano: I'm really curious to learn what those alternatives looks like.\nBecause yeah, I'm sure that there are more scalable ways today to deal with, especially if you start\nto have multiple accounts, lots of potential users, different kinds of access levels that you need\nto ensure.\nAnd you might have lots of people coming and going in the organization as well.\nSo yes, what are those better ways to deal with user access in general and permissions?\n\n\nConor: Yeah, so I think there's been previous episodes on creating a landing zone and then some of the infrastructure's code tooling around that.\nBut a core component of all of those modern landing zone setups is AWS IAM Identity Center\nand more so its integration with AWS organizations.\nSo what the Identity Center service lets you do is essentially implement the pattern we spoke\nabout.\nSo Identity Center has the concept of permission sets and permission sets are almost like a\nwrapper for a role and then it can have targets.\n\n\nSo the target could be another AWS account or an organizational unit.\nBut essentially, we now have single sign-on.\nOkay, Identity Center lets us create an individual identity.\nYou can use Identity Center's built-in Identity Store.\nIt's a great way to start.\nIf you're a small organization, you may not necessarily have an IDP, Azure or AD.\nAnd if you just want a sane way to manage users into your AWS organization, you can use the\nbuilt-in Identity Store.\n\n\nSo you then have your kind of standard, you know, groups.\nYou could have a group for certain application teams, security group, platform team.\nAnd you can map the groups and permission sets to accounts.\nAnd then Identity Center and its integration with organizations does the rest of the heavy\nlifting for you.\nSo if you have 10 accounts or 200 accounts, the admin role will be created automatically\nby the permission set.\n\n\nAnd then the appropriate users will be able to access it.\nOkay.\nSo Identity Center really does all of the heavy lifting for you here as regards access to a\nlarge AWS estate.\nSo the other option you have, which is convenient then, is that you can integrate most identity\nproviders.\nOkay.\nSo at 4th Erem, we have Google Workspace integrated.\nYou can have manual user management, or you can even use something like Skim to get full\nautomated user just-in-time provisioning into the Identity Center.\n\n\nSo it's quite flexible and, you know, nice to be able to integrate it with an existing\nidentity provider.\nSo that then gives you a lovely, you know, SSO landing page that you might be familiar with,\nand it'll present all of the AWS accounts and all of the roles that you have access to.\nSo it solves the console access problem quite well.\nAnd then I guess, you know, the next step is getting your programmatic access.\n\n\nSo one thing that the Identity Center portal will let you do is grab temporary credentials\nin the context of any role that you have access to.\nSo it's like the Identity Center console has essentially performed the, you know, STS assume\nrole or assume role with web identity operation for you.\nAnd it's going to give you back an access key, a secret key, and a session token.\nAnd if you want, you can just paste that in a shell or, you know, configure it in an AWS\ncredentials file, and you have your programmatic access.\n\n\nNow, that is fine for, you know, maybe a tiny investigation or access to an account you don't\noften use.\nBut the tool that we recommend to a lot of customers and we use internally ourselves is\na tool called Granted by CommonFate.\nAnd the really nice thing about that tool is that you sign in, you perform the single sign-on\nflow, and it's able to enumerate all of the accounts and roles you have access to.\n\n\nIt's able to automatically generate the config file.\nAnd now you have this tool that you can run in your shell, and it can inject credentials\ninto the shell for any role in any account that you have access to.\nSo we find it, you know, a really great way to access accounts.\nYou're only ever getting back a temporary credential.\nAnd it just, in an environment where you have lots of accounts, which we often encourage,\nit just makes the, you know, the user experience really nice.\n\n\nSo what else did we want to chat about there?\nSo when you, if you do that, like this is what we think great looks like, I guess.\nIt's a lot easier in a greenfields environment to create this.\nBut we would then recommend, you know, creating an SCP policy that blocks the creation of IAM\nusers.\nThat's a common security best practice now, I guess, because once you've gotten to this\npoint, you don't want people to be able to go off piste.\nYou know, you don't want them to be able to do the bad thing.\nLike we said, we want to make the good thing easy.\nSo we can just block the creation of IAM users.\nNow you've got to use your single point of entry into the AWS organization to do what you\nwanted to do.\nYeah, that sounds easy.\n\n\nLuciano: Again, when you are starting from scratch and you have total freedom, so you can set up\nthings in a way that looks modern and good from scratch.\nBut I would imagine that if you already have, I'm just going to say quite a legacy setup,\nmaybe with lots of stuff running there, you have been using it for years.\nLike how do you start to enforce this kind of things?\nI'm sure that there are cases where you cannot just get read from one day to the next one of\nall IAM users, you need to, I don't know, probably take a few shortcuts or a few half\nbaked solutions that maybe are still better than nothing.\nBut yeah, you're probably incrementally going to get to a great place to be, but it's not\ngoing to happen from like one day to the next.\nSo do you have any recommendations for people that might be more in this camp where it's not\ngoing to be easy to change things, but they can still have opportunities to make things\nbetter?\n\n\nConor: Yeah, absolutely.\nAnd it is a thankless job sometimes, you know, to say, look, we've got\na glaring security issue here.\nWe want to pay it down.\nSo I think it's just important to have buy-in from the team and the organization, you know,\nget it on the Kanban board, get it on the backlog and start to approach it systematically\nbecause it is worth doing.\nYou're closing off one of the biggest attack vectors you have in a modern cloud environment,\nI guess.\n\n\nSo how do you start, right?\nThe two problems we really usually have to solve is like the human access problem, which\ntends to be simpler, and then the machine access problem.\nOkay, like we spoke about in the intro, you're probably going to have a smattering of IAM\nusers that represent humans.\nYou might have ones that represent machines.\nAnd if you're really unlucky, you're going to have the ones that were intended for humans\nthat are now being used on machines.\n\n\nSo how do we approach it?\nThe IAM credential report is super helpful.\nWe'll put a link to that in the show notes.\nYou can generate this using the CLI or from the IAM console.\nThat will give you a CSV export, which is a great place to start.\nYou know, you can sort it by access key age, or you can start to examine some of the policies\nthat are attached to the users.\nAnd you can decide to organize them by danger, which sometimes makes sense, right?\n\n\nNow, often that first pass will be a big win.\nYou're going to find people that have left the company months ago or years ago.\nYou're going to find access keys that haven't been used in some window.\nYou're going to find machine users that are for services you no longer operate.\nAnd like very quickly, you might be able to shed half of the IAM users.\nAll right.\nNow, I would also recommend implementing identity center permission sets on board yourself and\nmaybe other people on the platform team or the security team.\n\n\nStart to kick the tires on that and introduce it for a trusted group at the organization.\nAnd then you can start to reimplement your role-based access control in identity center, right?\nSo you now have a permission set that represents maybe what the DevOps IAM user does.\nThat's a typical thing.\nYou might find an overprivileged IAM user that was used by a certain group or a group of individuals.\nSo you can start to take little slices off of the problem like that, okay?\n\n\nYou'll often find then, you know, you could find an IAM user called S3 backup.\nIt's quite obvious what it does, but you might find that it has the Amazon S3 full access\nmanaged policy attached.\nNow, you know, we could replace that with an IAM role and just move on.\nBut we know deep within our soul, the correct thing to do here is to also evaluate that workload\nand look at what access it actually needs.\n\n\nSo you might be able to talk to the team responsible.\nYou might be able to use IAM access analyzer, and you'll be able to create a really fine grain\npermission for that role, right?\nIt probably only accesses one bucket.\nIt might only access a soap key within the bucket, and you can give it a very strict,\nyou know, put object policy.\nAnd with every step you encounter, you're incrementally improving your posture, okay?\n\n\nSo the scream test is going to be your friend.\nYou're going to have a lot of rinse and repeat on this process.\nBut I have done it.\nIt is painful.\nBut it can be done.\nAnd it's definitely one of the best things you can do for your organization from a security\nstandpoint.\nThe other aspect then is, well, what if I have a different cloud?\nAnd what I mean by that usually is you could have CircleCI, GitHub Actions, HashiCorp Cloud.\n\n\nOften, these IAM users are created because you need access from a system that is not AWS,\nwhere we can't rely on assuming a role or something like that.\nSo that was a very common source of IAM users, you know, some sort of pipeline user.\nThankfully, in the last couple of years, OIDC identity providers have solved a lot of this.\nI believe there's previous episodes where they're covered, but stuff like GitHub Actions will\nnow give you, you know, fine-grained access to a role to a particular repo or even to a\nparticular operation in a repo, like the repo being tagged or a release being created or a\ncertain branch name being pushed to.\n\n\nSo you can get extremely fine-grained access from GitHub Actions into AWS now as well.\nSo that's kind of the tried and true method to replace those cloud-to-cloud IAM users.\nAnd I guess finally, you know, we can strive for perfection, but we're probably going to\nhave a handful of IAM users when we're finished.\nCould be some legacy system.\nThere could be a user with a customer or on a server somewhere in the wild that we don't\ncontrol anymore.\n\n\nUnfortunately, you might end up with a handful of IAM users.\nWhat we recommend there is the SCP to block creation of new ones so we can at least plug\nthe leak.\nAnd then we can document those users.\nWe could import them into Terraform or CloudFormation so that we have them in infrastructure as code.\nIt's easy to kind of ring fence the problem.\nWe could introduce rotation, again, if we have access to the system where they're used.\nAnd you could even implement, you know, event bridge notifications, AWS Config.\nYou can just really scrutinize those IAM users and make sure we know whenever they do something\ninteresting.\nSo I think that's all I have on the topic.\nIt's a challenge, but I would say it's definitely worth doing.\nIt's a game changer for your security posture, particularly in AWS environments that have\nbeen around for a couple of years.\nYeah.\n\n\nLuciano: Thank you for all these amazing suggestions.\nI especially like the last one about importing the users, making sure that they're well documented,\ncreating SCPs to avoid new users.\nAnd yeah.\nRotating credentials is something that most often, at least, is done manually.\nSo probably you want to have some kind of playbook where you kind of document very well\nall the steps.\nAt least you can do it in a repeatable way and have a process that kind of reminds yourself\nto do it often enough.\nSo with that, I think it's a wrap for today.\nI hope that we have convinced you that IAM users are not a good practice.\nI hope that we have given you enough suggestions and motivation to get started on getting rid of\nthem.\nAnd hopefully you enjoyed this episode.\nSo if you've done that, please give us feedback.\nAs always, thumbs up, share, like, subscribe, and all the usual things.\nAnd I want to conclude with a big thank you to Conor for bringing us a fresh perspective\nand to be our new host for this episode.\nSo until next time, thank you very much.\nAnd we'll see you in the next episode.\n"
    }
  ]
}