WEBVTT

1
00:00:00.000 --> 00:00:02.800
The hype around generative AI is dying down now,

2
00:00:02.800 --> 00:00:05.800
but we are beginning to see a growing ecosystem

3
00:00:05.800 --> 00:00:08.720
and lots of real-world use cases for the technology.

4
00:00:08.720 --> 00:00:10.880
We've just built some features using Amazon Bedrock

5
00:00:10.880 --> 00:00:12.640
that help us to produce this podcast

6
00:00:12.640 --> 00:00:14.400
and save some manual effort.

7
00:00:14.400 --> 00:00:15.680
So today we're going to talk about

8
00:00:15.680 --> 00:00:18.000
how we automate the process with Bedrock,

9
00:00:18.000 --> 00:00:20.600
and we'll give you an overview of Bedrock's features.

10
00:00:20.600 --> 00:00:22.000
We'll also talk about how to use it

11
00:00:22.000 --> 00:00:25.800
and share some tips and code on cost monitoring.

12
00:00:25.800 --> 00:00:27.720
My name is Eoin, I'm here with Luciano,

13
00:00:27.880 --> 00:00:31.000
and this is the latest episode of the AWS Bites podcast.

14
00:00:39.000 --> 00:00:41.640
This episode is sponsored by fourTheorem.

15
00:00:41.640 --> 00:00:43.360
If you're looking for somebody to accompany you

16
00:00:43.360 --> 00:00:46.720
on your cloud journey and need an AWS advanced partner,

17
00:00:46.720 --> 00:00:48.680
check us out at fourtheorem.com.

18
00:00:49.920 --> 00:00:52.360
Now, before we get started and talk about Bedrock

19
00:00:52.360 --> 00:00:55.560
and how we use it, let's talk about the manual process

20
00:00:55.600 --> 00:00:58.560
or semi-manual process we did before Luciano.

21
00:00:58.560 --> 00:01:01.280
What was the drudge work we wanted to remove?

22
00:01:01.280 --> 00:01:05.040
Yes, so we are talking about the work that is involved

23
00:01:05.040 --> 00:01:07.240
when you create, in this case, a podcast,

24
00:01:07.240 --> 00:01:09.200
but I think it applies to any video

25
00:01:09.200 --> 00:01:10.960
that you want to publish on YouTube.

26
00:01:10.960 --> 00:01:13.640
So you create this video and when you upload it,

27
00:01:13.640 --> 00:01:15.920
you need to provide a bunch of additional information,

28
00:01:15.920 --> 00:01:18.760
effectively metadata that makes your content

29
00:01:18.760 --> 00:01:21.000
more discoverable and provides all the necessary

30
00:01:21.000 --> 00:01:23.440
context information to your viewers,

31
00:01:23.440 --> 00:01:26.840
but also YouTube to make the content searchable.

32
00:01:26.840 --> 00:01:30.120
So we are talking, of course, about descriptions, tags,

33
00:01:30.120 --> 00:01:33.880
but also other things like the chapters that you can add.

34
00:01:33.880 --> 00:01:35.640
You probably have seen these in our videos

35
00:01:35.640 --> 00:01:39.200
or in other YouTube channels where you can add

36
00:01:39.200 --> 00:01:41.840
a specific type of text that says this section,

37
00:01:41.840 --> 00:01:44.960
starting at this point, is about, I don't know, introduction.

38
00:01:44.960 --> 00:01:48.480
This other section is about why do we need serverless?

39
00:01:48.480 --> 00:01:51.000
And what happens is that YouTube is gonna use

40
00:01:51.000 --> 00:01:53.000
that text information in the description

41
00:01:53.440 --> 00:01:55.280
to split your video into chapters

42
00:01:55.280 --> 00:01:58.040
and you will have those chapters as an easy way

43
00:01:58.040 --> 00:02:00.360
of jumping through different parts of the video.

44
00:02:00.360 --> 00:02:02.200
So all of this information is something

45
00:02:02.200 --> 00:02:04.240
we take the time to create for every single video

46
00:02:04.240 --> 00:02:06.720
we upload because I think it makes them more discoverable

47
00:02:06.720 --> 00:02:09.480
and it gives our viewer a better experience,

48
00:02:09.480 --> 00:02:11.360
but of course there's a lot of extra work

49
00:02:11.360 --> 00:02:13.440
that we have to do for every single episode.

50
00:02:13.440 --> 00:02:15.440
And the process that we have been doing so far

51
00:02:15.440 --> 00:02:19.040
before the feature that we are gonna be talking about today

52
00:02:19.040 --> 00:02:20.360
more or less looks like this.

53
00:02:20.400 --> 00:02:23.560
So we have already some automation,

54
00:02:23.560 --> 00:02:24.720
which is called PodWhisperer.

55
00:02:24.720 --> 00:02:26.480
We talked about that before and we'll link

56
00:02:26.480 --> 00:02:28.760
to the previous episode talking about that.

57
00:02:28.760 --> 00:02:31.400
And PodWhisperer, in short, what it does is able

58
00:02:31.400 --> 00:02:34.720
to extract a transcript of everything we said

59
00:02:34.720 --> 00:02:36.360
during a specific episode.

60
00:02:36.360 --> 00:02:38.400
And that information is something we can use

61
00:02:38.400 --> 00:02:39.360
in different ways.

62
00:02:39.360 --> 00:02:44.000
So all of that stuff is done through a Step Function.

63
00:02:44.000 --> 00:02:46.160
And again, you can check out that video.

64
00:02:46.160 --> 00:02:48.000
We'll give you some other details later.

65
00:02:48.000 --> 00:02:50.800
But other than that, the other piece of information we have

66
00:02:50.800 --> 00:02:53.440
is our notes that we keep in Notion.

67
00:02:53.440 --> 00:02:56.040
So we have already some kind of groundwork

68
00:02:56.040 --> 00:03:00.440
that we can use as every time we want to create a title

69
00:03:00.440 --> 00:03:01.880
or a description or tags,

70
00:03:01.880 --> 00:03:03.560
we can look at all of this information

71
00:03:03.560 --> 00:03:05.880
without having to watch the entire video again

72
00:03:05.880 --> 00:03:07.720
and decide, okay, what kind of title do we need?

73
00:03:07.720 --> 00:03:10.120
What kind of description and so on.

74
00:03:10.120 --> 00:03:13.680
I admit that sometimes we use ChatGPT just to summarize,

75
00:03:13.680 --> 00:03:16.280
for instance, all our notes and give us maybe a draft

76
00:03:16.280 --> 00:03:18.720
of the description or some ideas for titles.

77
00:03:18.720 --> 00:03:20.400
But then there is still a lot of manual work

78
00:03:20.400 --> 00:03:22.760
in trying to adjust that result

79
00:03:22.760 --> 00:03:25.600
until we are happy with the final outcome.

80
00:03:25.600 --> 00:03:28.440
And when it comes to creating YouTube chapters,

81
00:03:28.440 --> 00:03:30.640
it is a very manual process.

82
00:03:30.640 --> 00:03:34.160
What I used to do so far is I just watch the final version

83
00:03:34.160 --> 00:03:35.800
of the episode at 2X,

84
00:03:35.800 --> 00:03:38.280
and then I keep track of all the points where I see,

85
00:03:38.280 --> 00:03:39.640
okay, now we are changing topic,

86
00:03:39.640 --> 00:03:42.040
and this is probably worked a dedicated chapter.

87
00:03:42.040 --> 00:03:44.840
So I take track of the timestamp

88
00:03:44.840 --> 00:03:47.520
and I create the format that YouTube expects.

89
00:03:47.520 --> 00:03:48.960
So we do all of this stuff,

90
00:03:48.960 --> 00:03:51.040
and then when we upload the video on YouTube,

91
00:03:51.040 --> 00:03:54.400
we need to copy paste all of that information correctly.

92
00:03:54.400 --> 00:03:57.880
And some of this work, I think it's nice to do it manually

93
00:03:57.880 --> 00:03:59.400
because it's good that you can add

94
00:03:59.400 --> 00:04:01.040
a bit of a personal touch.

95
00:04:01.040 --> 00:04:02.840
And you might have seen that we are taking

96
00:04:02.840 --> 00:04:04.920
a bit of creative license when it comes.

97
00:04:04.920 --> 00:04:06.520
I don't know, sometimes we take a style

98
00:04:06.520 --> 00:04:09.040
that looks like Tesla, the scientist.

99
00:04:09.040 --> 00:04:11.000
Sometimes we think about, I don't know,

100
00:04:11.000 --> 00:04:12.600
let's give it a medieval touch

101
00:04:12.640 --> 00:04:14.920
because maybe we are using some medieval artwork.

102
00:04:14.920 --> 00:04:16.920
So it is nice that you can take the kind of freedom

103
00:04:16.920 --> 00:04:18.600
and make it a little bit more original.

104
00:04:18.600 --> 00:04:19.800
But at the same time,

105
00:04:19.800 --> 00:04:22.160
we thought that using some generative AI

106
00:04:22.160 --> 00:04:24.960
can help us to do all of that work faster

107
00:04:24.960 --> 00:04:26.760
and with less manual work.

108
00:04:26.760 --> 00:04:28.480
When Bedrock came out recently,

109
00:04:28.480 --> 00:04:30.240
this kind of was the inspiration.

110
00:04:30.240 --> 00:04:33.160
And especially, I'm gonna link some tutorials

111
00:04:33.160 --> 00:04:36.720
in the show notes because one of the notable things

112
00:04:36.720 --> 00:04:39.200
about Bedrock is that when Amazon announced it,

113
00:04:39.200 --> 00:04:42.400
they created lots of really good tutorials

114
00:04:43.040 --> 00:04:44.120
and workshops and example repos,

115
00:04:44.120 --> 00:04:47.120
some of which are like really, really impressive content.

116
00:04:47.120 --> 00:04:50.480
I don't think I've ever seen that for a new service before.

117
00:04:50.480 --> 00:04:52.200
But let's first talk about Bedrock.

118
00:04:52.200 --> 00:04:54.720
So Bedrock is Amazon's new service

119
00:04:54.720 --> 00:04:56.920
for building generative AI applications.

120
00:04:56.920 --> 00:04:59.800
It's quite a bit different to SageMaker

121
00:04:59.800 --> 00:05:02.600
in terms of experience where SageMaker

122
00:05:02.600 --> 00:05:04.960
is designed to remove some heavy lifting,

123
00:05:04.960 --> 00:05:07.200
but you still have to understand about containers

124
00:05:07.200 --> 00:05:11.240
and models and the model libraries like PyTorch, et cetera.

125
00:05:11.240 --> 00:05:14.520
With Bedrock, it's a lot more managed high level service.

126
00:05:14.520 --> 00:05:17.280
And the idea of Bedrock is that it gives you access

127
00:05:17.280 --> 00:05:20.040
to third party foundation models

128
00:05:20.040 --> 00:05:23.040
from lots of different companies through a single API.

129
00:05:23.040 --> 00:05:25.560
And you just use one API to get a response

130
00:05:25.560 --> 00:05:28.080
to an instruction or a prompt.

131
00:05:28.080 --> 00:05:31.240
And the models that are available on Bedrock right now

132
00:05:31.240 --> 00:05:34.400
are from Anthropic, the Claude large language model.

133
00:05:34.400 --> 00:05:37.240
So that's like a good one with a focus on safety,

134
00:05:37.840 --> 00:05:42.240
safety and security and non-toxic data

135
00:05:42.240 --> 00:05:46.200
and safe data from reliable sources is their focus there.

136
00:05:46.200 --> 00:05:48.280
And then you've got Cohere command model

137
00:05:48.280 --> 00:05:51.440
for cases like customer support and business scenarios.

138
00:05:51.440 --> 00:05:55.520
You have the AI21 Jurassic large language model,

139
00:05:55.520 --> 00:05:58.400
and then you have Amazon's own Titan models,

140
00:05:58.400 --> 00:06:01.440
which are like general purpose large language models.

141
00:06:01.440 --> 00:06:04.960
And those ones are really aiming to be the lower cost ones

142
00:06:04.960 --> 00:06:07.320
where you're just really trying to cost optimize.

143
00:06:07.320 --> 00:06:08.440
They're not fully available yet.

144
00:06:08.440 --> 00:06:11.000
Not all of the models are available yet.

145
00:06:11.000 --> 00:06:13.760
And then if you're doing image generation,

146
00:06:13.760 --> 00:06:16.880
you have the stable diffusion models available there as well.

147
00:06:17.920 --> 00:06:19.920
There are also other models planned

148
00:06:19.920 --> 00:06:22.280
like the Facebook/Meta Llama 2 model

149
00:06:22.280 --> 00:06:24.280
is also supposed to be coming soon

150
00:06:24.280 --> 00:06:26.920
and a lot more expected to arrive.

151
00:06:26.920 --> 00:06:29.880
So if you want to use a model that isn't on Bedrock,

152
00:06:29.880 --> 00:06:32.640
but is available elsewhere, like on Hugging Face,

153
00:06:32.640 --> 00:06:34.560
you would need to really host the model somewhere else

154
00:06:35.040 --> 00:06:38.080
like on SageMaker in the more traditional way.

155
00:06:38.080 --> 00:06:40.440
But going back to the ones that are available on Bedrock,

156
00:06:40.440 --> 00:06:43.400
then if you're just trying to start to build practical,

157
00:06:43.400 --> 00:06:45.960
like chat features or text summarization,

158
00:06:45.960 --> 00:06:49.320
image generation features, build it into your application,

159
00:06:49.320 --> 00:06:51.120
I would just say to people

160
00:06:51.120 --> 00:06:53.320
that I think it's a lot easier than you would expect.

161
00:06:53.320 --> 00:06:54.960
And there's very little work you have to do.

162
00:06:54.960 --> 00:06:57.560
It ultimately depends on your use case.

163
00:06:57.560 --> 00:07:00.600
And if you need to pull in additional data,

164
00:07:00.600 --> 00:07:02.000
but generally for the kind of use case

165
00:07:02.000 --> 00:07:03.320
we're describing here,

166
00:07:03.320 --> 00:07:05.440
it's really quite a simple addition.

167
00:07:05.440 --> 00:07:08.080
So it allows you to quickly use these models

168
00:07:08.080 --> 00:07:10.760
and then add things like chat applications,

169
00:07:10.760 --> 00:07:13.600
text summarization, knowledge search

170
00:07:13.600 --> 00:07:17.160
based on additional data you might have in documents,

171
00:07:17.160 --> 00:07:19.000
doing text to image creation

172
00:07:19.000 --> 00:07:21.200
or image to image creation as well.

173
00:07:21.200 --> 00:07:22.720
Now there's a very small bit of setup,

174
00:07:22.720 --> 00:07:25.800
which is that because you're using these third party models

175
00:07:25.800 --> 00:07:27.320
before you go and use them,

176
00:07:27.320 --> 00:07:28.880
you have to go into the console

177
00:07:28.880 --> 00:07:31.000
and go into Bedrock model settings

178
00:07:31.040 --> 00:07:34.120
and explicitly enable each model

179
00:07:34.120 --> 00:07:37.040
and agree to their end user license agreement.

180
00:07:37.040 --> 00:07:39.040
So I guess there's because of the nature

181
00:07:39.040 --> 00:07:40.640
of these applications,

182
00:07:40.640 --> 00:07:42.280
the fact that they're non-deterministic

183
00:07:42.280 --> 00:07:44.560
when you're using models and there's impact there,

184
00:07:44.560 --> 00:07:48.200
you just have to get the legal sign off on those pieces.

185
00:07:48.200 --> 00:07:50.320
Once you've done that, it's basically serverless.

186
00:07:50.320 --> 00:07:52.840
So you can start making requests without having to wait

187
00:07:52.840 --> 00:07:54.680
for any infrastructure to be set up.

188
00:07:55.880 --> 00:07:58.280
Now, if you're comparing Bedrock to other things

189
00:07:58.320 --> 00:08:01.880
like maybe OpenAI APIs, for example,

190
00:08:01.880 --> 00:08:03.600
the idea here is that with Bedrock,

191
00:08:03.600 --> 00:08:06.600
there's more of a focus on privacy and security.

192
00:08:06.600 --> 00:08:09.320
So the big difference compared to other alternatives

193
00:08:09.320 --> 00:08:11.240
is that your data is encrypted.

194
00:08:11.240 --> 00:08:13.400
It's not shared with other model providers.

195
00:08:14.440 --> 00:08:17.840
You can keep your data and traffic within your VPC as well

196
00:08:17.840 --> 00:08:18.720
using PrivateLink.

197
00:08:18.720 --> 00:08:22.560
So you've got encryption and your data isn't gonna be used

198
00:08:22.560 --> 00:08:24.360
to train the models further

199
00:08:24.360 --> 00:08:26.920
and that's part of the agreement that you get.

200
00:08:26.960 --> 00:08:29.000
So you can use those models all as is,

201
00:08:29.000 --> 00:08:31.280
but there's also a whole other set of APIs there

202
00:08:31.280 --> 00:08:33.400
for fine tuning them if you need to

203
00:08:33.400 --> 00:08:35.680
using your own training data on S3

204
00:08:35.680 --> 00:08:36.760
and simplifying that as well.

205
00:08:36.760 --> 00:08:40.560
But we're just talking really about using foundation models

206
00:08:40.560 --> 00:08:43.280
for inference, just for getting a response back

207
00:08:43.280 --> 00:08:47.200
and without any special training or any kind of fine tuning.

208
00:08:47.200 --> 00:08:48.240
So for our use case,

209
00:08:48.240 --> 00:08:52.480
we decided to use the Anthropic Claude V2 model

210
00:08:52.480 --> 00:08:55.920
because it supports the largest input size by far.

211
00:08:55.920 --> 00:08:58.680
It supports up to 100,000 tokens,

212
00:08:58.680 --> 00:09:01.920
which usually equates to around 75,000 words.

213
00:09:01.920 --> 00:09:04.960
And we want to be able to give it a full episode transcript

214
00:09:04.960 --> 00:09:06.920
and our episodes can be anything

215
00:09:06.920 --> 00:09:11.720
based on a historical evidence between 2000 and 30,000 words.

216
00:09:11.720 --> 00:09:14.160
So that might be 40,000 tokens.

217
00:09:15.200 --> 00:09:16.280
So that's what we started with.

218
00:09:16.280 --> 00:09:19.760
Luciano, what's then the goal of the design of the system?

219
00:09:19.760 --> 00:09:20.760
What do we want it to do?

220
00:09:20.760 --> 00:09:23.600
We've got the problem, we've got the model.

221
00:09:23.600 --> 00:09:25.760
What was our thinking from that point?

222
00:09:26.440 --> 00:09:27.920
We already had a piece of automation.

223
00:09:27.920 --> 00:09:30.520
We already mentioned PodWhisperer a couple of times

224
00:09:30.520 --> 00:09:32.760
and PodWhisperer is effectively a Step Function.

225
00:09:32.760 --> 00:09:36.200
So it's a workflow that orchestrates different things.

226
00:09:36.200 --> 00:09:40.600
And eventually what it does, it creates a transcript for us.

227
00:09:40.600 --> 00:09:43.480
So the idea was, okay, we have already the Step Function.

228
00:09:43.480 --> 00:09:46.440
We have already a step that gives us something

229
00:09:46.440 --> 00:09:47.520
that we can use as an input.

230
00:09:47.520 --> 00:09:50.280
So what we need to do next is basically extending

231
00:09:50.280 --> 00:09:52.320
that Step Function, taking that input

232
00:09:52.320 --> 00:09:54.400
and basically do more stuff with it.

233
00:09:54.440 --> 00:09:57.880
So once we have the transcript, what we can do

234
00:09:57.880 --> 00:10:00.440
as the new steps that we introduce in this automation

235
00:10:00.440 --> 00:10:02.200
is basically we can create a prompt

236
00:10:02.200 --> 00:10:04.040
that will instruct the large language model

237
00:10:04.040 --> 00:10:06.200
to generate few different things.

238
00:10:06.200 --> 00:10:09.560
One is the episode summary, then the YouTube chapters

239
00:10:09.560 --> 00:10:11.680
with precise timing and topic

240
00:10:11.680 --> 00:10:14.160
and a set of tags for the episode.

241
00:10:14.160 --> 00:10:15.640
I think here is worth mentioning also

242
00:10:15.640 --> 00:10:16.960
that when we generate the transcript,

243
00:10:16.960 --> 00:10:20.800
it's not just the text, but we also keep time references

244
00:10:20.800 --> 00:10:22.280
to all the different bits and pieces.

245
00:10:22.400 --> 00:10:25.360
And this is how we are basically capable

246
00:10:25.360 --> 00:10:28.040
of doing chapters with precise timing,

247
00:10:28.040 --> 00:10:29.760
because we are giving the model

248
00:10:29.760 --> 00:10:31.080
not just the text information,

249
00:10:31.080 --> 00:10:33.760
but also the timing of every text occurrence.

250
00:10:33.760 --> 00:10:38.080
So it can determine exactly which piece of text is said

251
00:10:38.080 --> 00:10:39.880
at which specific point in time.

252
00:10:41.000 --> 00:10:44.800
So once we create this prompt that needs to kind of summarize

253
00:10:44.800 --> 00:10:46.280
all of this instruction in a way

254
00:10:46.280 --> 00:10:48.280
that the model can really understand

255
00:10:48.280 --> 00:10:50.040
and give us the output we expect,

256
00:10:50.040 --> 00:10:51.320
we need to pass the prompt.

257
00:10:51.360 --> 00:10:54.480
So we need to actually make an API call to Bedrock.

258
00:10:54.480 --> 00:10:57.320
And we also need to give the full episode transcript,

259
00:10:57.320 --> 00:10:59.280
of course, because that's part of the context

260
00:10:59.280 --> 00:11:00.760
that we need to give it.

261
00:11:00.760 --> 00:11:04.280
And after a while, when this request is processed,

262
00:11:04.280 --> 00:11:07.200
we receive a response, we need to pass this response,

263
00:11:07.200 --> 00:11:10.520
and this becomes the next step in our integration.

264
00:11:10.520 --> 00:11:13.400
So what we finally want to do is basically

265
00:11:13.400 --> 00:11:15.840
we want to create a pull request

266
00:11:15.840 --> 00:11:19.000
to our website repository, which is something we manage

267
00:11:19.000 --> 00:11:21.680
with a static set generator called Eleventy.

268
00:11:21.680 --> 00:11:23.480
And it's all managed open source

269
00:11:23.480 --> 00:11:24.920
in a public repository on GitHub.

270
00:11:24.920 --> 00:11:26.840
So we will also have the link if you're curious

271
00:11:26.840 --> 00:11:29.280
to see exactly how we build our own website.

272
00:11:29.280 --> 00:11:30.720
So we create this pull request,

273
00:11:30.720 --> 00:11:33.720
and this pull request contains all this information

274
00:11:33.720 --> 00:11:37.720
nicely laid out in the PR description.

275
00:11:37.720 --> 00:11:39.760
So this is what we got from Bedrock,

276
00:11:39.760 --> 00:11:41.640
but also contains the transcript

277
00:11:41.640 --> 00:11:44.080
that we can incorporate in the website as well.

278
00:11:44.080 --> 00:11:45.520
And this is something we were doing before.

279
00:11:45.520 --> 00:11:48.640
So the new bit here is that the pull request description

280
00:11:49.160 --> 00:11:51.800
will contain all this additional information,

281
00:11:51.800 --> 00:11:54.000
description, chapters, and tags

282
00:11:54.000 --> 00:11:56.880
in a way that we can easily copy paste it into YouTube.

283
00:11:56.880 --> 00:11:59.320
And this way we're saving lots of time.

284
00:11:59.320 --> 00:12:02.560
Of course, we still take some manual time

285
00:12:02.560 --> 00:12:05.400
to review everything and decide whether we like it or not

286
00:12:05.400 --> 00:12:07.720
and add a little bit of personal touch,

287
00:12:07.720 --> 00:12:10.800
but I think that's saving us already a lot of time.

288
00:12:11.760 --> 00:12:13.800
I think one of the interesting bits here,

289
00:12:13.800 --> 00:12:16.320
which at least when we started to work on this

290
00:12:16.320 --> 00:12:17.720
wasn't obvious at all to me,

291
00:12:17.760 --> 00:12:20.720
is the part where we defined basically

292
00:12:20.720 --> 00:12:21.640
the prompt engineering.

293
00:12:21.640 --> 00:12:24.480
How do we tell the model, what do we want,

294
00:12:24.480 --> 00:12:26.520
and in which format it should give it to us?

295
00:12:26.520 --> 00:12:28.880
So do you want to talk a little bit more about that,

296
00:12:28.880 --> 00:12:30.200
or what?

297
00:12:30.200 --> 00:12:32.880
Yeah, the prompt syntax for every model is slightly different.

298
00:12:32.880 --> 00:12:34.920
For example, for the cloud one we're using,

299
00:12:34.920 --> 00:12:37.360
you need to specify like human colon,

300
00:12:37.360 --> 00:12:38.720
then your instruction,

301
00:12:38.720 --> 00:12:41.400
and then a new line with assistant colon,

302
00:12:41.400 --> 00:12:44.200
and then finish it with another two new lines.

303
00:12:44.200 --> 00:12:46.320
That's just the way that the model has been trained

304
00:12:46.320 --> 00:12:47.680
and expects input.

305
00:12:48.480 --> 00:12:51.240
Beyond that, it's kind of like trying to come up

306
00:12:51.240 --> 00:12:52.920
with the right phrases and instructions

307
00:12:52.920 --> 00:12:54.560
and restrictions and examples

308
00:12:55.520 --> 00:12:57.520
so that it has the best chance of giving you

309
00:12:57.520 --> 00:13:00.180
the kind of inference results you're looking for.

310
00:13:00.180 --> 00:13:01.040
And the way you can do that,

311
00:13:01.040 --> 00:13:03.320
you can start off with a Bedrock playground

312
00:13:03.320 --> 00:13:04.720
in the AWS console,

313
00:13:04.720 --> 00:13:07.040
and you can type instructions there.

314
00:13:07.040 --> 00:13:11.160
The API or SDK is really simple for models.

315
00:13:11.160 --> 00:13:13.200
You're just doing an invoke model request.

316
00:13:13.200 --> 00:13:14.560
That's what we're doing.

317
00:13:14.560 --> 00:13:16.840
And there's only a couple of parameters you need to pass in.

318
00:13:16.880 --> 00:13:18.800
You can look at the documentation for the parameters

319
00:13:18.800 --> 00:13:20.400
that you need to specify,

320
00:13:20.400 --> 00:13:23.180
and then it's just understanding how to format your prompt.

321
00:13:23.180 --> 00:13:27.900
So for us, it's just a string with this human start,

322
00:13:27.900 --> 00:13:29.640
and then we're asking it the instruction.

323
00:13:29.640 --> 00:13:33.320
So what we're saying is provide us with a episode summary,

324
00:13:33.320 --> 00:13:34.720
first person plural,

325
00:13:34.720 --> 00:13:37.660
and we're aiming for around 120 words.

326
00:13:37.660 --> 00:13:40.520
And then we say followed by 10 chapter summaries

327
00:13:41.880 --> 00:13:44.360
for this following transcript JSON, right?

328
00:13:44.360 --> 00:13:46.840
So we're gonna include the JSON in the instruction.

329
00:13:46.840 --> 00:13:51.600
And then we're also asking for the chapter summaries

330
00:13:51.600 --> 00:13:54.960
to be based off the timings in the transcript segments,

331
00:13:54.960 --> 00:13:57.720
and for those timestamps to be included

332
00:13:57.720 --> 00:14:00.880
exactly as they were, the same format from the segments.

333
00:14:00.880 --> 00:14:04.280
And we're also asking then for the tags,

334
00:14:04.280 --> 00:14:07.500
like up to 20 relevant tags for the YouTube video.

335
00:14:07.500 --> 00:14:09.020
But we're also doing this kind of,

336
00:14:09.020 --> 00:14:10.920
it's kind of a single shot inference

337
00:14:10.960 --> 00:14:15.080
where we're giving it an example as well

338
00:14:15.080 --> 00:14:17.520
of the output we want to receive.

339
00:14:17.520 --> 00:14:20.920
So we're giving it a sample JSON

340
00:14:20.920 --> 00:14:23.560
just to show the structure that we're looking for.

341
00:14:23.560 --> 00:14:26.680
And when we run that, then about 20 or 30 seconds later,

342
00:14:26.680 --> 00:14:29.320
we can get back our response.

343
00:14:29.320 --> 00:14:31.820
It starts off with a bit of text and then the JSON.

344
00:14:31.820 --> 00:14:35.440
So we just need to strip out the JSON and parse it.

345
00:14:35.440 --> 00:14:39.260
Now, you might wonder, this is a non-deterministic model.

346
00:14:39.300 --> 00:14:41.000
It can generate all sorts.

347
00:14:41.000 --> 00:14:43.380
Will it always generate valid JSON?

348
00:14:43.380 --> 00:14:45.860
That's something to be mindful of,

349
00:14:45.860 --> 00:14:48.860
but in our testing, it has always generated

350
00:14:48.860 --> 00:14:51.260
exactly perfectly formatted JSON,

351
00:14:51.260 --> 00:14:52.700
and we haven't had any issue there

352
00:14:52.700 --> 00:14:55.340
because we're using that example in the prompt.

353
00:14:55.340 --> 00:14:57.020
So then tying this all in briefly

354
00:14:57.020 --> 00:14:58.100
into the total architecture,

355
00:14:58.100 --> 00:15:00.780
which you can see on the website, there's a diagram there.

356
00:15:00.780 --> 00:15:02.940
We're using Step Function to orchestrate.

357
00:15:02.940 --> 00:15:04.940
It's a very simple two-step Step Function.

358
00:15:04.940 --> 00:15:07.680
It's triggered by S3 events in EventBridge.

359
00:15:07.680 --> 00:15:09.680
It runs the summarization lambda function

360
00:15:09.680 --> 00:15:12.320
that calls Bedrock with the prompt,

361
00:15:12.320 --> 00:15:13.960
passing in the full transcript

362
00:15:13.960 --> 00:15:17.180
and getting back the response, extracting the JSON.

363
00:15:17.180 --> 00:15:19.100
Then we pass that to the next step in the Step Function,

364
00:15:19.100 --> 00:15:21.020
which is our pull request lambda,

365
00:15:21.020 --> 00:15:23.480
which is the same one we had in the other project before.

366
00:15:23.480 --> 00:15:26.280
We just refactored this into the new repo,

367
00:15:26.280 --> 00:15:28.760
and that creates the pull request based on that JSON

368
00:15:28.760 --> 00:15:31.200
and gives us that nice GitHub description.

369
00:15:31.200 --> 00:15:32.020
And that's it.

370
00:15:32.020 --> 00:15:33.400
So I think it's pretty simple all in all,

371
00:15:33.400 --> 00:15:34.440
but it's quite powerful.

372
00:15:34.440 --> 00:15:37.780
And the results, I think, so far look pretty impressive.

373
00:15:37.780 --> 00:15:42.780
And recently we did the interview with Jeremy Daley.

374
00:15:43.120 --> 00:15:45.740
We got really a great amount of time to talk to Jeremy,

375
00:15:45.740 --> 00:15:48.240
but the more time you have, the more effort you have to do

376
00:15:48.240 --> 00:15:49.760
if you're trying to create chapters.

377
00:15:49.760 --> 00:15:51.640
So all of this automation really helps us

378
00:15:51.640 --> 00:15:53.760
because this podcast is only really possible

379
00:15:53.760 --> 00:15:56.720
because we've managed to find a format

380
00:15:56.720 --> 00:15:58.960
that doesn't take too much of our time.

381
00:15:58.960 --> 00:16:01.520
We do some preparation, we record the episodes,

382
00:16:01.520 --> 00:16:04.360
and then we try to keep the post-production

383
00:16:05.240 --> 00:16:06.560
process as lean as possible.

384
00:16:06.560 --> 00:16:08.160
Of course, all of this stuff is not free.

385
00:16:08.160 --> 00:16:10.760
We are using a service, we are using AWS.

386
00:16:10.760 --> 00:16:13.280
AWS is running servers and GPUs for us.

387
00:16:13.280 --> 00:16:15.240
So of course there is a cost to it.

388
00:16:15.240 --> 00:16:16.640
So what is the cost?

389
00:16:16.640 --> 00:16:19.340
And there are actually two different pricing models

390
00:16:19.340 --> 00:16:20.480
that you can pick.

391
00:16:20.480 --> 00:16:23.020
One is called provisioned and one is called on-demand.

392
00:16:23.020 --> 00:16:25.120
Provision, you basically pay per hour.

393
00:16:25.120 --> 00:16:26.040
And it's interesting enough

394
00:16:26.040 --> 00:16:28.040
that it's not supported for all models.

395
00:16:28.040 --> 00:16:29.920
So the idea is that you pay upfront,

396
00:16:29.920 --> 00:16:33.120
decide on which terms you want to commit,

397
00:16:33.120 --> 00:16:36.140
and then it looks a little bit like a compute saving plan

398
00:16:36.140 --> 00:16:40.480
where probably AWS is allocating something dedicated to you

399
00:16:40.480 --> 00:16:42.200
and then you are paying on the hour

400
00:16:42.200 --> 00:16:45.000
for that set of resources.

401
00:16:45.000 --> 00:16:46.880
And we actually didn't use this one.

402
00:16:46.880 --> 00:16:49.640
We used the on-demand just because it looks more flexible

403
00:16:49.640 --> 00:16:52.840
and it's, I think, better for us while we are experimenting

404
00:16:52.840 --> 00:16:54.440
and we don't really expect to use it

405
00:16:54.440 --> 00:16:56.200
in large volumes anyway.

406
00:16:56.200 --> 00:16:59.000
And the on-demand is what you would expect

407
00:16:59.000 --> 00:17:00.700
as kind of a serverless offering

408
00:17:00.700 --> 00:17:02.860
where you pay per the amount,

409
00:17:02.860 --> 00:17:04.980
the units that's being processed.

410
00:17:04.980 --> 00:17:07.880
And it varies a lot depending on the model you pick.

411
00:17:07.880 --> 00:17:12.060
For instance, we pay 0.3 of one cent to two cents

412
00:17:12.060 --> 00:17:13.480
per model for text.

413
00:17:13.480 --> 00:17:17.980
And then the most expensive is the stable diffusion one,

414
00:17:17.980 --> 00:17:20.060
probably because it's also the most expensive

415
00:17:20.060 --> 00:17:24.780
to run behind the scenes, and it's 7.2 cents per image.

416
00:17:24.780 --> 00:17:28.900
So based on that, it might not be very obvious

417
00:17:28.900 --> 00:17:31.340
to understand, especially because we are not generating

418
00:17:31.340 --> 00:17:33.380
images, but we are generating text,

419
00:17:33.380 --> 00:17:34.860
and text might vary a lot.

420
00:17:34.860 --> 00:17:36.060
You might have very short text.

421
00:17:36.060 --> 00:17:37.780
You might have very long text.

422
00:17:37.780 --> 00:17:40.080
And also it's not just the text that is generated,

423
00:17:40.080 --> 00:17:41.840
but even the input that you provide.

424
00:17:41.840 --> 00:17:44.800
So if you have longer episodes, you are providing more text.

425
00:17:44.800 --> 00:17:48.260
So it's very difficult to do a prediction and say,

426
00:17:48.260 --> 00:17:51.260
well, we're gonna be spending X per every episode.

427
00:17:51.260 --> 00:17:53.380
So how did we reason about cost?

428
00:17:53.380 --> 00:17:55.140
What did we do to try to make our costs

429
00:17:55.140 --> 00:17:57.380
a little bit more predictable?

430
00:17:57.500 --> 00:18:00.140
Because it's so difficult to understand this pricing model

431
00:18:00.140 --> 00:18:01.940
and it varies from model to model,

432
00:18:01.940 --> 00:18:04.460
and then the dimension is a bit strange as well.

433
00:18:04.460 --> 00:18:07.020
So this pricing example you gave,

434
00:18:07.020 --> 00:18:11.140
you mentioned 0.03 of one cent at the lower end,

435
00:18:11.140 --> 00:18:13.740
up to like one and a half, two cents.

436
00:18:13.740 --> 00:18:16.780
That's for a thousand input tokens

437
00:18:16.780 --> 00:18:19.220
for these different language models.

438
00:18:19.220 --> 00:18:20.180
So what's a token?

439
00:18:20.180 --> 00:18:22.900
Well, it's generally roughly one word,

440
00:18:22.900 --> 00:18:25.900
but the input, depending on if you're dynamically

441
00:18:25.900 --> 00:18:29.060
generating your input or if the output is extra long,

442
00:18:29.060 --> 00:18:30.020
your price is gonna change.

443
00:18:30.020 --> 00:18:32.900
So it's important to get more of a real-time handle on costs.

444
00:18:32.900 --> 00:18:36.580
So what we did to solve that was we created a real-time view

445
00:18:36.580 --> 00:18:39.840
of our pricing using a CloudWatch dashboard

446
00:18:39.840 --> 00:18:42.180
that is generated from a CDK application.

447
00:18:42.180 --> 00:18:44.100
So this CDK application is in the same repo.

448
00:18:44.100 --> 00:18:46.260
You can take a look at it and you can use it as an example

449
00:18:46.260 --> 00:18:49.500
to create your own Bedrock pricing dashboard too.

450
00:18:49.500 --> 00:18:53.560
And what we do is we hard code the pricing in there

451
00:18:53.560 --> 00:18:55.740
for a cloud model, because unfortunately,

452
00:18:56.420 --> 00:18:59.700
right now it's not available via the Amazon pricing API.

453
00:18:59.700 --> 00:19:01.080
So we just had to hard code it,

454
00:19:01.080 --> 00:19:04.620
but then we can just generate widgets for a dashboard

455
00:19:04.620 --> 00:19:08.400
that allow us to see a blink of an eye in real time,

456
00:19:08.400 --> 00:19:11.820
like based on a minute of granularity update,

457
00:19:11.820 --> 00:19:13.020
what's the price for input?

458
00:19:13.020 --> 00:19:14.220
What's the price for output?

459
00:19:14.220 --> 00:19:17.300
What's the total cost over the last week,

460
00:19:17.300 --> 00:19:20.280
over the last hour, over the last day, whatever.

461
00:19:20.280 --> 00:19:23.920
And then we can see based on the number of invocations,

462
00:19:23.920 --> 00:19:27.300
what does it cost for the average episode to be summarized?

463
00:19:27.300 --> 00:19:29.400
And we have that dashboard, but we also have alarms.

464
00:19:29.400 --> 00:19:33.460
So we can say, if this goes above $1 per hour

465
00:19:33.460 --> 00:19:36.760
for three consecutive hours, then send us notification.

466
00:19:36.760 --> 00:19:39.160
So we don't have to wait for our budgets to fire

467
00:19:39.160 --> 00:19:42.200
or for the end of day billing reports.

468
00:19:42.200 --> 00:19:43.760
We got much more real time alerts.

469
00:19:43.760 --> 00:19:45.360
And so it's an interesting model that you could apply,

470
00:19:45.360 --> 00:19:48.320
but I think it's particularly useful for this kind of stuff.

471
00:19:48.320 --> 00:19:50.260
By the way, in case you're wondering,

472
00:19:50.260 --> 00:19:53.100
it costs around 13, 14 cents per episode

473
00:19:53.140 --> 00:19:55.060
for us to do all this summarization.

474
00:19:55.060 --> 00:19:57.060
I think the cost is pretty good.

475
00:19:57.060 --> 00:20:01.140
We've run tens, almost a hundred so far

476
00:20:01.140 --> 00:20:05.020
and haven't spent more than $5, I think,

477
00:20:05.020 --> 00:20:08.140
or something like that, just with all our testing

478
00:20:08.140 --> 00:20:11.460
and running the latest few episodes through this engine.

479
00:20:12.460 --> 00:20:14.260
If anyone wants to get this up and running

480
00:20:14.260 --> 00:20:17.340
for their Gen AI with Bedrock, just check out the repo

481
00:20:17.340 --> 00:20:20.560
and you feel free to use that code as an example.

482
00:20:20.560 --> 00:20:22.660
And I think that's it for this episode.

483
00:20:23.060 --> 00:20:24.340
Please check out this episode or product.

484
00:20:24.340 --> 00:20:25.180
Let us know what you think.

485
00:20:25.180 --> 00:20:27.260
We'd love to have others contribute to it,

486
00:20:27.260 --> 00:20:30.420
add new features and give us some ideas as well.

487
00:20:30.420 --> 00:20:32.500
So thanks very much for joining us.

488
00:20:32.500 --> 00:20:33.420
I hope you're really enjoying

489
00:20:33.420 --> 00:20:36.500
those robot generated YouTube descriptions

490
00:20:36.500 --> 00:20:38.500
and we'll see you in the next episode.
