WEBVTT

1
00:00:00.000 --> 00:00:02.800
Following on from our last episode on Aurora,

2
00:00:02.800 --> 00:00:04.560
we are sticking with databases today.

3
00:00:04.560 --> 00:00:08.880
This time we are discussing one of the most requested topics by our listener, DynamoDB.

4
00:00:08.880 --> 00:00:12.480
We are going to give you our opinion on when and how to use DynamoDB,

5
00:00:12.480 --> 00:00:13.600
when you should avoid it,

6
00:00:13.600 --> 00:00:18.480
and whether the much talked about topic of single table design is actually worth the effort.

7
00:00:18.480 --> 00:00:21.680
By the end of today's episode, we hope you will have a comprehensive understanding

8
00:00:21.680 --> 00:00:26.480
of the main DynamoDB concepts and how to get most of the value from DynamoDB.

9
00:00:26.480 --> 00:00:28.080
I'm Luciano and I'm joined by Eoin,

10
00:00:28.080 --> 00:00:30.640
and this is another episode of AWS Bites podcast.

11
00:00:38.880 --> 00:00:41.600
AWS Bites is brought to you by fourTheorem.

12
00:00:41.600 --> 00:00:44.960
If you need someone to work with to build the best design,

13
00:00:44.960 --> 00:00:47.840
highly available databases on AWS, give us a shout.

14
00:00:47.840 --> 00:00:53.440
You can check us out at fourtheorem.com or contact us directly using the links you will find in the show notes.

15
00:00:53.440 --> 00:00:57.760
So maybe to get started, what we can do is give a little bit of background

16
00:00:57.760 --> 00:01:03.040
on what DynamoDB is and how does it compare with relational databases.

17
00:01:03.040 --> 00:01:08.640
So DynamoDB is well known as one of the best in class NoSQL databases in the cloud.

18
00:01:08.640 --> 00:01:12.240
And because we talked about relational databases in the previous episode,

19
00:01:12.240 --> 00:01:16.480
again, how does a NoSQL database compare with a SQL database?

20
00:01:16.480 --> 00:01:22.240
And it's not necessarily an easy description because NoSQL is a bit of a marketing term.

21
00:01:22.240 --> 00:01:24.880
So it's not like there is a canonical definition,

22
00:01:24.880 --> 00:01:29.680
but we'll try our best to try to describe the differences between those two classes of databases.

23
00:01:29.680 --> 00:01:31.760
So let's start with relational databases first.

24
00:01:31.760 --> 00:01:34.560
Relational databases traditionally optimize for storage.

25
00:01:34.560 --> 00:01:39.360
And after all, we have to think that they were invented at a time where storage was very expensive.

26
00:01:39.360 --> 00:01:44.000
So the goal at that point in history was to try to limit as much as possible

27
00:01:44.000 --> 00:01:48.080
the duplication of data because storage was effectively a scarce resource.

28
00:01:48.080 --> 00:01:52.960
So data is generally separated into normalized tables with defined relations between them.

29
00:01:52.960 --> 00:01:55.600
So well-organized structure and well-defined schema.

30
00:01:55.600 --> 00:02:01.440
Relational databases normally use a language called SQL and highly optimized query engine

31
00:02:01.440 --> 00:02:06.080
that basically allows to retrieve data across multiple tables in a very dynamic way.

32
00:02:06.080 --> 00:02:10.000
Allows you to combine data in different ways, filter data in different ways,

33
00:02:10.000 --> 00:02:13.440
do updates across multiple records at the same time.

34
00:02:13.440 --> 00:02:18.320
And this has become over the years some kind of lingua franca for databases.

35
00:02:18.320 --> 00:02:23.920
And incredibly popular, lots of people in the industry know SQL as a language,

36
00:02:23.920 --> 00:02:27.280
well understood, used in many products, even for reporting,

37
00:02:27.280 --> 00:02:29.760
not just for actually interacting with databases.

38
00:02:29.760 --> 00:02:34.720
And if you think it's been around for 15 years, 50 years, it actually makes sense

39
00:02:34.720 --> 00:02:39.360
that it's something so well-known and understood and adopted in the industry.

40
00:02:39.360 --> 00:02:45.840
So SQL is kind of a way to do arbitrary requests or ask arbitrary questions to your database.

41
00:02:45.920 --> 00:02:49.840
And that's a great thing. It's actually a great feature, especially in comparison with

42
00:02:49.840 --> 00:02:54.480
NoSQL databases because you generally don't need to know in advance what you're going to

43
00:02:54.480 --> 00:02:58.880
be using this database for. You just put all your data there, you give it some sensible structure,

44
00:02:58.880 --> 00:03:02.960
and then over time you can come up with new access patterns, with new questions to ask

45
00:03:02.960 --> 00:03:07.360
to your database and SQL is going to be flexible enough to allow you to express this kind of

46
00:03:07.360 --> 00:03:11.840
questions to your database. But this is a bit of a double-edged sword because it's so flexible,

47
00:03:11.840 --> 00:03:16.720
that also means that it cannot be optimized for performance for any use case or for any

48
00:03:16.720 --> 00:03:20.480
question that you might have. So sometimes you will find yourself, if you ever manage a SQL

49
00:03:20.480 --> 00:03:25.040
database, trying to figure out why this query was particularly slow, how do I optimize it.

50
00:03:25.040 --> 00:03:30.560
Sometimes that means maybe changing the data structure, maybe adding indices, maybe scaling

51
00:03:30.560 --> 00:03:35.280
hardware, maybe thinking how do I partition this data across maybe multiple instances of

52
00:03:35.280 --> 00:03:40.240
that database. So these are kind of the pros and cons of relational databases. Let's talk now about

53
00:03:40.320 --> 00:03:46.000
NoSQL. And we already mentioned that NoSQL is a bit of a marketing thing, so let's try to figure

54
00:03:46.000 --> 00:03:50.800
out what is the simplest definition that we can give that probably most people would agree with.

55
00:03:50.800 --> 00:03:56.240
And one of the main points of most NoSQL products is that they are schema-less. So that means that

56
00:03:56.240 --> 00:04:01.040
when you store data, you store it in a set, for lack of a better word, let's just call it a set,

57
00:04:01.040 --> 00:04:06.160
like one place where you put all your data and different records in that set can contain

58
00:04:06.240 --> 00:04:10.720
a different structure of data. So you can have different fields. That's what we mean by schema-less.

59
00:04:10.720 --> 00:04:15.360
You don't have to think in advance about a schema that will fulfill all the records that you want to

60
00:04:15.360 --> 00:04:21.680
put in that particular set, but every single item can have its own properties. And another interesting

61
00:04:21.680 --> 00:04:26.480
point is that generally NoSQL products will be a little bit more relaxed when it comes to

62
00:04:26.480 --> 00:04:31.040
ACID compliance. With ACID, we mean atomic consistent, isolated, and durable, which is

63
00:04:31.040 --> 00:04:37.440
a property that most relational database will try to guarantee. In NoSQL databases, generally,

64
00:04:37.440 --> 00:04:41.760
the producers are concerned about performance and making sure that the data can be easily

65
00:04:41.760 --> 00:04:46.400
distributed across multiple nodes. So there are some trade-offs that are made where generally,

66
00:04:46.400 --> 00:04:51.840
for instance, what NoSQL producers will do, they will give up on the idea of consistency and favor

67
00:04:51.840 --> 00:04:56.800
eventual consistency so that it's easier for them to be able to distribute the data in a durable way

68
00:04:56.800 --> 00:05:01.600
across multiple partitions. And the final point is that with NoSQL databases, you generally

69
00:05:01.600 --> 00:05:06.640
worry a lot less about storage cost. And this is probably because it's a much modern version of

70
00:05:06.640 --> 00:05:12.880
databases. So storage is a bit less of a problem since the 70s. So there is a lot more freedom to

71
00:05:13.520 --> 00:05:18.560
use storage in ways where you might end up duplicating data. But once you do that,

72
00:05:18.560 --> 00:05:23.440
you might be able to access that data much faster in some access patterns. If you want to think

73
00:05:23.440 --> 00:05:28.880
about what is the simplest NoSQL database that you can imagine, you can just think about a key

74
00:05:28.880 --> 00:05:34.560
value storage. So imagine you have a map in any programming language where you can start key value

75
00:05:34.560 --> 00:05:39.280
pairs. And you can imagine that the key is basically the thing that allows you to access

76
00:05:39.280 --> 00:05:44.000
records univocally. And then inside the value, you can store complex objects of any kinds with

77
00:05:44.000 --> 00:05:49.680
multiple attributes. And they can all be different between every record. And it's also worth mentioning

78
00:05:49.680 --> 00:05:53.440
that this is not necessarily the same thing when we mentioned document databases. You might have

79
00:05:53.440 --> 00:05:58.160
heard of MongoDB, which is generally classified as a document database because document-oriented

80
00:05:58.160 --> 00:06:02.800
databases are more of an extension of the key value concept. They tend to have a little bit

81
00:06:02.800 --> 00:06:08.960
of a more structured format and a more expressive query language. So when we talk about NoSQL,

82
00:06:08.960 --> 00:06:13.280
and especially in the context of DynamoDB, we are talking about something that maybe can be a little

83
00:06:13.280 --> 00:06:18.560
bit simpler than products like MongoDB. Hopefully that gives you a good introduction to the world

84
00:06:18.560 --> 00:06:24.240
of NoSQL and what we mean by NoSQL and how does it compare with relational databases. So let's now

85
00:06:24.240 --> 00:06:27.360
talk about specifically DynamoDB. Eoin, where do we start?

86
00:06:32.080 --> 00:06:36.880
Yeah, let's start with some of the terminology and concepts around DynamoDB so that we can take the discussion from there. Just like SQL databases, you start with a table. So this is the primary unit.

87
00:06:36.880 --> 00:06:42.560
And you don't really create databases in DynamoDB, but you create tables. And that's your starting

88
00:06:42.560 --> 00:06:47.200
point. Within tables, then you're going to be storing items. So an item is the term that is

89
00:06:47.200 --> 00:06:52.960
used to refer to essentially a row containing a key and attribute values. So then we talk about

90
00:06:52.960 --> 00:06:58.640
a key and a key is less of a trivial concept really. But it is, as we mentioned, a type of

91
00:06:58.640 --> 00:07:03.280
key value store, and every record is identified with a key. And there are two types that you can

92
00:07:03.280 --> 00:07:09.120
use. You can either use a simple key or a composite key. And that would be the primary key uniquely

93
00:07:09.120 --> 00:07:16.320
identifying an item in the table. A single key has a hash key only, which is also known as a partition

94
00:07:16.320 --> 00:07:21.120
key. So it's probably a good idea to understand both the term hash key and also partition key.

95
00:07:21.120 --> 00:07:25.920
And then if you're using a composite key, you'll have that same hash key. But you can also have

96
00:07:25.920 --> 00:07:31.680
then a range key, which is also known as a sort key. And when you're writing or reading DynamoDB

97
00:07:31.680 --> 00:07:37.040
data, you'll always use the partition key. If you have a sort key as well, so if you are using

98
00:07:37.040 --> 00:07:41.680
composite keys, you'll need to specify it when writing data, but you don't necessarily have to

99
00:07:41.680 --> 00:07:45.360
specify it when you're reading. So we'll talk a little bit more about all that later on.

100
00:07:46.000 --> 00:07:49.760
So that's your key. And then your value is composed of attributes. You can have multiple

101
00:07:49.760 --> 00:07:53.760
attributes in each item, as you mentioned already, and an attribute has a name and a value. It's a

102
00:07:53.760 --> 00:07:58.080
little bit different to the document storage option or just a simple key value storage option,

103
00:07:58.080 --> 00:08:02.160
where you have a single value or you have a document with awareness of the structure.

104
00:08:02.160 --> 00:08:06.960
In DynamoDB, you can have multiple attributes and each of those could be like a little document,

105
00:08:06.960 --> 00:08:11.680
but in a very unstructured way. There's a number of different types supported. It's worthwhile

106
00:08:11.680 --> 00:08:17.600
understanding what types are supported, especially when it gets into the multi-value types. So the

107
00:08:17.600 --> 00:08:22.480
simple values types, the literals you can store are string, number. You can also store binary

108
00:08:22.480 --> 00:08:29.360
data as a type. And then you have lists, maps, and sets. If you just write kind of JSON type data

109
00:08:29.360 --> 00:08:33.200
into an attribute, that's a map, a list is an array, and then you have sets where you don't

110
00:08:33.200 --> 00:08:37.840
get duplicate values. And you have three different types of sets supported. So you have string sets,

111
00:08:37.840 --> 00:08:42.320
number sets, and binary sets. And we'll talk a little bit more about how you use those.

112
00:08:42.320 --> 00:08:47.840
There is also a null type that's quite rarely used. Now it is important to note that unlike

113
00:08:47.840 --> 00:08:53.040
a lot of databases, the maximum item size in DynamoDB is 400 kilobytes. It's important to

114
00:08:53.040 --> 00:08:56.880
note that this is per item, right? For the whole record, not just per attribute. You might think

115
00:08:56.880 --> 00:09:01.760
this is a small comparison to like Cassandra or MongoDB, which lets you store gigabytes in

116
00:09:01.760 --> 00:09:06.640
records, but there's a lot of limitations like this in DynamoDB, which are there for a very good

117
00:09:06.640 --> 00:09:10.640
reason. And they're there because it helps them to deliver on the massive performance and

118
00:09:10.640 --> 00:09:15.440
scalability guarantees that they provide. So it seems sometimes working with DynamoDB that it's

119
00:09:15.440 --> 00:09:20.880
almost like working with a low level database engine because they're strict about giving you

120
00:09:20.880 --> 00:09:25.280
a limited set of features so that they can give you those guarantees in return. So if you want

121
00:09:25.280 --> 00:09:31.760
more data than 400 KB per item, it's difficult to offload that into an S3 object. So maybe before

122
00:09:31.760 --> 00:09:38.160
we start diving into more technical details, let's go up a level. Why and when would you use DynamoDB?

123
00:09:44.160 --> 00:09:50.320
I will say that one of the main reasons to think about using DynamoDB is because it's so nice and easy to get started with. You can create a table in a matter of seconds. Either you click offset,

124
00:09:50.320 --> 00:09:54.800
or you just write a few lines of YAML in your CloudFormation file, and you have a table that

125
00:09:54.800 --> 00:09:59.200
you can use to store data, read it, write it, and build an application on top of that. And this is

126
00:09:59.200 --> 00:10:03.840
something that I haven't seen in any other database, even when you can manage services.

127
00:10:03.840 --> 00:10:08.960
If those are relational databases especially, it takes a longer time to get started with.

128
00:10:08.960 --> 00:10:14.240
So for quick things, definitely a database to consider. It can also be very cost effective,

129
00:10:14.800 --> 00:10:20.240
especially when you don't need a data intensive application, when you don't expect to be reading

130
00:10:20.240 --> 00:10:24.560
and writing all the time, or when you don't expect large volumes of data. It can be very,

131
00:10:24.560 --> 00:10:28.640
very cost effective. And the main idea is that it's kind of a serverless database,

132
00:10:28.640 --> 00:10:32.320
at least from the pricing perspective. If you don't use it, you don't have to pay anything.

133
00:10:32.320 --> 00:10:37.440
In reality, there are different building modes. But again, we can think about it as if it's a

134
00:10:37.440 --> 00:10:42.000
serverless service. So they try to provide you with an interface that the more you use it,

135
00:10:42.000 --> 00:10:45.600
the more you're going to pay. If you don't use it, the cost is going to be very limited.

136
00:10:45.600 --> 00:10:49.120
And this can be a really big advantage, for instance, if you are a startup,

137
00:10:49.120 --> 00:10:53.840
where maybe at the very beginning, you are going to have very limited traffic as you try to build

138
00:10:53.840 --> 00:10:59.120
your first MVP. Then eventually, if your product is very successful, it might grow. And of course,

139
00:10:59.120 --> 00:11:03.840
your billing is going to grow with the success of your platform. It's very well integrated with

140
00:11:03.840 --> 00:11:08.320
other AWS services. That's another interesting point. For instance, we can talk about DynamoDB

141
00:11:08.320 --> 00:11:14.080
streams, which is a nice way to basically get all the changes that happen in a DynamoDB table

142
00:11:14.080 --> 00:11:18.080
and stream them for real-time processing, for instance, to Lambda. And this is something that

143
00:11:18.080 --> 00:11:22.640
allows you to do change data capture, and you can do all sorts of interesting things with it.

144
00:11:22.640 --> 00:11:27.760
Also, you can get very fine-grained access control because it's kind of a native AWS service,

145
00:11:27.760 --> 00:11:34.080
so to speak. You can use IAM policies to a very fine level of detail. You can really control

146
00:11:34.080 --> 00:11:40.160
what kind of, not just what kind of tables, but what kind of records different roles can actually

147
00:11:40.160 --> 00:11:44.080
access. And this can allow you to do very cool things. For instance, if you're building a

148
00:11:44.080 --> 00:11:49.520
multi-tenant application, you could limit, for example, a Lambda to be able to read only the

149
00:11:49.520 --> 00:11:53.920
records that are attributed to a specific tenant and not the other ones, which can be something

150
00:11:53.920 --> 00:11:58.080
really, really beneficial if you're trying to get some kind of stock compliance, or if you just want

151
00:11:58.080 --> 00:12:02.000
to be sure that you're not going to be leaking data across tenants by accident. And this will

152
00:12:02.000 --> 00:12:07.040
be something very, very difficult to achieve at this level with relational databases that are not

153
00:12:07.040 --> 00:12:12.800
so well integrated with the rest of the AWS ecosystem. And another few things that are very

154
00:12:12.800 --> 00:12:17.680
worth mentioning is that DynamoDB scales massively. And after all, we have to think that DynamoDB was

155
00:12:17.680 --> 00:12:21.600
built for Amazon, so to solve all the problems that they were having with their own massive

156
00:12:21.600 --> 00:12:27.920
e-commerce as it was growing in popularity. And it is powering today the entirety of the Amazon

157
00:12:27.920 --> 00:12:32.720
infrastructure. So you can imagine that if you can build something as complex as Amazon and as big as

158
00:12:32.720 --> 00:12:37.520
Amazon, there is a level of scale there that is not trivial to achieve with other technologies.

159
00:12:37.520 --> 00:12:43.680
It can be very simple to use, of course, for simple use cases. If you have access patterns

160
00:12:43.680 --> 00:12:49.200
that are pretty much key value based, then it is very simple to use. You just store your data by

161
00:12:49.200 --> 00:12:54.000
key, you read your data by key, super easy to get started. If you need very low latency,

162
00:12:54.000 --> 00:12:59.040
DynamoDB is one of the best databases for that out there. It has very consistent load and latency

163
00:12:59.040 --> 00:13:04.640
responses. For example, they promise you single digit milliseconds when you do a GET operation.

164
00:13:04.640 --> 00:13:09.520
So when you can access a key exactly, you get single digit millisecond response, which is

165
00:13:09.520 --> 00:13:13.680
amazing. And that's very consistent, regardless, for instance, of the size of your dataset.

166
00:13:13.680 --> 00:13:16.640
And this is something that can be a great candidate, for instance, when you're building

167
00:13:16.640 --> 00:13:21.440
web applications and you want to make sure your users have very snappy responses, there is a

168
00:13:21.440 --> 00:13:25.520
feeling that the application is very responsive, or other use cases where you need to guarantee

169
00:13:25.520 --> 00:13:29.600
that the access to the data is as fast as possible. But now let's talk very quickly on

170
00:13:29.600 --> 00:13:34.160
when you might not want to use DynamoDB, because of course it's not a silver bullet that is going

171
00:13:34.160 --> 00:13:39.280
to solve all your problems. So some cases that come to mind is the main one is probably when

172
00:13:39.840 --> 00:13:44.960
you need flexible querying capabilities. And this is actually very common for startups.

173
00:13:44.960 --> 00:13:49.760
Conversely, we say that DynamoDB is really good for startup environments because the pricing

174
00:13:49.760 --> 00:13:53.520
dynamics will scale with the growth of your company. But on the other end, you have to

175
00:13:53.520 --> 00:13:58.400
consider that DynamoDB requires you to understand really, really well the way you'd need to access

176
00:13:58.400 --> 00:14:03.200
the data. And when you're building a startup, sometimes you need to pivot multiple times before

177
00:14:03.200 --> 00:14:07.440
you figure out exactly what's your product market fit, what's the product that is really solving a

178
00:14:07.440 --> 00:14:11.920
problem for your customers. So you're going to go through significant different iterations of your

179
00:14:11.920 --> 00:14:16.000
product. And as such, you're probably going to change the way you use the data in your database

180
00:14:16.000 --> 00:14:20.560
multiple times. DynamoDB might not be the best database for that. It will require you a lot of

181
00:14:20.560 --> 00:14:25.200
hard work to always adjust changes to the structure of your database, change it to the way you query

182
00:14:25.200 --> 00:14:29.040
the data. Something like a relational database might be much more suitable for that kind of

183
00:14:29.040 --> 00:14:33.280
thing because with the flexibility of SQL, as long as you're keeping your data normalized,

184
00:14:33.280 --> 00:14:37.600
then you can easily adjust for different access patterns. So definitely worth considering if you

185
00:14:37.600 --> 00:14:42.800
don't really think you understand well your current and future access patterns, DynamoDB

186
00:14:42.800 --> 00:14:46.960
might create a little bit of friction for the evolution of your product. Other reasons why you

187
00:14:46.960 --> 00:14:51.040
might not want to use DynamoDB is, for instance, when you need to integrate with other systems that

188
00:14:51.040 --> 00:14:56.000
expect a SQL interface. DynamoDB is not going to give you a SQL interface or at least not a

189
00:14:56.080 --> 00:15:00.800
traditional one. So definitely it will make your life much harder if you need to integrate with

190
00:15:00.800 --> 00:15:06.000
something that is expecting SQL as a language. Another case is when you might have lots of

191
00:15:06.000 --> 00:15:10.720
relational data by nature. So you really need to use features like join and join data across

192
00:15:10.720 --> 00:15:15.440
multiple tables. That's not even something that is supported in DynamoDB natively. So you will need

193
00:15:15.440 --> 00:15:20.080
to do your own joins with code inside your application. And that's something that is not

194
00:15:20.080 --> 00:15:24.320
going to be very efficient and it's going to be very tricky to do well and to scale it.

195
00:15:24.320 --> 00:15:29.760
And finally, if you, for whatever reason, need to manage a database by yourself, like you need to

196
00:15:29.760 --> 00:15:34.320
host it yourself and run it yourself in your data center or even inside your own cloud account,

197
00:15:34.320 --> 00:15:40.160
DynamoDB doesn't offer that option. DynamoDB is only a managed service. Amazon will give you a

198
00:15:40.160 --> 00:15:44.720
single node local version that you can use and run it yourself, but that's meant to be used only for

199
00:15:44.720 --> 00:15:48.800
local testing and development, not to be run in production. Now, I think at this point, it might

200
00:15:48.800 --> 00:15:55.600
be very beneficial to try to explain a little bit more how DynamoDB works, because I think that

201
00:15:55.600 --> 00:16:00.000
is going to demystify why there are so many constraints, but at the same time also why

202
00:16:00.000 --> 00:16:03.600
DynamoDB can be so effective and performant in certain use cases.

203
00:16:03.600 --> 00:16:08.640
DynamoDB data is stored in partitions. You might've guessed this already, since we mentioned that

204
00:16:08.640 --> 00:16:14.000
data needs a partition key, that hash key we referred to earlier. So when you provide your

205
00:16:14.000 --> 00:16:18.960
value for your primary key, the partition key part at least, that key is going to be hashed

206
00:16:18.960 --> 00:16:24.800
by DynamoDB. And the hashed value is going to be used by DynamoDB to route it to the server nodes

207
00:16:24.800 --> 00:16:30.320
where the partition or that shard of data is stored. And it's the scalability of the partition

208
00:16:30.320 --> 00:16:36.560
model that gives DynamoDB its infinite scalability. Then each partition has a primary node that will

209
00:16:36.560 --> 00:16:40.960
handle writes just like many other databases, but it will also have two secondary nodes. And for

210
00:16:40.960 --> 00:16:45.440
data to be written, it has to be written by the primary and at least one other secondary node.

211
00:16:45.440 --> 00:16:50.320
The third node then can be updated asynchronously. So that will give you better performance on

212
00:16:50.320 --> 00:16:57.120
writes. But what this means is that because any of these nodes can handle reads and only one of

213
00:16:57.120 --> 00:17:02.720
the secondaries is updated synchronously, you might end up reading from a node that doesn't have

214
00:17:02.720 --> 00:17:08.480
the latest data. And this is part of DynamoDB's default mode of eventual consistency. And if this

215
00:17:08.480 --> 00:17:12.560
tradeoff is a problem for you, there is a way around it, you can explicitly request strong

216
00:17:12.560 --> 00:17:17.120
consistency when reading. And that may take a little longer because it has to wait for the third

217
00:17:17.120 --> 00:17:22.320
node to acknowledge. But you will pay an increased price for this mode, essentially double based on

218
00:17:22.320 --> 00:17:25.680
the billing model, which we'll explain a little bit later. And that billing model, the pricing

219
00:17:25.680 --> 00:17:31.760
model of DynamoDB is very tied into its performance and scalability. Because when you write, you

220
00:17:31.760 --> 00:17:37.600
consume a write capacity unit, when you read, you consume a read capacity unit. So WCUs and RCUs.

221
00:17:37.600 --> 00:17:42.960
One RCU will allow you a strongly consistent or two eventually consistent reads per second,

222
00:17:42.960 --> 00:17:47.920
and they can be up to four kilobytes. And a write capacity unit allows you to write one item up to

223
00:17:47.920 --> 00:17:52.640
one kilobyte. You have two pricing options. You've got provisioned mode where you can say, okay,

224
00:17:52.640 --> 00:17:58.560
I'm going to need 500 RCUs and 500 WCUs. And then you pay a fixed amount per hour as long as the

225
00:17:58.560 --> 00:18:04.240
table exists. The newer mode, which is more serverless is the on demand capacity. And that

226
00:18:04.240 --> 00:18:09.120
will scale the WCUs and RCUs up and down for you. If you don't use them, you don't pay. But if you

227
00:18:09.120 --> 00:18:13.280
do use them, the cost is generally higher than provisioned capacity. So you need to measure your

228
00:18:13.280 --> 00:18:18.960
own workload and decide which one works. Generally, we'd say start with on demand capacity, measure

229
00:18:18.960 --> 00:18:23.040
how much you're using, look at your bill and optimize accordingly. And the good news there

230
00:18:23.040 --> 00:18:28.400
actually is that I think just last week, AWS released a new feature, which will allow you to

231
00:18:28.400 --> 00:18:33.760
cap the maximum on demand capacity. So you can manage that maximum cost and don't have to lie

232
00:18:33.760 --> 00:18:38.720
awake at night worrying about it. Now, when we talk about partitions, you might have heard the

233
00:18:38.720 --> 00:18:43.040
concept of hot partitions, especially if you're reading older blog posts or content where your

234
00:18:43.040 --> 00:18:48.320
throughput could suffer if you didn't actually evenly distribute the partition keys across your

235
00:18:48.320 --> 00:18:53.520
whole data set. And if you do read anything like that, don't worry, because Amazon has since added

236
00:18:53.520 --> 00:18:58.560
an adaptive capacity feature a few years ago that automatically solves that for you. So they'll

237
00:18:58.560 --> 00:19:03.520
manage capacity according to the size of the partition keys on different nodes. But it is

238
00:19:03.520 --> 00:19:08.000
still important to note that each partition does have a maximum throughput. So it's 3000

239
00:19:08.000 --> 00:19:12.800
RCUs or 1000 WCUs. So if you are going to have a lot of traffic, you should make sure that you're

240
00:19:12.800 --> 00:19:17.600
not just using a small number of partition keys. And that will allow you to ensure that you get

241
00:19:17.600 --> 00:19:22.160
consistent performance across all of your data. So I think partitions, they're basically the

242
00:19:22.160 --> 00:19:26.880
fundamental concept to understand. We've talked about strong consistency and eventual consistency.

243
00:19:26.880 --> 00:19:31.600
Let's talk more practically, how do you get started and what you do to start using DynamoDB?

244
00:19:32.160 --> 00:19:37.200
If you're used to more traditional relational databases, one thing that might be surprising

245
00:19:37.200 --> 00:19:44.560
about DynamoDB is that it doesn't use something like an ODBC or JDBC type of connector. Instead,

246
00:19:44.560 --> 00:19:50.000
you just do HTTP requests. So in a way, it's like you have a web API to interact with when you use

247
00:19:50.000 --> 00:19:55.840
DynamoDB. In reality, you rarely want to use the web API directly. You will be using the AWS SDK,

248
00:19:55.840 --> 00:20:00.080
which of course abstract all of that communication in a much nicer way.

249
00:20:00.480 --> 00:20:04.560
When it comes to the SDK, there are actually two different types of client. And this is something

250
00:20:04.560 --> 00:20:08.560
that sometimes can be a little bit confusing, but the idea is that you have clients at two

251
00:20:08.560 --> 00:20:13.360
different levels. You have the main DynamoDB client where you still need to understand a

252
00:20:13.360 --> 00:20:18.240
little bit what is the protocol when it comes to specifying the different types of values that you

253
00:20:18.240 --> 00:20:24.400
need to read and write. Instead, when you use the Dogament client, that type of client is generally

254
00:20:24.400 --> 00:20:28.800
a little bit more integrated with your programming language of choice. It can understand the types

255
00:20:28.800 --> 00:20:32.880
directly from the type that you express in your programming language, and it's going to do an

256
00:20:32.880 --> 00:20:37.360
implicit conversion behind the scenes for you. So if you are trying to put a string, for instance,

257
00:20:37.360 --> 00:20:42.800
in an attribute, it's going to automatically create the correct shape of the object that the

258
00:20:42.800 --> 00:20:48.000
underlying client expects to say that that value is going to be persisted in DynamoDB as a string

259
00:20:48.000 --> 00:20:52.720
and not, for instance, as another data type. So generally speaking, I would recommend to use the

260
00:20:52.720 --> 00:20:57.840
Dogament client because it will make your life a little bit easier, and it will abstract some

261
00:20:57.840 --> 00:21:01.840
details that you don't necessarily have to worry about when it comes to the underlying protocol of

262
00:21:01.840 --> 00:21:06.640
DynamoDB. Let's talk a little bit more about how do you write data, what kind of options do you have

263
00:21:06.640 --> 00:21:12.080
there, and all the right actions that you need to do, as Eoin, you mentioned before, force you to

264
00:21:12.080 --> 00:21:16.640
provide the full primary key. So you need to explicitly say, this is the primary key that I'm

265
00:21:16.640 --> 00:21:20.960
going to be used to store this particular record or to update a particular record.

266
00:21:20.960 --> 00:21:25.440
And one interesting thing, and this is something of a pain point that I had a few times in the past,

267
00:21:25.440 --> 00:21:31.360
is that you have no way to do a query such as update all the records where this particular

268
00:21:31.360 --> 00:21:37.120
clause is true. You need to read and write based on a primary key that you need to know in advance.

269
00:21:37.120 --> 00:21:42.640
So you cannot just use arbitrary attributes to do that stuff. Now, when it comes to writing, you have

270
00:21:42.640 --> 00:21:47.920
a few different operations that you can do. The first one is put item, where basically you are

271
00:21:47.920 --> 00:21:54.080
either creating or overriding an existing item, so a single item. Then you have update item, which is,

272
00:21:54.080 --> 00:21:59.280
again, either write or update. You can specify a subset of attributes in this case,

273
00:21:59.280 --> 00:22:03.840
and you can also use this particular operation, for instance, if you want to delete existing

274
00:22:03.840 --> 00:22:08.080
attributes from an existing record. And you can also use it in interesting ways. For instance,

275
00:22:08.080 --> 00:22:14.000
if you have a record or an item that contains a set or a map, you can just insert or remove

276
00:22:14.000 --> 00:22:18.400
data from the underlying set and maps that exist in the attributes of your item.

277
00:22:18.400 --> 00:22:22.160
And finally, and this is something that can be actually very common, I've seen it a few times

278
00:22:22.160 --> 00:22:27.040
when using DynamoDB, if you have counters, you can use the update item to just say increase by

279
00:22:27.040 --> 00:22:32.560
one. And if you consider that this is a distributed database, you don't want to read the data first,

280
00:22:32.560 --> 00:22:36.640
then in your code increase by one and then start the data again, because you might have that

281
00:22:36.640 --> 00:22:41.840
operation happening simultaneously, potentially in a number of different concurrent executions,

282
00:22:41.840 --> 00:22:45.520
and therefore your counting might be overriding each other. So it's better to just let the

283
00:22:45.520 --> 00:22:50.560
database do the increment for you, because that way can be done consistently. Then, of course,

284
00:22:50.560 --> 00:22:56.080
we have delete item operations, and it's important to know that you can also do batch writing. So,

285
00:22:56.080 --> 00:23:00.800
for instance, if you need to insert lots of data into DynamoDB, maybe you are loading data from,

286
00:23:00.800 --> 00:23:04.800
I don't know, fixture data that you need to use in your application, you can do that,

287
00:23:04.800 --> 00:23:10.000
but there are limits. For instance, you can write up to 25 items if they're not wrong in a single

288
00:23:10.000 --> 00:23:14.400
batch. So you need to create multiple batches according to how many items you need to write.

289
00:23:14.400 --> 00:23:20.800
And finally, you can also use the transact write item, which allows you to write data as part of a

290
00:23:20.800 --> 00:23:25.600
transaction. And the other thing is that when you write something into DynamoDB, so when you do

291
00:23:25.600 --> 00:23:31.760
an update operation, you might be interested in receiving a response from DynamoDB, and you can

292
00:23:31.760 --> 00:23:36.960
actually specify what kind of response you want to get back from DynamoDB. So different options

293
00:23:36.960 --> 00:23:40.480
are, for instance, if you don't really care about anything, you can just say, no, just write the

294
00:23:40.480 --> 00:23:44.800
data, I don't care about the result of that operation. Then, for instance, when you're doing

295
00:23:44.800 --> 00:23:50.320
updates, it can be very interesting to know what was updated. So you have options like all old,

296
00:23:50.320 --> 00:23:56.880
or updated old, or all new, or updated new, that will allow you to select a subset of the data that

297
00:23:56.880 --> 00:24:00.640
was actually updated and compare it with the previous data. And going back to the case of the

298
00:24:00.640 --> 00:24:05.120
counter, if you want to say, for instance, increase this particular attribute by one,

299
00:24:05.120 --> 00:24:08.800
you don't necessarily know what's going to be the final value, because maybe you didn't read the

300
00:24:08.800 --> 00:24:13.280
value in the first place, or maybe the value that you have right now in memory in your program is

301
00:24:13.280 --> 00:24:18.000
outdated, because meanwhile, there have been other increases from other concurrent executions,

302
00:24:18.000 --> 00:24:23.360
you can get the new value as a response from your update operation when you select one of

303
00:24:23.360 --> 00:24:27.360
these attributes. So that can be a convenient use case. For instance, if you are building some kind

304
00:24:27.360 --> 00:24:31.680
of counter, increase the value, and you want to know what's the most recent count in your program.

305
00:24:31.680 --> 00:24:35.920
Another thing is that you can add condition expression. So when you write, you can say,

306
00:24:35.920 --> 00:24:41.360
write this record only if certain conditions are happening, and don't write it if those conditions

307
00:24:41.360 --> 00:24:46.640
are not satisfied. And this can be useful, for instance, if you want to guarantee data integrity,

308
00:24:46.640 --> 00:24:51.520
for instance, you might want to create a new user in a table, and maybe you want to make sure that

309
00:24:51.520 --> 00:24:56.160
there is only one user with a given email. Again, thinking that you might have concurrent execution

310
00:24:56.160 --> 00:25:00.400
of your program in different environments, maybe different lambdas, it's not unlikely that you can

311
00:25:00.400 --> 00:25:04.800
have a very similar request from two different lambdas in a very short amount of time. So for

312
00:25:04.800 --> 00:25:10.160
instance, if a user is submitting a form twice by mistake, you might end up creating two users with

313
00:25:10.160 --> 00:25:15.520
the same email. By using a condition expression, you can say, don't create a user a second time if

314
00:25:15.520 --> 00:25:20.640
this email is already existing in the primary key, for instance, of another record. Going into

315
00:25:20.640 --> 00:25:25.840
queries, you have different ways to query your data. The simplest one is probably get item.

316
00:25:25.840 --> 00:25:31.120
Another use case is scans, which basically allows you to iterate over the entire table.

317
00:25:31.120 --> 00:25:36.320
This is generally a very niche use case. You rarely need to do that. Or if you find yourself

318
00:25:36.320 --> 00:25:40.560
doing that, probably you should think twice because it's not always a good idea to do this.

319
00:25:40.560 --> 00:25:44.800
So unless you really know what you're doing, try to avoid scans as much as possible. And the main

320
00:25:44.800 --> 00:25:49.440
reason is that, especially if you have a large data set, a scan might take a very long time to

321
00:25:49.440 --> 00:25:54.160
complete, but also it's going to be very expensive for you. So just be sure that you are aware of

322
00:25:54.160 --> 00:25:58.720
that. If you find yourself using a scan, make sure you know what you're doing. And if you have other

323
00:25:58.720 --> 00:26:03.200
options, probably go with the other options. Of course, we have a concept of query as well,

324
00:26:03.200 --> 00:26:08.080
where you might want to retrieve multiple records together, but of course it is still somewhat

325
00:26:08.080 --> 00:26:14.480
limited to the partition key. So you can query only for a given partition key and then filter

326
00:26:14.480 --> 00:26:19.840
the subset of records from your sort key when you have a composite key. And you can have expressions

327
00:26:19.840 --> 00:26:26.000
such as equality, begins with, between, but you cannot do more generic expressions that you might

328
00:26:26.000 --> 00:26:31.280
find in SQLite, for instance, when you use the like operator and you cannot even do ends with.

329
00:26:31.280 --> 00:26:35.040
So you need to be very careful depending on the type of queries that you expect to do

330
00:26:35.040 --> 00:26:40.000
in structuring your keys so that the query operation allows you to do the queries that

331
00:26:40.000 --> 00:26:44.400
you need to do. And you can also use filter expressions, which are a little bit more flexible

332
00:26:44.400 --> 00:26:48.720
and they can be applied to any attribute, not just the primary key and the secondary key.

333
00:26:48.720 --> 00:26:53.840
And these filters are a little bit funny. They work in a way that you might not expect

334
00:26:54.800 --> 00:26:59.840
the first time you use them, because if you're using again to SQL, the filtering happens at the

335
00:26:59.840 --> 00:27:06.160
database level where the database is just going to give you the data that matches the conditions

336
00:27:06.160 --> 00:27:10.480
that you are looking for and ignore everything else. While here with DynamoDB, when you use

337
00:27:10.480 --> 00:27:14.800
filter expressions, you are actually still getting all the data, effectively discarding the records

338
00:27:14.800 --> 00:27:19.920
that don't match that particular filter expression. Finally, each query has one megabyte read limit.

339
00:27:19.920 --> 00:27:24.560
If you need to read more, you need to use pagination. Thankfully, the SDK these days

340
00:27:24.560 --> 00:27:29.600
makes that much easier than it used to be, especially in dynamic programming languages

341
00:27:29.600 --> 00:27:34.720
like JavaScript, you can use async-atorators and that's a relatively easier experience to go

342
00:27:34.720 --> 00:27:38.240
through all the different pages. But of course, you need to be aware that you are making multiple

343
00:27:38.240 --> 00:27:43.440
requests to DynamoDB. You're sending multiple HTTP requests. So the more data you read, the more

344
00:27:43.440 --> 00:27:47.360
time is going to be required to read the entire dataset. Now we should probably talk a little bit

345
00:27:47.360 --> 00:27:51.840
about indices because that's such another interesting topic in DynamoDB and it's something

346
00:27:51.840 --> 00:27:56.640
that can allow for other access patterns.

347
00:27:56.640 --> 00:28:00.960
And we've talked about one type of index, kind of, so far because we mentioned primary keys, but you can actually add additional keys to

348
00:28:00.960 --> 00:28:05.280
support querying by fields that are not in the primary key. As we've said, and you've talked

349
00:28:05.280 --> 00:28:10.640
through the query semantics, you need to specify the partition key. If you want to do more granular

350
00:28:10.640 --> 00:28:16.000
filtering, you need to use a key expression, but what about different access patterns?

351
00:28:16.000 --> 00:28:20.160
What if you need to query by something else entirely? Well, that's where indexes come in and they're called

352
00:28:20.160 --> 00:28:25.920
secondary indexes in this case. There's two types of secondary index. There's the local secondary

353
00:28:25.920 --> 00:28:31.200
index, which is stored together on the same partition as your database. And because of that,

354
00:28:31.200 --> 00:28:35.520
the partition key is always the same as your primary key's partition key. And only the sort

355
00:28:35.520 --> 00:28:39.840
key is different. Then you have global secondary indexes, which are stored separately, and they can

356
00:28:39.840 --> 00:28:43.680
have a different partition key and sort key. For that reason, they're a lot more common.

357
00:28:43.680 --> 00:28:48.640
Local secondary indexes have a few more limitations and they also share the table's capacity,

358
00:28:48.640 --> 00:28:53.040
whereas global secondary indexes have their own capacity. And when you hear people talking about

359
00:28:53.040 --> 00:28:56.800
global secondary indexes and local secondary indexes, because they're a bit of a mouthful,

360
00:28:56.800 --> 00:29:02.320
they'll normally say GSI and any kind of secondary index allows you to retrieve atom attributes from

361
00:29:02.320 --> 00:29:07.680
a set of related records by different keys. You can imagine having a DynamoDB table that stores

362
00:29:07.680 --> 00:29:13.040
customer orders. And normally you retrieve it by customer ID and maybe date for the sort key so

363
00:29:13.040 --> 00:29:17.680
that you can filter by date. But you might also want to retrieve by product ID and amount.

364
00:29:17.680 --> 00:29:22.480
So you could put product ID and amount in as a separate global secondary index. One of the cool things

365
00:29:22.480 --> 00:29:27.120
about indexes is that they can actually be sparse. So what does that mean? Well, if some of the

366
00:29:27.120 --> 00:29:31.760
attributes in your index aren't present in any item that you're inserting into a table, your

367
00:29:31.760 --> 00:29:35.760
index doesn't actually need to store that record at all. So the volume of data in an index could be

368
00:29:35.760 --> 00:29:40.800
much less than in the table itself. And because of that, indexes can actually be used as like a

369
00:29:40.800 --> 00:29:46.720
materialized view or a filter on data because it's already pre-filtered based on whether those

370
00:29:46.720 --> 00:29:52.880
attributes are present or not. And that's quite a common pattern for GSIs. You can also use indexes

371
00:29:52.880 --> 00:29:57.600
to store different but related entities together in the one table. So we talked about storing

372
00:29:57.600 --> 00:30:03.040
customer orders, but what if you wanted to store customers' orders and products and query them

373
00:30:03.040 --> 00:30:08.000
together? You can actually do that in DynamoDB. And you do that by overloading the partition keys

374
00:30:08.000 --> 00:30:13.200
and sort key values so that you can query them individually and using more indexes as well to be

375
00:30:13.200 --> 00:30:18.000
able to support more and more query patterns. And this approach is called single table design.

376
00:30:18.560 --> 00:30:24.800
And it typically means having a naming convention in your keys, like having a partition key,

377
00:30:24.800 --> 00:30:29.760
which has a syntax like customer hash, and then a customer ID. And then maybe in your sort key,

378
00:30:29.760 --> 00:30:34.080
you'll have a order hash order ID. And then you might have a separate product ID column,

379
00:30:34.080 --> 00:30:39.760
which is used in a secondary index to query the product. It's a total shift from the simplicity

380
00:30:39.760 --> 00:30:44.000
of the default DynamoDB approach. And it's incorporating relational modeling from relational

381
00:30:44.000 --> 00:30:49.600
databases, but it allows you to get the best of both worlds with some trade-offs, but you can

382
00:30:49.600 --> 00:30:54.800
actually implement relational design in this way. And this all came about, well, it's been around

383
00:30:54.800 --> 00:30:59.600
for a while, I guess, but it was popularized when Rick Houlihan, who used to work at AWS advising

384
00:30:59.600 --> 00:31:04.720
all of their amazon.com teams on how to do this. He gave a series of very famous re-invent talks

385
00:31:04.720 --> 00:31:08.880
describing advanced DynamoDB modeling. And this really gave a lot of momentum to the idea of single table design.

386
00:31:08.880 --> 00:31:12.320
I remember seeing this talk and thinking, wow, this is amazing, but

387
00:31:12.880 --> 00:31:17.520
I had to watch it a few times to really understand it because it's kind of mind-meltingly high speed

388
00:31:17.520 --> 00:31:24.320
and deep dive, like the most level 400 talk I've seen. And then Alex DeBrie gave a much more

389
00:31:24.320 --> 00:31:28.720
accessible guide on it in his great DynamoDB book. Yeah, he has got a lot of great content

390
00:31:28.720 --> 00:31:33.040
around DynamoDB, so much so that I'm surprised they haven't renamed it to DynamoDeBrie at this point!

391
00:31:33.040 --> 00:31:37.760
So the fundamental idea with single table design is if you know your access patterns ahead

392
00:31:37.760 --> 00:31:42.160
of time, you can design your DynamoDB table indexes and keys to store all of this data,

393
00:31:42.160 --> 00:31:45.520
related data together. So that could be created together and it can allow you to do all this

394
00:31:45.520 --> 00:31:50.400
relational modeling, but still gain from the performance and scalability of DynamoDB.

395
00:31:51.120 --> 00:31:55.920
Unfortunately, it's not really very easy to grasp and do well. I'm still afraid of it,

396
00:31:55.920 --> 00:32:00.080
to be honest. Even if you do do it, it can be difficult for others on your team to understand

397
00:32:00.080 --> 00:32:04.960
and troubleshoot when they join the team. Even I've seen single table designs, which I've

398
00:32:04.960 --> 00:32:08.880
implemented and understood, and then gone back to it a few months later and thought,

399
00:32:08.880 --> 00:32:13.600
what is this schema? I can't remember how this is modeled. And people have tried to provide

400
00:32:13.600 --> 00:32:18.000
tooling around that to make it easier. And that has helped to design it, but I still don't see

401
00:32:18.000 --> 00:32:22.160
a great solution to ultimately making it accessible and understandable for everybody. Of course,

402
00:32:22.160 --> 00:32:26.640
we mentioned that you need your access patterns well-documented and understood ahead of time. So

403
00:32:26.640 --> 00:32:30.560
if they change, you need to be able to plan and execute a schema change and a migration later.

404
00:32:30.560 --> 00:32:34.320
So it's not a silver bullet. And while it looks really cool and it's very appealing,

405
00:32:34.320 --> 00:32:38.720
I would tend to say, don't get caught up in it and don't worry about it too much. What do you

406
00:32:38.720 --> 00:32:43.520
think, Luciano? I mostly agree with what you said there.

407
00:32:43.520 --> 00:32:48.160
The only thing I can add is that I found that it might help a little bit if you try to abstract all of that stuff in your code,

408
00:32:48.160 --> 00:32:51.600
meaning that you are going to use something like the repository pattern to say,

409
00:32:51.600 --> 00:32:57.120
well, I have a code layer where I can just say, give me all the, I don't know, products,

410
00:32:57.120 --> 00:33:02.560
or give me what's in the cart for this customer. And behind the scenes, you have abstracted all the

411
00:33:02.560 --> 00:33:07.840
necessary logic to integrate with DynamoDB from a team perspective that may make things a little

412
00:33:07.840 --> 00:33:11.920
bit easier because you are not necessarily required to go and look under the wood to exactly

413
00:33:11.920 --> 00:33:16.640
see what's happening with DynamoDB. But of course, as you say, then if you eventually find yourself

414
00:33:16.640 --> 00:33:20.320
in the position where you need to change the data structure to accommodate for different access

415
00:33:20.320 --> 00:33:24.240
patterns, then somebody will need to be able to touch that layer and make the necessary

416
00:33:24.240 --> 00:33:29.200
adjustments. So this is not necessarily a silver bullet. It's just, I guess, good code practices

417
00:33:29.200 --> 00:33:33.840
that might create abstraction layers that can be more accessible to a larger group of people in the

418
00:33:33.840 --> 00:33:39.440
team. So that's maybe something else to consider if you do find yourself using the single table

419
00:33:39.440 --> 00:33:43.440
design pattern, if you see value in it, and there is definitely value, that can be one of the

420
00:33:43.440 --> 00:33:47.760
practices you can use to make your life as a team a little bit easier.

421
00:33:47.760 --> 00:33:52.800
And I think at this point, we've covered enough ground when it comes to DynamoDB. This was a longer episode that we

422
00:33:52.800 --> 00:33:58.400
generally do, and hopefully you enjoyed it anyway. We tried to share as much as we could about the

423
00:33:58.400 --> 00:34:03.520
basics of DynamoDB, what it is, how does it compare with relational databases, how do you use it,

424
00:34:03.520 --> 00:34:09.120
even up to talking about the single table design pattern. And of course, don't forget that if you

425
00:34:09.120 --> 00:34:13.200
decide to use DynamoDB, don't forget that relational databases are still pretty ubiquitous.

426
00:34:13.520 --> 00:34:18.160
In a way, if you're using AWS, it makes sense to adopt DynamoDB, but you always need to look at

427
00:34:18.160 --> 00:34:21.760
your requirements and make sure you make it a conscious decision. Definitely, there are many

428
00:34:21.760 --> 00:34:26.800
advantages in using DynamoDB, but also we can say the same when it comes to traditional relational

429
00:34:26.800 --> 00:34:33.200
databases and SQL. So don't be feeling like you are missing out if you prefer to use a relational

430
00:34:33.200 --> 00:34:38.480
database rather than DynamoDB. I think there are still many ways to use relational databases and

431
00:34:38.480 --> 00:34:45.200
make them scale in the cloud even at very high scale. So I think we will love to hear more from

432
00:34:45.200 --> 00:34:49.600
you if you're using DynamoDB, if you totally ditch relational databases, or if you are still

433
00:34:49.600 --> 00:34:54.880
feeling more attached to relational databases than to DynamoDB. And maybe hear about the stories that

434
00:34:54.880 --> 00:34:59.600
you might have, if you have any scar from DynamoDB or any scar from relational databases. It would be

435
00:34:59.600 --> 00:35:05.200
nice to put these ideas into context because I think the context is really the key here.

436
00:35:05.200 --> 00:35:09.440
It's not really like one technology is better than the other. Different use cases might be more suitable

437
00:35:09.440 --> 00:35:13.360
for different types of technologies. With that, we will leave you some additional resources in

438
00:35:13.360 --> 00:35:18.640
the show notes. We will link the DynamoDB book that we mentioned by Alex DeBrie, but we will also

439
00:35:18.640 --> 00:35:23.440
link Alex's podcast and YouTube channel where you can find additional content and we will share some

440
00:35:23.440 --> 00:35:27.440
of the talks we mentioned about Rick Houlihan. So thank you very much for being with us and we look

441
00:35:27.440 --> 00:35:35.520
forward to seeing you in the next episodes.
