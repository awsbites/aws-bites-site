WEBVTT

1
00:00:00.000 --> 00:00:07.360
Hello and welcome. This is AWS Bites episode 136. We were not planning on doing a pre-invent episode,

2
00:00:07.560 --> 00:00:14.060
but there has been so much new and exciting stuff coming out of AWS in the last few weeks that we

3
00:00:14.060 --> 00:00:18.760
really wanted to share our take, what we think are the best announcements, and really there has been

4
00:00:18.760 --> 00:00:23.500
a fire hose of announcements. But by the way, don't worry, we're not going to cover any GenAI stuff.

5
00:00:23.720 --> 00:00:28.760
This is going to be purely a Gen AI free episode, so please stick around and hopefully you'll enjoy

6
00:00:28.760 --> 00:00:32.480
what we have to share. Of course, we are not going to cover every single announcement, but again,

7
00:00:32.600 --> 00:00:38.540
only what we think are our top picks from the last few weeks. My name is Luciano and today I'm joined

8
00:00:38.540 --> 00:00:53.960
by Eoin for another episode of AWS Bites podcast.

9
00:00:54.260 --> 00:00:58.680
AWS Bites is brought to you as always by fourTheorem. Sometimes AWS can be a little bit overwhelming and you might need someone to provide you a

10
00:00:58.680 --> 00:01:04.000
clear guidance in the fog of the cloud offerings. That someone is fourTheorem, so if that sounds

11
00:01:04.000 --> 00:01:10.080
interesting, check us out at fourtheorem.com. By the way, we have been updating our AWS Bites website.

12
00:01:10.320 --> 00:01:15.660
We added an amazing feature that allows you to search across all our episodes, including the

13
00:01:15.660 --> 00:01:21.100
transcripts, and this is powered by Orama Search. Our website is also all open source, so if you're

14
00:01:21.100 --> 00:01:25.460
curious, you can check out how we implemented all of that. We'll have a link in the show notes to the

15
00:01:25.460 --> 00:01:31.480
repo, and hopefully with this new feature, you can find all our episodes a little bit more easily.

16
00:01:31.840 --> 00:01:36.580
And if there is any topic that interests you with more than 130 episodes, hopefully there will be

17
00:01:36.580 --> 00:01:41.780
one episode covering what you're interested about. So please let us know what you think and if you find

18
00:01:41.780 --> 00:01:46.900
this feature useful. Okay, so let's get started. I think we want to cover a lot of announcements, but

19
00:01:46.900 --> 00:01:53.280
what's your first one, Eoin?

20
00:01:53.280 --> 00:02:00.140
We do a lot of stuff with Step Functions, and anyone who's worked with Step Functions will know that dealing with the JSONPath stuff, input path, output path, result selector,

21
00:02:00.360 --> 00:02:04.900
can be a little bit difficult. Now there's a whole new way to deal with state in Step Functions.

22
00:02:05.200 --> 00:02:11.320
So using JSONata, which is a standard I wasn't familiar with before, you can now have much larger

23
00:02:11.320 --> 00:02:17.200
state. You can have up to 10 megabytes of state stored in variables, each of which can be 256k,

24
00:02:17.200 --> 00:02:22.320
and you don't necessarily have to pass the state all the way through each step in your function.

25
00:02:22.880 --> 00:02:27.520
You can instead just have these variables in one state that you then reference later in another

26
00:02:27.520 --> 00:02:32.100
state. Now JSONata is much more powerful than JSON path, so it supports additional functions for

27
00:02:32.100 --> 00:02:37.880
transforming data, string manipulation, mathematics, doing daytime stuff, etc. And it's a lot more intuitive,

28
00:02:38.440 --> 00:02:42.520
and there's lots of like online playgrounds and stuff you can use to get familiar with it.

29
00:02:42.520 --> 00:02:47.220
Speaking of Step Functions, you can now also generate SAM and CloudFormation templates from

30
00:02:47.220 --> 00:02:50.440
the Step Functions console. So if you want to play around with the workflow editor,

31
00:02:50.860 --> 00:02:53.160
and then generate your SAM template, you can now do that.

32
00:02:53.360 --> 00:02:59.780
That's really cool. My first one is EC2 autoscaling, that now is much faster when doing target tracking.

33
00:03:00.140 --> 00:03:05.080
Used to be with the minute granularity, I believe, and now is going to the second.

34
00:03:05.620 --> 00:03:09.980
So that basically gives you much faster scaling up and down depending on your threshold.

35
00:03:09.980 --> 00:03:14.880
So that's pretty cool if you still rely on EC2 and you do lots of autoscaling groups.

36
00:03:15.100 --> 00:03:20.480
But going into a more serverless one, we have a couple of news regarding Lambda,

37
00:03:20.860 --> 00:03:26.820
and specifically regarding runtimes such as Node.js and Python, because finally we have two new

38
00:03:26.820 --> 00:03:32.780
amazing versions supported. So speaking of Node.js, we have Node.js 22 available for Lambda.

39
00:03:33.120 --> 00:03:36.420
And this is a release that I really like for a couple of reasons.

40
00:03:36.420 --> 00:03:42.600
The first one is that Node.js 22 gives you native fetch, so you don't need to do polyfilling anymore.

41
00:03:43.040 --> 00:03:48.500
And the other one that I care about even more is that Node.js 22 gives you experimental require,

42
00:03:48.900 --> 00:03:53.580
which is basically a feature that allows you to use the require for ESM.

43
00:03:53.660 --> 00:03:57.800
So if you are working on ESM mode, you can still require CommonJS modules.

44
00:03:58.340 --> 00:04:02.760
And this used to be a little bit of a problem for Middy, because when we published Middy 5,

45
00:04:02.760 --> 00:04:09.780
I think it was last year at some point, we decided to go a little bit wild and not support

46
00:04:09.780 --> 00:04:15.740
CommonJS anymore, because we wanted to be ESM first and actually ESM only, just for the simplicity

47
00:04:15.740 --> 00:04:20.880
of maintaining a package like that. And this had a little bit of a backlash in the community,

48
00:04:20.880 --> 00:04:25.820
because lots of people are still using CommonJS. And even if they want to migrate to ESM,

49
00:04:25.920 --> 00:04:32.320
it's not something you can just do overnight. So lots of people were effectively stuck with Middy 4.

50
00:04:32.320 --> 00:04:39.040
Now, to be fair, Middy 5 wasn't really adding anything new. So sticking with Middy 4 was effectively

51
00:04:39.040 --> 00:04:47.600
the same experience. And we kept maintaining both for a long time. But now you can just use Middy 5 and

52
00:04:47.600 --> 00:04:53.300
stay up to date with the latest version and still use CommonJS if you have to. By the way,

53
00:04:53.340 --> 00:04:59.960
we just published Middy 6 as well, which comes with some polish and some improvement. So check it out,

54
00:04:59.960 --> 00:05:06.120
we'll have the release note in the show notes if you're curious. But yeah, excited about Node.js 22

55
00:05:06.120 --> 00:05:12.720
and Middy 6 as well. And speaking about another runtime, Python as well got a new major release of

56
00:05:12.720 --> 00:05:19.900
the runtime. Now Python 3.13 is available as well. Now I don't use Python nearly as much as Node.js,

57
00:05:20.160 --> 00:05:24.960
so I wouldn't be able to say which new amazing features are becoming available with this new release.

58
00:05:24.960 --> 00:05:30.160
But I'm sure that if you are a Pythonista, you are excited about this one. And I'm sure that there

59
00:05:30.160 --> 00:05:33.460
are some amazing features that you can start to use in your Lambdas as well.

60
00:05:33.760 --> 00:05:40.440
I know that one of the things that 3.13 introduces, but it's still behind an experimental flag,

61
00:05:40.620 --> 00:05:45.560
is removing the GIL, the global interpreter lock from Python and adding free threading support.

62
00:05:45.560 --> 00:05:52.620
But it's really like the first step on the journey of adding that free threading support. So it doesn't

63
00:05:52.620 --> 00:05:57.820
apply if you're using Lambda because you won't get that experimental flag. But I think there's a lot

64
00:05:57.820 --> 00:06:03.040
of other improvements, nice language typing improvements, and also just performance improvements

65
00:06:03.040 --> 00:06:03.440
as well.

66
00:06:03.640 --> 00:06:08.940
That's nice. I wonder when the cold start benchmarks are going to come out, if there is any major

67
00:06:08.940 --> 00:06:15.220
revelation. But I don't know anything about that just yet. Okay, the next one I have is another

68
00:06:15.220 --> 00:06:21.040
exciting one in the serverless space, which is Aurora Serverless now scales to zero, which has been

69
00:06:21.040 --> 00:06:26.920
another big complaint for a long time. Because when Aurora Serverless v2 was announced, everyone was

70
00:06:26.920 --> 00:06:32.800
basically saying, well, this is not really serverless because it doesn't scale to zero. And now this is no

71
00:06:32.800 --> 00:06:38.300
longer the case. Now you can scale to zero and scale back up again. Basically takes around 15-20 seconds.

72
00:06:38.300 --> 00:06:44.720
So that makes Aurora truly a serverless database. And that's really exciting news. And I'm looking

73
00:06:44.720 --> 00:06:51.560
forward to start to play around with Aurora Serverless in, I guess, more context, because before I was only

74
00:06:51.560 --> 00:06:58.360
using it mostly for production use cases. Now it starts to become the case that you can use it even for

75
00:06:58.360 --> 00:07:02.800
development environments, because it's going to scale to zero when you are not actually using it.

76
00:07:02.900 --> 00:07:03.580
What else do we have?

77
00:07:04.140 --> 00:07:07.900
Well, on the topic of Lambda, Snapstart has been available for Java for a while.

78
00:07:07.900 --> 00:07:14.180
And now it's available for Python and .NET. And we got access to the beta for this before launch,

79
00:07:14.180 --> 00:07:19.300
and we were able to run some benchmarks. And with some heavy enough Python functions,

80
00:07:19.460 --> 00:07:23.680
we were able to get a 4x speedup, which is pretty impressive. You might remember we talked about

81
00:07:23.680 --> 00:07:28.300
Python cold start performance in a previous episode. So this is a really a good thing.

82
00:07:28.520 --> 00:07:32.160
Now, there's always a downside when you've got these new features, there's always some trade-offs to

83
00:07:32.160 --> 00:07:38.240
consider. So just to mention, you have to use published versions for functions.

84
00:07:38.520 --> 00:07:43.940
So you can't just use $LATEST. And it's only available for ZIP-packaged functions. It's not

85
00:07:43.940 --> 00:07:49.540
available for OCI container images at the moment. Now, some people were surprised by the fact that

86
00:07:49.540 --> 00:07:54.540
you have to pay extra for Snapstart for Python and .NET, because that wasn't the case with Java. But

87
00:07:54.540 --> 00:08:00.800
since Lambda has to store your snapshots, and then when it's restoring them, it has to allocate resources,

88
00:08:01.600 --> 00:08:07.180
I suppose it makes sense. And maybe Java developers were just lucky to get a bit of a free ride on that

89
00:08:07.180 --> 00:08:12.120
one as it was introduced. But yeah, I think it's definitely worth checking it out if you're really

90
00:08:12.120 --> 00:08:18.200
concerned about cold starts in Python and .NET. Now, another thing we covered in a previous episode

91
00:08:18.200 --> 00:08:25.160
was Mountpoint for S3. We did a whole episode on it in 1995. Right now, you can use, you can,

92
00:08:25.560 --> 00:08:31.420
now you have extra caching available for Mountpoint for S3. So you can use an Express One Zone bucket

93
00:08:31.420 --> 00:08:37.760
as a read cache for Mountpoint. So I think Express One Zone was announced, announced that reinvented last

94
00:08:37.760 --> 00:08:42.480
year. It's a single availability zone bucket. So you don't get the high availability, but you get much

95
00:08:42.480 --> 00:08:48.140
better performance. So if you're using this as a read cache with Mountpoint, then your reads will be up to

96
00:08:48.140 --> 00:08:53.080
seven times faster, they say. And speaking of Express One Zone again, there's a really cool new

97
00:08:53.080 --> 00:09:00.060
feature for this, which is that you can now append data to existing objects. So if you're doing things

98
00:09:00.060 --> 00:09:05.720
like creating log files and appending to them, or doing live video streaming, this could definitely

99
00:09:05.720 --> 00:09:10.480
be very useful. And then how do you do it? Well, it's actually just a normal put object call,

100
00:09:10.700 --> 00:09:18.040
and now you just can add an offset header, and that will write an append block to your object.

101
00:09:18.140 --> 00:09:23.800
And on the topic of S3, if you have a serious bucket addiction, AWS definitely has you covered,

102
00:09:23.920 --> 00:09:27.960
because you can now create up to 1 million buckets. Coincidentally, this is about the same

103
00:09:27.960 --> 00:09:33.480
number of announcements AWS made in the last year, featuring the phrase Amazon Q. Now, like any good

104
00:09:33.480 --> 00:09:39.020
enabler, they give you the first 2,000 buckets for free, but there is actually a price per bucket

105
00:09:39.020 --> 00:09:42.980
after that, so don't go to town on your 1 million bucket limit.

106
00:09:42.980 --> 00:09:49.480
The next one I have is about DynamoDB, and it's about price slashing.

107
00:09:49.760 --> 00:09:54.900
Because now, if you use on-demand throughput, it's half the price than what it used to be. And also,

108
00:09:55.000 --> 00:10:00.440
global tables got interesting price slashing, because now they are one-third of the price.

109
00:10:00.640 --> 00:10:06.400
So this is super interesting to me, because I always try to use on-demand, but then I'm always a

110
00:10:06.400 --> 00:10:12.120
little bit concerned about price in production. And then I try to think, okay, where is the point

111
00:10:12.120 --> 00:10:17.500
where I need to start moving to provision mode and do all the calculation, and then make sure I

112
00:10:17.500 --> 00:10:22.500
scale accordingly and manage all of the stuff that, with the serverless mindset, I really don't want

113
00:10:22.500 --> 00:10:27.340
to manage. I just want the system to do everything on its own and be cheap at the same time. So I think

114
00:10:27.340 --> 00:10:31.080
now we are getting a little bit closer to that vision, and maybe I wouldn't have to worry

115
00:10:31.080 --> 00:10:37.100
so much anymore about provision concurrency. So I think now it's going to be easy for most people

116
00:10:37.100 --> 00:10:41.940
to just stick with on-demand throughput and have reasonable prices. Probably they would be even

117
00:10:41.940 --> 00:10:47.500
cheaper than on-demand prices for most cases, and they will scale much better without you having to

118
00:10:47.500 --> 00:10:52.120
do a lot of homework on your site. We'll have a link on the show notes if you are curious to read

119
00:10:52.120 --> 00:10:56.720
more about this particular announcement. But moving on to the next one, we have ALB headers,

120
00:10:56.720 --> 00:11:02.800
which is an interesting feature that seems like a small thing, but can actually be quite useful.

121
00:11:03.240 --> 00:11:08.720
But basically now with application load balancers, you can inject custom headers when effectively

122
00:11:08.720 --> 00:11:14.100
targeting your load balancer targets.

123
00:11:14.100 --> 00:11:19.620
It was a pretty unexpected move because AWS has given Cognito a lot of attention and just announced a whole lot of new features. And another unusual move

124
00:11:19.620 --> 00:11:24.920
was that they added these kind of SaaS-like pricing tiers, which is a bit of a departure from AWS's

125
00:11:24.920 --> 00:11:32.420
usual pricing model. So now there are three tiers, Lite, Essential, and Plus. So Lite is essentially

126
00:11:32.420 --> 00:11:37.820
what you used to get before. It's the legacy tier, really. And then all the new features are in the

127
00:11:37.820 --> 00:11:44.000
new tiers. So Essential will give you a new managed login UI. It has support for passwordless

128
00:11:44.000 --> 00:11:49.920
authentication, and it allows you to do things like customize access tokens as well. So if you're creating

129
00:11:49.920 --> 00:11:54.300
a new user pool now, it'll be in the Essentials tier rather than the Lite tier. And then there's a

130
00:11:54.300 --> 00:11:59.120
the plus tier is essentially for enterprise features like compromised password detection,

131
00:11:59.480 --> 00:12:05.900
audit logging, and risk-based adaptive authentication, which is basically trying to detect if it may be a

132
00:12:05.900 --> 00:12:12.260
malicious login and seeking extra verification from the user. So going on to the new features then,

133
00:12:12.740 --> 00:12:17.500
the managed login is an interesting one because previously we've used the hosted UI that they used

134
00:12:17.500 --> 00:12:24.060
to have. It's still there, but as part of the Lite package. The new managed sign-in allows you to do

135
00:12:24.060 --> 00:12:29.520
much more richer UI customization. I think there's a drag and drop UI as well. So I'm looking forward to

136
00:12:29.520 --> 00:12:34.760
looking at that and seeing how well that works. Then you also get passwordless authentication with

137
00:12:34.760 --> 00:12:43.440
pass keys, magic links, and SMS. And those are really, I think, great new additions. It's good to see

138
00:12:43.440 --> 00:12:48.860
Cognito getting this level of attention. There's also some add-ons that you can pay for, like if you

139
00:12:48.860 --> 00:12:52.720
want to do machine-to-machine authorization. And then there's also a whole set of packages you can

140
00:12:52.720 --> 00:12:57.680
buy at like fixed price per month that will allow you to get higher throughput on specific operations

141
00:12:57.680 --> 00:13:02.180
you want. So if you're really running Cognito at scale, that's definitely worth a look. Now,

142
00:13:02.660 --> 00:13:08.840
Lite and Essentials both have a free tier for 1,000, sorry, not 1,000, but 10,000 monthly active users.

143
00:13:08.840 --> 00:13:13.840
That's still a pretty generous free tier, I think. Now, the old Cognito pricing model,

144
00:13:14.060 --> 00:13:19.100
people often said it was ridiculously cheap. It was a real competitive advantage, actually,

145
00:13:19.400 --> 00:13:25.280
that kind of allowed people to adopt Cognito and take some of the pain that was associated with the

146
00:13:25.280 --> 00:13:30.800
Cognito developer experience. But I think if they are going to improve that developer experience and

147
00:13:30.800 --> 00:13:37.480
add new features and give it continuous love, then it makes sense to fund that work by having a more

148
00:13:37.480 --> 00:13:43.820
sensible pricing tier. So the pricing difference, Essentials is about two and a half times per user

149
00:13:43.820 --> 00:13:50.600
compared to the old pricing. So I don't think it's outrageous, but you'll have to do your own

150
00:13:50.600 --> 00:13:54.300
mathematics on it. And then if you're using the enterprise tier, it seems to be about an extra

151
00:13:54.300 --> 00:14:01.400
half a cent for monthly active user.

152
00:14:01.400 --> 00:14:07.460
The next one I have is VPC block public access. So if you heard about block public access for S3, it's basically the same thing for VPC, which means that you have a

153
00:14:07.460 --> 00:14:13.980
central control that blocks internet access to your VPC, regardless of any internet gateway that you

154
00:14:13.980 --> 00:14:19.980
can have in those VPCs. So pretty interesting if you are a network security nerd, I think you might

155
00:14:19.980 --> 00:14:25.200
like this one. And sticking with networking, we have another interesting feature announcement

156
00:14:25.200 --> 00:14:32.180
involving Lattice. So we spoke about VPC Lattice in episode 88. So check that out if you want to have

157
00:14:32.180 --> 00:14:37.960
refresher. And we talked about how you need to have a load balancer in front of an ECS in order to use

158
00:14:37.960 --> 00:14:42.780
it with Lattice. And we also said that maybe that would be improved in the future. So you wouldn't

159
00:14:42.780 --> 00:14:48.160
have to pay the extra cost and latency. Now that future is finally here. I don't know if it was just

160
00:14:48.160 --> 00:14:52.520
a coincidence or maybe somebody at AWS actually listened to that episode. We'd like to think that

161
00:14:52.520 --> 00:14:58.600
they did. But yeah, you can now plug ECS directly into Lattice without a load balance. So yeah,

162
00:14:58.660 --> 00:15:02.700
again, check out the announcement in the show notes if you're curious to find out more.

163
00:15:02.960 --> 00:15:06.360
Yeah, that's a good one. I think Lattice is underappreciated and underutilized. That was a

164
00:15:06.360 --> 00:15:11.040
blocker for people in the past. I can see more adoption now. There's a new type of policy. We've

165
00:15:11.040 --> 00:15:15.980
also done episodes, I think, on all of the different types of policy you can have with IAM and SCPs,

166
00:15:16.040 --> 00:15:20.840
permissions, boundaries. And now we have another one. So it's called resource control policies.

167
00:15:20.840 --> 00:15:25.940
And it's essentially like a service control policy, but for resources. So service control

168
00:15:25.940 --> 00:15:31.260
policies are policies that apply to all the principles in an account or in an organizational

169
00:15:31.260 --> 00:15:36.640
unit. RCPs, resource control policies, can be used to apply policies to all supported resources

170
00:15:36.640 --> 00:15:43.460
like S3 buckets, SQS Queues, that sort of thing. So with it, you can do things like centrally deny access

171
00:15:43.460 --> 00:15:48.760
to buckets from outside your organization. So it's a pretty nice way to do it rather than having to

172
00:15:48.760 --> 00:15:54.540
monitor each individual bucket's resource policy. Now, we seem to be getting a lot of free stuff

173
00:15:54.540 --> 00:16:00.640
and price reductions in this pre-reinvent, which is really nice. And one of them is that event

174
00:16:00.640 --> 00:16:05.420
bridge is even faster. I think back in episode 23, we talked about EventBridge. And one of the

175
00:16:05.420 --> 00:16:10.760
points we made then was how it was not the best compared to SNS, Kinesis, et cetera, when it came

176
00:16:10.760 --> 00:16:14.660
to latency. That was over two years ago. And performance has now improved massively,

177
00:16:14.660 --> 00:16:20.800
like over 90%, I think. So you can now expect delivery in around 130 milliseconds,

178
00:16:21.300 --> 00:16:23.780
which is pretty cool. So hats off to the EventBridge team.

179
00:16:24.140 --> 00:16:31.360
Nice. And the last one I have is AppSync WebSocket support. So AppSync now has a new way to do WebSockets.

180
00:16:31.440 --> 00:16:37.940
Previously, you had three options for WebSockets on AWS, AWS IoT, API Gateway, and AppSync with GraphQL.

181
00:16:38.480 --> 00:16:43.640
Now there is a whole dedicated service called AppSync Events, which should be much simpler to set up

182
00:16:43.640 --> 00:16:50.720
than all the other options I mentioned. So that's basically all we have for our pre-invent selection.

183
00:16:51.080 --> 00:16:56.400
There might be maybe some last minute pre-invent announcement. Otherwise, we'd just be curious to

184
00:16:56.400 --> 00:17:01.020
see what comes up at re-invent. Oh, and by the way, I will personally go and attend re-invent.

185
00:17:01.400 --> 00:17:05.240
So if you are also there and you would like to catch up in person, I'd love to do that.

186
00:17:05.240 --> 00:17:10.420
So please reach out on our social channel and let's schedule something together.

187
00:17:10.420 --> 00:17:16.400
So that's all we have for today. I'm curious to know if you have any favorite pre-invent announcement.

188
00:17:16.700 --> 00:17:20.340
Let us know as well. And as always, we'd love to hear your opinion.

189
00:17:20.740 --> 00:17:22.900
And until that, we'll see you next time.
