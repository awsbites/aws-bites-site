WEBVTT

1
00:00:00.000 --> 00:00:03.260
Hello and welcome to another episode of AWS Bites.

2
00:00:03.340 --> 00:00:05.080
My name is Luciano and I'm joined by Eoin.

3
00:00:05.560 --> 00:00:08.120
And we wanted to talk about DuckDB for a while.

4
00:00:08.280 --> 00:00:10.600
And today we finally get to do just that.

5
00:00:11.020 --> 00:00:13.480
So a few years ago, we started to hear lots of excitement

6
00:00:13.480 --> 00:00:16.220
from our data scientists and analysts friends

7
00:00:16.220 --> 00:00:19.660
and all colleagues and everyone was talking about DuckDB

8
00:00:19.660 --> 00:00:21.480
and they were super excited

9
00:00:21.480 --> 00:00:23.960
and we couldn't figure out exactly why.

10
00:00:24.100 --> 00:00:26.760
But finally, we have used it in a few different projects

11
00:00:26.760 --> 00:00:28.240
and now we really understand

12
00:00:28.240 --> 00:00:30.040
why everyone is excited about that.

13
00:00:30.300 --> 00:00:31.620
So today we want to share with you

14
00:00:31.620 --> 00:00:34.200
how DuckDB is becoming a must-have tool

15
00:00:34.200 --> 00:00:37.460
for anyone who has some data on a S3 bucket somewhere

16
00:00:37.460 --> 00:00:40.020
and will need to do some kind of analytical job with that data.

17
00:00:40.340 --> 00:00:42.780
So we'll share some use cases and how to get started.

18
00:00:43.360 --> 00:00:46.320
And by the way, there is a little bit of a treat at the end.

19
00:00:46.660 --> 00:00:50.140
If you are missing the recently retired S3 Select feature on AWS,

20
00:00:50.760 --> 00:00:53.340
stay tuned until the end, we might have a surprise for you.

21
00:00:53.340 --> 00:00:55.820
Hopefully a better option that you can use today.

22
00:00:56.120 --> 00:00:57.480
So let's get into it.

23
00:00:58.240 --> 00:01:09.520
AWS Bites is brought to you by Forteorem, our sponsor.

24
00:01:10.300 --> 00:01:13.060
And if you're looking for a partner to architect, develop,

25
00:01:13.200 --> 00:01:15.820
and modernize on AWS, give Forteorem a call.

26
00:01:16.220 --> 00:01:18.920
You can check out all the details at fourtheorem.com.

27
00:01:19.140 --> 00:01:22.440
So Eoin, maybe we can start by giving a little bit of an explanation

28
00:01:22.440 --> 00:01:23.860
of what DuckDB is.

29
00:01:23.860 --> 00:01:28.240
Well, it's a database, but specifically it's an in-memory analytical database.

30
00:01:28.900 --> 00:01:31.860
So when you think in-memory, you might think of things like SQLite,

31
00:01:32.300 --> 00:01:37.700
sometimes known as SQLite, but this is for analytical or OLAP workloads.

32
00:01:37.820 --> 00:01:40.460
So you can therefore run it locally because it's in-memory,

33
00:01:40.640 --> 00:01:43.280
but you can also run it in the cloud, basically anywhere.

34
00:01:44.140 --> 00:01:45.600
And it's SQL first.

35
00:01:45.600 --> 00:01:49.980
So anyone with knowledge of SQL can pick it up and use it really very quickly.

36
00:01:50.360 --> 00:01:52.320
It works with a lot of different storage formats.

37
00:01:53.080 --> 00:01:58.660
It has its own format, but you can also use it with Parquet, Iceberg, CSV data, JSON data,

38
00:01:58.980 --> 00:01:59.220
Arrow.

39
00:01:59.560 --> 00:02:03.980
And one of the really big draws of it is that it's very simple to use,

40
00:02:04.040 --> 00:02:06.140
and it has managed to stay very simple to use.

41
00:02:06.320 --> 00:02:10.840
It doesn't have any dependencies, works on pretty much any modern OS or CPU,

42
00:02:10.840 --> 00:02:14.400
and it has language bindings for almost everything you might need.

43
00:02:14.640 --> 00:02:15.480
It's also pretty modular.

44
00:02:16.060 --> 00:02:19.820
So new features are automatically enabled by installing extensions,

45
00:02:20.160 --> 00:02:22.880
and you can even write your own custom extensions as well.

46
00:02:23.660 --> 00:02:24.580
And it's open source.

47
00:02:25.440 --> 00:02:28.000
I heard an interview at one of the founders of DuckDB recently,

48
00:02:28.300 --> 00:02:30.700
and they were talking about the different model, the governance model,

49
00:02:30.920 --> 00:02:33.840
but they basically set up a separate foundation holding all of its IP,

50
00:02:34.480 --> 00:02:37.220
keeping it clear of all the commercial entities

51
00:02:37.220 --> 00:02:42.580
to ensure that the community doesn't get any nasty license-switching surprises in the future.

52
00:02:42.860 --> 00:02:44.220
So that's the intro.

53
00:02:44.540 --> 00:02:46.220
Luciano, what is DuckDB good for?

54
00:02:46.480 --> 00:02:49.080
Yeah, you already mentioned that you can run it anywhere.

55
00:02:49.320 --> 00:02:52.440
That's probably one of the main things worth to mention.

56
00:02:52.700 --> 00:02:55.320
So you can run it on your laptop, on an EC2 instance,

57
00:02:55.820 --> 00:02:59.360
in a container running, again, on your machine or somewhere in the cloud.

58
00:02:59.760 --> 00:03:01.060
You can even run it in Lambda,

59
00:03:01.320 --> 00:03:03.480
which is probably the coolest use case, if you ask me.

60
00:03:03.480 --> 00:03:08.540
So one of the biggest benefits is how fast it is on modest hardware.

61
00:03:08.660 --> 00:03:13.900
So you don't need to run it in high-end machines or buy expensive VMs.

62
00:03:14.040 --> 00:03:17.300
You can run it pretty much in almost any reasonable hardware.

63
00:03:17.720 --> 00:03:21.540
And it has a very efficient multicore columnar execution engine,

64
00:03:21.700 --> 00:03:25.640
which is able to parallelize fetching and execution of the queries

65
00:03:25.640 --> 00:03:27.560
that you provide to analyze your data.

66
00:03:27.940 --> 00:03:31.140
So you can also work with more data than available in memory,

67
00:03:31.140 --> 00:03:34.720
which is another interesting detail for something that is defined

68
00:03:34.720 --> 00:03:36.060
as an in-memory database.

69
00:03:36.460 --> 00:03:41.520
And effectively, we are used to massive clusters or data warehouse

70
00:03:41.520 --> 00:03:43.520
required for querying big data.

71
00:03:44.440 --> 00:03:46.560
DuckDB kind of changes that a little bit

72
00:03:46.560 --> 00:03:49.640
because it solves a lot of the problem that we typically would solve

73
00:03:49.640 --> 00:03:50.800
with these massive clusters,

74
00:03:51.100 --> 00:03:53.140
but with a much simpler approach

75
00:03:53.140 --> 00:03:56.560
and a much more cost-effective approach.

76
00:03:56.560 --> 00:04:01.280
And I think that's probably one of the main reasons

77
00:04:01.280 --> 00:04:04.540
why everyone is so excited about DuckDB.

78
00:04:05.340 --> 00:04:08.860
So they say that one of the main reasons

79
00:04:08.860 --> 00:04:10.400
why DuckDB can be so efficient

80
00:04:10.400 --> 00:04:14.260
is that the engine is particularly well engineered,

81
00:04:14.480 --> 00:04:18.160
it's a columnar vectorized type of engine,

82
00:04:18.380 --> 00:04:22.360
and it can process large numbers of rows for a column at once.

83
00:04:22.360 --> 00:04:27.100
So this is why it's very well suited for analytical type of workloads.

84
00:04:27.500 --> 00:04:30.840
So because DuckDB can support more data than it can fit in memory,

85
00:04:31.000 --> 00:04:34.020
you often hear stories of people processing terabytes of data,

86
00:04:34.320 --> 00:04:35.420
even in their laptops.

87
00:04:35.900 --> 00:04:39.560
And I'm seeing lots of small examples and use cases

88
00:04:39.560 --> 00:04:40.840
of people talking online.

89
00:04:40.960 --> 00:04:42.960
For instance, one common use case would be

90
00:04:42.960 --> 00:04:45.680
if you want to reimplement your own version of Google Analytics,

91
00:04:46.140 --> 00:04:47.740
people are trying to do that with DuckDB

92
00:04:47.740 --> 00:04:50.040
and seems to be extremely easy to build

93
00:04:50.040 --> 00:04:51.920
and extremely efficient and cost-effective.

94
00:04:52.180 --> 00:04:53.660
So that's just to give you an idea

95
00:04:53.660 --> 00:04:55.220
of the kind of things you could be building

96
00:04:55.220 --> 00:04:56.400
with something like DuckDB.

97
00:04:56.700 --> 00:04:58.040
So where do we get started?

98
00:04:58.640 --> 00:05:00.720
Yeah, the best way to take it for a test drive

99
00:05:00.720 --> 00:05:02.400
is just to install the CLI.

100
00:05:02.560 --> 00:05:06.200
So you can do that for any Linux, Mac OS, Windows.

101
00:05:06.800 --> 00:05:09.000
Once you do that, you can run the DuckDB command

102
00:05:09.000 --> 00:05:12.420
and you have an interactive SQL REPL.

103
00:05:12.600 --> 00:05:13.580
So you can start writing SQL

104
00:05:13.580 --> 00:05:15.320
and you can run in memory,

105
00:05:15.440 --> 00:05:16.760
which it would do by default,

106
00:05:16.760 --> 00:05:18.640
or use a DuckDB file.

107
00:05:18.880 --> 00:05:23.380
I think 95% of the use cases I've seen,

108
00:05:23.700 --> 00:05:25.160
you're just using the in-memory case.

109
00:05:25.640 --> 00:05:27.400
But there's a lot of advantages

110
00:05:27.400 --> 00:05:29.160
if you want to persist the data as well.

111
00:05:29.780 --> 00:05:31.540
So if you just want to query data on S3,

112
00:05:31.840 --> 00:05:32.540
in-memory is fine.

113
00:05:32.860 --> 00:05:35.020
You could start with something like select star from

114
00:05:35.020 --> 00:05:38.440
and then just give the URI for your S3 object.

115
00:05:38.620 --> 00:05:39.660
And that could be a single file

116
00:05:39.660 --> 00:05:40.940
or it could be a group of files.

117
00:05:41.980 --> 00:05:44.380
And then you can get into more complex stuff

118
00:05:44.380 --> 00:05:45.500
like partitioning.

119
00:05:45.500 --> 00:05:47.880
So if you're in the big data space,

120
00:05:48.140 --> 00:05:50.620
you'll be familiar with hive partitioning

121
00:05:50.620 --> 00:05:51.760
on stores like S3,

122
00:05:51.960 --> 00:05:54.020
where you have certain parts of the path,

123
00:05:54.100 --> 00:05:56.920
which represent basically partitions of data,

124
00:05:57.100 --> 00:06:00.640
like date equals 2025, 02, 18.

125
00:06:01.520 --> 00:06:05.120
And when you have those dates in your where expression,

126
00:06:05.320 --> 00:06:07.320
it means it only has to read in that folder.

127
00:06:07.840 --> 00:06:09.560
You can do that with DuckDB as well.

128
00:06:09.720 --> 00:06:11.480
You can also create in-memory tables

129
00:06:11.480 --> 00:06:12.740
for intermediate results

130
00:06:12.740 --> 00:06:14.140
or indeed for assisted tables

131
00:06:14.140 --> 00:06:15.600
if you're not using in-memory mode.

132
00:06:15.600 --> 00:06:16.860
And then you can do joins.

133
00:06:17.340 --> 00:06:18.480
And it has, you know,

134
00:06:18.560 --> 00:06:21.520
very rich SQL support,

135
00:06:21.660 --> 00:06:23.980
but it also has custom functions

136
00:06:23.980 --> 00:06:26.240
for doing things like reading Parquet data

137
00:06:26.240 --> 00:06:27.420
with specific parameters

138
00:06:27.420 --> 00:06:32.080
and scanning lots of different partitions

139
00:06:32.080 --> 00:06:33.640
on a store like S3.

140
00:06:33.640 --> 00:06:34.620
So on AWS,

141
00:06:34.880 --> 00:06:36.980
then if you want to start using it in your application,

142
00:06:37.360 --> 00:06:39.640
there's lots of options for integrating it.

143
00:06:40.160 --> 00:06:41.780
Lambda, you mentioned already, Luciano,

144
00:06:42.200 --> 00:06:44.940
it lets you do simple and cheap data lake querying

145
00:06:44.940 --> 00:06:46.700
or even ETL pipelines

146
00:06:46.700 --> 00:06:48.080
if you're doing transformations.

147
00:06:48.080 --> 00:06:50.140
That's one of the places

148
00:06:50.140 --> 00:06:51.660
where I've found it very useful

149
00:06:51.660 --> 00:06:52.800
because you can do ETL

150
00:06:52.800 --> 00:06:54.240
a couple of lines of SQL

151
00:06:54.240 --> 00:06:55.840
just running in a Lambda function.

152
00:06:56.040 --> 00:06:57.340
And that's your job done.

153
00:06:57.500 --> 00:06:58.620
You don't have to stand up

154
00:06:58.620 --> 00:07:00.560
any kind of complicated

155
00:07:00.560 --> 00:07:03.820
or just heavier ETL infrastructure.

156
00:07:04.220 --> 00:07:05.740
And you can obviously run it in containers,

157
00:07:06.020 --> 00:07:08.460
for example, on ECS, Fargate, EKS,

158
00:07:08.620 --> 00:07:09.140
something like that.

159
00:07:09.540 --> 00:07:10.960
You can use it in AWS Glue,

160
00:07:11.320 --> 00:07:12.940
if you like, in your Python shell jobs.

161
00:07:13.360 --> 00:07:15.780
And you can also integrate it into step functions.

162
00:07:15.980 --> 00:07:16.840
And that's one of the things

163
00:07:16.840 --> 00:07:19.060
that I've found it really good for, actually,

164
00:07:19.140 --> 00:07:20.440
is putting it into step functions

165
00:07:20.440 --> 00:07:22.780
to do one step in your step function,

166
00:07:22.880 --> 00:07:24.640
which will do massive transformations

167
00:07:24.640 --> 00:07:25.660
on a lot of data.

168
00:07:26.240 --> 00:07:28.360
And you gave a preview at start.

169
00:07:28.780 --> 00:07:29.960
You said that we might have an alternative

170
00:07:29.960 --> 00:07:30.800
for S3 Select.

171
00:07:31.840 --> 00:07:34.440
Unfortunately, S3 Select has been retired

172
00:07:34.440 --> 00:07:37.480
since late last year for new AWS accounts.

173
00:07:38.160 --> 00:07:40.340
If you have existing accounts,

174
00:07:40.520 --> 00:07:41.840
you might still be able to use it.

175
00:07:42.320 --> 00:07:44.400
But it was a very handy feature.

176
00:07:44.500 --> 00:07:46.740
It was a console tool and an API.

177
00:07:46.840 --> 00:07:48.620
That you could use from the SDKs

178
00:07:48.620 --> 00:07:49.820
that would allow you to query data

179
00:07:49.820 --> 00:07:50.840
on S3 directly.

180
00:07:51.480 --> 00:07:52.880
It was quite a limited subset

181
00:07:52.880 --> 00:07:55.300
of SQL supported in S3 Select.

182
00:07:55.440 --> 00:07:57.240
You could basically just select

183
00:07:57.240 --> 00:07:59.900
with a where clause on a single file.

184
00:08:00.400 --> 00:08:02.420
But people really miss it since it's gone.

185
00:08:02.780 --> 00:08:03.760
And the good news with DuckDB

186
00:08:03.760 --> 00:08:05.040
is that you can replace it.

187
00:08:05.780 --> 00:08:07.560
Essentially, you can use the CLI

188
00:08:07.560 --> 00:08:08.680
as a drop-in replacement

189
00:08:08.680 --> 00:08:10.360
for what you would have done

190
00:08:10.360 --> 00:08:11.240
in the AWS console.

191
00:08:11.560 --> 00:08:13.360
But you can also use DuckDB

192
00:08:13.360 --> 00:08:15.960
as a programmatic replacement

193
00:08:15.960 --> 00:08:18.740
for the AWS SDK usage of S3 Select.

194
00:08:19.320 --> 00:08:20.380
And the good news is that

195
00:08:20.380 --> 00:08:21.380
you'll have a lot more features

196
00:08:21.380 --> 00:08:24.040
than you would have had with S3 Select.

197
00:08:24.460 --> 00:08:26.480
So we'll talk a little bit about that more

198
00:08:26.480 --> 00:08:28.120
because one of the recent use cases

199
00:08:28.120 --> 00:08:29.160
I came across was

200
00:08:29.160 --> 00:08:30.740
I wanted to use S3 Select

201
00:08:30.740 --> 00:08:31.520
in a step function

202
00:08:31.520 --> 00:08:33.000
just to do some filtering on data.

203
00:08:33.400 --> 00:08:34.520
It was in a new account,

204
00:08:34.680 --> 00:08:35.960
so that wasn't available to me.

205
00:08:35.960 --> 00:08:38.940
And I was able to replace it with DuckDB

206
00:08:38.940 --> 00:08:41.180
and get really good results.

207
00:08:41.340 --> 00:08:42.660
So I'll come back to that in a bit.

208
00:08:42.920 --> 00:08:43.880
But maybe we should talk about

209
00:08:43.880 --> 00:08:46.560
before that how S3...

210
00:08:46.560 --> 00:08:46.780
Sorry.

211
00:08:47.240 --> 00:08:47.980
Maybe we should talk about

212
00:08:47.980 --> 00:08:50.680
how DuckDB compares to other things

213
00:08:50.680 --> 00:08:54.700
like Athena, SQLite, Pandas, Polars.

214
00:08:55.700 --> 00:08:56.720
Yeah, let me try to give

215
00:08:56.720 --> 00:08:57.720
a quick overview on that.

216
00:08:57.860 --> 00:08:59.160
So comparing it to Athena,

217
00:08:59.540 --> 00:09:01.520
DuckDB is much faster and simpler

218
00:09:01.520 --> 00:09:03.280
in terms of setting it up

219
00:09:03.280 --> 00:09:04.960
and the code that you need to write.

220
00:09:04.960 --> 00:09:06.520
I think it's fair to say

221
00:09:06.520 --> 00:09:08.320
that Athena will probably support

222
00:09:08.320 --> 00:09:09.360
much more data

223
00:09:09.360 --> 00:09:11.060
like if you are at massive scale,

224
00:09:11.320 --> 00:09:13.000
but you can get very far with DuckDB.

225
00:09:13.340 --> 00:09:14.520
And the reason why Athena

226
00:09:14.520 --> 00:09:15.820
or even Redshift,

227
00:09:15.940 --> 00:09:17.460
if we want to bring that

228
00:09:17.460 --> 00:09:18.560
in the conversation,

229
00:09:19.100 --> 00:09:21.180
they will support more data

230
00:09:21.180 --> 00:09:22.880
is because they are large-scale

231
00:09:22.880 --> 00:09:23.920
multi-node systems

232
00:09:23.920 --> 00:09:26.480
while DuckDB is a single-node system.

233
00:09:26.640 --> 00:09:28.500
So unless you want to write

234
00:09:28.500 --> 00:09:31.360
your own crazy demultiplexer,

235
00:09:31.400 --> 00:09:32.840
multiplexer type of approach

236
00:09:32.840 --> 00:09:33.780
where you can shard

237
00:09:33.780 --> 00:09:35.180
multiple nodes

238
00:09:35.180 --> 00:09:36.180
and manage all that

239
00:09:36.180 --> 00:09:37.180
querying in between,

240
00:09:37.700 --> 00:09:39.140
I think DuckDB is more suitable

241
00:09:39.140 --> 00:09:40.440
for things that will fit

242
00:09:40.440 --> 00:09:41.220
a single node.

243
00:09:41.680 --> 00:09:42.840
But I think with that,

244
00:09:42.880 --> 00:09:43.740
you can still solve

245
00:09:43.740 --> 00:09:44.840
lots of problems,

246
00:09:44.940 --> 00:09:46.580
even at relatively big scale.

247
00:09:46.780 --> 00:09:49.120
So that just keeps the comparison

248
00:09:49.120 --> 00:09:50.240
between Athena

249
00:09:50.240 --> 00:09:51.520
and even Redshift,

250
00:09:51.640 --> 00:09:51.980
I guess,

251
00:09:52.220 --> 00:09:52.920
versus DuckDB.

252
00:09:53.140 --> 00:09:54.300
Another one that it's

253
00:09:54.300 --> 00:09:55.600
very often compared to

254
00:09:55.600 --> 00:09:56.460
is SQLite.

255
00:09:56.620 --> 00:09:57.780
And I think it's fair to say

256
00:09:57.780 --> 00:09:58.660
that these days

257
00:09:58.660 --> 00:10:00.240
there is an alternative

258
00:10:00.240 --> 00:10:01.240
called LibSQL.

259
00:10:01.940 --> 00:10:03.780
So if you are not aware,

260
00:10:04.540 --> 00:10:05.800
LibSQL is a fork

261
00:10:05.800 --> 00:10:06.840
of SQLite.

262
00:10:07.140 --> 00:10:07.620
So in a way,

263
00:10:07.660 --> 00:10:08.260
it's very similar,

264
00:10:08.400 --> 00:10:08.960
tries to solve

265
00:10:08.960 --> 00:10:09.740
the same problems

266
00:10:09.740 --> 00:10:11.140
as a slightly different

267
00:10:11.140 --> 00:10:12.420
governance than SQLite.

268
00:10:12.560 --> 00:10:13.560
But apparently,

269
00:10:13.640 --> 00:10:14.760
I'm seeing it more and more.

270
00:10:14.880 --> 00:10:15.600
So possibly,

271
00:10:15.780 --> 00:10:16.520
it's going to eventually

272
00:10:16.520 --> 00:10:17.900
become a big contender

273
00:10:17.900 --> 00:10:18.600
in this space.

274
00:10:18.920 --> 00:10:20.400
So both SQLite

275
00:10:20.400 --> 00:10:21.180
and LibSQL,

276
00:10:21.500 --> 00:10:22.360
if you compare them

277
00:10:22.360 --> 00:10:23.040
with DuckDB,

278
00:10:23.260 --> 00:10:24.920
I think the approach

279
00:10:24.920 --> 00:10:25.820
is very similar.

280
00:10:26.100 --> 00:10:27.400
So I think it makes sense

281
00:10:27.400 --> 00:10:28.240
to compare them.

282
00:10:28.620 --> 00:10:29.320
But at the same time,

283
00:10:29.340 --> 00:10:30.280
they are trying to solve

284
00:10:30.280 --> 00:10:31.360
very different problems.

285
00:10:31.600 --> 00:10:32.800
So where SQLite

286
00:10:32.800 --> 00:10:33.600
or LibSQL

287
00:10:33.600 --> 00:10:34.540
are giving you

288
00:10:34.540 --> 00:10:36.540
effectively an easy way

289
00:10:36.540 --> 00:10:37.340
to have a local

290
00:10:37.340 --> 00:10:38.000
embeddable

291
00:10:38.000 --> 00:10:39.360
transactional database,

292
00:10:39.660 --> 00:10:41.220
DuckDB is much more focused

293
00:10:41.220 --> 00:10:42.600
on large-scale analytics.

294
00:10:43.100 --> 00:10:44.220
So effectively,

295
00:10:44.360 --> 00:10:45.260
the way that the data

296
00:10:45.260 --> 00:10:46.140
is being stored

297
00:10:46.140 --> 00:10:46.980
and retrieved

298
00:10:46.980 --> 00:10:48.200
is very, very different.

299
00:10:48.420 --> 00:10:48.920
And therefore,

300
00:10:49.580 --> 00:10:50.400
I think you need to pick

301
00:10:50.400 --> 00:10:50.880
the right tool

302
00:10:50.880 --> 00:10:51.600
for the right job.

303
00:10:51.680 --> 00:10:52.740
So if you're doing analytics,

304
00:10:53.200 --> 00:10:53.940
stick with DuckDB.

305
00:10:54.520 --> 00:10:54.840
Otherwise,

306
00:10:55.580 --> 00:10:56.440
probably SQLite

307
00:10:56.440 --> 00:10:57.600
will give you better results.

308
00:10:58.100 --> 00:10:58.840
Then if we bring

309
00:10:58.840 --> 00:10:59.500
into the pictures

310
00:10:59.500 --> 00:11:00.220
Pandas

311
00:11:00.220 --> 00:11:01.120
and the more recent

312
00:11:01.120 --> 00:11:01.720
Polars,

313
00:11:01.920 --> 00:11:03.600
which is pretty much...

314
00:11:03.600 --> 00:11:04.260
So Pandas is

315
00:11:04.260 --> 00:11:05.660
the original one

316
00:11:05.660 --> 00:11:06.320
written in Python

317
00:11:06.320 --> 00:11:07.660
and Polars is kind of

318
00:11:07.660 --> 00:11:09.400
a more modern rewrite

319
00:11:09.400 --> 00:11:10.360
written in Rust.

320
00:11:10.700 --> 00:11:11.660
So more focused

321
00:11:11.660 --> 00:11:12.240
on bringing,

322
00:11:12.380 --> 00:11:12.700
I guess,

323
00:11:12.860 --> 00:11:14.040
additional performance

324
00:11:14.040 --> 00:11:15.020
that the Rust programming

325
00:11:15.020 --> 00:11:15.920
language can bring.

326
00:11:16.180 --> 00:11:17.040
So these are two

327
00:11:17.040 --> 00:11:17.760
very popular

328
00:11:17.760 --> 00:11:19.040
data processing

329
00:11:19.040 --> 00:11:20.380
libraries

330
00:11:20.380 --> 00:11:21.620
that are used

331
00:11:21.620 --> 00:11:22.180
for analytics.

332
00:11:22.580 --> 00:11:24.280
I think the use case

333
00:11:24.280 --> 00:11:24.940
is very similar,

334
00:11:24.940 --> 00:11:25.860
but the approach

335
00:11:25.860 --> 00:11:27.060
is probably very different.

336
00:11:27.360 --> 00:11:28.180
So while Pandas

337
00:11:28.180 --> 00:11:28.680
and Polars

338
00:11:28.680 --> 00:11:29.520
effectively are

339
00:11:29.520 --> 00:11:30.800
programming languages

340
00:11:30.800 --> 00:11:31.280
libraries,

341
00:11:31.500 --> 00:11:32.560
so they offer you

342
00:11:32.560 --> 00:11:34.000
like a programmatic interface,

343
00:11:34.340 --> 00:11:35.240
you import a library

344
00:11:35.240 --> 00:11:36.260
in a piece of code

345
00:11:36.260 --> 00:11:36.740
you're writing,

346
00:11:37.120 --> 00:11:37.980
and then you have

347
00:11:37.980 --> 00:11:38.520
this concept

348
00:11:38.520 --> 00:11:39.160
of an object

349
00:11:39.160 --> 00:11:39.760
that represents

350
00:11:39.760 --> 00:11:40.460
a data frame,

351
00:11:40.840 --> 00:11:41.540
and you can use

352
00:11:41.540 --> 00:11:42.180
this data frame

353
00:11:42.180 --> 00:11:43.040
to do all sorts

354
00:11:43.040 --> 00:11:43.820
of kind of things

355
00:11:43.820 --> 00:11:44.800
using different

356
00:11:44.800 --> 00:11:45.600
programming languages

357
00:11:45.600 --> 00:11:46.600
like Python or Rust.

358
00:11:47.180 --> 00:11:47.900
You write different

359
00:11:47.900 --> 00:11:48.400
instructions,

360
00:11:48.560 --> 00:11:49.020
effectively you write

361
00:11:49.020 --> 00:11:49.620
your own script,

362
00:11:49.820 --> 00:11:50.400
and that way

363
00:11:50.400 --> 00:11:51.300
you can create

364
00:11:51.300 --> 00:11:52.780
even fairly complex

365
00:11:52.780 --> 00:11:54.380
analytical pipelines.

366
00:11:55.320 --> 00:11:56.140
I don't think

367
00:11:56.140 --> 00:11:56.760
those will be

368
00:11:56.760 --> 00:11:57.440
completely replaced

369
00:11:57.440 --> 00:11:58.020
by DuckDB.

370
00:11:58.300 --> 00:12:00.120
I think on one side

371
00:12:00.120 --> 00:12:01.580
maybe Pandas

372
00:12:01.580 --> 00:12:02.160
and Polars

373
00:12:02.160 --> 00:12:03.100
can give you

374
00:12:03.100 --> 00:12:03.700
an extra bit

375
00:12:03.700 --> 00:12:04.260
of flexibility.

376
00:12:04.600 --> 00:12:05.640
Maybe people just prefer

377
00:12:05.640 --> 00:12:07.060
the more programmatic

378
00:12:07.060 --> 00:12:07.560
approach

379
00:12:07.560 --> 00:12:08.780
so that there is

380
00:12:08.780 --> 00:12:10.160
definitely a difference

381
00:12:10.160 --> 00:12:10.900
in the things

382
00:12:10.900 --> 00:12:11.460
you can do

383
00:12:11.460 --> 00:12:12.220
and the way

384
00:12:12.220 --> 00:12:12.780
you approach

385
00:12:12.780 --> 00:12:13.340
the problem.

386
00:12:14.060 --> 00:12:15.600
But with that being said,

387
00:12:15.740 --> 00:12:16.640
I think you can

388
00:12:16.640 --> 00:12:18.160
be able to replace

389
00:12:18.160 --> 00:12:19.880
some of the work

390
00:12:19.880 --> 00:12:20.660
that you might be doing

391
00:12:20.660 --> 00:12:21.720
with Pandas and Polars

392
00:12:21.720 --> 00:12:22.840
with just a query

393
00:12:22.840 --> 00:12:23.920
or a single SQL

394
00:12:23.920 --> 00:12:25.980
even in DuckDB.

395
00:12:26.200 --> 00:12:27.080
So I think it's worth

396
00:12:27.080 --> 00:12:27.820
figuring out

397
00:12:27.820 --> 00:12:28.800
where is the sweet spot

398
00:12:28.800 --> 00:12:30.400
where maybe DuckDB

399
00:12:30.400 --> 00:12:31.000
will give you

400
00:12:31.000 --> 00:12:32.260
a much simpler solution

401
00:12:32.260 --> 00:12:33.260
and you don't have to

402
00:12:33.260 --> 00:12:34.420
write and maintain code.

403
00:12:34.620 --> 00:12:35.440
You probably just focus

404
00:12:35.440 --> 00:12:36.840
on writing a SQL statement.

405
00:12:37.380 --> 00:12:37.760
Yeah, I think

406
00:12:37.760 --> 00:12:38.460
my rule of thumb

407
00:12:39.020 --> 00:12:41.100
is like when you run into

408
00:12:41.100 --> 00:12:42.060
multiple lines of SQL,

409
00:12:42.280 --> 00:12:42.780
then eventually

410
00:12:42.780 --> 00:12:43.660
it gets more complicated.

411
00:12:44.040 --> 00:12:44.900
You feel like you're back

412
00:12:44.900 --> 00:12:47.020
in store procedure land

413
00:12:47.020 --> 00:12:48.740
in the early 2000s

414
00:12:48.740 --> 00:12:50.440
and at that point

415
00:12:50.440 --> 00:12:50.940
you might be better

416
00:12:50.940 --> 00:12:51.720
off using something

417
00:12:51.720 --> 00:12:52.600
a bit more imperative

418
00:12:52.600 --> 00:12:54.720
like Python.

419
00:12:56.480 --> 00:12:57.620
So you mentioned step functions before.

420
00:12:57.780 --> 00:12:58.660
I'm really curious

421
00:12:58.660 --> 00:13:00.000
to see how you use

422
00:13:00.000 --> 00:13:01.620
DuckDB with step functions.

423
00:13:02.420 --> 00:13:02.820
Yeah, I mean there's no way

424
00:13:02.820 --> 00:13:03.780
you can create

425
00:13:03.780 --> 00:13:04.940
like a custom

426
00:13:04.940 --> 00:13:06.840
user-defined task

427
00:13:06.840 --> 00:13:07.540
or state

428
00:13:07.540 --> 00:13:08.460
in step functions

429
00:13:08.460 --> 00:13:09.540
apart from using

430
00:13:09.540 --> 00:13:10.340
something like Lambda

431
00:13:10.340 --> 00:13:12.220
or a container task.

432
00:13:13.520 --> 00:13:14.880
So I mentioned

433
00:13:14.880 --> 00:13:16.340
that we had this issue

434
00:13:16.340 --> 00:13:17.100
where we couldn't

435
00:13:17.100 --> 00:13:18.000
use S3 select

436
00:13:18.000 --> 00:13:20.060
and we trialed

437
00:13:20.060 --> 00:13:21.220
using DuckDB instead.

438
00:13:21.440 --> 00:13:22.220
It worked really well

439
00:13:22.220 --> 00:13:23.360
and we actually decided

440
00:13:23.360 --> 00:13:24.880
to make this

441
00:13:24.880 --> 00:13:26.160
a general purpose pattern

442
00:13:26.160 --> 00:13:27.300
so that anyone

443
00:13:27.300 --> 00:13:28.860
can really easily

444
00:13:28.860 --> 00:13:29.600
integrate things

445
00:13:29.600 --> 00:13:30.140
like DuckDB

446
00:13:30.140 --> 00:13:31.200
into step functions

447
00:13:31.200 --> 00:13:32.040
and other workflows.

448
00:13:32.660 --> 00:13:33.220
The way we did that

449
00:13:33.220 --> 00:13:34.000
was creating

450
00:13:34.000 --> 00:13:34.980
a Lambda runtime

451
00:13:34.980 --> 00:13:35.680
for DuckDB.

452
00:13:36.260 --> 00:13:36.840
And this might sound

453
00:13:36.840 --> 00:13:37.580
a little bit strange

454
00:13:37.580 --> 00:13:39.080
but here is out.

455
00:13:40.680 --> 00:13:42.260
The layer allows you

456
00:13:42.260 --> 00:13:43.080
to basically deploy

457
00:13:43.080 --> 00:13:43.700
a Lambda function

458
00:13:43.700 --> 00:13:45.080
that doesn't have any code.

459
00:13:45.080 --> 00:13:46.460
So it just has

460
00:13:46.460 --> 00:13:47.380
a DuckDB engine

461
00:13:47.380 --> 00:13:48.240
and we created

462
00:13:48.240 --> 00:13:48.940
this runtime

463
00:13:48.940 --> 00:13:50.120
layer on top of it

464
00:13:50.120 --> 00:13:50.780
that basically

465
00:13:50.780 --> 00:13:51.740
accepts a query

466
00:13:51.740 --> 00:13:53.320
and allows you

467
00:13:53.320 --> 00:13:54.460
to then do something

468
00:13:54.460 --> 00:13:55.060
with the results.

469
00:13:55.500 --> 00:13:56.380
So it's really lightweight.

470
00:13:56.780 --> 00:13:57.560
It's written in C

471
00:13:57.560 --> 00:13:58.680
because DuckDB's

472
00:13:58.680 --> 00:13:59.260
core itself

473
00:13:59.260 --> 00:13:59.980
is written in C

474
00:13:59.980 --> 00:14:00.800
so it was

475
00:14:00.800 --> 00:14:01.800
fairly straightforward

476
00:14:01.800 --> 00:14:03.080
to create a thin wrapper

477
00:14:03.080 --> 00:14:04.340
around that written in C as well.

478
00:14:04.700 --> 00:14:05.720
So it's very tiny

479
00:14:05.720 --> 00:14:06.300
to deploy

480
00:14:06.300 --> 00:14:07.220
quite lightweight

481
00:14:07.220 --> 00:14:08.000
in terms of startup

482
00:14:08.000 --> 00:14:09.320
and then you really

483
00:14:09.320 --> 00:14:10.420
just have to incur

484
00:14:10.420 --> 00:14:11.760
the analytical time

485
00:14:11.760 --> 00:14:12.820
for whatever query

486
00:14:12.820 --> 00:14:13.380
you're running.

487
00:14:13.860 --> 00:14:14.580
So it means you don't

488
00:14:14.580 --> 00:14:15.600
have to have a Python runtime

489
00:14:15.600 --> 00:14:16.740
or a Node.js runtime.

490
00:14:17.040 --> 00:14:17.620
You don't have to worry

491
00:14:17.620 --> 00:14:18.300
about dependencies

492
00:14:18.300 --> 00:14:19.160
and upgrades.

493
00:14:19.620 --> 00:14:20.740
It's just a single binary

494
00:14:20.740 --> 00:14:22.320
and you can then

495
00:14:22.320 --> 00:14:23.340
use this

496
00:14:23.340 --> 00:14:24.760
as a response

497
00:14:24.760 --> 00:14:25.840
to an event bridge event

498
00:14:25.840 --> 00:14:27.340
or integrate it

499
00:14:27.340 --> 00:14:28.100
into step functions

500
00:14:28.100 --> 00:14:29.260
and all of a sudden

501
00:14:29.260 --> 00:14:30.160
then you have

502
00:14:30.160 --> 00:14:30.620
the ability

503
00:14:30.620 --> 00:14:31.860
to do DuckDB

504
00:14:31.860 --> 00:14:33.300
in your step function

505
00:14:33.300 --> 00:14:34.160
definition

506
00:14:34.160 --> 00:14:35.340
and you can just

507
00:14:35.340 --> 00:14:36.540
write your SQL

508
00:14:36.540 --> 00:14:38.480
in the input

509
00:14:38.480 --> 00:14:39.220
parameters

510
00:14:39.220 --> 00:14:40.280
for the Lambda task

511
00:14:40.280 --> 00:14:41.620
and interpolate

512
00:14:41.620 --> 00:14:42.800
any variables

513
00:14:42.800 --> 00:14:43.460
you might need

514
00:14:43.460 --> 00:14:44.260
from your step function

515
00:14:44.260 --> 00:14:44.660
state.

516
00:14:45.120 --> 00:14:46.020
And we've been able

517
00:14:46.020 --> 00:14:46.600
to use this

518
00:14:46.600 --> 00:14:47.340
then for just

519
00:14:47.340 --> 00:14:48.200
doing simple things

520
00:14:48.200 --> 00:14:49.560
like taking

521
00:14:49.560 --> 00:14:50.860
a CSV file

522
00:14:50.860 --> 00:14:51.820
and converting it

523
00:14:51.820 --> 00:14:52.320
into JSON

524
00:14:52.320 --> 00:14:53.340
and then doing

525
00:14:53.340 --> 00:14:53.920
something else

526
00:14:53.920 --> 00:14:54.580
with the response.

527
00:14:54.840 --> 00:14:55.240
But you can also

528
00:14:55.240 --> 00:14:56.180
do different filtering,

529
00:14:56.380 --> 00:14:57.360
grouping, aggregation

530
00:14:57.360 --> 00:14:58.220
before you return

531
00:14:58.220 --> 00:14:58.680
your JSON.

532
00:14:58.960 --> 00:14:59.780
So it's really powerful.

533
00:15:00.180 --> 00:15:00.680
It is,

534
00:15:01.520 --> 00:15:02.480
I think it's worthy

535
00:15:02.480 --> 00:15:03.020
of a runtime

536
00:15:03.020 --> 00:15:03.640
because DuckDB

537
00:15:03.640 --> 00:15:04.260
is so powerful

538
00:15:04.260 --> 00:15:04.780
and you can do

539
00:15:04.780 --> 00:15:05.560
so much with it.

540
00:15:06.100 --> 00:15:06.860
And I think a lot

541
00:15:06.860 --> 00:15:07.660
of us who use Lambda

542
00:15:07.660 --> 00:15:08.060
heavily

543
00:15:08.060 --> 00:15:08.840
are always looking

544
00:15:08.840 --> 00:15:09.640
for ways to reduce

545
00:15:09.640 --> 00:15:10.460
the number of functions

546
00:15:10.460 --> 00:15:11.440
you need to maintain

547
00:15:11.440 --> 00:15:12.320
because it just seems

548
00:15:12.320 --> 00:15:12.740
like you've got

549
00:15:12.740 --> 00:15:13.340
a lot of cattle

550
00:15:13.340 --> 00:15:14.620
to herd then.

551
00:15:15.360 --> 00:15:16.860
And this is a good

552
00:15:16.860 --> 00:15:17.580
way to do it.

553
00:15:17.620 --> 00:15:18.220
So we'll have a link

554
00:15:18.220 --> 00:15:19.080
to the repo

555
00:15:19.080 --> 00:15:20.280
in the show notes.

556
00:15:20.440 --> 00:15:21.000
It's essentially

557
00:15:21.000 --> 00:15:21.760
a SAR app,

558
00:15:21.900 --> 00:15:22.380
so a serverless

559
00:15:22.380 --> 00:15:23.280
application repository

560
00:15:23.280 --> 00:15:24.680
app that you can use

561
00:15:24.680 --> 00:15:25.240
to install

562
00:15:25.240 --> 00:15:26.600
the runtime

563
00:15:26.600 --> 00:15:27.600
in your own account.

564
00:15:27.940 --> 00:15:28.540
And then you just

565
00:15:28.540 --> 00:15:29.240
create a simple

566
00:15:29.240 --> 00:15:30.720
function that you

567
00:15:30.720 --> 00:15:31.180
can deploy.

568
00:15:31.740 --> 00:15:32.380
You might need

569
00:15:32.380 --> 00:15:32.680
to give it

570
00:15:32.680 --> 00:15:33.180
some permissions.

571
00:15:34.120 --> 00:15:35.120
So normally,

572
00:15:35.300 --> 00:15:36.180
rather than returning

573
00:15:36.180 --> 00:15:37.040
analytical data

574
00:15:37.040 --> 00:15:38.120
in the response

575
00:15:38.120 --> 00:15:39.120
of your function

576
00:15:39.120 --> 00:15:39.560
invocation,

577
00:15:39.560 --> 00:15:40.840
you might write

578
00:15:40.840 --> 00:15:41.460
it to S3

579
00:15:41.460 --> 00:15:42.780
just because of

580
00:15:42.780 --> 00:15:43.660
the size of the data

581
00:15:43.660 --> 00:15:44.260
or the format

582
00:15:44.260 --> 00:15:44.780
of the data.

583
00:15:45.740 --> 00:15:46.660
So you might want

584
00:15:46.660 --> 00:15:48.320
to give that function

585
00:15:48.320 --> 00:15:48.780
you deploy

586
00:15:48.780 --> 00:15:50.040
some access

587
00:15:50.040 --> 00:15:50.860
to S3,

588
00:15:51.380 --> 00:15:51.900
like read access

589
00:15:51.900 --> 00:15:52.560
to some buckets

590
00:15:52.560 --> 00:15:54.060
and maybe write access

591
00:15:54.060 --> 00:15:55.300
to a staging area

592
00:15:55.300 --> 00:15:56.240
where your intermediate

593
00:15:56.240 --> 00:15:57.340
results are stored.

594
00:15:58.400 --> 00:15:58.940
And once you have

595
00:15:58.940 --> 00:15:59.160
that,

596
00:15:59.500 --> 00:16:00.520
you can do

597
00:16:00.520 --> 00:16:01.060
some pretty mad

598
00:16:01.060 --> 00:16:01.700
things with

599
00:16:01.700 --> 00:16:02.800
DuckDB and step

600
00:16:02.800 --> 00:16:03.140
functions,

601
00:16:03.360 --> 00:16:04.760
like you can even

602
00:16:04.760 --> 00:16:05.720
call APIs using

603
00:16:05.720 --> 00:16:06.580
DuckDB because it

604
00:16:06.580 --> 00:16:07.420
has an HTTP

605
00:16:07.420 --> 00:16:08.600
extension and you

606
00:16:08.600 --> 00:16:09.240
can even do

607
00:16:09.240 --> 00:16:10.380
like authenticated

608
00:16:10.380 --> 00:16:11.580
requests to APIs,

609
00:16:11.860 --> 00:16:12.740
transform the response

610
00:16:12.740 --> 00:16:13.100
in SQL.

611
00:16:13.400 --> 00:16:14.060
You can convert

612
00:16:14.060 --> 00:16:15.420
tabular data on S3

613
00:16:15.420 --> 00:16:15.880
to JSON,

614
00:16:16.060 --> 00:16:16.620
like I mentioned,

615
00:16:16.740 --> 00:16:17.240
that you can then

616
00:16:17.240 --> 00:16:17.880
use in your step

617
00:16:17.880 --> 00:16:18.420
function state.

618
00:16:18.680 --> 00:16:19.600
You can do

619
00:16:19.600 --> 00:16:21.320
like fairly significant

620
00:16:21.320 --> 00:16:23.300
ETL or ELT

621
00:16:23.300 --> 00:16:24.140
type workloads

622
00:16:24.140 --> 00:16:25.560
and then just

623
00:16:25.560 --> 00:16:26.520
anything else in terms

624
00:16:26.520 --> 00:16:27.200
of analytics

625
00:16:27.200 --> 00:16:28.100
aggregation.

626
00:16:28.500 --> 00:16:28.600
You know,

627
00:16:28.660 --> 00:16:29.420
if you've got a

628
00:16:29.420 --> 00:16:30.500
step function that's

629
00:16:30.500 --> 00:16:31.040
doing a map

630
00:16:31.040 --> 00:16:31.880
or even a distributed

631
00:16:31.880 --> 00:16:32.700
map to process

632
00:16:32.700 --> 00:16:33.620
lots and lots of

633
00:16:33.620 --> 00:16:34.380
units of data,

634
00:16:34.860 --> 00:16:35.620
you could then use

635
00:16:35.620 --> 00:16:36.940
DuckDB to collate

636
00:16:36.940 --> 00:16:37.480
all that data,

637
00:16:37.600 --> 00:16:38.200
aggregate it all

638
00:16:38.200 --> 00:16:39.160
together and get

639
00:16:39.160 --> 00:16:39.520
results.

640
00:16:40.080 --> 00:16:40.900
So, yeah,

641
00:16:40.940 --> 00:16:41.760
I'm curious to hear

642
00:16:41.760 --> 00:16:42.780
how people get on

643
00:16:42.780 --> 00:16:43.360
with it and if they

644
00:16:43.360 --> 00:16:44.920
can try it out and

645
00:16:44.920 --> 00:16:45.480
find some really

646
00:16:45.480 --> 00:16:46.760
good use cases

647
00:16:46.760 --> 00:16:47.560
because it really is

648
00:16:47.560 --> 00:16:48.120
kind of a Swiss

649
00:16:48.120 --> 00:16:49.020
army knife type

650
00:16:49.020 --> 00:16:51.100
tool and it's just

651
00:16:51.100 --> 00:16:51.660
limited by your

652
00:16:51.660 --> 00:16:52.140
imagination.

653
00:16:52.480 --> 00:16:52.960
That's awesome.

654
00:16:53.160 --> 00:16:54.020
I think this is

655
00:16:54.020 --> 00:16:54.800
everything we wanted

656
00:16:54.800 --> 00:16:55.640
to share today.

657
00:16:55.980 --> 00:16:57.280
We believe as a

658
00:16:57.280 --> 00:16:58.240
final conclusion that

659
00:16:58.240 --> 00:16:59.000
DuckDB is really

660
00:16:59.000 --> 00:16:59.600
good, really

661
00:16:59.600 --> 00:16:59.980
promising,

662
00:17:00.160 --> 00:17:00.780
probably we'll be

663
00:17:00.780 --> 00:17:01.420
seeing more and

664
00:17:01.420 --> 00:17:02.540
more of it and

665
00:17:02.540 --> 00:17:03.460
especially in the

666
00:17:03.460 --> 00:17:04.560
cloud, in Lambda,

667
00:17:04.880 --> 00:17:05.340
in AWS.

668
00:17:05.340 --> 00:17:06.560
So, hopefully you

669
00:17:06.560 --> 00:17:07.180
got some value

670
00:17:07.180 --> 00:17:07.960
from this episode

671
00:17:07.960 --> 00:17:08.340
today.

672
00:17:08.700 --> 00:17:09.320
If you did,

673
00:17:09.500 --> 00:17:10.600
please remember to

674
00:17:10.600 --> 00:17:11.380
share it with your

675
00:17:11.380 --> 00:17:12.120
friends, like,

676
00:17:12.220 --> 00:17:13.320
subscribe, everything

677
00:17:13.320 --> 00:17:14.500
else and also

678
00:17:14.500 --> 00:17:15.740
please give us a

679
00:17:15.740 --> 00:17:17.040
star to our new

680
00:17:17.040 --> 00:17:17.980
repository if you

681
00:17:17.980 --> 00:17:18.940
like this project.

682
00:17:19.420 --> 00:17:20.060
So, thank you very

683
00:17:20.060 --> 00:17:21.060
much and we'll see

684
00:17:21.060 --> 00:17:21.740
you in the next

685
00:17:21.740 --> 00:17:22.080
episode.

686
00:17:35.340 --> 00:17:36.340
and we'll see you next

687
00:17:36.380 --> 00:17:37.180
big one.

688
00:17:37.880 --> 00:17:37.960
Bye.

689
00:17:38.100 --> 00:17:38.140
Bye.

690
00:17:38.220 --> 00:17:38.280
Bye.

691
00:17:39.060 --> 00:17:39.220
Bye.

692
00:17:39.360 --> 00:17:40.520
Bye.

693
00:17:40.560 --> 00:17:41.180
Bye.

694
00:17:41.680 --> 00:17:42.740
Bye.

695
00:17:43.060 --> 00:17:43.240
Bye.

696
00:17:43.440 --> 00:17:44.020
Bye.

697
00:17:44.400 --> 00:17:44.500
Bye.

698
00:17:44.500 --> 00:17:44.760
Bye.

699
00:17:44.960 --> 00:17:46.020
Bye.

700
00:17:47.600 --> 00:17:48.100
Bye.

701
00:17:56.140 --> 00:17:56.740
Bye.

702
00:17:56.920 --> 00:17:57.140
Bye.

703
00:17:57.140 --> 00:17:58.140
Bye.

704
00:17:58.140 --> 00:17:58.180
Bye.

705
00:17:59.080 --> 00:17:59.120
Bye.

706
00:17:59.180 --> 00:17:59.440
Bye.

707
00:17:59.440 --> 00:17:59.980
Bye.

708
00:18:00.180 --> 00:18:00.520
Bye.

709
00:18:00.540 --> 00:18:00.560
Bye.

710
00:18:00.560 --> 00:18:00.820
Bye.

711
00:18:00.860 --> 00:18:01.080
Bye.

712
00:18:01.420 --> 00:18:01.580
Bye.

713
00:18:01.700 --> 00:18:02.380
Bye.

714
00:18:02.400 --> 00:18:02.940
Bye.

715
00:18:02.960 --> 00:18:03.640
Bye.

716
00:18:03.740 --> 00:18:03.760
Bye.

717
00:18:03.840 --> 00:18:03.980
Bye.

718
00:18:04.100 --> 00:18:04.660
Bye.
