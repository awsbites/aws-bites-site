WEBVTT

1
00:00:00.000 --> 00:00:06.720
Hello and happy 2026. In our latest episode of AWS Bites, we covered ECS managed instances,

2
00:00:07.040 --> 00:00:12.160
which is a new way to power your ECS cluster with managed EC2 capacity. This basically means

3
00:00:12.160 --> 00:00:18.220
that you still use EC2 instances under the hood, but in this case, AWS takes care of all the usual

4
00:00:18.220 --> 00:00:24.520
instance chores like picking the base image, operating system lifecycle, security patching,

5
00:00:24.880 --> 00:00:29.240
ongoing maintenance, and you basically focus on describing what you need, for example,

6
00:00:29.240 --> 00:00:34.240
if you want a GPU, or maybe if you want specific storage profiles, or maybe particular networking

7
00:00:34.240 --> 00:00:39.380
characteristics. And then at that point, AWS provisions the right instance for your cluster

8
00:00:39.380 --> 00:00:44.960
in exchange for a management fee. Now, coming straight out of reInvent in Las Vegas, which is

9
00:00:44.960 --> 00:00:50.660
AWS's biggest conference of the year, AWS has taken this very idea, managed instances, and applied it

10
00:00:50.660 --> 00:00:58.260
to, drumroll, Lambda. So the name of the announcement is Lambda managed instances, and it has been a

11
00:00:58.260 --> 00:01:03.140
little bit controversial. It has sparked a lot of curiosity, plus a fair amount of confusion, because

12
00:01:03.140 --> 00:01:09.900
let's be honest, why in the world would you want to bring EC2 instances, aka servers, into one of the

13
00:01:09.900 --> 00:01:15.320
most serverless compute services out there? So you might be wondering, what does it really change for

14
00:01:15.320 --> 00:01:20.820
Lambda? Do you gain something that you could not do before? Honestly, we were as well curious as much

15
00:01:20.820 --> 00:01:26.340
excited by this announcement. So we took Lambda MI for a proper test drive during the holidays, and in this

16
00:01:26.340 --> 00:01:31.140
episode, we're going to share our take on it. So we want to share what Lambda managed instances enables

17
00:01:31.140 --> 00:01:36.340
that default Lambda could not do before. And there are a few spoilers I want to give you here, just to

18
00:01:36.340 --> 00:01:42.100
capture your attention. There are no more cold starts, kinda, we'll talk more about that. And a single

19
00:01:42.100 --> 00:01:46.740
Lambda environment can now handle multiple requests concurrently, which is also very interesting, and we'll

20
00:01:46.740 --> 00:01:52.100
cover that in detail. We'll also talk about how to set it up and make the most out of it. What are the

21
00:01:52.100 --> 00:01:57.140
different ways to scale the underlying EC2 capacity, limitations and pitfalls we noticed, and there are

22
00:01:57.140 --> 00:02:02.020
quite a few of them to be aware of. The use cases, when we think this is actually a good fit and a good

23
00:02:02.020 --> 00:02:07.060
idea to use this new technology. And of course, we'll talk about pricing and whether it's worth the cost

24
00:02:07.060 --> 00:02:12.020
and the effort. Now, since we built a realistic application for this exercise, we'll also talk about

25
00:02:12.020 --> 00:02:18.340
our use case and our example application as well. My name is Luciano and I'm joined by Eoin, and this is AWS Bites.

26
00:02:22.100 --> 00:02:30.900
AWS Bites is brought to you by fourTheorem. Stay tuned to hear more about fourTheorem at the end of the show.

27
00:02:30.900 --> 00:02:36.420
So, Eoin, maybe we should start by clarifying again, what is the difference between default, I guess we

28
00:02:36.420 --> 00:02:42.340
should call them, Lambdas, and Lambda on EC2?

29
00:02:42.340 --> 00:02:48.420
Yeah, let's call the old model the default, that's what AWS calls it in the documentation. So let's roll with that. Your default Lambda is the fully managed, just run my

30
00:02:48.420 --> 00:02:52.260
code service that we know and love and most people refer to when they say Lambda functions.

31
00:02:52.820 --> 00:02:57.700
And a function is invoked in response to an event, right? An object gets uploaded to S3,

32
00:02:57.700 --> 00:03:02.900
that can be an event that triggers a Lambda that resizes an image, extracts metadata. That's a canonical

33
00:03:02.900 --> 00:03:08.900
example. So when an event arrives, AWS is running your code in this isolated execution environment.

34
00:03:08.900 --> 00:03:13.540
Where does this environment exist in terms of compute? It doesn't matter. Of course, it will be in a

35
00:03:13.540 --> 00:03:17.780
server somewhere, but we don't get to see that. We don't care. That's what we like. The important

36
00:03:17.780 --> 00:03:23.220
detail is that each environment processes only one event or one invocation at a time. If more events

37
00:03:23.220 --> 00:03:28.580
come in, Lambda scales out by creating more environments to handle concurrency. And then if

38
00:03:28.580 --> 00:03:35.700
traffic scales back, Lambda will manage scaling back the execution environments, often effectively to zero.

39
00:03:35.700 --> 00:03:42.020
The main trade-off is cold starts then, because all of that scaling that Lambda is doing behind the scenes,

40
00:03:42.020 --> 00:03:47.220
you don't pay for that. You don't think about it. But it means that sometimes there isn't a warm

41
00:03:47.220 --> 00:03:51.460
environment ready to serve your event. And Lambda will need to create a brand new environment, which

42
00:03:51.460 --> 00:03:56.420
takes time. Sometimes it can be a few seconds. Sometimes it's as low as 100 milliseconds or even

43
00:03:56.420 --> 00:04:02.900
less if you're using Rust, for example. But often it can be significant. We've talked about that in

44
00:04:02.900 --> 00:04:08.820
previous episodes when we dealt with Python and large data science packages. Now, let's talk about the

45
00:04:08.820 --> 00:04:15.540
new way of doing things. The new option, at least. Lambda managed instances. This one is about the

46
00:04:15.540 --> 00:04:21.620
Lambda service keeping the same programming model and integrations, but it changes the how and where

47
00:04:21.620 --> 00:04:27.860
your code runs element. Now your function executes in containers on EC2 instances in your account.

48
00:04:29.300 --> 00:04:34.980
These EC2 instances are chosen via something called a capacity provider. And we'll talk about more about

49
00:04:34.980 --> 00:04:39.940
that in a second. So the AWS is still managing provisioning, patching, routing, load balancing,

50
00:04:39.940 --> 00:04:44.420
the scaling mechanics, and the lifecycle. That's important. So it's not about necessarily taking on

51
00:04:44.420 --> 00:04:51.940
all the burden of managing EC2 instances. It is about just changing the scaling and provisioning capacity

52
00:04:51.940 --> 00:04:54.900
and also the cost model for Lambda, which we'll also talk about.

53
00:04:54.900 --> 00:05:00.500
So it's not just Lambda, but on different hardware. There are a few big behavioral changes that will

54
00:05:00.500 --> 00:05:06.340
materially affect how your functions perform, how they scale, and how you need to write and operate

55
00:05:06.340 --> 00:05:11.460
them. So it's definitely something you need to go into with your mind fully informed on how all these

56
00:05:11.460 --> 00:05:17.300
things work. So what changes in practice is, well, for one, the concurrency model. One execution environment

57
00:05:17.300 --> 00:05:23.380
can now handle multiple concurrent invocations, unlike default Lambda's single invocation per environment

58
00:05:23.380 --> 00:05:29.140
model. This is a big change in how Lambda operates and something that people might have been asking

59
00:05:29.140 --> 00:05:34.100
about for a long time. And it reminds us a little bit of Vercel's fluid compute. It also makes Lambda

60
00:05:34.100 --> 00:05:40.820
suitable as a back end for high throughput APIs. We've seen people who are familiar with, say, the Node.js

61
00:05:40.820 --> 00:05:47.540
ecosystem and how it can handle high concurrency, asynchronous I/O in one process. This is one of the great

62
00:05:47.540 --> 00:05:51.860
benefits of Node.js when it came out originally. And then when you move to Lambda, you're wondering,

63
00:05:51.860 --> 00:05:57.220
well, I'm losing all of that power now because I can only do one event in a process. But now you

64
00:05:57.220 --> 00:06:02.100
can do multiple events per process so you get a little bit more of that benefit back. Other things

65
00:06:02.100 --> 00:06:06.420
that have changed with managed instances, now we now have always on environments. So environments can

66
00:06:06.420 --> 00:06:13.060
stay continuously active, no freezing between invocations, and this helps to mitigate most of the

67
00:06:13.060 --> 00:06:20.100
traditional cold starts. Scaling behavior now is asynchronous and driven by things like CPU utilization.

68
00:06:20.100 --> 00:06:25.460
It doesn't scale to zero because it maintains a configured minimum capacity. And fast traffic

69
00:06:25.460 --> 00:06:28.740
ramps can outpace scaling briefly. So you'll need to think about that.

70
00:06:30.260 --> 00:06:34.900
And then when you publish a function to run on managed instances, Lambda by default is going to

71
00:06:34.900 --> 00:06:40.740
launch three instances, three EC2 instances in your account by default for resiliency. It's good practice.

72
00:06:40.740 --> 00:06:46.260
And it'll bring those environments up before making the version active. Now, there are ways,

73
00:06:46.260 --> 00:06:51.940
which we might touch on a little bit later on how to avoid there being three instances all the time,

74
00:06:51.940 --> 00:06:58.740
but that's just generally a good practice. When we talk about lifecycle as well, the instance

75
00:06:58.740 --> 00:07:03.060
lifecycle, these instances will stay up for a maximum of 14 days. That's what Lambda is saying,

76
00:07:03.060 --> 00:07:07.860
but then they're going to rotate them. So that's something you should plan for if it's important.

77
00:07:07.860 --> 00:07:13.860
And now with this model, you can pick instance characteristics. So you can think about the

78
00:07:13.860 --> 00:07:20.020
latest CPUs you want, like Graviton 4, you can get here now. Configurable memory to CPU ratios,

79
00:07:20.020 --> 00:07:23.220
if you want high bandwidth networking, which is something you didn't have that much control over

80
00:07:23.220 --> 00:07:28.260
before, now you have that option. So these are all new considerations. What stays the same,

81
00:07:28.260 --> 00:07:32.420
it's still Lambda in how you build and integrate it. You should treat your code more like a concurrent

82
00:07:32.420 --> 00:07:37.300
long lived service process because the execution environment can handle parallel invocations and sticks

83
00:07:37.300 --> 00:07:42.740
around. So if you used to, functions that didn't hang around, sandboxes that didn't hang around,

84
00:07:43.300 --> 00:07:47.300
these short lived execution environments, you may not have noticed your memory leaks before.

85
00:07:47.300 --> 00:07:50.980
Now with these 14 day instances, it might be something you'll have to think about.

86
00:07:50.980 --> 00:07:55.860
Should we talk now about how scaling works? I think a little bit more in detail. What do you think?

87
00:07:55.860 --> 00:08:01.300
Okay, I'll try to cover up for that. So I guess the first mental model shift is that Lambda

88
00:08:01.300 --> 00:08:06.820
managed instances scales proactively, as you explained, and not on demand, which is the case for

89
00:08:06.820 --> 00:08:12.660
the default Lambda. And with default Lambda, let's repeat that just for clarity. When an invocation arrives,

90
00:08:12.660 --> 00:08:18.420
Lambda looks for a free execution environment. And if there is not available, it creates a new one on demand.

91
00:08:18.420 --> 00:08:25.300
And this is where you can see that famous or infamous cold start. With managed instances, Lambda does not scale

92
00:08:25.300 --> 00:08:34.180
because an invocation arrived. It scales asynchronously, basically up front, watching things like CPU utilization and multi-concurrency saturation.

93
00:08:34.180 --> 00:08:41.940
And it's basically trying to determine how busy is the environment and do I need to effectively provide more room for executing Lambdas.

94
00:08:42.340 --> 00:08:50.340
And we'll talk about what that actually means in practice in a bit more detail in a few minutes. But just think about these two differences.

95
00:08:50.340 --> 00:08:55.620
In the case of default Lambda, you basically don't have to think about anything. You just know that when event arrives,

96
00:08:55.620 --> 00:09:02.660
if there are no environments, they will be created on demand. With Lambda MI, basically those environments are created

97
00:09:02.660 --> 00:09:09.460
basically in the background up front before your code is executed. And one analogy that I think can somewhat

98
00:09:09.460 --> 00:09:14.340
explain this idea a little bit better is we can think about managing a restaurant. And you can imagine that

99
00:09:14.340 --> 00:09:21.620
if you have, of course, you need to have tables in a restaurant and you can imagine that those are your EC2 instances.

100
00:09:21.620 --> 00:09:28.020
Then you need to have execution environments and we can compare those to your staff working and serving those tables.

101
00:09:28.020 --> 00:09:34.020
And then max concurrency is basically the idea of how many guests can a single server handle at once.

102
00:09:34.020 --> 00:09:40.580
And if we use this analogy, which is a little bit of a stretch, but I think it's still useful to get the mental model right,

103
00:09:40.580 --> 00:09:45.140
So we can think about scaling in these terms. So when demand goes up,

104
00:09:45.140 --> 00:09:52.580
what Lambda MI can do is basically scale two different layers. It can either add more execution environments on existing instances.

105
00:09:52.580 --> 00:09:58.020
So this is like if you are hiring more staff in the same exact restaurant, the same space available,

106
00:09:58.020 --> 00:10:05.700
you just have more staff available to do more things. And then if your instances are running out, you can add more managed instances.

107
00:10:05.700 --> 00:10:13.220
So basically you are adding more EC2 capacity under the hood with your capacity provider. And this is basically like adding more tables to the restaurant,

108
00:10:13.220 --> 00:10:18.420
which of course it doesn't, you have to think that as not necessarily cramming more tables in the same space,

109
00:10:18.420 --> 00:10:24.260
but probably more taking more space. Maybe you're putting the tables outside, or maybe you are expanding the restaurant somehow,

110
00:10:24.260 --> 00:10:32.580
right? Maybe taking the next building or something like that. So that these are kind of the two dimensions that are used for increasing your capacity for running more Lambda code.

111
00:10:32.580 --> 00:10:38.260
And this is where why scaling can feel a little bit different with this new way of running Lambda code.

112
00:10:38.820 --> 00:10:47.940
And also because this scaling is asynchronous, it's basically trying to scale upfront. But sometimes if you have lots of demand happening very quickly,

113
00:10:47.940 --> 00:10:54.020
what happens is that you might not have that capacity available when you need it. So you might actually see throttles.

114
00:10:54.020 --> 00:11:00.260
So you try to execute Lambda code, there is no capacity available, your Lambda execution might effectively be throttled.

115
00:11:00.260 --> 00:11:13.380
So there are some default mechanisms that AWS put in place where basically, if your capacity doesn't double within five minutes, you should be okay. But I think if you go over that, so if your traffic doubles very quickly,

116
00:11:13.940 --> 00:11:21.380
then maybe it's where you start to see throttles. You need to play around with that to see exactly how it works. But this is what we can infer from the documentation.

117
00:11:21.380 --> 00:11:29.060
So again, just to remark, the main change is that instead of thinking about the first request is going to be slower, which is the case of a cold start,

118
00:11:29.060 --> 00:11:34.820
the failure mode is more like maybe you have a sudden spike and you might see throttles. But in general,

119
00:11:34.820 --> 00:11:45.380
if you have predictable traffic and your capacity is enough, you are not going to see cold starts or throttles. So that gives you a little bit of a more predictable and always available environment to run on,

120
00:11:45.380 --> 00:11:58.500
which is nice, especially if you're using languages that tend to have a quite long cold start like Java, Python, or Node.js sometimes, or maybe if you have lots of dependencies that might take a long time to just keep the environment up for the first time.

121
00:11:58.500 --> 00:12:02.980
This can be actually a nice use case where you effectively can eliminate that problem.

122
00:12:03.620 --> 00:12:09.060
Now, if we want to deep dive even a little bit more, I think there are a few moving pieces that we also need to mention.

123
00:12:09.380 --> 00:12:16.660
And these are the router and scaler and a Lambda agent. This is more to explain how AWS implemented all these things under the hood.

124
00:12:16.660 --> 00:12:24.580
So in practice, when you publish a new version with a capacity provider, Lambda launches and manage instances in your account or multiple instances.

125
00:12:24.580 --> 00:12:28.500
Of course, if we consider that they will be available in different availability zones.

126
00:12:29.300 --> 00:12:35.300
And as we say, by default, there are three instances for resiliency in different AZs.

127
00:12:35.300 --> 00:12:42.740
And this version will eventually become active. So effectively, you publish a new Lambda, the instances are created in different availability zones.

128
00:12:42.740 --> 00:12:44.580
That Lambda is now considered active.

129
00:12:44.580 --> 00:12:48.820
When an invocation comes in, your environment is going to start to consume CPU and memory.

130
00:12:48.820 --> 00:12:57.540
And that's where you have a Lambda agent, which is running within the EC2 instances and reporting this consumption to what AWS calls the scaler.

131
00:12:57.540 --> 00:13:02.660
And the scaler is effectively the component that decides, do we need to add more environments or more instances?

132
00:13:02.660 --> 00:13:08.580
So it's this continuous conversation between all these different moving parts to try to determine, are we using enough capacity?

133
00:13:08.580 --> 00:13:12.580
Do we need more? Is there still space to run more Lambda functions?

134
00:13:12.580 --> 00:13:24.260
So that's another dimension to consider.

135
00:13:25.300 --> 00:13:30.180
Again, when the traffic goes down, that's another thing we need to consider.

136
00:13:30.180 --> 00:13:36.420
The agent report that too, the whole system can decide to scale down environments and instances accordingly.

137
00:13:36.420 --> 00:13:41.420
So I think that gives you the general idea of how things work more at an abstract level.

138
00:13:41.420 --> 00:13:47.420
When we talked about the restaurant analogy, we also spoke a little bit more in the actual implementation details with the different components.

139
00:13:47.420 --> 00:13:54.420
But what probably matters the most is what can you control as a developer building your applications with this new model in mind?

140
00:13:54.420 --> 00:13:59.420
What kind of tweaks and toggles can you touch?

141
00:13:59.420 --> 00:14:04.420
Well, at the function level, you can pick how big your execution environment is.

142
00:14:04.420 --> 00:14:07.420
That's in terms of vCPUs and memory.

143
00:14:07.420 --> 00:14:12.420
The smallest supported size is 2 gigabytes and 1 vCPU.

144
00:14:12.420 --> 00:14:19.420
The key is to choose a size that supports your intended multi-concurrency because each environment is meant to handle multiple invocations.

145
00:14:19.420 --> 00:14:25.420
The other big thing here is that previously you had a 10 gigabyte memory limit for default Lambda mode.

146
00:14:25.420 --> 00:14:31.420
Now you can have 32 gigabytes available to a Lambda invocation, which is a big change.

147
00:14:31.420 --> 00:14:41.420
The rule of thumb, I think, if you're doing CPU heavy work with not that much IO, you typically want more vCPUs rather than just cranking concurrency.

148
00:14:41.420 --> 00:14:45.420
And then you can specify the maximum concurrency per environment.

149
00:14:45.420 --> 00:14:48.420
So with default in Lambda, that would be a one-to-one ratio.

150
00:14:48.420 --> 00:14:54.420
But here you can have up to 64 concurrent invocations per vCPU.

151
00:14:54.420 --> 00:14:58.420
So you can increase it if each invocation is light on CPU and maybe more IO bound.

152
00:14:58.420 --> 00:15:02.420
Then you get more throughput per environment and you'll get more cost benefit as well.

153
00:15:03.420 --> 00:15:05.420
And you can decrease it if you're memory heavy and CPU light.

154
00:15:05.420 --> 00:15:08.420
Then that's at the function level.

155
00:15:08.420 --> 00:15:13.420
And then at the capacity provider level, you can specify your target resource utilization.

156
00:15:13.420 --> 00:15:16.420
So how much headroom do you want really?

157
00:15:16.420 --> 00:15:22.420
So a higher target would be higher utilization, potentially lower cost, but less headroom.

158
00:15:22.420 --> 00:15:29.420
And then a lower target you could use if you want more spare capacity for bursts, but you'll pay for more idle compute.

159
00:15:29.420 --> 00:15:34.420
And then you can specify your instance types so you can constrain instance types.

160
00:15:34.420 --> 00:15:39.420
But AWS recommends letting Lambda choose for best availability.

161
00:15:39.420 --> 00:15:43.420
So don't be too restrictive and specific on what instance types you support.

162
00:15:43.420 --> 00:15:48.420
So when it comes to the two scaling modes, manual versus automatic scaling,

163
00:15:48.420 --> 00:15:51.420
at the capacity provider level, you can specify which one you want.

164
00:15:51.420 --> 00:15:52.420
Auto is the default.

165
00:15:52.420 --> 00:15:56.420
Manual exists when you want precise control over the scaling threshold.

166
00:15:56.420 --> 00:16:04.420
Now from what we've seen, that's basically just a CPU scaling threshold, CPU utilization scaling threshold.

167
00:16:04.420 --> 00:16:14.420
I haven't seen any ways yet, haven't really tried it either, but of using other maybe custom metrics or other metrics to scale like you could with an auto scaling group.

168
00:16:14.420 --> 00:16:19.420
Then separately at the function level, you can specify the minimum and maximum execution environments.

169
00:16:19.420 --> 00:16:27.420
So this is a particularly important one, because you need to, if you want your function to be invokable, you have to specify a non-zero minimum.

170
00:16:27.420 --> 00:16:31.420
Then AWS might add more as it sees fit.

171
00:16:31.420 --> 00:16:37.420
But if you set it to zero, as we'll discuss later, that's basically turning off your function, making it not invokable.

172
00:16:37.420 --> 00:16:44.420
But you can change the scaling characteristics with a put function scaling config API.

173
00:16:44.420 --> 00:16:52.420
And that allows, that'll allow you to have like more brute force or manual control over scaling before a batch of background processing.

174
00:16:52.420 --> 00:16:56.420
If you wanted to do some large scale processing during the night, for example.

175
00:16:56.420 --> 00:17:00.420
So given that, I hope that made some sense. Luciano, do you want to talk through what we built?

176
00:17:00.420 --> 00:17:03.420
Yes, we like to do practical test drives.

177
00:17:03.420 --> 00:17:13.420
And so we needed to find an excuse and think, what can we actually build that maybe makes a little bit of sense when it comes into the context of Lambda MI and its particular characteristics.

178
00:17:13.420 --> 00:17:23.420
So what we thought is, again, video processing, which seems to come up a lot in our examples, maybe because we think about this podcast and how to optimize the production of the podcast itself.

179
00:17:23.420 --> 00:17:36.420
And to be fair, this is not like a full implementation. We didn't really implement like, I don't know, wiring FFmpeg or ML vision models or I don't know, subtitles type of things.

180
00:17:36.420 --> 00:17:40.420
It's more the idea that the processing is simulated, but we built everything around it.

181
00:17:40.420 --> 00:17:45.420
So in the computation bit, you could plug in whatever logic you like.

182
00:17:45.420 --> 00:17:57.420
So if you want to use this, what we built, for instance, to, I don't know, extract the audio from a video or convert the video or whatever else you want to do that you can do with FFmpeg or something else, you can definitely do it.

183
00:17:57.420 --> 00:18:01.420
So the idea is that we built a service with three main components.

184
00:18:01.420 --> 00:18:10.420
So there is a REST API that allows you to manage videos. So effectively it's like a CRUD API where you can create a video entry, you can list them, you can get the details.

185
00:18:10.420 --> 00:18:15.420
And most importantly, you can trigger processing and we'll see how that works in a second.

186
00:18:15.420 --> 00:18:20.420
So one detail that might be important to know is that we built a little bit of a lambdalith.

187
00:18:20.420 --> 00:18:27.420
So effectively it's one Lambda that can respond to all the routes and it's behind an HTTP API gateway.

188
00:18:27.420 --> 00:18:39.420
So then we have a simulated video processor. So whenever you call the process Lambda, sorry, the process API endpoint that we mentioned before, effectively you want to trigger the processing of a video.

189
00:18:39.420 --> 00:18:46.420
So that happens in this other component, which is a simulated video processor that effectively is where you will put all the heavy lifting.

190
00:18:46.420 --> 00:18:56.420
Like, as we say, different use cases might come to mind, thumbnail generation, transcoding, content analysis, subtitle generation, I don't know, chapter generation, whatever you think it makes sense.

191
00:18:56.420 --> 00:19:00.420
So in a real system, this is generally something that will be CPU intensive work.

192
00:19:00.420 --> 00:19:08.420
So particularly sensible for this use case where you might spin up a lot of EC2 capacity just to be able to do that at scale.

193
00:19:08.420 --> 00:19:15.420
And then you still have the convenience of Lambda to package your code in a way that gives you a nice developer experience.

194
00:19:15.420 --> 00:19:20.420
And then finally, we have a step function, which may be a little bit unexpected. We'll explain why we did that.

195
00:19:20.420 --> 00:19:24.420
But the idea is that with this step function, we can orchestrate everything.

196
00:19:24.420 --> 00:19:38.420
And the idea is that we don't want to have, so we actually want to have capacity always available for the rest API, but for the processor component, we only want to have capacity on demand while still having the convenience of Lambda MI.

197
00:19:38.420 --> 00:19:43.420
So the idea is that we, I don't know if this is actually a little bit of a hack, it feels a little bit like that.

198
00:19:43.420 --> 00:19:53.420
But the idea is that we keep the capacity for the processor to zero, which means that by default, effectively, that Lambda is deactivated.

199
00:19:53.420 --> 00:20:01.420
But then anytime somebody is calling the process endpoint, we actually use the step function to spin up more capacity.

200
00:20:01.420 --> 00:20:11.420
So we basically change at runtime the capacity definition to actually go from zero to a different range that, of course, you can configure to whatever makes sense to you.

201
00:20:11.420 --> 00:20:13.420
But of course, one to whatever you like.

202
00:20:13.420 --> 00:20:24.420
So at that point, we can start to see the instances appearing and the step function monitors to the point that there is enough capacity to start doing the processing and effectively run our code.

203
00:20:24.420 --> 00:20:33.420
So it is a little bit of a hack to effectively get that scale to zero, which of course only makes sense if you control the event that triggers, in this case, the processing.

204
00:20:33.420 --> 00:20:36.420
It wouldn't make too much sense, for example, in a REST API.

205
00:20:36.420 --> 00:20:47.420
So this is how we tested the two different scenarios, one where we have some kind of manual control of the capacity and the other one where we are just letting the service manage all of that.

206
00:20:47.420 --> 00:20:58.420
And the way that we do that scaling up mechanism or effectively changing, controlling the details of the scaling configuration of the processor function is that there is an API called put function scaling config.

207
00:20:58.420 --> 00:21:01.420
And that's where you can define the minimum and the maximum capacity.

208
00:21:01.420 --> 00:21:06.420
If you set the minimum to zero, you are basically saying this environment is disabled.

209
00:21:06.420 --> 00:21:13.420
And when you set it to more like one or whatever you like, then effectively you are creating the EC2 instances.

210
00:21:13.420 --> 00:21:15.420
You are letting the capacity provider create the EC2 instances.

211
00:21:15.420 --> 00:21:20.420
And then that's where your Lambda function becomes active and you can invoke it.

212
00:21:20.420 --> 00:21:22.420
Other small details that might be relevant.

213
00:21:22.420 --> 00:21:24.420
We use CDK with TypeScript.

214
00:21:24.420 --> 00:21:28.420
And if you use the latest versions, everything we just mentioned is supported out of the box.

215
00:21:28.420 --> 00:21:31.420
So there are no weird hacks that we could see.

216
00:21:31.420 --> 00:21:36.420
We use Node.js and Node.js 24 on ARM64.

217
00:21:36.420 --> 00:21:38.420
We store metadata in DynamoDB.

218
00:21:38.420 --> 00:21:40.420
So all pretty standard, I would say.

219
00:21:40.420 --> 00:21:42.420
All this code is available on GitHub.

220
00:21:42.420 --> 00:21:43.420
So publicly available.

221
00:21:43.420 --> 00:21:45.420
You'll find the link in the show notes.

222
00:21:45.420 --> 00:21:55.420
So feel free to check it out and let us know if you like it, or even feel free to submit a PR if there is something else that you find doesn't work or maybe you want to change and improve.

223
00:21:55.420 --> 00:22:01.420
So based on all of that, Eoin, what are our impressions or maybe limitations or pitfalls that we discovered?

224
00:22:07.420 --> 00:22:16.420
Well, given that we just did the ECS managed instances review in our last episode, selecting instances here with Lambda MI is a bit more limited because in ECS MI you have more of a query system where you specify characteristics of your instance requirements.

225
00:22:16.420 --> 00:22:22.420
And AWS will pick the right instance type for that, which kind of abstracts you from having to think about specific instance types.

226
00:22:22.420 --> 00:22:28.420
And if new instance types become available, they'll automatically get included in the potential query results.

227
00:22:28.420 --> 00:22:40.420
But with Lambda MI, you can only specify a very limited set of parameters, initially maximum vCPU count, and the instance type filter, which is basically allowed instance types or excluded instance types.

228
00:22:40.420 --> 00:22:46.420
So an inclusion list and an exclusion list, and you still have to think about actual instance types.

229
00:22:46.420 --> 00:22:48.420
So it's strange they didn't follow the same model.

230
00:22:48.420 --> 00:22:50.420
Region availability is limited for now.

231
00:22:50.420 --> 00:22:55.420
If your workload is multi-region or you want to use specific regions, right now we've got what?

232
00:22:55.420 --> 00:23:00.420
US East 1, US East 2, US West 2, AP Northeast 1, and EU West 1.

233
00:23:00.420 --> 00:23:02.420
So just five regions.

234
00:23:02.420 --> 00:23:06.420
In terms of runtime, the support is for the latest version only.

235
00:23:06.420 --> 00:23:12.420
Some people, especially enterprises, rely on the ability to pin specific versions of managed runtimes.

236
00:23:12.420 --> 00:23:17.420
So if you're migrating existing functions, anything on older runtimes will not qualify.

237
00:23:17.420 --> 00:23:20.420
You have to use the latest supported one.

238
00:23:20.420 --> 00:23:25.420
Anything that's on the deprecated list or previous list is not possible.

239
00:23:25.420 --> 00:23:28.420
VPC networking is now mandatory.

240
00:23:28.420 --> 00:23:31.420
Of course it is because you know you have to run EC2 instances.

241
00:23:31.420 --> 00:23:33.420
EC2 instances, there's no exceptions.

242
00:23:33.420 --> 00:23:35.420
They always need a VPC.

243
00:23:35.420 --> 00:23:39.420
So if you put them in a private subnet with no egress, you're going to have an issue.

244
00:23:39.420 --> 00:23:46.420
You'd have to have, make sure your VPC has reachability through VPC endpoints to AWS services your Lambda function might need.

245
00:23:46.420 --> 00:23:49.420
Think about S3, DynamoDB, SSM.

246
00:23:49.420 --> 00:23:54.420
So you'll need either a NAT gateway, internet gateway, or a VPC endpoint.

247
00:23:54.420 --> 00:24:00.420
This is exactly the same concern when you have a normal Lambda function in a VPC.

248
00:24:00.420 --> 00:24:05.420
But you just don't have an option to avoid VPCs now.

249
00:24:05.420 --> 00:24:09.420
Deployments with Lambda MI can be noticeably slower.

250
00:24:09.420 --> 00:24:16.420
The first published on a new capacity provider has to launch managed instances and bring up execution environments before the version becomes active.

251
00:24:16.420 --> 00:24:24.420
AWS explicitly says this can take several minutes and we've encountered examples of around eight minutes for end-to-end deployment.

252
00:24:24.420 --> 00:24:28.420
And then there's a minimum size jump.

253
00:24:28.420 --> 00:24:33.420
You don't have the option of small utility Lambda functions with 128 megabytes of RAM anymore.

254
00:24:33.420 --> 00:24:37.420
With managed instances, the smallest one is two gigabytes with one vCPU.

255
00:24:37.420 --> 00:24:43.420
So it could be a surprise if you're just trying it out and trying to keep things minimal in terms of resources and costs.

256
00:24:43.420 --> 00:24:52.420
Also important to know that creating a capacity provider with manual scaling still spins up baseline capacity.

257
00:24:52.420 --> 00:25:03.420
We saw in our cases when we were doing testing, even when we created a capacity provider without attaching any functions to it, AWS was starting two instances.

258
00:25:03.420 --> 00:25:07.420
It's worth knowing if you expect it to be zero until you're attached to a function.

259
00:25:07.420 --> 00:25:14.420
We also had cases where after scaling down instances remained active for arbitrary lengths of time.

260
00:25:14.420 --> 00:25:16.420
It's definitely something to keep an eye on.

261
00:25:16.420 --> 00:25:17.420
Anything else to add to that, Luciano?

262
00:25:17.420 --> 00:25:23.420
Yeah, I think it's worth remarking that scale to zero is possible, but it is complex, so to speak.

263
00:25:23.420 --> 00:25:32.420
Like we have an example in our repo and people can check out the way we achieve it, but I don't think it's like a general purpose way that you can use it for any use case.

264
00:25:32.420 --> 00:25:40.420
In our use case, it makes sense just because we have a very clear execution path that determines when we need that compute capacity available.

265
00:25:40.420 --> 00:25:51.420
And we effectively get it almost like a cold start in our example, just on demand, if you want to call it like that, using this instance under the hood and by changing the min max execution environment on the fly.

266
00:25:51.420 --> 00:25:56.420
But I don't think, yeah, as we were saying that that's something you can use, for instance, for an API that wouldn't work, right?

267
00:25:56.420 --> 00:26:03.420
So you just need to be aware that while it is possible, it is complex and it's not something you can use as a general purpose mechanism.

268
00:26:03.420 --> 00:26:09.420
And the main reason is because when you set the min execution environment to zero, effectively your Lambda function becomes deactivated.

269
00:26:09.420 --> 00:26:18.420
And there is a very clear indication when you go in the web UI on that particular Lambda function, you will see a blue banner saying this version is deactivated.

270
00:26:18.420 --> 00:26:22.420
To activate it, you need to set the scaling to a value that is non-zero.

271
00:26:22.420 --> 00:26:24.420
So just be aware of that.

272
00:26:24.420 --> 00:26:28.420
If you were expecting automatic scale to zero, that's not necessarily the case.

273
00:26:28.420 --> 00:26:35.420
The other thing is that because now you get concurrent execution, that comes with a few more headaches from a developer perspective,

274
00:26:35.420 --> 00:26:38.420
which I don't think is necessarily a bad thing.

275
00:26:38.420 --> 00:26:45.420
It's probably useful, but it's just something you need to be aware and consider in your code because otherwise you might have unexpected bugs or side effects.

276
00:26:45.420 --> 00:26:51.420
And this is the same thing that you need to worry about anytime you are building an environment.

277
00:26:51.420 --> 00:26:55.420
It can even be a container, it can be effectively anything where you're running code concurrently,

278
00:26:55.420 --> 00:27:00.420
where you might end up with race conditions or issues of that kind.

279
00:27:00.420 --> 00:27:05.420
In the case, for instance, of Node.js, which is what we used, what can happen is, for instance,

280
00:27:05.420 --> 00:27:10.420
when you have global state that is shared across execution environments.

281
00:27:10.420 --> 00:27:17.420
So imagine you have a global variable that you put outside your handler, and then you reference that variable inside your handler.

282
00:27:17.420 --> 00:27:23.420
Because you might have multiple instances of that handler running at the same time concurrently,

283
00:27:23.420 --> 00:27:32.420
if they are both changing that value, then you might end up with inconsistent state where one handler suddenly is seeing the data that was changed from the previous handler.

284
00:27:32.420 --> 00:27:37.420
Imagine this is like, I don't know, a user session. You might have two users that are effectively overriding each other.

285
00:27:37.420 --> 00:27:41.420
And this is something that might lead to very serious bugs. So just be aware of that.

286
00:27:41.420 --> 00:27:44.420
Of course, there are other ways to avoid this problem.

287
00:27:44.420 --> 00:27:48.420
We're not going to go into detail here on how you can solve this particular problem, but just be aware the problem exists.

288
00:27:48.420 --> 00:27:55.420
And there are tons of best practices that you can find online for your specific language so that you don't run into kind of threading or concurrency issues,

289
00:27:55.420 --> 00:27:59.420
which you now might have depending on your language of choice, just because you have concurrency.

290
00:27:59.420 --> 00:28:06.420
And another similar issue is that if you're using the TMP folder, that's also a shared thing between concurrent executions.

291
00:28:06.420 --> 00:28:10.420
So again, the same issue might happen in the sense that if you are creating a file from an instance,

292
00:28:10.420 --> 00:28:15.420
and then another instance is also trying to create that file, they might end up overriding each other.

293
00:28:15.420 --> 00:28:21.420
So just be aware and make sure you select file names that don't conflict, maybe using UUIDs or something like that.

294
00:28:21.420 --> 00:28:26.420
Logs can also be problematic in that sense because they will interleave.

295
00:28:26.420 --> 00:28:33.420
And I suppose this is the reason why structured JSON logs are enabled by default and you don't get to change that.

296
00:28:33.420 --> 00:28:38.420
And AWS says they will include a request ID by default in every JSON line.

297
00:28:38.420 --> 00:28:43.420
So that should make it a little bit easier to avoid confusion between logs when you're just looking at the logs.

298
00:28:43.420 --> 00:28:49.420
But you can still see interleaved lines, so it's up to you to filter by request ID.

299
00:28:49.420 --> 00:28:59.420
There are lots more potential pitfalls and it's nice that AWS has put a documentation page that goes pretty much in detail also, not just with the problems, but with detailed solutions.

300
00:28:59.420 --> 00:29:02.420
So we'll just give you a link that you can find in the show notes if you're curious.

301
00:29:02.420 --> 00:29:04.420
And they are also organized by programming language.

302
00:29:04.420 --> 00:29:08.420
So probably that removes a lot of the noise depending on your language of choice.

303
00:29:08.420 --> 00:29:11.420
You'll be focused on what really matters for that particular language.

304
00:29:11.420 --> 00:29:16.420
Now, I suppose the last topic and probably one of the most interesting for most people is how much is this going to cost me?

305
00:29:16.420 --> 00:29:30.420
Yeah, a big change here really because one of the biggest mindset shifts with Lambda managed instances is that the billing is no longer your memory times duration default Lambda computer model.

306
00:29:30.420 --> 00:29:34.420
So you're not paying for that dimension at all anymore.

307
00:29:34.420 --> 00:29:42.420
But of course, you're paying for the underlying EC2 instances which are running in your account for a duration that you don't necessarily control to a fine level.

308
00:29:42.420 --> 00:29:48.420
So with managed instances for Lambda, the official pricing model has three dimensions running in parallel.

309
00:29:48.420 --> 00:29:51.420
So you still have request charges just like with default Lambda.

310
00:29:51.420 --> 00:29:54.420
You pay 20 cents per million requests.

311
00:29:54.420 --> 00:29:55.420
That's simple, familiar.

312
00:29:55.420 --> 00:29:58.420
It's independent from how long each request runs.

313
00:29:58.420 --> 00:30:04.420
And generally in any bills I've seen, that's a tiny negligible component compared to the other dimensions.

314
00:30:04.420 --> 00:30:06.420
Then you have your EC2 instance charge.

315
00:30:06.420 --> 00:30:13.420
And now you're paying the EC2 instance that backs your capacity providers using standard on-demand EC2 pricing.

316
00:30:13.420 --> 00:30:23.420
The key benefit is that now you can apply EC2 instance savings plans and reserved instances and any other EC2 discount mechanisms that might be applicable.

317
00:30:23.420 --> 00:30:31.420
The new thing is just like with ECS MI, now you've got kind of a management fee, managed instance tax, if you like.

318
00:30:31.420 --> 00:30:32.420
With ECS, that was 12%.

319
00:30:32.420 --> 00:30:33.420
We worked it out.

320
00:30:33.420 --> 00:30:40.420
With Lambda, it's a 15% premium calculated on the EC2 on-demand price of the instance.

321
00:30:40.420 --> 00:30:47.420
So the important nuance to this is EC2 discounts applied to the compute portion, not to the management fee.

322
00:30:47.420 --> 00:30:53.420
You're always paying 15% of the on-demand list price for the management fee.

323
00:30:53.420 --> 00:31:00.420
And also critically, spot instances are not yet supported, just like ECS, managed instances.

324
00:31:00.420 --> 00:31:08.420
What this all means is that if you've got steady state high volume workloads, you might have massive cost savings with Lambda MI,

325
00:31:08.420 --> 00:31:10.420
but you'll have to measure and have a look.

326
00:31:10.420 --> 00:31:15.420
Like you really need a consistent load or to be doing something like we did with the video processing example,

327
00:31:15.420 --> 00:31:21.420
where you're scaling it up for a certain amount of time, doing a large volume of batch processing and then scaling it down.

328
00:31:21.420 --> 00:31:27.420
But you can also, things like multi-concurrency and like you said Luciano, with the high volume API,

329
00:31:27.420 --> 00:31:30.420
you've also potential for cost savings too.

330
00:31:30.420 --> 00:31:39.420
So your mileage will vary, but high volume requests, longer running Lambdas really can benefit here.

331
00:31:39.420 --> 00:31:46.420
And of course, remember that you can leverage existing savings plans with EC2 and reserved instances to save even more.

332
00:31:46.420 --> 00:31:49.420
Yeah, I guess let's jump to the conclusions.

333
00:31:49.420 --> 00:31:54.420
I think what's important to mention here is that Lambda MI, it isn't like a new version of Lambda.

334
00:31:54.420 --> 00:31:58.420
It isn't Lambda replacing Lambda or Lambda V2 or whatever you want to call it.

335
00:31:58.420 --> 00:32:00.420
It's just a different execution model.

336
00:32:00.420 --> 00:32:03.420
So it's like a new option for executing Lambda code.

337
00:32:03.420 --> 00:32:06.420
So it's still the same Lambda developer experience and integrations.

338
00:32:06.420 --> 00:32:10.420
It's just the compute, the underlying compute is something that you get to control.

339
00:32:10.420 --> 00:32:13.420
Before it was just happening magically behind the scenes.

340
00:32:13.420 --> 00:32:15.420
That's why we love to call it serverless.

341
00:32:15.420 --> 00:32:23.420
Now it's a little bit less serverless, but it's an option and there are benefits and cases where you might want to use this particular option.

342
00:32:23.420 --> 00:32:25.420
But I personally, I don't know if I like it or not.

343
00:32:25.420 --> 00:32:37.420
I think it's a bit sad that makes my mental model or decision making process or decision tree, if you want to call it like that, a lot more complicated because now there are more options and more dimensions to think about.

344
00:32:37.420 --> 00:32:42.420
But at the same time, it's also a good thing because there are definitely cases where something like this is useful.

345
00:32:42.420 --> 00:32:51.420
So now you have that option without having to leave the comfort of Lambda or without having to do a massive refactor if you already have a solution that runs on Lambda.

346
00:32:51.420 --> 00:32:54.420
So again, good and bad things.

347
00:32:54.420 --> 00:32:55.420
You have more options to decide on.

348
00:32:55.420 --> 00:32:59.420
But at the same time, those options can be very useful in certain particular cases.

349
00:32:59.420 --> 00:33:02.420
I still expect those cases are maybe limited.

350
00:33:02.420 --> 00:33:13.420
Maybe it's more enterprises with big workloads or maybe cases where you're doing cost optimizations or maybe cases where you need a significant amount of concurrency.

351
00:33:13.420 --> 00:33:17.420
But those cases exist. So now be aware that this option exists.

352
00:33:17.420 --> 00:33:26.420
So as we said, it shines where you have steady state, predictable loads, high throughput APIs, CPU heavy or long running work, batch workflows and so on.

353
00:33:26.420 --> 00:33:38.420
I think it might not be the best use case if you are worried about fast spikes, for example, and those spikes can cause throttling.

354
00:33:38.420 --> 00:33:43.420
So depending on your use case, the more traditional Lambda approach might be better suited for that.

355
00:33:43.420 --> 00:33:49.420
You have more knobs to tune. So for instance, you have to think about max concurrency, utilization targets, instance shape.

356
00:33:49.420 --> 00:33:53.420
So definitely something else that's worth considering and adds complexity.

357
00:33:53.420 --> 00:34:01.420
And then the minimum function size is also something you need to be aware of because if you have, I really like, for instance, to do single purpose Lambda functions.

358
00:34:01.420 --> 00:34:04.420
So I sometimes end up with even hundreds of small Lambda functions.

359
00:34:04.420 --> 00:34:10.420
I think this approach pushes you a little bit more into the lambdalith land as we did in our particular example.

360
00:34:10.420 --> 00:34:13.420
So just be aware that there is nothing necessarily wrong with it.

361
00:34:13.420 --> 00:34:15.420
It's just something you need to consider.

362
00:34:15.420 --> 00:34:30.420
And I'd really like if there was an option here where if it was scaling slowly and maybe you're getting some throttling because it's trying to provision a new EC2 managed instance for you.

363
00:34:30.420 --> 00:34:34.420
If then you could say, well, in the meantime, just use default Lambda for that function, right?

364
00:34:34.420 --> 00:34:41.420
And handle the scale by using the normal execution mode, but you can't mix the two with Lambda-MI.

365
00:34:41.420 --> 00:34:44.420
You're either using one or the other, which is a bit of a shame.

366
00:34:44.420 --> 00:34:47.420
It would be nice to be able to have a blend.

367
00:34:47.420 --> 00:34:52.420
Yeah, maybe we can consider this a feature request if anyone from AWS is listening, but I definitely agree with you.

368
00:34:52.420 --> 00:34:59.420
So just to close things off, the bottom line is that this is a new tool and it might be a very good tool for the right workloads.

369
00:34:59.420 --> 00:35:09.420
It's not necessarily something we should consider as an upgrade to Lambda, but then yet again, it's definitely a useful tool and it's worth to know when to use it and how you can use it.

370
00:35:09.420 --> 00:35:12.420
I remind you that we have a repository with an example.

371
00:35:12.420 --> 00:35:13.420
You can find it in the show notes.

372
00:35:13.420 --> 00:35:15.420
So if you have tried it, let us know.

373
00:35:15.420 --> 00:35:17.420
If you like it, let us know why.

374
00:35:17.420 --> 00:35:19.420
If you don't like it, also let us know why.

375
00:35:19.420 --> 00:35:25.420
We are always open to talk to you and hear your opinion and see if maybe you found other use cases that we didn't think about.

376
00:35:25.420 --> 00:35:26.420
One last thing.

377
00:35:26.420 --> 00:35:30.420
Thanks to fourTheorem for sponsoring yet another episode of AWS Bites.

378
00:35:30.420 --> 00:35:32.420
fourTheorem is an AWS partner.

379
00:35:32.420 --> 00:35:33.420
It's a consulting company.

380
00:35:33.420 --> 00:35:36.420
We can help you with your AWS architecture.

381
00:35:36.420 --> 00:35:40.420
We can make sure that your implementation are simple, scalable, cost-sane.

382
00:35:40.420 --> 00:35:47.420
So if you're curious, check out fourTheorem.com and find our case studies and get in touch if you want to know more.

383
00:35:47.420 --> 00:35:50.420
So thank you very much and we'll see you in the next episode.
