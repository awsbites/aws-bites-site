WEBVTT

1
00:00:00.000 --> 00:00:02.880
The public cloud gives you amazing machine learning powers

2
00:00:02.880 --> 00:00:04.300
with a low barrier to entry.

3
00:00:04.300 --> 00:00:05.480
Once you know where to begin,

4
00:00:05.480 --> 00:00:08.240
you can quickly build solutions to process images,

5
00:00:08.240 --> 00:00:10.760
video, text, and audio, as well as data.

6
00:00:10.760 --> 00:00:12.560
Today, you will hear about

7
00:00:12.560 --> 00:00:15.040
all the available managed AI services on AWS

8
00:00:15.040 --> 00:00:18.080
that require zero machine learning expertise,

9
00:00:18.080 --> 00:00:20.080
services you can use to run custom models,

10
00:00:20.080 --> 00:00:21.840
some different use cases,

11
00:00:21.840 --> 00:00:23.160
and some of the things you need to consider

12
00:00:23.160 --> 00:00:25.360
before you do machine learning in production.

13
00:00:25.360 --> 00:00:27.340
My name is Eoin, I'm joined by Luciano,

14
00:00:27.340 --> 00:00:29.920
and this is the AWS Bites podcast.

15
00:00:29.920 --> 00:00:34.920


16
00:00:40.040 --> 00:00:41.280
So, okay, when we're talking about machine learning, I'm always very confused

17
00:00:41.280 --> 00:00:43.720
because it's a very broad and loose term.

18
00:00:43.720 --> 00:00:46.000
So today, what kind of machine learning

19
00:00:46.000 --> 00:00:46.920
are we talking about?

20
00:00:46.920 --> 00:00:49.980
Let's give it a somewhat of definition.

21
00:00:49.980 --> 00:00:51.400
I think we're mainly focusing today

22
00:00:51.400 --> 00:00:53.880
on the latest generation of machine learning,

23
00:00:53.880 --> 00:00:56.280
so built on deep neural networks

24
00:00:56.280 --> 00:00:58.480
or deep learning, as it's called.

25
00:00:58.480 --> 00:00:59.840
So in the last 10 years,

26
00:00:59.840 --> 00:01:02.360
there's been a big leap forward in machine learning,

27
00:01:02.360 --> 00:01:04.240
mainly down to three things.

28
00:01:04.240 --> 00:01:07.820
One is the availability of massive amounts of data

29
00:01:07.820 --> 00:01:11.280
from internet scale web and mobile applications out there.

30
00:01:11.280 --> 00:01:13.840
Another one is the availability of the cloud

31
00:01:13.840 --> 00:01:15.600
and scalable compute resources

32
00:01:15.600 --> 00:01:18.560
to do training and run machine learning.

33
00:01:18.560 --> 00:01:20.200
And the third one is the improvements

34
00:01:20.200 --> 00:01:21.800
in the machine learning algorithms themselves,

35
00:01:21.800 --> 00:01:25.000
and specifically the advancements in deep neural networks,

36
00:01:25.000 --> 00:01:27.280
which have allowed us to do all sorts of things

37
00:01:27.280 --> 00:01:31.800
like natural language recognition, image recognition,

38
00:01:31.800 --> 00:01:33.200
and those are the kinds of things

39
00:01:33.200 --> 00:01:35.360
that drive a lot of the machine learning services

40
00:01:35.360 --> 00:01:37.680
we see today, like Alexa,

41
00:01:37.680 --> 00:01:41.180
or maybe GitHub Copilot.

42
00:01:42.440 --> 00:01:44.320
Dali is another one

43
00:01:44.320 --> 00:01:46.120
which people might be familiar with now, right,

44
00:01:46.120 --> 00:01:47.720
which allows you to generate images

45
00:01:47.720 --> 00:01:49.400
from an arbitrary description.

46
00:01:49.400 --> 00:01:51.080
Yeah, no, that's super cool.

47
00:01:51.080 --> 00:01:54.060
One thing that I'm always fascinated about

48
00:01:54.060 --> 00:01:55.640
when talking about machine learning

49
00:01:55.640 --> 00:01:59.680
is that there seems to be like a very big and long process

50
00:01:59.680 --> 00:02:02.160
every time you want to come up with, I don't know,

51
00:02:02.160 --> 00:02:04.460
a new model that solves a specific problem.

52
00:02:04.460 --> 00:02:06.480
So what are the different steps

53
00:02:06.480 --> 00:02:10.120
or the different main areas that are always present

54
00:02:10.120 --> 00:02:12.000
when you want to do ML?

55
00:02:12.000 --> 00:02:14.600
Every machine learning expert I talk to

56
00:02:14.600 --> 00:02:18.120
or data scientist will say that preparation of data,

57
00:02:18.120 --> 00:02:21.220
so getting your data, preparing it, cleaning it,

58
00:02:21.220 --> 00:02:25.280
normalizing it is at least 80% of the effort.

59
00:02:25.280 --> 00:02:26.720
So that's the first one.

60
00:02:26.720 --> 00:02:28.040
And then you have training.

61
00:02:28.040 --> 00:02:30.380
So creating the model from that data.

62
00:02:30.380 --> 00:02:33.120
So that could be the bulk of the rest of your effort.

63
00:02:33.120 --> 00:02:34.540
And then the inference part itself,

64
00:02:34.540 --> 00:02:36.980
which is like running your model in production,

65
00:02:36.980 --> 00:02:38.600
that's like 5% of the effort really.

66
00:02:38.600 --> 00:02:42.060
So it's kind of an 80, 15, five split.

67
00:02:43.360 --> 00:02:46.300
So there's a huge amount of effort required to train,

68
00:02:46.300 --> 00:02:48.960
including acquiring and preparing all your data.

69
00:02:48.960 --> 00:02:51.140
And that makes kind of pre-trained managed services

70
00:02:51.140 --> 00:02:51.980
very appealing.

71
00:02:51.980 --> 00:02:53.600
I mean, specifically for me,

72
00:02:53.600 --> 00:02:55.240
because I'm not a machine learning expert.

73
00:02:55.240 --> 00:02:57.120
I'm very interested in the topic,

74
00:02:57.120 --> 00:02:59.180
but I'm always looking for managed services

75
00:02:59.180 --> 00:03:00.560
that take all that heavy lifting

76
00:03:00.560 --> 00:03:04.120
and the need to have very specialist skills away from me.

77
00:03:04.120 --> 00:03:05.720
Yeah, I suppose also the cost.

78
00:03:05.720 --> 00:03:09.880
Like when you put all this time into that particular phase,

79
00:03:09.880 --> 00:03:11.820
if you can just use that as a service,

80
00:03:11.820 --> 00:03:13.320
you're probably saving a lot of costs

81
00:03:13.320 --> 00:03:15.320
and a lot of time to production, right?

82
00:03:15.320 --> 00:03:18.840
So what kind of services do we find in AWS

83
00:03:18.840 --> 00:03:20.760
that give us that kind of experience?

84
00:03:20.760 --> 00:03:23.280
You don't need to think about models

85
00:03:23.280 --> 00:03:25.260
and preparing all the data upfront,

86
00:03:25.260 --> 00:03:28.180
but more as a user, I just need this feature.

87
00:03:28.180 --> 00:03:30.680
Just give me a service that is gonna do that well enough.

88
00:03:30.680 --> 00:03:32.280
And maybe I can fine tune a few things

89
00:03:32.280 --> 00:03:33.920
for my particular use case.

90
00:03:33.920 --> 00:03:34.960
There's a lot here.

91
00:03:34.960 --> 00:03:36.540
So let's run through them really quickly.

92
00:03:36.540 --> 00:03:39.200
So AWS Recognition is one of the most popular

93
00:03:39.200 --> 00:03:40.440
and that's the computer vision one.

94
00:03:40.440 --> 00:03:42.860
So if you've got images or videos,

95
00:03:42.860 --> 00:03:45.880
it can process those and recognize people

96
00:03:45.880 --> 00:03:49.040
or text within photographs or objects.

97
00:03:49.040 --> 00:03:50.240
It's even got a feature that allows you

98
00:03:50.240 --> 00:03:53.440
to recognize celebrities in an image.

99
00:03:53.440 --> 00:03:56.320
So that works on images or video recordings

100
00:03:56.320 --> 00:03:58.160
and also streaming video.

101
00:03:58.160 --> 00:04:01.560
Then you have text to speech and speech to text.

102
00:04:01.560 --> 00:04:04.880
So Polly is the Amazon service that does text to speech

103
00:04:04.880 --> 00:04:08.400
and Transcribe is the one that does speech to text.

104
00:04:08.400 --> 00:04:10.000
There's a couple of the newer ones

105
00:04:10.000 --> 00:04:12.200
that are more kind of business oriented.

106
00:04:12.200 --> 00:04:15.780
Like forecast is about time series forecasting.

107
00:04:15.780 --> 00:04:17.200
So if you can imagine a use case,

108
00:04:17.200 --> 00:04:19.400
if you have all of your historical sales data

109
00:04:19.400 --> 00:04:21.120
for your e-commerce platform,

110
00:04:21.120 --> 00:04:23.040
and you want to project and predict your sales

111
00:04:23.040 --> 00:04:28.040
for Q4 2022, you could use forecast to help you to do that.

112
00:04:28.680 --> 00:04:31.560
Of course, like events can change that.

113
00:04:31.560 --> 00:04:35.420
I heard Mark our fourth year machine learning expert

114
00:04:35.420 --> 00:04:37.920
mentioned that, you know, at the start of the pandemic,

115
00:04:37.920 --> 00:04:40.680
a lot of people were using forecasting models like this

116
00:04:40.680 --> 00:04:42.720
and they all became completely useless

117
00:04:42.720 --> 00:04:44.740
in the face of world events.

118
00:04:45.800 --> 00:04:48.320
Personalize is another one then, which is also based,

119
00:04:48.320 --> 00:04:50.800
a lot of these services are based on amazon.com

120
00:04:50.800 --> 00:04:53.600
retail models that they've used and have trained

121
00:04:53.600 --> 00:04:56.240
based on their data personalizes one of those.

122
00:04:56.240 --> 00:04:59.300
So if you're browsing amazon.com,

123
00:04:59.300 --> 00:05:01.440
you often see product recommendations

124
00:05:01.440 --> 00:05:04.520
based on your browsing history and your purchase history.

125
00:05:04.520 --> 00:05:07.040
So personalize gives you the ability to do that

126
00:05:07.040 --> 00:05:07.960
within your own service.

127
00:05:07.960 --> 00:05:11.160
So if you're building the next version of Netflix

128
00:05:11.160 --> 00:05:13.500
and you want to do recommendations on video titles,

129
00:05:13.500 --> 00:05:15.380
that's the service you'd go for.

130
00:05:16.220 --> 00:05:17.720
Comprehend is one I've used quite a lot,

131
00:05:17.720 --> 00:05:20.120
which is for text analysis.

132
00:05:20.120 --> 00:05:22.560
And that allows you to do named entity recognition.

133
00:05:22.560 --> 00:05:25.100
So if you've got a document and you're looking to identify

134
00:05:25.100 --> 00:05:27.340
the people, the places, the dates,

135
00:05:27.340 --> 00:05:29.080
the locations in that document,

136
00:05:29.080 --> 00:05:31.320
Comprehend will do that for you.

137
00:05:31.320 --> 00:05:33.560
And it'll also do sentiment analysis.

138
00:05:33.560 --> 00:05:36.360
So if you want to monitor social media feeds

139
00:05:36.360 --> 00:05:39.440
about your company and figure out if people are complaining

140
00:05:39.440 --> 00:05:41.320
or very happy and respond accordingly,

141
00:05:41.320 --> 00:05:42.960
you could use Comprehend for that.

142
00:05:44.240 --> 00:05:46.040
If you want to do something more high level,

143
00:05:46.040 --> 00:05:48.240
kind of on that theme, Lex is the chatbot,

144
00:05:48.240 --> 00:05:50.220
one that a lot of people might be familiar with.

145
00:05:50.220 --> 00:05:52.600
So this is if you want to have an interactive chatbot

146
00:05:52.600 --> 00:05:55.180
in your mobile app or a webpage,

147
00:05:55.180 --> 00:05:58.040
that's machine learning driven because it'll understand

148
00:05:58.040 --> 00:06:00.080
the intent of what the user is trying to say

149
00:06:00.080 --> 00:06:01.760
and allow you to direct a conversation.

150
00:06:01.760 --> 00:06:04.520
So it's like an orchestration for an interaction

151
00:06:04.520 --> 00:06:09.400
between a customer and a robot representing your company.

152
00:06:10.560 --> 00:06:13.360
The last couple we've mentioned there are Textract,

153
00:06:13.360 --> 00:06:15.680
which is very useful for document processing.

154
00:06:15.680 --> 00:06:17.960
So if you've got images, PDFs,

155
00:06:17.960 --> 00:06:20.640
and you want to extract all the text out of them,

156
00:06:20.640 --> 00:06:24.800
they might have tables and you want to extract that out

157
00:06:24.800 --> 00:06:27.920
as structured data, Textract will do that for you.

158
00:06:27.920 --> 00:06:29.420
And then there's Translate.

159
00:06:29.420 --> 00:06:33.160
So Translate is for translating from one language to another.

160
00:06:33.160 --> 00:06:34.120
That's super interesting.

161
00:06:34.120 --> 00:06:36.240
But I suppose that all the services,

162
00:06:36.240 --> 00:06:39.320
they are available to all different kinds of industries.

163
00:06:39.320 --> 00:06:43.460
So probably there is like a good enough expectation

164
00:06:43.460 --> 00:06:44.820
in terms of results.

165
00:06:44.820 --> 00:06:47.280
But I assume that given more specific use cases,

166
00:06:47.280 --> 00:06:49.080
you probably want to fine tune something.

167
00:06:49.080 --> 00:06:52.160
So what are the options there to adapt these services

168
00:06:52.160 --> 00:06:54.900
and be more accurate for specific use cases?

169
00:06:56.760 --> 00:07:00.280
Yes, some of those services allow you to cross train

170
00:07:00.280 --> 00:07:01.260
with your own data.

171
00:07:01.260 --> 00:07:04.000
So one example of that would be recognition.

172
00:07:04.000 --> 00:07:06.320
Again, so if you're processing images

173
00:07:06.320 --> 00:07:07.720
and you want to identify something

174
00:07:07.720 --> 00:07:09.820
that it doesn't recognize out of the box,

175
00:07:09.820 --> 00:07:12.240
now the default set of labels that it does recognize

176
00:07:12.240 --> 00:07:13.080
is quite large.

177
00:07:13.080 --> 00:07:15.200
You can go onto the website and download a CSV

178
00:07:15.200 --> 00:07:17.680
of all the labels of things they identify.

179
00:07:17.680 --> 00:07:18.720
It's a long list.

180
00:07:20.820 --> 00:07:21.800
But you can also add your own.

181
00:07:21.800 --> 00:07:23.960
So depending on your own business case.

182
00:07:23.960 --> 00:07:26.440
One example of that, and I've seen a few companies

183
00:07:26.440 --> 00:07:29.440
try to do this is if let's say you're a global

184
00:07:29.440 --> 00:07:33.240
consumer goods company, and you've got 1500 brands

185
00:07:33.240 --> 00:07:37.600
in your portfolio, and you want to monitor social media

186
00:07:37.600 --> 00:07:39.820
for people taking pictures of your product,

187
00:07:39.820 --> 00:07:42.120
and maybe commenting on your product.

188
00:07:42.120 --> 00:07:44.820
So you could process images you see on Twitter,

189
00:07:44.820 --> 00:07:46.600
pass them through to recognition,

190
00:07:46.600 --> 00:07:49.200
but train recognition to recognize your logo

191
00:07:49.200 --> 00:07:52.040
or your products, images of your product,

192
00:07:52.040 --> 00:07:53.760
and then label them accordingly.

193
00:07:53.760 --> 00:07:55.920
Then you could maybe use comprehensive sentiment,

194
00:07:55.920 --> 00:07:58.100
analyze the text.

195
00:07:58.100 --> 00:08:01.720
And if there's negative sentiment about your brands,

196
00:08:01.720 --> 00:08:06.000
you might route that through as a support query

197
00:08:06.000 --> 00:08:09.000
to the customer support department of individual brands.

198
00:08:09.000 --> 00:08:13.000
We actually, the book, myself and Peter wrote,

199
00:08:13.000 --> 00:08:14.400
the AI as a Service book.

200
00:08:14.400 --> 00:08:17.000
We have examples that do something like that

201
00:08:17.000 --> 00:08:19.760
with Comprehend for social media sentiment analysis.

202
00:08:19.760 --> 00:08:21.720
This is another few use cases in that book actually.

203
00:08:21.720 --> 00:08:23.860
So we'll link to that in the show notes.

204
00:08:23.860 --> 00:08:25.800
Yeah, that's super cool.

205
00:08:25.800 --> 00:08:28.560
And I suppose being AWS, one of the cool things

206
00:08:28.560 --> 00:08:31.200
is that probably all the services are very well integrated

207
00:08:31.200 --> 00:08:33.120
with all the AWS ecosystem.

208
00:08:33.120 --> 00:08:36.120
I'm going to assume that you just have,

209
00:08:36.120 --> 00:08:38.760
as part of the SDK, access to all these different services.

210
00:08:38.760 --> 00:08:41.560
So you can, for instance, call different features

211
00:08:41.560 --> 00:08:42.960
of the service from a Lambda

212
00:08:42.960 --> 00:08:45.200
or from some other compute service.

213
00:08:45.200 --> 00:08:46.040
Is that the case?

214
00:08:46.040 --> 00:08:47.480
Or is there something else to be aware of there?

215
00:08:47.480 --> 00:08:48.320
Yeah, exactly.

216
00:08:48.320 --> 00:08:49.140
That's how you use it.

217
00:08:49.140 --> 00:08:50.480
It's all through the SDK.

218
00:08:50.480 --> 00:08:53.400
Of course, many of these services have console access

219
00:08:53.400 --> 00:08:56.400
if you need to do very ad hoc workflows with them,

220
00:08:56.400 --> 00:08:58.360
but the SDK is the way to go really.

221
00:08:58.360 --> 00:08:59.360
If you're going to integrate it

222
00:08:59.360 --> 00:09:02.040
into any kind of a application workflow,

223
00:09:02.040 --> 00:09:04.800
and it then is a very good fit for serverless

224
00:09:04.800 --> 00:09:07.960
because you can imagine that if you've got images

225
00:09:07.960 --> 00:09:09.400
with data arriving in S3,

226
00:09:09.400 --> 00:09:12.160
and you want to respond to that and analyze it,

227
00:09:12.160 --> 00:09:14.760
things like Lambda and Step Functions really help you

228
00:09:14.760 --> 00:09:17.060
to stitch that all together very well

229
00:09:17.060 --> 00:09:18.440
without having to put in a load

230
00:09:18.440 --> 00:09:20.340
of additional compute infrastructure.

231
00:09:21.280 --> 00:09:22.100
Awesome.

232
00:09:22.100 --> 00:09:25.440
What if instead you want to actually go really

233
00:09:25.440 --> 00:09:28.120
into the depths and build your own models

234
00:09:28.120 --> 00:09:32.440
or do more advanced research and more of your own ML?

235
00:09:32.440 --> 00:09:34.520
What kind of tools are worth considering

236
00:09:34.520 --> 00:09:36.280
in the AWS ecosystem?

237
00:09:36.280 --> 00:09:38.240
Tools or services, of course.

238
00:09:38.240 --> 00:09:42.360
You have a lot of options there outside of AWS and on AWS,

239
00:09:42.360 --> 00:09:43.920
but of course, when you start getting

240
00:09:43.920 --> 00:09:45.160
into custom models and training,

241
00:09:45.160 --> 00:09:47.440
then you start to think about large amounts of compute

242
00:09:47.440 --> 00:09:49.200
and also GPUs.

243
00:09:49.200 --> 00:09:50.940
So there is SageMaker,

244
00:09:50.940 --> 00:09:53.880
and SageMaker is difficult enough to comprehend

245
00:09:53.880 --> 00:09:55.240
when you're coming into it for the first time

246
00:09:55.240 --> 00:09:57.720
because it has a large number of features

247
00:09:57.720 --> 00:09:59.520
with very confusing names.

248
00:09:59.520 --> 00:10:03.360
So maybe we'll try and digest that a little bit

249
00:10:03.360 --> 00:10:07.640
in a somewhat clear way, and we can start with notebooks.

250
00:10:07.640 --> 00:10:09.720
So anyone used to data science

251
00:10:09.720 --> 00:10:10.880
and machine learning development is used

252
00:10:10.880 --> 00:10:11.960
to working with notebooks.

253
00:10:11.960 --> 00:10:13.960
So that could be Python, Jupyter notebooks.

254
00:10:13.960 --> 00:10:16.320
Now you also have RStudio in SageMaker.

255
00:10:16.320 --> 00:10:17.160
I don't know what that's like,

256
00:10:17.160 --> 00:10:18.640
but I've used the Jupyter notebooks version

257
00:10:18.640 --> 00:10:20.000
and it works pretty well.

258
00:10:20.000 --> 00:10:22.720
You also have now SageMaker Studio,

259
00:10:22.720 --> 00:10:25.220
which is like a notebooks, but it's in its own,

260
00:10:25.220 --> 00:10:28.320
has its own domain and account system.

261
00:10:28.320 --> 00:10:30.880
So you can actually use it inside of your AWS accounts.

262
00:10:30.880 --> 00:10:32.520
It's a bit like Google Colab.

263
00:10:32.520 --> 00:10:34.480
I'm reading you one called SageMaker Canvas,

264
00:10:34.480 --> 00:10:36.120
which is their attempt to build

265
00:10:36.120 --> 00:10:38.040
like a no code machine learning.

266
00:10:38.040 --> 00:10:39.760
And this is mainly concerned with processing

267
00:10:39.760 --> 00:10:40.940
like tabular data.

268
00:10:42.320 --> 00:10:44.140
From what I saw when it first came out,

269
00:10:44.140 --> 00:10:47.300
it's still a little bit limited in what it can do

270
00:10:47.300 --> 00:10:49.600
and its feature sets, but the idea is good, right?

271
00:10:49.600 --> 00:10:51.440
Eventually you want people

272
00:10:51.440 --> 00:10:52.720
who have business domain knowledge,

273
00:10:52.720 --> 00:10:54.060
but no machine learning knowledge

274
00:10:54.060 --> 00:10:56.060
to be able to train their own models.

275
00:10:57.820 --> 00:10:59.920
If you are training, then a big part of the job,

276
00:10:59.920 --> 00:11:02.580
you mentioned 80% of the data of the effort is in

277
00:11:02.580 --> 00:11:03.660
preparing data.

278
00:11:03.660 --> 00:11:06.700
So there is a service in SageMaker called Ground Truth

279
00:11:06.700 --> 00:11:09.440
and Ground Truth will allow you to label that data

280
00:11:09.440 --> 00:11:12.680
and manage all of the human interaction that's required

281
00:11:12.680 --> 00:11:16.420
to take individual items in your data set,

282
00:11:16.420 --> 00:11:19.400
label them, mark them as labeled,

283
00:11:19.400 --> 00:11:22.640
quality control your labeling, also using Mechanical Turk,

284
00:11:22.640 --> 00:11:25.440
if you want to outsource a lot of that effort

285
00:11:25.440 --> 00:11:28.000
and also with some machine learning assistance as well.

286
00:11:28.000 --> 00:11:30.760
So it can even do things like creation of synthetic data

287
00:11:30.760 --> 00:11:32.160
samples within your data set.

288
00:11:33.480 --> 00:11:35.060
Then when it comes to actually training itself,

289
00:11:35.060 --> 00:11:37.200
so SageMaker, the main really, I would say,

290
00:11:37.200 --> 00:11:39.760
if you're a real serious machine learning developer,

291
00:11:39.760 --> 00:11:42.240
probably the main benefit is the training infrastructure,

292
00:11:42.240 --> 00:11:45.680
because it will allow you to build clusters

293
00:11:45.680 --> 00:11:49.400
of GPU containers that can be used to train your model.

294
00:11:49.400 --> 00:11:52.000
And then it can also help you to tune your model.

295
00:11:52.000 --> 00:11:54.360
So with like deep learning, the typical processes,

296
00:11:54.360 --> 00:11:55.720
you have hyper parameter tuning

297
00:11:55.720 --> 00:11:58.340
where you're constantly tweaking different configuration

298
00:11:58.340 --> 00:12:01.160
parameters to your model, you rerun your training model,

299
00:12:01.160 --> 00:12:03.040
then you test it against your test data,

300
00:12:03.040 --> 00:12:06.220
and you see if your model is improving or disimproving.

301
00:12:06.220 --> 00:12:09.760
And the SageMaker training platform is designed

302
00:12:09.760 --> 00:12:11.920
to make all of that a lot more automated compared

303
00:12:11.920 --> 00:12:13.400
to the typical manual flow.

304
00:12:13.400 --> 00:12:15.440
The most important thing at the end of it is getting it

305
00:12:15.440 --> 00:12:18.360
into users hands, and that's where inference comes in.

306
00:12:18.360 --> 00:12:21.240
And that's basically when you run the model in production,

307
00:12:21.240 --> 00:12:23.440
you pass the data and you get results.

308
00:12:23.440 --> 00:12:26.320
So is there a cat in this picture being

309
00:12:26.320 --> 00:12:28.460
a canonical example maybe?

310
00:12:28.460 --> 00:12:31.220
And this is also container based.

311
00:12:31.220 --> 00:12:35.040
You deploy your model, you deploy like a Python wrapper file

312
00:12:35.040 --> 00:12:37.440
and you get back a HTTP endpoint.

313
00:12:37.440 --> 00:12:39.360
And you can access that publicly,

314
00:12:39.360 --> 00:12:41.120
maybe from your web application,

315
00:12:41.120 --> 00:12:43.880
or you can access it just from another system

316
00:12:43.880 --> 00:12:46.960
within your microservices architecture,

317
00:12:46.960 --> 00:12:48.980
however you're building your application.

318
00:12:48.980 --> 00:12:51.000
That was typically like a provisioned mode

319
00:12:51.000 --> 00:12:53.600
where you pick a size for your infrastructure,

320
00:12:53.600 --> 00:12:57.000
whether you need GPUs or not, and you run an endpoint

321
00:12:57.000 --> 00:12:59.520
and 20 minutes later it becomes available

322
00:12:59.520 --> 00:13:01.920
and you can call the HTTP endpoint.

323
00:13:01.920 --> 00:13:03.800
But now they also support a serverless mode.

324
00:13:03.800 --> 00:13:07.740
So you deploy your model and then SageMaker will scale

325
00:13:07.740 --> 00:13:10.500
the infrastructure for you based on the mental traffic.

326
00:13:11.540 --> 00:13:14.160
Awesome, so one thing that is interesting is that the work

327
00:13:14.160 --> 00:13:18.880
that we do at Fortier, it's not as much on the side

328
00:13:18.880 --> 00:13:22.120
of let's do ML research, create new models,

329
00:13:22.120 --> 00:13:25.360
but it's more on how do we take existing models

330
00:13:25.360 --> 00:13:28.240
and to put them in production the most optimal way.

331
00:13:28.240 --> 00:13:30.280
So generally we think about, I don't know,

332
00:13:30.280 --> 00:13:32.740
what kind of AWS infrastructure do we need to use

333
00:13:32.740 --> 00:13:36.600
for data pipelines, model deployment, recording results?

334
00:13:36.600 --> 00:13:38.640
What kind of costs are we talking about?

335
00:13:38.640 --> 00:13:40.920
So can we optimize anything to save money,

336
00:13:40.920 --> 00:13:43.780
like not overspend on all this infrastructure?

337
00:13:43.780 --> 00:13:47.260
And yeah, we generally start by taking an existing model

338
00:13:47.260 --> 00:13:49.520
and think how do we put this in AWS

339
00:13:49.520 --> 00:13:50.880
in the best possible way?

340
00:13:50.880 --> 00:13:54.100
So do you have any, I don't know, comment or suggestion

341
00:13:54.100 --> 00:13:56.560
that what are the options or the different topics

342
00:13:56.560 --> 00:13:59.460
that we generally explore with this kind of use cases?

343
00:14:01.240 --> 00:14:03.200
Yeah, it's very common that people have these really

344
00:14:03.200 --> 00:14:05.880
interesting models and the next step is like, okay,

345
00:14:05.880 --> 00:14:08.400
how do we integrate, how do we give this to our customers?

346
00:14:08.400 --> 00:14:12.840
And you end up with some prototype that works

347
00:14:12.840 --> 00:14:15.880
on somebody's laptop or in a data center somewhere.

348
00:14:15.880 --> 00:14:17.400
And the question is, okay, how do we make this

349
00:14:17.400 --> 00:14:18.680
production grade?

350
00:14:18.680 --> 00:14:23.280
And it's really typical applying typical software engineering

351
00:14:23.280 --> 00:14:26.980
best practices and just applying it to this machine learning

352
00:14:26.980 --> 00:14:29.960
application, so just thinking about, okay,

353
00:14:29.960 --> 00:14:31.420
how do you manage your data?

354
00:14:31.420 --> 00:14:33.600
What's the deployment pipeline for your model?

355
00:14:33.600 --> 00:14:36.260
How do you manage very different versions of your model

356
00:14:36.260 --> 00:14:39.460
and the process for going from one version to another,

357
00:14:39.460 --> 00:14:41.420
testing and rolling back?

358
00:14:41.420 --> 00:14:42.520
What do you do with your results?

359
00:14:42.520 --> 00:14:44.600
So one good thing you might want to put into practice

360
00:14:44.600 --> 00:14:46.800
is that every time you do inference,

361
00:14:46.800 --> 00:14:49.740
you record that result and store it with the data

362
00:14:49.740 --> 00:14:51.000
so that you can feed that back

363
00:14:51.000 --> 00:14:53.240
into future training exercises.

364
00:14:55.120 --> 00:14:56.680
You mentioned like performance optimization

365
00:14:56.680 --> 00:14:58.140
and cost optimization.

366
00:14:58.140 --> 00:15:00.680
These are like typical well-architected things,

367
00:15:00.680 --> 00:15:03.360
but you're also applying them to machine learning as well.

368
00:15:03.360 --> 00:15:05.840
So looking at the compute infrastructure

369
00:15:05.840 --> 00:15:08.400
and how scalable is it, how manageable is it,

370
00:15:08.400 --> 00:15:10.440
what observability do you need,

371
00:15:10.440 --> 00:15:13.000
and how do you optimize that cost

372
00:15:13.000 --> 00:15:14.600
versus performance trade-off?

373
00:15:15.440 --> 00:15:16.880
And yeah, it's quite typical.

374
00:15:16.880 --> 00:15:18.020
Yeah, the people give us a model

375
00:15:18.020 --> 00:15:19.400
and it's like a PyTorch model.

376
00:15:19.400 --> 00:15:21.320
There's something trained in TensorFlow.

377
00:15:21.320 --> 00:15:23.120
And then we're saying, okay,

378
00:15:23.120 --> 00:15:25.120
well, what's the best workflow for this?

379
00:15:25.120 --> 00:15:26.640
Do we need to build training infrastructure

380
00:15:26.640 --> 00:15:28.680
or is just inference infrastructure?

381
00:15:28.680 --> 00:15:30.200
And if it's inference infrastructure,

382
00:15:30.200 --> 00:15:32.800
then there's quite a few compute options actually.

383
00:15:32.800 --> 00:15:36.480
And surprisingly, maybe we can discuss this,

384
00:15:36.480 --> 00:15:39.140
but SageMaker inference is something we've used

385
00:15:39.140 --> 00:15:42.400
quite a lot, but it's not our number one go-to service

386
00:15:42.400 --> 00:15:43.240
for inference.

387
00:15:44.660 --> 00:15:46.580
Yeah, that's super interesting.

388
00:15:46.580 --> 00:15:49.040
What about other considerations?

389
00:15:49.040 --> 00:15:50.480
Like, I don't know.

390
00:15:50.480 --> 00:15:54.360
I know that every time we talk about AI and ML ethics,

391
00:15:54.360 --> 00:15:56.440
for instance, it's one of those big things

392
00:15:56.440 --> 00:15:59.440
that you'll need to address somehow.

393
00:15:59.440 --> 00:16:02.760
What does AWS do from that perspective?

394
00:16:02.760 --> 00:16:04.080
Or maybe the customers on there

395
00:16:04.080 --> 00:16:06.680
needs to be aware when using AWS.

396
00:16:06.680 --> 00:16:08.240
Yes, if you've been reading about machine learning,

397
00:16:08.240 --> 00:16:10.720
you probably have seen many, many instances

398
00:16:10.720 --> 00:16:14.800
of bias problems in all aspects of machine learning.

399
00:16:14.800 --> 00:16:16.680
And just because you choose a managed service

400
00:16:16.680 --> 00:16:19.080
doesn't mean you don't have any responsibility there.

401
00:16:19.080 --> 00:16:21.920
There's a couple of interesting studies into this

402
00:16:21.920 --> 00:16:23.520
that we can also link in the show notes.

403
00:16:23.520 --> 00:16:27.200
There was one from MIT that looked into recognition

404
00:16:27.200 --> 00:16:31.800
and demonstrated gender and ethnic bias in its accuracy

405
00:16:31.800 --> 00:16:34.600
in doing person recognition.

406
00:16:36.160 --> 00:16:38.480
There's also the case of recognition is actually used

407
00:16:38.480 --> 00:16:41.920
in law enforcement and security industry

408
00:16:41.920 --> 00:16:43.160
in a number of places.

409
00:16:43.160 --> 00:16:46.680
And back in 2020, Amazon announced like a moratorium.

410
00:16:46.680 --> 00:16:49.160
So they were suspending recognition for police use

411
00:16:49.160 --> 00:16:50.680
in the States for one year,

412
00:16:50.680 --> 00:16:54.760
because they were asking for the legislation to catch up

413
00:16:54.760 --> 00:16:56.760
to make sure that the technology would be applied

414
00:16:56.760 --> 00:16:58.200
in an ethical way.

415
00:16:58.200 --> 00:17:01.280
So these kinds of things kind of show

416
00:17:01.280 --> 00:17:02.320
that we have a long way to go

417
00:17:02.320 --> 00:17:04.760
before we can just deploy these things and use them,

418
00:17:04.760 --> 00:17:07.640
particularly when it relates to classifying people

419
00:17:07.640 --> 00:17:08.640
or labeling people.

420
00:17:09.880 --> 00:17:12.720
So this is everybody's responsibility.

421
00:17:12.720 --> 00:17:14.200
Just because you're using a managed service

422
00:17:14.200 --> 00:17:16.680
doesn't mean you can just say it's somebody else's problem.

423
00:17:17.600 --> 00:17:18.840
What are the other considerations?

424
00:17:18.840 --> 00:17:20.440
So beyond the ethical one,

425
00:17:20.440 --> 00:17:22.400
which is probably the most important one,

426
00:17:23.960 --> 00:17:24.960
we mentioned compute, right?

427
00:17:24.960 --> 00:17:27.920
And I said, SageMaker Infosys isn't the number one.

428
00:17:27.920 --> 00:17:29.680
The reason for that is that, as I mentioned,

429
00:17:29.680 --> 00:17:31.880
it could take like 20 minutes for your endpoint

430
00:17:31.880 --> 00:17:32.760
to come up and running.

431
00:17:32.760 --> 00:17:35.120
It's also can be quite expensive.

432
00:17:35.120 --> 00:17:38.120
Number one, service for inference,

433
00:17:38.120 --> 00:17:39.440
a go-to is Lambda actually,

434
00:17:39.440 --> 00:17:43.160
because if your model can fit in 10 gigabytes of space

435
00:17:43.160 --> 00:17:45.320
or on a BFS volume

436
00:17:45.320 --> 00:17:48.320
and can run with less than 10 gigabytes of RAM,

437
00:17:48.320 --> 00:17:51.960
I would choose Lambda every time, really,

438
00:17:51.960 --> 00:17:54.960
because it scales so quickly and is so responsive

439
00:17:54.960 --> 00:17:57.840
as your inference workload rises and falls,

440
00:17:57.840 --> 00:17:59.840
much better compared to SageMaker

441
00:17:59.840 --> 00:18:02.760
and much easier to set up and much easier to deploy.

442
00:18:02.760 --> 00:18:04.760
So I've just had a way better experience

443
00:18:04.760 --> 00:18:06.800
deploying machine learning to Lambda.

444
00:18:06.800 --> 00:18:09.400
As a developer, for the customer, in production,

445
00:18:09.400 --> 00:18:10.800
for performance and scalability,

446
00:18:10.800 --> 00:18:13.520
it's just always ticking all the boxes.

447
00:18:13.520 --> 00:18:15.520
So SageMaker will be further down the list.

448
00:18:16.600 --> 00:18:18.080
Another thing then, when you're looking at cost

449
00:18:18.080 --> 00:18:21.240
and performance is maybe look at edge machine learning

450
00:18:21.240 --> 00:18:23.600
rather than doing all of the inference in the cloud.

451
00:18:23.600 --> 00:18:25.360
So if you have a mobile app,

452
00:18:25.360 --> 00:18:27.640
maybe it's better to do the machine learning

453
00:18:27.640 --> 00:18:28.600
on the mobile device,

454
00:18:28.600 --> 00:18:31.440
leverage the consumer's computing power

455
00:18:31.440 --> 00:18:33.160
rather than your computing power,

456
00:18:33.160 --> 00:18:35.840
then you can get some cost benefits, performance,

457
00:18:35.840 --> 00:18:38.040
because you don't have any round trip to the cloud.

458
00:18:38.040 --> 00:18:40.640
And there's also a data privacy element there as well,

459
00:18:40.640 --> 00:18:41.640
because the customer's data

460
00:18:41.640 --> 00:18:43.360
doesn't have to leave their device.

461
00:18:43.360 --> 00:18:47.480
So there is SageMaker Edge, another SageMaker service,

462
00:18:47.480 --> 00:18:50.800
which allows you to run these models on the edge,

463
00:18:50.800 --> 00:18:51.920
as well as a lot of alternatives.

464
00:18:51.920 --> 00:18:53.160
Like all the deep learning frameworks

465
00:18:53.160 --> 00:18:55.040
have a mobile equivalent.

466
00:18:55.040 --> 00:18:57.200
I think that's pretty much it.

467
00:18:57.200 --> 00:18:58.800
And then going back to like the typical

468
00:18:58.800 --> 00:19:00.320
software engineering practices,

469
00:19:00.320 --> 00:19:02.720
considerations before you go into production is

470
00:19:02.720 --> 00:19:05.520
all of those continuous deployment, best practices,

471
00:19:05.520 --> 00:19:07.680
change management, observability,

472
00:19:09.640 --> 00:19:11.320
governance, reliability,

473
00:19:12.840 --> 00:19:14.840
all of those well-architected pillars

474
00:19:14.840 --> 00:19:17.000
have to be applied to machine learning as well.

475
00:19:17.000 --> 00:19:20.360


476
00:19:20.360 --> 00:19:22.800
Okay, last question that I have is, I guess up to which point it is correct,

477
00:19:22.800 --> 00:19:25.440
I guess up to which point it is convenient

478
00:19:25.440 --> 00:19:28.680
to go with managed services compared to

479
00:19:28.680 --> 00:19:30.520
let's build everything from scratch ourselves.

480
00:19:30.520 --> 00:19:33.440
So obviously talking about pricing,

481
00:19:33.440 --> 00:19:34.960
like what's the price equation?

482
00:19:34.960 --> 00:19:37.440
Is it convenient to just go with AWS

483
00:19:37.440 --> 00:19:39.800
because the price is reasonable for most use cases,

484
00:19:39.800 --> 00:19:43.160
or there are maybe the price is just enough expensive

485
00:19:43.160 --> 00:19:45.280
that sometimes you might want to consider,

486
00:19:45.280 --> 00:19:46.240
I'm going to spend more time,

487
00:19:46.240 --> 00:19:47.920
but build my own thing in the long run

488
00:19:47.920 --> 00:19:48.760
is going to be cheaper.

489
00:19:48.760 --> 00:19:50.240
Well, it depends on your use case.

490
00:19:50.240 --> 00:19:51.960
Maybe we can give two examples.

491
00:19:51.960 --> 00:19:55.160
So let's say example one is you're a technology provider

492
00:19:55.160 --> 00:19:59.040
that is selling cutting edge devices

493
00:19:59.040 --> 00:20:04.040
that do Fox recognition for small holders with chickens.

494
00:20:05.160 --> 00:20:08.160
So you have some device with a camera

495
00:20:08.160 --> 00:20:09.520
that you put in the chicken run.

496
00:20:09.520 --> 00:20:11.960
And if it spots a Fox in the field of vision,

497
00:20:11.960 --> 00:20:14.440
it's going to maybe play some sort of sound

498
00:20:14.440 --> 00:20:16.560
that will deter the Fox.

499
00:20:16.560 --> 00:20:18.960
So if you connect that up to AWS recognition,

500
00:20:18.960 --> 00:20:21.600
you can imagine that

501
00:20:21.600 --> 00:20:22.840
if you're doing a number of images

502
00:20:22.840 --> 00:20:27.840
and the number of Foxes are being detected every night,

503
00:20:27.840 --> 00:20:28.720
that's not too bad, right?

504
00:20:28.720 --> 00:20:29.560
Your volume is low

505
00:20:29.560 --> 00:20:32.080
and then each small holder is playing a subscription.

506
00:20:32.080 --> 00:20:35.120
So it would probably cover your cost with AWS recognition.

507
00:20:35.120 --> 00:20:37.400
The other one is like if you're airport security

508
00:20:37.400 --> 00:20:39.920
and you're scanning all of the incoming passengers

509
00:20:39.920 --> 00:20:42.240
in real time on a video stream, right?

510
00:20:42.240 --> 00:20:44.880
So then you're running multiple images per second.

511
00:20:44.880 --> 00:20:48.400
So a million images in recognition is about a thousand dollars.

512
00:20:48.400 --> 00:20:52.120
So for your chicken cam,

513
00:20:52.120 --> 00:20:55.120
that's probably very achievable,

514
00:20:55.120 --> 00:20:56.880
but for your airport security,

515
00:20:56.880 --> 00:20:58.160
then it's probably a case where you would say,

516
00:20:58.160 --> 00:21:00.400
look, now it's time to run our own infrastructure

517
00:21:00.400 --> 00:21:01.880
and do machine learning.

518
00:21:01.880 --> 00:21:04.040
It's something more cost-effective

519
00:21:04.040 --> 00:21:06.680
because I think a recognition at scale,

520
00:21:06.680 --> 00:21:08.160
it would want to be very valuable

521
00:21:08.160 --> 00:21:11.280
to justify the cost of million images for a thousand dollars.

522
00:21:12.200 --> 00:21:13.840
When you're talking about your own inference

523
00:21:13.840 --> 00:21:15.560
like SageMaker versus Lambda,

524
00:21:15.560 --> 00:21:17.680
SageMaker, if you look at the price,

525
00:21:17.680 --> 00:21:22.680
it's like four zeros and a two cents.

526
00:21:23.560 --> 00:21:25.080
So what is that?

527
00:21:25.080 --> 00:21:29.040
20,000th, two 10,000th of a cent,

528
00:21:30.240 --> 00:21:32.440
but that's about a thousand times more expensive

529
00:21:32.440 --> 00:21:34.840
than Lambda from what I can see in the per second cost.

530
00:21:34.840 --> 00:21:36.640
Now, a thousand times sounds extreme,

531
00:21:36.640 --> 00:21:39.040
but remember that a single SageMaker endpoint

532
00:21:39.040 --> 00:21:41.080
can process multiple images concurrently.

533
00:21:41.080 --> 00:21:43.200
A Lambda processes one event at a time.

534
00:21:43.200 --> 00:21:46.360
So it's not exactly comparing apples to apples,

535
00:21:46.360 --> 00:21:49.360
but again, another reason to go with Lambda

536
00:21:49.360 --> 00:21:51.520
or even Fargate if you want to use containers,

537
00:21:51.520 --> 00:21:55.040
because I don't know, SageMaker, unless you need a GPU,

538
00:21:55.040 --> 00:21:57.040
which you don't always for inference,

539
00:21:57.880 --> 00:21:59.680
that will come down to your performance requirement.

540
00:21:59.680 --> 00:22:03.040
But if you don't need a GPU, then SageMaker,

541
00:22:03.040 --> 00:22:03.880
you don't need it.

542
00:22:04.840 --> 00:22:06.880


543
00:22:06.880 --> 00:22:09.560
Right, is there any resource that we can suggest to people as a final thing?

544
00:22:10.880 --> 00:22:13.600


545
00:22:13.600 --> 00:22:16.280
Yeah, so myself and Peter Elger wrote the book, AI as a Service, and it's all really about the AI.

546
00:22:16.280 --> 00:22:17.120
It's all about that topic

547
00:22:17.120 --> 00:22:19.960
and how to use these managed services in a serverless way.

548
00:22:19.960 --> 00:22:21.680
So we'll link to that book.

549
00:22:21.680 --> 00:22:24.680
And I think our YouTube channel from Julien Simon

550
00:22:24.680 --> 00:22:26.640
is a really good one for people

551
00:22:26.640 --> 00:22:28.520
who are trying to explore the space,

552
00:22:28.520 --> 00:22:30.880
because he worked at AWS for a long time,

553
00:22:30.880 --> 00:22:32.320
and he has lots of really practical

554
00:22:32.320 --> 00:22:34.940
kind of use case driven scenarios

555
00:22:34.940 --> 00:22:36.720
where he shows you how to use these services.

556
00:22:36.720 --> 00:22:38.760
And it's very technical.

557
00:22:38.760 --> 00:22:40.920
He gives you very unbiased opinion.

558
00:22:40.920 --> 00:22:42.520
It's not full of AWS spin.

559
00:22:42.520 --> 00:22:46.200
It's really very factual and honest.

560
00:22:46.200 --> 00:22:47.760
And he's since left AWS,

561
00:22:47.760 --> 00:22:49.320
but he's still doing this kind of content

562
00:22:49.320 --> 00:22:52.920
and using SageMaker and reviewing these services.

563
00:22:52.920 --> 00:22:55.200
So I think his YouTube channel is really good resource

564
00:22:55.200 --> 00:22:56.880
for anyone doing ML on AWS.

565
00:22:57.800 --> 00:22:59.400
Awesome, and I think with that,

566
00:22:59.400 --> 00:23:01.200
we have covered everything we wanted to cover.

567
00:23:01.200 --> 00:23:03.900
Let us know what you think in the comments.

568
00:23:03.900 --> 00:23:05.560
And yeah, we look forward to know

569
00:23:05.560 --> 00:23:07.000
if you use any of these services,

570
00:23:07.000 --> 00:23:10.560
if you found any particular issue that is worth highlighting,

571
00:23:10.560 --> 00:23:12.060
or if you're actually just having fun

572
00:23:12.060 --> 00:23:14.280
and learning a lot of things, please share with us

573
00:23:14.280 --> 00:23:15.720
what are you learning

574
00:23:15.720 --> 00:23:18.560
and what are maybe the next topics you want us to cover.

575
00:23:18.560 --> 00:23:20.680
Until then, see you in the next episode.

576
00:23:20.680 --> 00:23:45.680
And we'll see you in the next one.
