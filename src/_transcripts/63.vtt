WEBVTT

1
00:00:00.000 --> 00:00:04.420
We have updated awsbites.com and added transcripts to every single episode.

2
00:00:04.700 --> 00:00:09.920
We don't have a massive number of episodes yet, but we're proud to have published 62 episodes so far.

3
00:00:10.260 --> 00:00:13.020
So it's no small amount of audio to transcribe.

4
00:00:13.020 --> 00:00:17.380
So how did we go about doing all this work and how are we going to be able to keep doing this consistently in the future?

5
00:00:17.620 --> 00:00:20.140
Well, there's a simple answer. Automation and AI.

6
00:00:20.860 --> 00:00:24.340
For the longer and more elaborate answer, you'll have to stick around until the end of this episode.

7
00:00:24.340 --> 00:00:29.880
We'll tell you how to build your own transcription automation pipeline and we'll also share all of the code for our solution.

8
00:00:29.880 --> 00:00:33.960
My name is Eoin, I'm with Luciano and this is the AWS Bites podcast.

9
00:00:40.580 --> 00:00:42.580
AWS Bites is sponsored by fourTheorem.

10
00:00:42.580 --> 00:00:48.100
fourTheorem is an AWS consulting partner offering training, cloud migration and modern application architecture.

11
00:00:48.100 --> 00:00:51.380
Find out more at fourtheorem.com You'll find the link in the show notes.

12
00:00:51.380 --> 00:00:54.580
Since this is the first episode of 2023, happy new year to everyone.

13
00:00:54.580 --> 00:00:57.540
We're glad to be back publishing new episodes after the break.

14
00:00:57.540 --> 00:00:59.940
So back to transcripts for this podcast.

15
00:00:59.940 --> 00:01:04.580
Luciano, can you start maybe by giving a quick recap of what the actual need is?

16
00:01:04.580 --> 00:01:06.580
What problem are we solving?

17
00:01:06.580 --> 00:01:11.140
Of course. So yeah, this is a podcast and basically in every episode we talk a lot.

18
00:01:11.140 --> 00:01:15.140
We say a lot of nonsense. Sometimes we say also something interesting, at least I hope.

19
00:01:15.700 --> 00:01:19.700
And of course it would be great if we could provide together with the videos

20
00:01:19.700 --> 00:01:23.860
and the audio files also proper transcripts.

21
00:01:23.860 --> 00:01:29.060
And that would be nice if we can do that consistently for every single episode.

22
00:01:29.060 --> 00:01:33.540
When the episode comes out it is also available in a text format basically.

23
00:01:33.540 --> 00:01:38.900
So people who prefer to read this conversation rather than just consuming the video or the audio format,

24
00:01:38.900 --> 00:01:43.540
they can just simply use that transcript as a way to consume the information we are trying to share.

25
00:01:44.820 --> 00:01:49.460
Also transcripts are very useful because they can be used for search engine optimization

26
00:01:49.460 --> 00:01:53.460
we embedded them in our own website with the hope that that contributes

27
00:01:53.460 --> 00:01:58.740
to make our content more discoverable because on the web we provide a better description for

28
00:01:58.740 --> 00:02:04.020
the kind of content we are producing. And in general transcripts can also help people

29
00:02:04.020 --> 00:02:09.380
even just watching the video or listening to the audio to easily find exactly what is the place

30
00:02:09.380 --> 00:02:13.700
where we were talking about a specific concept. Maybe they are listening to an episode again

31
00:02:13.700 --> 00:02:17.460
because there is something they want to refresh so maybe they remember that we talk somewhere

32
00:02:17.460 --> 00:02:21.780
about Step Functions they can easily search in the transcript just to exactly figure out at

33
00:02:21.780 --> 00:02:28.740
which point we start to talk about that particular topic. So definitely there is value in creating

34
00:02:28.740 --> 00:02:33.780
all these transcripts. But yeah the main question is how did we do that? How can we generate

35
00:02:33.780 --> 00:02:39.700
transcripts in general? What did we do?

36
00:02:39.700 --> 00:02:44.740
Yeah this is something we've looked at a few times in the past and never found any ideal option until very recently. So some of the options are like doing

37
00:02:44.740 --> 00:02:50.500
it manually hiring somebody who's professional at this. The other one is grabbing the closed

38
00:02:50.500 --> 00:02:53.940
captions that are automatically generated by YouTube because all our episodes are already

39
00:02:53.940 --> 00:02:59.140
on YouTube or we can generate them in another way. So having someone do it manually like having a

40
00:02:59.140 --> 00:03:03.060
professional individual like a freelancer or a company who specializes in this is appealing

41
00:03:03.060 --> 00:03:07.380
because it leads to really high quality results by people who do this all the time.

42
00:03:07.380 --> 00:03:11.540
The disadvantage really the main hurdle is that it takes time to find somebody who's reliable enough

43
00:03:11.540 --> 00:03:15.300
and build a relationship with them and set up that whole process. It can also be expensive

44
00:03:15.300 --> 00:03:19.780
depending on your budget and then you have communication back and forth that can introduce

45
00:03:19.780 --> 00:03:24.980
a delay every time you need to publish an episode. So overall because it adds to the lead time it's

46
00:03:24.980 --> 00:03:31.940
something we were pretty reluctant to do. Regarding YouTube closed captions, we could have done this

47
00:03:31.940 --> 00:03:36.980
pretty I suppose integrated this into our workflow after we publish a new video on YouTube we could

48
00:03:36.980 --> 00:03:41.700
wait for some time for YouTube to generate those closed captions and then try and integrate some

49
00:03:41.700 --> 00:03:46.180
code to download them and integrate them into the build process. That seems like a decent enough

50
00:03:46.180 --> 00:03:50.740
solution but there's two major problems with it. Number one, the quality of the transcripts isn't

51
00:03:50.740 --> 00:03:54.740
that great it lacks kind of punctuation and grammar and sentences and that sort of stuff.

52
00:03:55.780 --> 00:04:00.900
Additionally, the YouTube transcripts don't identify different speakers so if you just

53
00:04:00.900 --> 00:04:05.700
converted it into a blog a wall of text it would be literally that just a wall of kind of stream

54
00:04:05.700 --> 00:04:12.180
of consciousness text without any punctuation or identification of speakers. So the last

55
00:04:12.180 --> 00:04:17.140
solution left is to kind of generate the transcripts ourselves somehow and since this is

56
00:04:17.140 --> 00:04:22.740
an AWS podcast it's to be expected that we would use something like Amazon Transcribe which is AWS's

57
00:04:22.740 --> 00:04:29.220
managed service to perform speech to text. So you give it an audio file and it gives you back text

58
00:04:29.780 --> 00:04:33.460
and we like the simplicity of that we're always advocating for using managed services

59
00:04:33.460 --> 00:04:38.260
you can use the Transcribe SDK or the API to generate the transcription in a programmatic way.

60
00:04:40.260 --> 00:04:45.140
With a transcribed client, you call the start transcription API and provide reference to

61
00:04:45.140 --> 00:04:53.540
an audio file and an output prefix and it will generate that as a JSON file it can also generate

62
00:04:53.540 --> 00:05:00.180
subtitles formats like SRT and WebVTT. So it runs in batch mode it can also do real-time

63
00:05:00.180 --> 00:05:04.820
transcriptions but we would be using batch mode for this since it's kind of for on-demand content

64
00:05:04.820 --> 00:05:10.500
and you can get notified with EventBridge when it's finished. Luciano, do you want to talk about

65
00:05:10.500 --> 00:05:17.380
what the pros and cons of Transcribe are and like why we ultimately ended up kind of using it but

66
00:05:17.380 --> 00:05:25.140
not entirely?

67
00:05:25.140 --> 00:05:29.380
Yeah of course so Transcribe is quite good because it gives us that feature that we really liked and we we felt it was missing on YouTube which is basically you get different

68
00:05:29.380 --> 00:05:33.700
speakers you get a label that tells you this person is starting to talk another person is

69
00:05:33.700 --> 00:05:39.140
starting to talk from this point so we can retain in a text format that feeling that this is just

70
00:05:39.140 --> 00:05:45.460
not a wall of text but it's an actual conversation between multiple people. Unfortunately actually

71
00:05:45.460 --> 00:05:51.300
there is another one good thing that you can customize it so you can add custom models and

72
00:05:51.300 --> 00:05:58.580
vocabularies to fine-tune the results so if you have a very specific domain you can basically put

73
00:05:58.580 --> 00:06:04.340
more work into it and get more accurate results but in general we were not satisfied with the

74
00:06:04.340 --> 00:06:10.580
level of quality. It is not that bad I think it's still quite a good tool so you can use it for most

75
00:06:10.580 --> 00:06:18.500
things but for the kind of scope that we had in mind we feel that good and perfect are very

76
00:06:18.500 --> 00:06:27.780
noticeable points like we were aiming for the 99% good while transcribed maybe around the 97%

77
00:06:27.780 --> 00:06:34.180
good and we feel that that 2% of a difference is actually quite noticeable when you are reading

78
00:06:34.180 --> 00:06:39.940
some text and you expect it to be higher quality. So we were looking for something that could be a

79
00:06:39.940 --> 00:06:46.340
little bit better so that basically led us to explore other avenues. And pretty much during the

80
00:06:46.340 --> 00:06:53.140
same time where we were looking for an alternative, there was a blog announcement by OpenAI that

81
00:06:53.140 --> 00:06:58.660
introduced this new tool called Whisper which is effectively another tool to do text-to-speech so

82
00:06:58.660 --> 00:07:04.980
to try to recognize speech and convert it to text. So this came around

83
00:07:06.500 --> 00:07:11.460
last September I think and we are going to be linking the announcement blog post in the show notes.

84
00:07:11.460 --> 00:07:17.620
It's the same group of people that created ChatGPT and DALL-E so you probably heard of them

85
00:07:17.620 --> 00:07:23.060
because right now their products are all the rage and Whisper is probably the least known of these

86
00:07:23.060 --> 00:07:28.100
three but nonetheless it's a very interesting product. And we were really excited to try it

87
00:07:28.100 --> 00:07:34.980
so we quickly spin it up and tested it and we were definitely blown away by the level of accuracy.

88
00:07:35.780 --> 00:07:40.660
So we immediately thought okay we want to use this because this is giving us the level of quality

89
00:07:40.660 --> 00:07:45.380
that we want to provide in the end and if we can automate all of that process this is going to be

90
00:07:45.380 --> 00:07:50.100
something that we can keep doing very easily without too much overhead in our existing process.

91
00:07:51.060 --> 00:07:56.340
Still, it wasn't perfect unfortunately there were a few small problems one is that it did not

92
00:07:56.340 --> 00:08:00.820
distinguish between speakers. So on one side we're getting more accuracy but again we are losing that

93
00:08:00.820 --> 00:08:06.340
ability to distinguish the speakers and the other thing is that it's not built in in AWS as a managed

94
00:08:06.340 --> 00:08:11.940
offering so if we were to productionize so to speak this solution in AWS, we'll need to figure

95
00:08:11.940 --> 00:08:19.140
out exactly how do we take the model and run it in AWS. So Eoin, do you want to detail our solution in the end?

96
00:08:24.500 --> 00:08:31.220
Yeah exactly, so we wanted to get the best of both worlds right so we have OpenAI Whisper which is this fantastic model that you can run it's basically they deliver it as a container

97
00:08:31.220 --> 00:08:36.180
that you can run and as a very nice user-friendly developer friendly interface where you just give

98
00:08:36.180 --> 00:08:40.420
it an audio file and it gives you the transcript. It might be worth mentioning that can also do

99
00:08:40.420 --> 00:08:45.380
translations as well so if you want to transcribe but also generate Italian text or even transcribe

100
00:08:45.380 --> 00:08:52.500
from different languages into English this is something that's really good at too. We did run

101
00:08:52.500 --> 00:08:57.780
it standalone. It comes in different sizes so you have it depending on your compute resources

102
00:08:57.780 --> 00:09:03.060
available to you you can run the tiny model, small model, the medium model or the large model but if

103
00:09:03.060 --> 00:09:07.060
you want to use anything and get a result within a reasonable period of time like even less than 30

104
00:09:07.060 --> 00:09:12.420
minutes you probably need a GPU so that's something that's worth bearing in mind with OpenAI. So our

105
00:09:12.420 --> 00:09:17.940
solution - what we wanted to do was use the accuracy of the OpenAI Whisper transcript but take the

106
00:09:17.940 --> 00:09:24.420
speaker labels from the Amazon Transcribe output output so that we'd have an accurate labeled time

107
00:09:24.420 --> 00:09:29.940
linked transcript and merge the results and end up with a JSON file that we could use to generate

108
00:09:29.940 --> 00:09:37.380
a transcript for the website with sections that say this is what Eoin said this is what Luciano

109
00:09:37.380 --> 00:09:44.340
said and make it readable for people almost like a blog post, right? So we built this using Step Functions.

110
00:09:44.340 --> 00:09:49.940
It's using SageMaker so that we can run that Whisper model with the GPU and it's also using

111
00:09:49.940 --> 00:09:57.140
Lambda for lots of little transformation efforts like if the input audio file isn't an MP3 we

112
00:09:57.140 --> 00:10:03.060
convert it to MP3 because Transcribe - it's one of the formats it supports and sometimes we're using

113
00:10:03.060 --> 00:10:10.260
M4A audio and Transcribe doesn't natively support that. So, how do we do this? How do we even kick off

114
00:10:10.260 --> 00:10:14.180
this process? Well, after we finish recording an episode of AWS Bites we do a bit of editing we

115
00:10:14.180 --> 00:10:19.860
create a video and we create an audio file. That audio file gets pushed up to Anchor which

116
00:10:19.860 --> 00:10:24.900
distributes the podcast to all the podcast channels but we also take that audio now and we

117
00:10:24.900 --> 00:10:29.940
copy it into an S3 bucket and that kicks off a whole automated process with this step function.

118
00:10:31.300 --> 00:10:37.060
We of course we had the previous 61 episodes or so to consider so we also had to do a backfilling

119
00:10:37.060 --> 00:10:44.100
process so we pulled down the RSS feed and kicked off this process for each of the 61 previous

120
00:10:44.100 --> 00:10:50.660
episodes by copying that audio up to S3. Interestingly, I suppose it's worth mentioning

121
00:10:50.660 --> 00:10:55.380
that you know there is a cost associated and probably a lot of people will be wondering what

122
00:10:55.380 --> 00:11:00.180
is the cost to run this because we have to run SageMaker with a GPU we also have to use Transcribe

123
00:11:00.180 --> 00:11:04.100
both of those things can be expensive if you use them at scale we talked about that in our previous

124
00:11:04.100 --> 00:11:09.380
episode on AI services.  We did work this out I can't recall exactly the channel was around

125
00:11:09.380 --> 00:11:15.300
it was definitely less than a dollar per episode for the whole process to run so it's not too

126
00:11:15.300 --> 00:11:20.900
ominous compared to other alternatives at all I would say. So is it worthwhile talking about some

127
00:11:20.900 --> 00:11:24.660
of the orchestration here? How does it all fit together? We mentioned we have Step Functions

128
00:11:24.660 --> 00:11:29.620
right. So we pre-process the input if needed with FFmpeg, we trigger the two transcription jobs at

129
00:11:29.620 --> 00:11:36.740
the same time so the Transcribe job and the SageMaker job transcribe means we have to use

130
00:11:36.740 --> 00:11:42.900
the AWS SDK within step functions and then kind of poll until it's complete. With SageMaker we've

131
00:11:42.900 --> 00:11:46.500
got like a more native integration with Step Functions where we could just say run this batch

132
00:11:46.500 --> 00:11:52.100
transform job that'll kick off a docker container in the background with the right um compute

133
00:11:52.100 --> 00:11:58.900
resources it will pass our input audio into it it can run a batch of jobs the way we set it up

134
00:11:58.900 --> 00:12:03.700
we generally just do one transcription at a time because we're only doing one a week and when we

135
00:12:03.700 --> 00:12:08.260
get both results then Step Functions will allow us to take both of those inputs and kick off a

136
00:12:08.260 --> 00:12:14.660
Lambda to process the results. So that's like essentially taking these two sets.

137
00:12:14.660 --> 00:12:20.500
Both systems will give you a set of segments with start times and end times one of them has speaker labels

138
00:12:20.500 --> 00:12:25.780
so we have to do this run this algorithm essentially to merge the two what else is there

139
00:12:31.620 --> 00:12:34.780
to mention in this process. What other bells and whistles do we have?

140
00:12:35.000 --> 00:12:43.140
There are some additional things that we do to try to to get a little bit higher quality with our final result, so for

141
00:12:37.780 --> 00:12:43.140
instance we noticed very common mistakes. For some reason, Whisper doesn't like our names

142
00:12:43.140 --> 00:12:48.180
like it was getting my name wrong a few times it was mostly getting your name correctly, Eoin, but

143
00:12:48.180 --> 00:12:55.060
spelled in a different way I think that is that I just yeah just too many ways so it was interesting

144
00:12:55.060 --> 00:12:59.780
that it was getting correctly but just the spelling of course yeah you need to guess which

145
00:12:59.780 --> 00:13:04.900
which spelling is the right one so we basically figured it out there are also some other cases

146
00:13:04.900 --> 00:13:10.900
where for instance name of services in AWS sometimes would be consistently wrong or some

147
00:13:10.900 --> 00:13:15.540
small things like that so basically, by reviewing the first results we created a dictionary

148
00:13:15.540 --> 00:13:22.900
and we pass the output and apply what substitution wherever we see these common errors and we apply

149
00:13:22.900 --> 00:13:27.620
the correction so all of that is somewhat automated and we can keep improving our dictionary

150
00:13:27.620 --> 00:13:34.900
as we find more issues like that. Then, the other thing is that at the end of the day our website

151
00:13:34.900 --> 00:13:39.700
is the place where we want to output this result in a way that is visible to people and

152
00:13:39.700 --> 00:13:46.100
they can consume it so we need somehow to hook this entire project into the process that builds

153
00:13:46.100 --> 00:13:52.020
our website and our website is also open source. We'll put the link in the show notes. It is a static

154
00:13:52.020 --> 00:13:57.700
website built with Eleventy so what we do is basically every week every time there is a new episode we

155
00:13:57.700 --> 00:14:03.140
trigger a new build and that will generate a new version of the entire website, all the html pages

156
00:14:03.140 --> 00:14:10.500
, assets and so on and publish that online on a CDN. So what we wanted to do is somehow integrate

157
00:14:10.500 --> 00:14:17.700
this process to be able to hook into our website build process and we thought that it would be very

158
00:14:17.700 --> 00:14:24.260
nice if the Step Function could just do a PR just trying to send the generated file directly into

159
00:14:24.260 --> 00:14:29.380
the repository for our website so we did all of that with an additional Lambda at the end of the

160
00:14:29.380 --> 00:14:36.580
process so you might be wondering at this point did we manage to fully automate everything and

161
00:14:36.580 --> 00:14:42.900
I will say unfortunately not entirely yes but I think we are close enough. We definitely reduced

162
00:14:42.900 --> 00:14:48.900
all the manual work to the bare minimum and but what's left to do. What we still need to do

163
00:14:48.900 --> 00:14:55.140
manually or at least we want to do manually to retain a decent level of quality there so this is

164
00:14:55.140 --> 00:15:00.500
also the reason why we we do a PR because first of all it gives us an opportunity to review the

165
00:15:00.500 --> 00:15:06.180
result of our transcript before it gets merged and the other interesting thing is that the PR

166
00:15:06.180 --> 00:15:12.340
effectively is just publishing trying to publish a JSON file in our website repository. This JSON file

167
00:15:12.340 --> 00:15:19.620
is not ready to go straight away because the speaker identification is just telling us something

168
00:15:19.620 --> 00:15:24.820
like speaker 0 and speaker 1 is not able to tell we which one is which depending on the voice is

169
00:15:24.820 --> 00:15:30.260
just distinguishing between between two different people so we need to quickly look up who is the

170
00:15:30.260 --> 00:15:36.740
first person to talk and just assign the name to to the right label. So this is something we can

171
00:15:36.740 --> 00:15:43.220
easily do manually directly from the GitHub UI by editing the PR and in the process, we also quickly

172
00:15:43.220 --> 00:15:48.340
review, we just eyeball the entire text and if we spot any other obvious mistake we can easily fix

173
00:15:48.340 --> 00:15:57.380
it manually before merging the PR. So I think that describes more or less the process and what do we

174
00:16:03.700 --> 00:16:06.000
do in an automated fashion and what we still do manually. What else do we want to share?

175
00:16:07.000 --> 00:16:09.140
Yeah I think just to summarize I think this has really been a step forward in the transcription

176
00:16:09.140 --> 00:16:12.660
technology and I'm really happy with the level of automation we now have I think it's just the

177
00:16:12.660 --> 00:16:18.500
right balance between manual effort and automation it's great that you can now use AI and be really

178
00:16:18.500 --> 00:16:22.740
confident that you've got a result that people can read without finding it kind of jarring or

179
00:16:22.740 --> 00:16:29.780
distracting to read. Some of the things that it really surprised me with OpenAI Whisper is

180
00:16:29.780 --> 00:16:34.980
how you mentioned, like, product names and AWS service names it seems to just know what they

181
00:16:34.980 --> 00:16:39.940
are and get them right most of the time some of the things where it's less accurate is just things

182
00:16:39.940 --> 00:16:45.620
that are hard to predict. Like, AWS Bites isn't exactly a top international brand yet so sometimes

183
00:16:45.620 --> 00:16:50.180
it would spell it with b y t e s instead of b i t e s so there are some things where you'll always

184
00:16:50.180 --> 00:16:55.540
have to do those vocabulary substitutions you mentioned but overall I think this is just mostly

185
00:16:55.540 --> 00:17:00.260
mostly hands off and you end up with a really good result for very little cost. So if this is

186
00:17:00.260 --> 00:17:04.500
something you want to do for your own podcast, the good news is that everything we just told you

187
00:17:04.500 --> 00:17:10.740
about is open source so you can find a repo on GitHub. It's called PodWhisperer as a tribute

188
00:17:10.740 --> 00:17:15.140
to OpenAI Whisper because this is primarily aimed at podcasts but of course you can use this for

189
00:17:15.140 --> 00:17:20.420
transcribing meetings any other kind of audio you could think of. You can follow the instructions in

190
00:17:20.420 --> 00:17:24.340
the readme and deploy this into your own AWS account so feel free to contribute back to the

191
00:17:24.340 --> 00:17:27.460
project if you think there's something missing, improvements you'd like to make, something you'd

192
00:17:27.460 --> 00:17:32.660
like to change and we'd really love to hear from you. And we'd gratefully appreciate the chance to

193
00:17:32.660 --> 00:17:37.860
grow this and spread it around even further. This is all we have for this episode. We hope you liked

194
00:17:37.860 --> 00:17:42.180
it and we look forward to hearing your feedback on our transcripts. By the way, if you happen to

195
00:17:42.180 --> 00:17:46.100
find a mistake in one of our transcripts you can easily submit a PR like Luciano said. The link

196
00:17:46.100 --> 00:17:51.460
will be in the show notes to the AWS Bites static website repo. It will help us fix the issue and

197
00:17:51.460 --> 00:17:55.780
improve the quality of what we're doing. It's really nice to have everybody contributing to

198
00:17:55.780 --> 00:18:21.940
the podcast. We're really enjoying that so far so thank you and we'll see you in the next episode.
