WEBVTT

1
00:00:00.000 --> 00:00:04.200
Using AWS Lambda together with SQS is a very common serverless pattern

2
00:00:04.200 --> 00:00:07.920
that has always suffered from some special limitations.

3
00:00:07.920 --> 00:00:10.560
We covered SQS in a dedicated episode last year,

4
00:00:10.560 --> 00:00:14.120
but recently we've had a significant new feature solving a common pain.

5
00:00:14.120 --> 00:00:17.840
And today we want to dive deeper into using SQS and Lambda together

6
00:00:17.840 --> 00:00:21.560
and tell you all you need to know about using SQS triggers,

7
00:00:21.560 --> 00:00:23.360
about scaling and concurrency.

8
00:00:23.360 --> 00:00:24.800
I'm Eoin, I'm here with Luciana,

9
00:00:24.800 --> 00:00:30.200
and this is another episode of the AWS Bites podcast.

10
00:00:30.200 --> 00:00:38.880
AWS Bites is sponsored by fourTheorem.

11
00:00:38.880 --> 00:00:41.760
fourTheorem is an AWS consulting partner offering training,

12
00:00:41.760 --> 00:00:44.120
cloud migration and modern application architecture.

13
00:00:44.120 --> 00:00:46.040
You can find out more at fourtheorem.com

14
00:00:46.040 --> 00:00:48.280
and you can find that link in the show notes.

15
00:00:48.280 --> 00:00:50.520
Luciano, let's start with addressing the basics

16
00:00:50.520 --> 00:00:54.200
for anyone not intimately familiar with AWS Lambda and SQS.

17
00:00:54.200 --> 00:00:56.560
We have a lot of seasoned experts out there listening,

18
00:00:56.560 --> 00:01:00.200
but we also know that plenty of people are listening and watching

19
00:01:00.200 --> 00:01:03.560
or taking their first steps with AWS or these serverless offerings.

20
00:01:03.560 --> 00:01:08.000
Do you want to give a quick overview and a quick elevator pitch for SQS and Lambda?

21
00:01:08.000 --> 00:01:11.040
Okay, I'll try my best and I'm going to start with Lambda.

22
00:01:11.040 --> 00:01:14.000
So Lambda is basically a function as a service offering

23
00:01:14.000 --> 00:01:15.880
that is available on AWS.

24
00:01:15.880 --> 00:01:19.360
So what that means is that it is effectively a managed compute service

25
00:01:19.360 --> 00:01:21.480
with a very simple abstraction.

26
00:01:21.480 --> 00:01:24.480
As developers, we are familiar with the concept of a function,

27
00:01:24.480 --> 00:01:26.920
which is basically a piece of code that takes some inputs,

28
00:01:26.920 --> 00:01:29.280
executes some logics and returns some output.

29
00:01:29.280 --> 00:01:31.560
And Lambda basically takes that particular model

30
00:01:31.560 --> 00:01:35.280
and provides it as a managed on-demand compute layer.

31
00:01:35.280 --> 00:01:37.680
So as a user, you write your own Lambda function,

32
00:01:37.680 --> 00:01:41.080
you can use many languages, so pick the language of your choice.

33
00:01:41.080 --> 00:01:44.280
You write your own business logic in that particular shape,

34
00:01:44.280 --> 00:01:46.200
so in that particular function format,

35
00:01:46.200 --> 00:01:49.360
and then you just tell AWS when to run that particular function.

36
00:01:49.360 --> 00:01:52.320
And generally, this is in response to a particular event.

37
00:01:52.320 --> 00:01:55.160
Just to give you an example, that event can be an HTTP call

38
00:01:55.160 --> 00:01:56.760
if you're using something like API Gateway

39
00:01:56.760 --> 00:02:00.080
and you are trying to implement an API in a serverless fashion.

40
00:02:00.080 --> 00:02:03.440
It can be a schedule, I don't know, every Monday at 9am,

41
00:02:03.440 --> 00:02:04.760
maybe you want to do something,

42
00:02:04.760 --> 00:02:07.680
so you can trigger the Lambda that way on that schedule.

43
00:02:07.680 --> 00:02:10.760
Or maybe you want to react to certain files being created in S3,

44
00:02:10.760 --> 00:02:12.200
that can be another trigger.

45
00:02:12.200 --> 00:02:15.680
Or maybe, because we are going to be talking about the integration with a queue,

46
00:02:15.680 --> 00:02:19.280
you can trigger your Lambda as a response to a new message

47
00:02:19.280 --> 00:02:23.520
being available in SQS, which is a queueing system available in AWS.

48
00:02:23.520 --> 00:02:26.720
So let's just talk more about SQS as well.

49
00:02:26.720 --> 00:02:30.200
So, very similar to Lambda, SQS is another managed,

50
00:02:30.200 --> 00:02:33.280
scalable service provided by AWS.

51
00:02:33.280 --> 00:02:36.280
So if you need a queueing system and you don't want to manage

52
00:02:36.280 --> 00:02:40.920
all of the deployment updates, security patches, scalability by yourself,

53
00:02:40.920 --> 00:02:44.680
you just go on AWS and you provision a new SQS queue.

54
00:02:44.680 --> 00:02:47.440
And just to explain why you would want to do that,

55
00:02:47.440 --> 00:02:50.560
let's present an example.

56
00:02:50.560 --> 00:02:55.200
Or actually, in a more generic sense, let's just say that you have some

57
00:02:55.200 --> 00:02:58.760
piece of functionality going on, but also you might want to do

58
00:02:58.760 --> 00:03:01.120
more work on demand in the background.

59
00:03:01.120 --> 00:03:03.960
For instance, I don't know, you are sending transactional emails,

60
00:03:03.960 --> 00:03:07.560
or you need to resize some pictures, or you need to run some workloads.

61
00:03:07.560 --> 00:03:11.400
For instance, you have some text available in picture format,

62
00:03:11.400 --> 00:03:15.120
and you want to extract the text, maybe running an OCR algorithm.

63
00:03:15.120 --> 00:03:17.840
So all things that you don't want to do in line,

64
00:03:17.840 --> 00:03:19.640
you probably want to offload in the background,

65
00:03:19.640 --> 00:03:22.440
and maybe you want to parallelize that kind of compute.

66
00:03:22.440 --> 00:03:26.640
So what you could do there is you could create a queueing system,

67
00:03:26.640 --> 00:03:30.440
maybe use just SQS, and then every time a new job becomes available,

68
00:03:30.440 --> 00:03:31.920
you get the definition of that job.

69
00:03:31.920 --> 00:03:35.160
Rather than doing it straight away in the process that receives

70
00:03:35.160 --> 00:03:38.280
the definition of the jobs, you just send it to the queue,

71
00:03:38.280 --> 00:03:42.080
and then the queue is going to keep it in storage somehow,

72
00:03:42.080 --> 00:03:45.280
and other workers in the background can just ask to the queue,

73
00:03:45.280 --> 00:03:46.760
is there something I can do?

74
00:03:46.760 --> 00:03:49.640
They can pick up the work and just do it in the background.

75
00:03:49.640 --> 00:03:52.720
Now, this brings a few interesting advantages.

76
00:03:52.720 --> 00:03:56.240
The first one is that you are not blocking the core application.

77
00:03:56.240 --> 00:03:59.760
If it's a web server, the web server can reply to the user as fast as possible.

78
00:03:59.760 --> 00:04:01.960
This is what the user expects on the web,

79
00:04:01.960 --> 00:04:05.000
while all the heavy work is offloaded to the background.

80
00:04:05.000 --> 00:04:08.880
If you have a peak of traffic, maybe that creates a lot of work

81
00:04:08.880 --> 00:04:10.520
that you'll need to do in the background.

82
00:04:10.520 --> 00:04:14.520
So by having a queue and having workers, you have a decoupled system,

83
00:04:14.520 --> 00:04:17.240
and basically you can decide to scale up the workers' part.

84
00:04:17.240 --> 00:04:19.800
So add more and more workers to be able to respond

85
00:04:19.800 --> 00:04:22.760
to that increased demand for background work.

86
00:04:22.760 --> 00:04:25.640
And that can be very elastic, so when the demand is over,

87
00:04:25.640 --> 00:04:27.160
maybe you go back to a normal,

88
00:04:27.160 --> 00:04:30.080
you can remove all the workers that you don't need anymore.

89
00:04:30.080 --> 00:04:33.000
And also that adds resiliency, because if something fails,

90
00:04:33.000 --> 00:04:37.040
the queuing mechanism can automatically recognize that a job failed

91
00:04:37.040 --> 00:04:39.400
and put it back in the queue, so that means that another worker

92
00:04:39.400 --> 00:04:42.960
would pick it up again later and you can retry it automatically.

93
00:04:42.960 --> 00:04:46.000
And even more interesting, if that particular job keeps failing,

94
00:04:46.000 --> 00:04:49.680
you can add rules to basically move that job on the site.

95
00:04:49.680 --> 00:04:51.640
They are generally called dead letter queues.

96
00:04:51.640 --> 00:04:54.080
So it's basically another queue where you store all the jobs

97
00:04:54.080 --> 00:04:57.200
that you were not able to process, and a human can just go there

98
00:04:57.200 --> 00:04:59.560
and try to figure out why this consistently failed.

99
00:04:59.560 --> 00:05:00.680
Maybe there is a bug.

100
00:05:00.680 --> 00:05:03.320
You can fix that bug in your code and then push the message back

101
00:05:03.320 --> 00:05:05.360
to the original queue, and at that point,

102
00:05:05.360 --> 00:05:08.000
you are able to reprocess that message correctly.

103
00:05:08.000 --> 00:05:11.360
So that's basically giving you ways to never lose jobs

104
00:05:11.360 --> 00:05:15.560
and consistently be able to deliver on what the user expects.

105
00:05:15.560 --> 00:05:17.680
We want to talk about Lambda and SQS together.

106
00:05:17.680 --> 00:05:19.440
So how do they work together?

107
00:05:19.440 --> 00:05:22.520
So with SQS, you're always using a poll-based model.

108
00:05:22.520 --> 00:05:24.640
You would need something to poll the queue,

109
00:05:24.640 --> 00:05:27.600
retrieve events, process them, and then delete them.

110
00:05:27.600 --> 00:05:30.240
It's a fairly simple API, really,

111
00:05:30.240 --> 00:05:32.840
when it comes to consuming messages from SQS,

112
00:05:32.840 --> 00:05:35.640
and we covered that in detail in the previous episodes.

113
00:05:35.640 --> 00:05:38.480
So traditionally, you'd use EC2 or a container

114
00:05:38.480 --> 00:05:41.440
or some other piece of long-lived compute running on AWS

115
00:05:41.440 --> 00:05:44.120
or even on-premises or anywhere else.

116
00:05:44.120 --> 00:05:47.080
With AWS Lambda, it's a lot simpler

117
00:05:47.080 --> 00:05:49.360
because you don't have to run a poller yourself.

118
00:05:49.360 --> 00:05:51.120
The polling service is actually provided

119
00:05:51.120 --> 00:05:54.480
as part of Lambda's Event Source Mapping feature.

120
00:05:54.480 --> 00:05:57.240
And you may have used SQS and Lambda together

121
00:05:57.240 --> 00:05:59.400
without knowing that there was such a feature

122
00:05:59.400 --> 00:06:02.000
because a lot of things like the serverless framework or SAM

123
00:06:02.000 --> 00:06:04.360
kind of create this for you transparently under the hood

124
00:06:04.360 --> 00:06:06.400
when you create that trigger.

125
00:06:06.400 --> 00:06:08.000
So within the AWS Lambda service,

126
00:06:08.000 --> 00:06:09.920
you've got this Event Source Mapping feature,

127
00:06:09.920 --> 00:06:11.480
and this is the bit that's doing the polling.

128
00:06:11.480 --> 00:06:15.040
It's also the same feature that handles Lambda triggers

129
00:06:15.040 --> 00:06:20.040
from Kinesis and Kafka, MQ, and DynamoDB streams.

130
00:06:20.080 --> 00:06:22.600
So if you imagine a simple architecture diagram,

131
00:06:22.600 --> 00:06:24.200
you've got your queue on the left

132
00:06:24.200 --> 00:06:26.120
and a Lambda function on the right.

133
00:06:26.120 --> 00:06:28.600
The Event Source Mapping is essentially a box in the middle

134
00:06:28.600 --> 00:06:31.720
that's running that polling and passing messages from the queue

135
00:06:31.720 --> 00:06:35.120
and invoking Lambda functions with the messages.

136
00:06:35.120 --> 00:06:37.360
And those invocations are actually synchronous.

137
00:06:37.360 --> 00:06:38.880
So they're not, with Lambda,

138
00:06:38.880 --> 00:06:41.280
you've got synchronous and asynchronous invocations.

139
00:06:41.280 --> 00:06:43.960
Event Source Mapping is using a synchronous invocation

140
00:06:43.960 --> 00:06:46.560
and waiting for the function invocation to complete.

141
00:06:48.040 --> 00:06:49.440
So Event Source Mappings are very good

142
00:06:49.440 --> 00:06:51.600
because they give you a few neat features for free

143
00:06:51.600 --> 00:06:53.320
that you would have to implement yourself.

144
00:06:53.320 --> 00:06:55.960
Like you can control the batch size and the batching window

145
00:06:55.960 --> 00:06:59.520
in terms of the number of events that arrive in a batch,

146
00:06:59.520 --> 00:07:02.360
what kind of time interval they need to arrive within.

147
00:07:03.280 --> 00:07:05.520
And since about a year or so ago as well,

148
00:07:05.520 --> 00:07:07.640
you can also specify filters.

149
00:07:07.640 --> 00:07:08.960
Some messages are filtered out

150
00:07:08.960 --> 00:07:10.640
before they reach your function.

151
00:07:10.640 --> 00:07:12.200
So that can save you a lot of execution time

152
00:07:12.200 --> 00:07:13.120
and cost as well.

153
00:07:14.760 --> 00:07:17.520
So if you imagine, if you've got an instance

154
00:07:17.520 --> 00:07:19.720
or a number of EC2 instances polling from a queue,

155
00:07:19.720 --> 00:07:22.640
you're in control of the processing rate and the concurrency

156
00:07:22.640 --> 00:07:23.760
because it's directly linked

157
00:07:23.760 --> 00:07:26.520
to the number of workers you have, right?

158
00:07:26.520 --> 00:07:27.960
You can retrieve a batch of messages

159
00:07:27.960 --> 00:07:30.920
and process them with whatever cluster size

160
00:07:30.920 --> 00:07:33.360
or worker pool size you have running.

161
00:07:33.360 --> 00:07:35.120
Now with Lambda, the Event Source Mapping

162
00:07:35.120 --> 00:07:36.000
is doing this for you.

163
00:07:36.000 --> 00:07:38.520
So it's in control of the concurrency.

164
00:07:38.520 --> 00:07:40.400
And it's this fact that's been a source of pain

165
00:07:40.400 --> 00:07:41.920
for a lot of users.

166
00:07:41.920 --> 00:07:45.280
And this was the case until very recently

167
00:07:45.280 --> 00:07:49.520
when the AWS announced a feature

168
00:07:49.520 --> 00:07:53.400
called SQS maximum concurrency support.

169
00:07:53.400 --> 00:07:54.720
That makes a lot of sense to me,

170
00:07:54.720 --> 00:07:57.040
but maybe we can provide an example

171
00:07:57.040 --> 00:07:59.640
of what is the pain we are talking about

172
00:07:59.640 --> 00:08:02.680
so we can make it more obvious to everyone.

173
00:08:02.680 --> 00:08:05.360
And also how is this new feature helping

174
00:08:05.360 --> 00:08:07.560
to kind of ease this particular type of pain?

175
00:08:08.600 --> 00:08:10.120
Yeah, no, this is good.

176
00:08:10.120 --> 00:08:11.920
Let's try and give some sort of an example.

177
00:08:11.920 --> 00:08:15.500
So let's say you've got a queue that has messages relating

178
00:08:15.500 --> 00:08:17.520
to signups for your SaaS application.

179
00:08:17.520 --> 00:08:20.360
So user fills in a form, signs up,

180
00:08:20.360 --> 00:08:22.760
they're now customer reviewers.

181
00:08:22.760 --> 00:08:24.360
As part of this whole signup flow,

182
00:08:24.360 --> 00:08:27.120
maybe you've got this event driven mailing list

183
00:08:27.120 --> 00:08:28.080
subscription features.

184
00:08:28.080 --> 00:08:30.320
So when a user signs up, you go off

185
00:08:30.320 --> 00:08:34.440
and you want to make an API call to MailChimp, for example,

186
00:08:34.440 --> 00:08:35.840
so that they're going to receive

187
00:08:35.840 --> 00:08:38.400
your weekly user mailing list.

188
00:08:38.400 --> 00:08:40.040
Now let's say in this contrived example

189
00:08:40.040 --> 00:08:41.680
that MailChimp has a rate limit

190
00:08:41.680 --> 00:08:44.560
of 10 invocations per second to this API.

191
00:08:44.560 --> 00:08:47.020
So you've got a queue and a Lambda function

192
00:08:47.020 --> 00:08:51.240
that takes the signup event and initiates a subscription

193
00:08:51.240 --> 00:08:53.040
with the MailChimp API.

194
00:08:53.040 --> 00:08:56.360
So you want this to scale as users sign up,

195
00:08:56.360 --> 00:08:58.880
you want to have this resiliency you talked about,

196
00:08:58.880 --> 00:09:01.640
but you don't want to flood this API

197
00:09:01.640 --> 00:09:03.440
because it's got a rate limit.

198
00:09:03.440 --> 00:09:04.540
So let's talk about the behavior

199
00:09:04.540 --> 00:09:06.240
before we've got this recent change

200
00:09:06.240 --> 00:09:08.800
in how Lambda and SQS work together.

201
00:09:08.800 --> 00:09:10.760
So the Event Source Mapping in Lambda

202
00:09:10.760 --> 00:09:13.380
starts five pollers by default,

203
00:09:13.380 --> 00:09:15.120
reading messages in batches from the queue.

204
00:09:15.120 --> 00:09:17.760
So let's say you've just got a batch size of one,

205
00:09:17.760 --> 00:09:20.720
but you've got more than 10 messages coming in per second.

206
00:09:20.720 --> 00:09:22.180
So when messages are available,

207
00:09:22.180 --> 00:09:23.700
the Event Source Mapping will pass these

208
00:09:23.700 --> 00:09:25.440
to running Lambda containers.

209
00:09:25.440 --> 00:09:27.140
But if you've got more messages coming in

210
00:09:27.140 --> 00:09:28.680
because your service is really popular

211
00:09:28.680 --> 00:09:31.480
and people are signing up at a really, really fast rate,

212
00:09:32.640 --> 00:09:35.000
the Event Source Mapping is still going to scale,

213
00:09:35.000 --> 00:09:37.280
try and scale up the number of Lambda workers

214
00:09:37.280 --> 00:09:40.080
by making synchronous invocation requests.

215
00:09:40.080 --> 00:09:42.000
And it's going to increase that concurrency level

216
00:09:42.000 --> 00:09:44.760
by a factor of 60 every minute.

217
00:09:44.760 --> 00:09:49.400
And it will keep going up to the account concurrency limit

218
00:09:49.400 --> 00:09:54.400
or the reserve concurrency or 1000, whichever is the lowest.

219
00:09:55.920 --> 00:09:57.600
So in order to prevent your function

220
00:09:57.600 --> 00:10:01.080
from exceeding that relative API limit,

221
00:10:01.080 --> 00:10:03.440
you might set the reserve concurrency to five

222
00:10:03.440 --> 00:10:04.380
because you're thinking, okay,

223
00:10:04.380 --> 00:10:07.520
maybe this function takes about 500 milliseconds to invoke.

224
00:10:07.520 --> 00:10:10.160
So in order to keep it at 10 per second,

225
00:10:10.160 --> 00:10:12.240
I'm going to just have five concurrency.

226
00:10:12.240 --> 00:10:14.560
But Event Source Mappings don't seem to know anything

227
00:10:14.560 --> 00:10:16.280
about your functions reserve concurrency

228
00:10:16.280 --> 00:10:17.840
or the account level concurrency.

229
00:10:17.840 --> 00:10:19.320
It just keeps scaling up.

230
00:10:19.320 --> 00:10:22.020
So the Lambda for service will stop there

231
00:10:22.020 --> 00:10:24.220
from being more than that number of concurrent workers,

232
00:10:24.220 --> 00:10:25.060
but Event Source Mapping

233
00:10:25.060 --> 00:10:27.320
just keeps trying to invoke functions anyway.

234
00:10:28.600 --> 00:10:31.200
And this results in invocations being throttled.

235
00:10:32.220 --> 00:10:33.860
So this can happen in a lot of different cases,

236
00:10:33.860 --> 00:10:35.040
like when other functions

237
00:10:35.040 --> 00:10:36.520
are consuming the account concurrency

238
00:10:36.520 --> 00:10:39.000
and there just isn't the available capacity as well.

239
00:10:39.000 --> 00:10:40.900
It can even happen if you've got cases

240
00:10:40.900 --> 00:10:43.000
where you've got multiple Event Source Mappings

241
00:10:43.000 --> 00:10:45.880
invoking the same function, which is also possible

242
00:10:45.880 --> 00:10:47.980
because each one is scaling independently.

243
00:10:47.980 --> 00:10:50.980
So this has been a source of a lot of pain

244
00:10:50.980 --> 00:10:52.500
for use cases like this.

245
00:10:52.500 --> 00:10:55.700
So what happens when the throttling actually occurs

246
00:10:55.700 --> 00:10:59.220
and how can we actually leverage

247
00:10:59.220 --> 00:11:01.020
this new maximum concurrency feature

248
00:11:01.020 --> 00:11:03.660
to make our life easier?

249
00:11:03.660 --> 00:11:05.780
Yes, when the throttling occurs,

250
00:11:05.780 --> 00:11:08.300
messages are going to go back onto the queue

251
00:11:08.300 --> 00:11:11.020
once the visibility time has been reached.

252
00:11:11.020 --> 00:11:13.780
And if this keeps happening after a number of retries,

253
00:11:13.780 --> 00:11:17.200
which is configurable, the message will be discarded,

254
00:11:17.200 --> 00:11:18.340
or if you've got a dead letter queue,

255
00:11:18.340 --> 00:11:20.620
it will end up in your dead letter queue.

256
00:11:20.620 --> 00:11:22.980
So the new maximum concurrency feature

257
00:11:24.100 --> 00:11:26.100
is really doing exactly what it implies.

258
00:11:26.100 --> 00:11:28.620
It's specifying a maximum number of invocations

259
00:11:28.620 --> 00:11:30.180
at the Event Source Mapping level.

260
00:11:30.180 --> 00:11:31.740
So you don't need to use a reserved concurrency

261
00:11:31.740 --> 00:11:32.700
for the function,

262
00:11:32.700 --> 00:11:36.060
although you can use both together in combination.

263
00:11:36.060 --> 00:11:37.300
So it solves the problem by essentially

264
00:11:37.300 --> 00:11:39.940
capping the number of concurrent invocations

265
00:11:39.940 --> 00:11:41.900
and reducing the excess of throttling

266
00:11:41.900 --> 00:11:44.500
that can happen with the default behavior.

267
00:11:44.500 --> 00:11:45.800
So it's helpful for our example,

268
00:11:45.800 --> 00:11:48.180
when you don't want to flood the third party API

269
00:11:48.180 --> 00:11:50.580
with requests that might cause a rate limiting error,

270
00:11:50.580 --> 00:11:52.740
it means you don't have to use a reserve concurrency,

271
00:11:52.740 --> 00:11:54.740
which can be annoying for people,

272
00:11:54.740 --> 00:11:56.780
because when you use reserve concurrency,

273
00:11:56.780 --> 00:11:59.860
you're also taking away capacity from other functions.

274
00:12:01.060 --> 00:12:02.940
It's also nice for anyone using multiple events

275
00:12:02.940 --> 00:12:04.660
or mapping with the same function

276
00:12:04.660 --> 00:12:06.260
that you can now control the concurrency

277
00:12:06.260 --> 00:12:08.220
of each trigger independently,

278
00:12:08.220 --> 00:12:10.520
instead of just the function as a whole.

279
00:12:10.520 --> 00:12:12.020
Yeah, that makes a lot of sense.

280
00:12:12.020 --> 00:12:15.300
Basically before we were effectively hacking the system,

281
00:12:15.300 --> 00:12:18.300
trying to limit the number of execution

282
00:12:18.300 --> 00:12:20.700
with something that was not necessarily meant

283
00:12:20.700 --> 00:12:22.020
to be used in that way.

284
00:12:22.020 --> 00:12:23.580
And that was creating the side effect

285
00:12:23.580 --> 00:12:26.380
that the event source mapper

286
00:12:26.380 --> 00:12:28.220
was still trying to trigger your Lambda

287
00:12:28.220 --> 00:12:30.540
and probably you would end up with a lot messages

288
00:12:30.540 --> 00:12:31.920
in the dead letter queue,

289
00:12:31.920 --> 00:12:35.000
just because there wasn't capacity to execute them,

290
00:12:35.000 --> 00:12:37.460
not because the messages were actually failing.

291
00:12:37.460 --> 00:12:40.140
So probably this will lead to a lot of false positives

292
00:12:40.140 --> 00:12:42.700
and then somebody needs to look at them, retry them,

293
00:12:42.700 --> 00:12:44.160
and a lot of like overhead

294
00:12:44.160 --> 00:12:47.300
for just because for lacking the capacity of saying,

295
00:12:47.300 --> 00:12:48.540
I don't want you to run more

296
00:12:48.540 --> 00:12:50.820
than a certain number of Lambdas at any given time

297
00:12:50.820 --> 00:12:53.500
for this particular source of events.

298
00:12:53.500 --> 00:12:55.260
So that makes a lot of sense.

299
00:12:55.260 --> 00:12:57.200
Is there any other improvement

300
00:12:57.200 --> 00:12:58.900
that we would like to see in this integration

301
00:12:58.900 --> 00:13:01.060
between Lambda and SQS?

302
00:13:01.060 --> 00:13:02.120
One of the things I mentioned

303
00:13:02.120 --> 00:13:06.020
was that the scaling rate of Lambda and SQS,

304
00:13:06.020 --> 00:13:10.440
it's adding 60 concurrent function executions per minute.

305
00:13:10.440 --> 00:13:14.320
Now this is pretty slow scaling rate.

306
00:13:14.320 --> 00:13:16.240
And if you've got like batch processing workloads

307
00:13:16.240 --> 00:13:18.000
when suddenly you've got tens of thousands

308
00:13:18.000 --> 00:13:20.320
of requests coming in and you want to scale out

309
00:13:20.320 --> 00:13:21.920
to maybe hundreds of thousands

310
00:13:21.920 --> 00:13:23.560
of Lambda functions concurrently,

311
00:13:23.560 --> 00:13:26.160
adding 60 every minute is really slow.

312
00:13:26.160 --> 00:13:27.940
And I've encountered this myself

313
00:13:27.940 --> 00:13:30.180
and then had to use other mechanisms.

314
00:13:30.180 --> 00:13:32.960
So if you have just the Lambda API

315
00:13:32.960 --> 00:13:37.240
and you call invoke directly with the async mode,

316
00:13:37.240 --> 00:13:40.160
then you can scale to thousands of concurrent functions

317
00:13:40.160 --> 00:13:41.120
in seconds.

318
00:13:41.120 --> 00:13:43.580
And I know for a fact that that's using SQS internally

319
00:13:43.580 --> 00:13:45.880
to manage that queue of invocations as well.

320
00:13:45.880 --> 00:13:48.300
So it's still a bit strange that SQS

321
00:13:48.300 --> 00:13:51.600
seems to be really slowing down your scaling rate

322
00:13:51.600 --> 00:13:53.000
and other events sources don't,

323
00:13:53.000 --> 00:13:56.440
like with Kinesis, it's tied to the number of shards.

324
00:13:56.440 --> 00:13:58.500
So this is a bit limiting.

325
00:13:58.500 --> 00:14:01.360
So I would like to see if there was a new future coming out

326
00:14:01.360 --> 00:14:04.500
in this integration, I'd like to see that changed

327
00:14:04.500 --> 00:14:07.560
and make it more configurable so that you could at least,

328
00:14:07.560 --> 00:14:08.680
if you choose to,

329
00:14:08.680 --> 00:14:10.920
you can scale up much faster than that.

330
00:14:10.920 --> 00:14:14.080
That would be my number one next feature.

331
00:14:14.080 --> 00:14:14.980
That makes a lot of sense.

332
00:14:14.980 --> 00:14:17.600
I also have a slightly related comment

333
00:14:17.600 --> 00:14:20.040
that another feature that is available in Lambda

334
00:14:20.040 --> 00:14:22.760
is that you can consume messages in batches,

335
00:14:22.760 --> 00:14:24.480
not necessarily just one by one.

336
00:14:24.480 --> 00:14:27.420
Now this is not necessarily going to solve this problem

337
00:14:27.420 --> 00:14:29.560
because yeah, this problem still exists,

338
00:14:29.560 --> 00:14:31.380
but it's another dimension that you might use,

339
00:14:31.380 --> 00:14:33.520
for instance, to handle throttling

340
00:14:33.520 --> 00:14:36.460
or to handle cases where you want to,

341
00:14:36.460 --> 00:14:38.040
where maybe the task that you need to perform

342
00:14:38.040 --> 00:14:40.000
is very small and therefore it makes sense

343
00:14:40.000 --> 00:14:41.720
to try to get this task together.

344
00:14:41.720 --> 00:14:43.960
So you pull once from the queue and then you,

345
00:14:43.960 --> 00:14:46.200
that Lambda that gets executed can do

346
00:14:46.200 --> 00:14:47.680
a certain number of dams together

347
00:14:47.680 --> 00:14:49.820
rather than just doing that one by one.

348
00:14:49.820 --> 00:14:51.780
So this is just something to keep in mind

349
00:14:51.780 --> 00:14:53.840
and something we mentioned in the other SQS episodes.

350
00:14:53.840 --> 00:14:57.040
So maybe check out that particular feature

351
00:14:57.040 --> 00:14:59.240
if you're trying to figure out what kind of patterns

352
00:14:59.240 --> 00:15:01.020
you can use when using SQS.

353
00:15:01.920 --> 00:15:02.840
With that being said,

354
00:15:02.840 --> 00:15:05.660
are there resources that we want to recommend people

355
00:15:05.660 --> 00:15:07.400
if they want to deep dive?

356
00:15:08.720 --> 00:15:10.600
A lot of people have been writing and talking about this maximum concurrency feature recently,

357
00:15:10.600 --> 00:15:13.400
but I think the best place to go is the series of articles

358
00:15:13.400 --> 00:15:14.720
written by Zach Charles,

359
00:15:14.720 --> 00:15:16.840
who described this problem very well

360
00:15:16.840 --> 00:15:19.880
when he originally encountered it,

361
00:15:19.880 --> 00:15:21.400
described how to reproduce that problem

362
00:15:21.400 --> 00:15:23.360
and has now written a followup in that series

363
00:15:23.360 --> 00:15:25.780
about how this solves maximum concurrent,

364
00:15:25.780 --> 00:15:27.960
how this maximum concurrency feature solves the problem,

365
00:15:27.960 --> 00:15:30.160
but also some other things you might want to watch out for.

366
00:15:30.160 --> 00:15:32.360
So that is definitely the go-to guide here.

367
00:15:32.360 --> 00:15:34.900
We will also include a link to the AWS blog post

368
00:15:34.900 --> 00:15:37.000
and sample code provided with the announcement.

369
00:15:37.000 --> 00:15:39.080
There's a SAM template that you can use

370
00:15:39.080 --> 00:15:42.240
to explore the new feature.

371
00:15:42.240 --> 00:15:45.400
And of course, do check out our previous episode on SQS

372
00:15:45.400 --> 00:15:49.160
and all our other series on all the AWS event services.

373
00:15:49.160 --> 00:15:51.080
So that's it for today's episode.

374
00:15:51.080 --> 00:15:52.400
Thank you very much for joining us

375
00:15:52.400 --> 00:15:54.200
and we'll see you in the next episode.
